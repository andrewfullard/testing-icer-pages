{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"#getting-access-to-the-hpcc","text":"<p>For potential users with an MSU NetID, accounts must be requested by a MSU tenure-track faculty member. Researchers at partner institutions should use the mechanism specified by their institution's  agreement with MSU. For more information, see: Obtain an HPCC Account and on the ICER website.</p>","title":"Getting Access to the HPCC"},{"location":"#cpu-and-gpu-time-limits","text":"<p>Non-buyin users are limited to\u00a0500,000 CPU hours (30,000,000 minutes) and 10,000\u00a0GPU hours (600,000 minutes)\u00a0every year (from January 1st to December 31st). More information is available at Job Policies.</p>","title":"CPU and GPU Time Limits"},{"location":"#buy-in-options","text":"<p>With each cluster purchase, ICER offers researchers the oppportunity to purchase buy-in nodes for priority access to the cluster.</p>","title":"Buy-in Options"},{"location":"#questions","text":"<p>If you have questions and couldn\u2019t find answers in this documentation, please submit a ticket.</p>","title":"Questions?"},{"location":"#online-helpdesk-hours","text":"<p>Monday and Thursday, 1-2pm at our ICER Public Help Desk Channel . More information about virtual support is available.</p>","title":"Online Helpdesk Hours"},{"location":"#hpcc-workshops-and-training","text":"<p>Our monthly workshops and our D2L training materials cover introductions to Linux, HPCC, and some popular software tools. Check out our training calendar for scheduled events and our course content available on Desire2Learn.</p>","title":"HPCC Workshops and Training"},{"location":"#acknowledgements","text":"<p>We encourage HPCC users to acknowledge ICER and MSU in publications based on work that was done with HPCC resources. A sample statement:\u00a0\"This work was supported in part through computational resources and services provided by the Institute for Cyber-Enabled Research at Michigan State University.\" </p> <p>Let us know that we have been referenced, and we will link to your publication on our\u00a0publication site,\u00a0which will further increase the visibility of your work.</p>","title":"Acknowledgements"},{"location":"1._Variables_-_Part_I/","text":"<p>The shell is a program that takes commands from the input device (usually, keyboard) and gives them to the operating system to perform.\u00a0On most Linux system including HPC at MSU, sh works as the shell. Besides sh, other shells are available, but here, we will focus on sh.</p> <p>This tutorial assumes you have:</p> <ul> <li>minimal programming knowledge</li> <li>minimal Linux shell knowledge</li> </ul>","title":"1. Variables - Part I"},{"location":"1._Variables_-_Part_I/#a-first-script","text":"<p>Let's create a file 'first.sh' on the terminal using your favorite editor. If you rarely use any editor on Linux, this is a good chance to start using one of them (Linux text editors ).</p> <p>first.sh</p> <pre><code>#!/bin/sh\n# This is a comment!\necho Hello World         # This is a comment, too!\n</code></pre> <p>The first line tells Linux that the file is to be executed by /bin/bash. '#!' will be explained later. The second line begins with '#'. This special character makes the line as a comment, and it is ignored by the shell. The only exception is when the first\u00a0line of the file starts with '#!'</p> <p>The third line runs a command echo, with two parameters/arguments 'Hello' and 'World'. The symbol '#' on line 3 makes the rest of the line as a comment.</p> <p>Now, run 'chmod 755 first.sh' to make the text file executable, then run './first.sh'.</p> <pre><code>$ chmod 755 first.sh\n$ ./first.sh\nHello World\n</code></pre> <p>Next, let's expand 'first.sh' with variables.\u00a0</p> <p>var1.sh</p> <pre><code>#!/bin/bash\nMY_MESSAGE=\"Hello World\"\necho $MY_MESSAGE\n</code></pre> <p>This assigns the string 'Hello World' to the variable 'MY_MESSAGE' then echo command prints the value of the variable. Note that you need the quotes around the string.</p> <p>To use variables, '$' is required in front of variables. If you use 'echo MY_MESSAGE' in the above, it will \u00a0print 'MY_MESSAGE' instead of 'Hello World'. The scope of the variable 'MY_MESSAGE\" is only inside of the script, and when the script finished the variable is empty (don't forget to use chmod 755 to make a script as an executable).</p> <pre><code>$ ./var1.sh\nHello World\n$ echo $MY_MESSAGE\n\n$\n</code></pre> <p>In addition, if you use a variable without declaration, it returns empty string. There is no warning or error message. Let's create a shell script 'var2.sh'.</p> <p>var2.sh</p> <pre><code>#!/bin/sh \necho \"MYVAR is: $MYVAR\"\nMYVAR=\"hi there\"\necho \"MYVAR is: $MYVAR\"\n</code></pre> <p>Then run the script. You can use chmod 755 to make var2.sh as an executable and run it as previous exampels or use bash command:</p> <pre><code>$ sh var2.sh\nMYVAR is:\nMYVAR is: hi there\n</code></pre> <p>The first MYVAR is empty because it is not declared. The second MYVAR has the value we expected. The scope of the variables in a script is only inside the script. For example, MYVAR is only valid inside 'var2.sh' and when the script finishes, MYVAR is empty again.</p> <pre><code>$ ./var2.sh\nMYVAR is:\nMYVAR is: hi there\n$ echo $MYVAR\n$\n</code></pre> <p>You can declare variables with export command in a shell. Check the scope of variables.</p> <pre><code>$ MYVAR=\"hello there\"\n$ export MYVAR\n$ ./var2.sh\nMYVAR is: hello there\nMYVAR is: hi there\n$ echo $MYVAR\nhello there\n</code></pre> <p>You can use variables in many ways. Here is one example.</p> <p>var3.sh</p> <pre><code>#!/bin/sh\necho \"What is your name?\"\nread USER_NAME\necho \"Hello $USER_NAME\"\necho \"I will create a file called ${USER_NAME}_file\"\ntouch ${USER_NAME}_file\n</code></pre> <p>Let's run the script.</p> <pre><code>$ chmod 755 var3.sh\n$ ./var3.sh\nWhat is your name?\nICER\nHello ICER\nI will create a file called ICER_file\n$ls -l ICER_file\n-rw-r--r--  1 choiyj  staff  0 Jan  5 14:08 ICER_file\n</code></pre> <p>You would notice that we use curly braces for a file name. If you use '$USER_NAME_file' instead of '{USER_NAME}_file', the shell returns empty string because there is no variable called 'USER_NAME_file' in the script.</p>","title":"A first script"},{"location":"2._Variables_-_Part_II/","text":"<p>Linux offers a set of pre-defined variables. These pre-defined variables contain useful information.</p> <p>The first set of variables are $0 ... $9 and $#.</p> <p>The variable $0 is the name of the program as it was called. For example, if you run 'example.sh' which has the variable $0, it will return 'example.sh'. $1 ... $9 are the first 9 additional parameters the script was called with. $* and $@ both will act the same unless they are enclosed in double quotes, \"\". $@ special parameter takes the entire list and separates it into separate arguments. The variable $@ is all parameters $1 ... $any_number. \u00a0The variables $* is similar but does not preserve any whitespace, and quoting, so \"File with spaces\" becomes \"File\" \"with\" \"spaces\". This is similar to the echo command.</p> <p>Let's do a hand on example.</p> <pre><code>#!/bin/sh\necho \"Number of parameters from input: $# parameters\"\necho \"My name is $0\"\necho \"My first parameter is $1\"\necho \"My second parameter is $2\"\necho \"All parameters are $@\"\n</code></pre> <p>Here is a sample run for the above script.</p> <pre><code>$ sh ./var4.sh\nNumber of parameters from input: 0 parameters\nMy name is ./var4.sh\nMy first parameter is\nMy second parameter is\nAll parameters are\n\n$ sh ./var4.sh My lazy fox\nNumber of parameters from input: 3 parameters\nMy name is ./var4.sh\nMy first parameter is My\nMy second parameter is lazy\nAll parameters are My lazy fox\n</code></pre> <p>$# and $1 ... $9 are set by the shell. We can take more than 9 parameters by using the shift command. Look at the next example.</p> <pre><code>#!/bin/sh\nwhile [ \"$#\" -gt \"0\" ]\ndo\n  echo \"\\$1 is $1\"\n  shift\ndone    \n</code></pre> <p>The backslash \\ character is used to mark $ so that it is not interpreted by the shell. This script uses shift until $# is down to zero.</p> <p>Here is a sample run for the above script.</p> <pre><code>$ sh ./test.sh The quick brown fox jumps over the lazy dog.\n$1 is The\n$1 is quick\n$1 is brown\n$1 is fox\n$1 is jumps\n$1 is over\n$1 is the\n$1 is lazy\n$1 is dog.\n</code></pre> <p>We can write a script with $* to get a same result.</p> <pre><code>#!/bin/sh\n\nfor TOKEN in $*\ndo\n    echo \\$1 is $TOKEN\ndone  \n</code></pre> <p>Here, for and do...done\u00a0are loop commands which will be covered later.</p> <p>The\u00a0$?\u00a0variable represents the exit status of the previous command.\u00a0Exit status is a numerical value returned by every command upon its completion. Most commands return 0 if they were successful, and 1 if they were unsuccessful.</p> <pre><code>#!/bin/sh\n\nfor TOKEN in $*\ndo\n    echo $1 $TOKEN\ndone\necho $?\n</code></pre> <p>Here is the result of the sample run.</p> <pre><code>$ sh ./test.sh The quick brown fox jumps over the lazy dog.\n$1 is The\n$1 is quick\n$1 is brown\n$1 is fox\n$1 is jumps\n$1 is over\n$1 is the\n$1 is lazy\n$1 is dog.\n0\n</code></pre>","title":"2. Variables - Part II"},{"location":"2018_cluster_resources/","text":"<p>HPCC maintains several clusters. They are named according to the year of installation. Each cluster has very similar hardware with specific processors but has some variety in configuration, such as different coprocessors, memory capacity, or number of CPUs. However, with many different kinds of configurations, HPCC uses a single-queue system managed by SLURM, a resource management software. Jobs submitted to SLURM job queue can run on any possible nodes, unless there are specifications on cluster constraints. Users only have to specify resource requirements and our scheduler can assign your job to an appropriate cluster.</p> <p>All clusters currently run CentOS 7 and use the SLURM resource manager. They are connected to each other with file systems through Infiniband. Your home, research, and scratch is available and identical on all nodes.</p> <p>The following table lists nodes\u00a0that are currently available to run jobs; jobs can be submitted from any of our dev-nodes.</p>    Cluster_Type Node_Name Node_Count Processors Core Memory Disk_Size GPUs     intel14 csm (11 nodes) &amp; css (10 nodes) 18 Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz 20 240 GB 416 GB     css-[002-003,020,023,032-035] 8 Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz 20 115 GB 416 GB     css nodes 65 Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz 20 52 GB 416 GB    intel14-k20 csn-[001-039] 37 Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz 20 115 GB 416 GB k20 (2)   intel14-phi csp-[006,016-020,025-026] 8 Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz 20 115 GB 416 GB Phi card (2)   intel14-xl qml-003 1 Four Intel Xeon CPU E7-8857 v2 @ 3.00GHz 48 969 GB 1.8 TB     qml-[002] 1 Four Intel Xeon CPU E7-8857 v2 @ 3.00GHz 48 1.45 TB 897 GB     qml-000 1 Four Intel Xeon CPU E7-8857 v2 @ 3.00GHz 48 2.93TB 1.1 TB    intel16 lac-[250-253,256-261,302-317] 26 Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz 28 492 GB 190 GB     lac-[224-225,228-248, 278-285,294-301] 39 Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz 28 240 GB 190 GB     lac-[000-023,032-191,200-223, 254,255,276,277,318-341, 350-369,372,372-445] 313 Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz 28 115 GB 190 GB    intel16-k80 lac-[024-031,080-087,136-143, 192-199,286-293,342-349] 48 Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz 28 240 GB 190 GB k80 (8)   intel16-xl vim-[000,001] 2 Intel(R) Xeon(R) CPU E7-8867 v3 @ 2.50GHz 64 2.93 TB 860 GB     vim-002 1 Intel(R) Xeon(R) CPU E7-8867 v4 @ 2.40GHz 144 5.86 TB 3.7 TB    intel18 skl-[000-112] 113 Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz 40 83 GB 413 GB     skl-[113-131] 19 Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz 40 178 GB 413 GB     skl-[132-139,148-167] 28 Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz 40 367 GB 413 GB     skl-[140-147] 8 Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz 40 745 GB 413 GB    intel18-v100 nvl-[000-007] 8 Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz 40 367 GB 413 GB v100(8)   amd20 amr-[000-101], amr-[136-238], amr-[252-253] 207 AMD EPYC 7H12 Processor @2.595 GHz 128 493 GB 412 GB     amr-[104-135], amr-[239-251] 45 AMD EPYC 7H12 Processor @2.595 GHz 128 996 GB 412 GB     amr-[102-103] 2 AMD EPYC 7H12 Processor @2.595 GHz 128 2005 GB 412 GB    amd20-v100 nvf-[000-016], nvf-[018-020] 20 Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz 48 178 GB 412 GB v100s(4)    nvf-017 1 Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz 48 565 GB 412 GB v100s(4)    nal-[000-010] 11 AMD 7713 @ 2.0 GHz 128 512 GB 1.92 TB A100(4)    nif-[000-005] 6 Intel Xeon  8358 @ 2.6GHz 64 256 GB 1.92 TB A100(4)","title":"Cluster resources"},{"location":"20220924-LabNotebook_Overlays/","tags":["lab notebook","conda","singularity"],"text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","title":"Lab Notebook: Singularity Overlays"},{"location":"20220924-LabNotebook_Overlays/#singularity","tags":["lab notebook","conda","singularity"],"text":"<p>Singularity is a versitle tool to give researchers more flexibility installing software and running their workflows on the HPC.   Most workflows don't need singilarity but it can be extreamly helpful to solve certain weridly difficult problems.  Some common examples for researchers using singularity on the HPC include:</p> <ul> <li>Installing software that needs a special/different base operating system.</li> <li>Installing software that requires adminstrator privliges (aka root, su and/or sudo). </li> <li>Installing complex dependancy trees (like python and R)</li> <li>Using existing software inside a pre-built vitural machine.</li> <li>Working with lots of tiny files on the HPC filesystems which are better designed for smaller numbers of big files. </li> <li>Building workflows that can easily move between different resources.</li> </ul> <p>NOTE This overview is specific to the High Performace Computing Center (HPCC) at Michigan State University (MSU).  For a complete tutorial see the Singularity documentation.  This overview assumes that you have an HPCC account and know how to navigate to and use a development node. </p> <p>The remainder of this overview will walk you through the first steps in using Singularity. However, if you want to skip the details just try running the following three powertools commands.  The first two will create a read/writable overlay and install miniconda. The Third one will start this overlay inside a CentOS singularity image.  You will only need to run the first two commands once to build the overlay file (with conda), then you can just use the third command anytime you want to start the overlay:</p> <pre><code>overlay_build\noverlay_install_conda\noverlay_start\n</code></pre> <p>To exit singularity just type <code>exit</code></p>","title":"Singularity"},{"location":"20220924-LabNotebook_Overlays/#step-1-get-a-singularity-image","tags":["lab notebook","conda","singularity"],"text":"<p>As a starting point we need a singularity image, also known as a container or virtual machine.  You can think of a singularity image as a \"software hard drive\" that contains an entire opperating system in a file. There are three main ways to get these images:</p> <ol> <li>Use one of the Singularity images already on the HPCC. </li> <li>Download an image form one of the many online libraries.</li> <li>Build your own image.</li> </ol> <p>If you don't know which one of the above to use, I recommend that you pick number 1 and just use the singularity image we already have on the system.</p>","title":"Step 1: Get a singularity image"},{"location":"20220924-LabNotebook_Overlays/#1-use-one-of-the-singularity-images-already-on-the-hpcc","tags":["lab notebook","conda","singularity"],"text":"<p>For this introduction, we can keep things simple and just use one of the singilarity images already on the HPCC. This image runs CentOS 7 linux and is a good starting point.  Use the following command to start singularity in a \"shell\" using the provided image:</p> <p><code>singularity shell --env TERM=vt100 /opt/software/CentOS.container/7.4/bin/centos</code></p> <p>Once you run this command you should see the \"Singularity\" prompt which will look something like the following:</p> <p><code>Singularity&gt;</code></p> <p>You did it!  You are running a different operating system (OS) than the base opporating system.  All of the main HPCC folders are still accessable from this \"container\" (ex. /mnt/home, /mnt/research, /mnt/scratch/, etc) so it shouldn't look much different than before (except for the different prompt and you no longer have access to some of the base HPCC software). </p> <p>At this point, if you know what you need, you should be able use files in your home directory and it will compile/run using the singularity OS instead of the base OS.  </p> <p>NOTE: You can just install software in your <code>/mnt/home/$USER</code> and/or <code>/mnt/research</code> folders. The software you install will probably only work from \"inside\" this singularity image. However, you will also be able to see and minipulate the files from within your standard HPC account.  This is fine for many researchers but I recommend you jump down to \"Step 3: Overlays\" to make Singularity even more flexible. </p>","title":"1. Use one of the Singularity images already on the HPCC."},{"location":"20220924-LabNotebook_Overlays/#2-download-an-image-form-one-of-the-many-online-libraries","tags":["lab notebook","conda","singularity"],"text":"<p>Many people publish singularity images and post them on public \"libraries\" for easy install.  Here is a list of online libraries you can browse (this section of the tutorial may need more work, not all of these may work on the HPCC):</p>","title":"2. Download an image form one of the many online libraries"},{"location":"20220924-LabNotebook_Overlays/#sylabs-library","tags":["lab notebook","conda","singularity"],"text":"<p>Link to Browse Sylabs example: <pre><code>singularity pull alpine.sif library://alpine:latest\n\nsingularity shell alpine.sif\n</code></pre></p>","title":"Sylabs Library"},{"location":"20220924-LabNotebook_Overlays/#dockerhub","tags":["lab notebook","conda","singularity"],"text":"<p>Link to Browse Dockerhub example: <pre><code>singularity pull tensorflow.sif docker://tensorflow/tensorflow:latest\n\nsingularity shell tensorflow.sif\n</code></pre></p>","title":"Dockerhub"},{"location":"20220924-LabNotebook_Overlays/#singularity-hub-aka-shub","tags":["lab notebook","conda","singularity"],"text":"<p>Link to Browse Singularity Hub example: <pre><code>singularity pull shub_image.sif shub://vsoch/singularity-images\n\nsingularity shell shub_image.sif\n</code></pre></p>","title":"Singularity Hub (aka shub)"},{"location":"20220924-LabNotebook_Overlays/#3-build-your-own-image","tags":["lab notebook","conda","singularity"],"text":"<p>This one is more complex and outside the scope of the overview. However, if you are intersted I recommend you try using the build command with a Docker image since it is fairly easy to install on your personal computer. Here is a link to how to use docker to make a singularity image:</p> <ul> <li>Link to singularity build command</li> </ul>","title":"3. build your own image"},{"location":"20220924-LabNotebook_Overlays/#step-2-running-commands-in-singularity","tags":["lab notebook","conda","singularity"],"text":"<p>In Step 1 we showed you how to start a singularity \"shell\".  You can also just \"execute\" a command inside the singularity image and resturn the results.  For example, to run </p> <p><code>singularity exec /opt/software/CentOS.container/7.4/bin/centos &lt;&lt;COMMAND&gt;&gt;</code></p> <p>Where you replace <code>&lt;&lt;COMMAND&gt;&gt;</code> whith whatever command you need to run.  This option will become very helpful when you want to run singularity inside a submission script \"See Step 4\" below.</p> <p>For example, the <code>df -hT</code> command will report file system disk space usage. So running the <code>df -hT</code> will give a different result running inside or outside a singularity image. You can test this using the following commands:</p> <p><code>df -hT</code> <code>singularity exec /opt/software/CentOS.container/7.4/bin/centos df -hT</code></p>","title":"Step 2: Running commands in Singularity"},{"location":"20220924-LabNotebook_Overlays/#step-3-overlays","tags":["lab notebook","conda","singularity"],"text":"<p>One problem we often encounter on the HPCC is \"lots-of-small-files\" (hundreds of files where each one is &lt; 50MB).  The filesystem is optmized for large files.  Lots of small files end up \"clogging\" things up which can slow things down for everyone.  One useful trick of singularity is you can make a single large file called an \"Overlay\" which can be attached to a singularity session. You can use an Overlay as a \"filesystem inside a single file\" where you can store lots of the small files inside the single overlay file. From the user point of view you can have as many small files as you want accessable from the singularity image (within reasonable limits). However, these small files act as a single file from the HPCC point of view and dosn't clog things up.</p> <p>This technique is really helpful if you are using complex software installs such as lots of Python, R or Conda installs.  It can also be helpful if your research data is lots of small files. </p>","title":"Step 3: Overlays"},{"location":"20220924-LabNotebook_Overlays/#make-your-overlay-file","tags":["lab notebook","conda","singularity"],"text":"<p>Making an overlay is not hard but takes multiple steps. For details on how to make an overlay we recommend viewing the singularity overlay documentation.</p> <p>Fortunatly the HPCC has a \"powertool\" that can make a basic overlay for you.  All you need to do is run the following command:</p> <pre><code>overlay_build\n</code></pre> <p>This overlay can be applied to a singularity image using the <code>--overlay</code> option as follows:</p> <pre><code>singularity shell --overlay overlay.img --env TERM=vt100 /opt/software\n/CentOS.container/7.4/bin/centos\n</code></pre> <p>If you have an overlay called overlay.img in your current directory you can use the following powertool shortcut to run it inside the centos image:</p> <pre><code>overlay_start\n</code></pre> <p>You can also view the amount of filespace avaliable on an overlay (using the <code>df -hT</code> command we used above by using the following powertool:</p> <pre><code>overlay_size\n</code></pre>","title":"Make your overlay file"},{"location":"20220924-LabNotebook_Overlays/#writing-to-your-overlay","tags":["lab notebook","conda","singularity"],"text":"<p>Once you are in the singularity shell you can now write to the overlay as if you were adding files to the \"root\" directory (/).  For example, running the following commands from inside of your singularity image should allow you to install miniconda3: </p> <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh \n\n./Miniconda3-py39_4.12.0-Linux-x86_64.sh -b -p /miniconda3/\n</code></pre> <p>Since we install miniconda3 a lot there is yet another powertool that will do this installation for you. Just run the following command:</p> <pre><code>overlay_install_conda\n</code></pre> <p>Once miniconda is installed in the <code>/miniconda3/</code> directory you need to add the the folder <code>/miniconda3/</code> to the path with the following command:</p> <pre><code>export PATH=/miniconda3/bin:$PATH\n\nconda --init\n</code></pre> <p>Or, just use the powertool from before and it will automatically add <code>/miniconda3</code> to your path:</p> <pre><code>overlay_start\n</code></pre> <p>At this point you can use <code>pip</code> and <code>conda</code> installs as much as you like.  These generate hundreds of small files but it dosn't matter because everything will be stored in the overlay.img file as one big file.  </p> <p>To exit singularity just type '''exit'''. To start your overlay image just type <code>overlay_start</code></p>","title":"Writing to your overlay"},{"location":"20220924-LabNotebook_Overlays/#step-4-submitting-jobs","tags":["lab notebook","conda","singularity"],"text":"<p>Once we have our image and our conda overlay working in a development node we can execute a script inside of the singularity image, \"in batch mode\" using the <code>exec</code> from above. For example, this script uses our miniconda installed overlay and runs the python3 script called \"mypython.py\" which is stored in my home directory on the HPCC.</p> <pre><code>singularity exec --overlay overlay.img /opt/software/CentOS.container/7.4/bin/centos python3 /mnt/home/$USER/mypython.py\n</code></pre> <p>Once the above is running on a development node we can just submit this as a job to the HPCC using the following submissions script:</p> <p><pre><code>#!/bin/bash\n#SBATCH --walltime=04:00:00\n#SBATCH --mem=5gb\n#SBATCH -c 1\n#SBATCH -N 1\n\nsingularity exec --overlay overlay.img --env PATH=/miniconda3/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/sysbin/ /opt/software/CentOS.container/7.4/bin/centos python3 /mnt/home/$USER/mypython.py\n</code></pre> Again, we have a powertool to help clean this up for common workflows.  using the <code>overlay_exec</code> command you can simply the above submission script using the following:</p> <pre><code>#!/bin/bash\n#SBATCH --walltime=04:00:00\n#SBATCH --mem=5gb\n#SBATCH -c 1\n#SBATCH -N 1\n\noverlay_exec python3 /mnt/home/$USER/mypython.py\n</code></pre>","title":"Step 4: Submitting Jobs"},{"location":"20220924-LabNotebook_Overlays/#job-arrays","tags":["lab notebook","conda","singularity"],"text":"<p>If you need to have multiple jobs running the same software (such as for a job array). You can't have them all writting to the same overlay file.  The following script still allows the overlay to work but all of the changes will be discared after the run so make sure you copy your results back to your home drive:</p> <pre><code>#!/bin/bash\n#SBATCH --walltime=04:00:00\n#SBATCH --mem=5gb\n#SBATCh --array 1-10\n#SBATCH -c 1\n#SBATCH -N 1\n\noverlayrun python3 /mnt/home/$USER/mypython.py $SLURM_ARRAY_ID\n</code></pre> <p>This overview of singularity was inicially written by Dirk Colbry.  Please contact the ICER User Support Team if you need any help getting your workflow up and running.</p> <p>link to ICER User Support Team online contact form</p>","title":"Job Arrays"},{"location":"3._Expansion/","text":"<p>When we type a command with arguments/inputs and press the enter key, shell does several things to the arguments/input text before it actually carries out the command. This action is called expansion. With expansion, the arguments expands into something else before the shell acts on it with the command. Let's see an example with echo command. We already learned echo command in the previous section. As you know, echo prints out its text arguments on standard output.</p> <pre><code>$ echo hello world!\nhello world!\n</code></pre> <p>Let's use echo with '*':</p> <pre><code>$ echo *\nhello.c hello.qsub hello.sb README\n</code></pre> <p>Instead of print '*', it prints out all file names in the directory because shell expands '*' into something else before the echo command acts to the argument (in this case '*').</p> <pre><code>$ ls\nhello.c hello.qsub hello.sb README\n\n$ echo h*\nhello.c hello.qsub hello.sb\n</code></pre> <p>'\\~' is another special character with a special meaning. It expands into the name of the home directory of the user:</p> <pre><code>$ echo ~\n$ temp_user_01\n</code></pre> <p>The shell allows arithmetic by expansion. Arithmetic expansion uses the form $((expression)). Look at the example.</p> <pre><code>$ echo $((1 + 1))\n2\n</code></pre> <p>Please keep in mind that arithmetic expansions allows only integers. Arithmetic expression can be nested, and spaces are allowed.</p> <pre><code>$ echo $(( 7*( 2 + 2 ) ))\n28\n$ echo $((7*(2+2)))\n28\n</code></pre> <p>Brace expansion is useful when you write a shell script or batch script. The brace expression can contain a comma separated list of characters, strings, or integers. Here is a few examples.</p> <pre><code>$ echo srt-{a,b,c}-end\nsrt-a-end srt-b-end srt-c-end\n\n$ echo number_{1..5}\nnumber_1 number_2 number_3 number_4 number_5\n\n$ echo {A..Z}\nA B C D E F G H I J K L M N O P Q R S T U V W X Y Z\n\n$ echo a{A{1,5},B{6..10}}b\naA1b aA5b aB6b aB7b aB8b aB9b aB10b\n</code></pre> <p>The next example is creating multiple directories with a brace expansion.</p> <pre><code>$ mkdir Photos\n$ cd Photos/\nPhotos$ mkdir {2020..2021}-{01..12}\nPhotos$ ls\n2020-01/  2020-03/  2020-05/  2020-07/  2020-09/  2020-11/  2021-01/  2021-03/  2021-05/  2021-07/  2021-09/  2021-11/\n2020-02/  2020-04/  2020-06/  2020-08/  2020-10/  2020-12/  2021-02/  2021-04/  2021-06/  2021-08/  2021-10/  2021-12/\nPhotos$\n</code></pre> <p>Expansion also allows us to use the output of a command.</p> <pre><code>$ echo $(ls |grep 2020)\n2020-01/ 2020-02/ 2020-03/ 2020-04/ 2020-05/ 2020-06/ 2020-07/ 2020-08/ 2020-09/ 2020-10/ 2020-11/ 2020-12/\n</code></pre> <p>Next, let's learn how to control expansion.\u00a0</p> <pre><code>$ echo This is a      test\nThis is a test\n\n$ echo The total is $100.00\nThe total is 00.00\n</code></pre> <p>In the first example, the shell removes extra space from the echo command's argument. In the second example, '$1' is interpreted the first input parameter which is not defined here, and therefore, it is replaced as empty string. With quoting, we can suppress unwanted expansions.</p> <p>First, let's learn about double quotes. If we place text inside double quotes, all special characters lose their special meaning, and treated as ordinary characters. However \"$\" \"\\\" (backslash) and \"`\" (back quote) are exceptions.</p> <pre><code>$ echo \"This is a     test\"\nThis is a     test\n\n$ ls two words.txt\nls: cannot access two: No such file or directory\nls: cannot access words.txt: No such file or directory\n(base) dev-intel16-k80:shell$ ls \"two words.txt\"\ntwo words.txt\n</code></pre> <p>If you want to suppress all expansions, you need to use single quotes. Next three examples show how quoting give different results.</p> <pre><code>$ echo text ~/*.txt {1..5} $(echo foo) $((2+2)) $(date)\ntext /mnt/home/user_name/hostfile.txt /mnt/home/choiyj/powertools.txt 1 2 3 4 5 foo 4 Tue Jan 19 15:10:00 EST 2021\n\n$ echo \"text ~/*.txt {1..5} $(echo foo) $((2+2)) $(date)\"\ntext ~/*.txt {1..5} foo 4 Tue Jan 19 15:10:09 EST 2021\n$ echo 'text ~/*.txt {1..5} $(echo foo) $((2+2)) $(date)'\ntext ~/*.txt {1..5} $(echo foo) $((2+2)) $(date)\n</code></pre> <p>A backslash is useful when we wan to quote a single character. A backslash is called the escape character. Next example shows how quoting and an escape character work.</p> <pre><code>$ echo The balance of $(date) is $100\nThe balance of Tue Jan 19 15:15:36 EST 2021 is 00\n\n$ echo \"The balance of $(date) is $100\"\nThe balance of Tue Jan 19 15:15:48 EST 2021 is 00\n\n$ echo 'The balance of $(date) is $100'\nThe balance of $(date) is $100\n\n$ echo \"The balance of $(date) is \\$100\"\nThe balance of Tue Jan 19 15:16:09 EST 2021 is $100\n\n$ echo 'The balance of $(date) is \\$100'\nThe balance of $(date) is \\$100\n</code></pre> <p>The table show the most frequently used escape characters.</p>           Escape Character Name usage   \\n newline Adding blank lines to text   \\t tab Inserting horizontal tabs to text   \\a alert Making the user terminal beep   \\\\ backslash Inserting a backslash","title":"3. Expansion"},{"location":"4._Conditional_statements/","text":"","title":"4. Conditional statements"},{"location":"4._Conditional_statements/#iffiifelsefiifelifelsefi","text":"","title":"if...fi/if...else...fi/if...elif...else...fi"},{"location":"4._Conditional_statements/#iffi","text":"<pre><code>if [ expression ]\nthen\n   Statement(s) to be executed if expression is true\nfi\n</code></pre>   <p>Example:</p>   <pre><code>#!/bin/sh\n\nif [ $1 == $2 ]\nthen\n   echo \"$1 is equal to $2\"\nfi\n</code></pre>   <p>This is the result of a sample run.</p>   <pre><code>sh ./sample.sh\n is equal to\n$ sh ./test.sh 1 2\n$ sh ./test.sh 12 12\n12 is equal to 12\n</code></pre>","title":"if...fi"},{"location":"4._Conditional_statements/#ifelsefi","text":"<pre><code>if [ expression ]\nthen\n   Statement(s) to be executed if expression is true\nelse\n   Statement(s) to be executed if expression is not true\nfi\n</code></pre>   <p>Example:</p>   <pre><code>#!/bin/sh\n\nif [ $1 == $2 ]\nthen\n   echo \"$1 is equal to $2\"\nelse\n   echo \"$1 is not equal to $2\"\nfi\n</code></pre>   <p>This is the result of a sample run.</p>   <pre><code>$ sh ./test.sh\n is equal to\n\n$ sh ./test.sh 1 2\n1 is not equal to 2\n\n$ sh ./test.sh 12 12\n12 is equal to 12\n</code></pre>","title":"if...else...fi"},{"location":"4._Conditional_statements/#ifelifelsefi","text":"<pre><code>if [ expression 1 ]\nthen\n   Statement(s) to be executed if expression 1 is true\nelif [ expression 2 ]\nthen\n   Statement(s) to be executed if expression 2 is true\nelif [ expression 3 ]\nthen\n   Statement(s) to be executed if expression 3 is true\nelse\n   Statement(s) to be executed if no expression is true\nfi\n</code></pre>   <p>Example:</p>   <pre><code>#!/bin/sh\n\nif [ $1 == $2 ]\nthen\n   echo \"$1 is equal to $2\"\nelif [ $1 -gt $2 ]\nthen\n   echo \"$1 is greater than $2\"\nelif [ $1 -lt $2 ]\nthen\n   echo \"$1 is less than $2\"\nelse\n   echo \"None of the condition met\"\nfi\n</code></pre>   <p>This is the result of a sample run.</p>   <pre><code>#!/bin/sh\nsh ./test.sh\n is equal to\n$ sh ./test.sh 1 2\n1 is less than 2\n$ sh ./test.sh 12 2\n12 is greater than 2\n$ sh ./test.sh 12 12\n12 is equal to 12\n</code></pre>","title":"if...elif...else...fi"},{"location":"4._Conditional_statements/#caseesac","text":"<pre><code>case word in\n    patterns ) commands ;;\nesac\n</code></pre>   <p>Example:</p>   <pre><code>#!/bin/sh\n\nread -p \"Enter a number between 1 and 3 inclusive &gt; \" character\ncase $character in\n    1 ) echo \"You entered one.\"\n        ;;\n    2 ) echo \"You entered two.\"\n        ;;\n    3 ) echo \"You entered three.\"\n        ;;\n    * ) echo \"You did not enter a number between 1 and 3.\"\nesac\n</code></pre>   <p>This is the result of a sample run.</p>   <pre><code>$ sh test.sh\nEnter a number between 1 and 3 inclusive &gt; 1\nYou entered one.\n\n$ bash test.sh\nEnter a number between 1 and 3 inclusive &gt; 2\nYou entered two.\n\n$sh test.sh\nEnter a number between 1 and 3 inclusive &gt; 3\nYou entered three.\n\n$sh test.sh\nEnter a number between 1 and 3 inclusive &gt; 4\nYou did not enter a number between 1 and 3.\n</code></pre>","title":"case...esac"},{"location":"5._Loops/","text":"","title":"5. Loops"},{"location":"5._Loops/#for-loops","text":"<pre><code>for [ expression ]\ndo\n   Statement(s) to be executed\ndone\n</code></pre>   <p>Example:</p>   <pre><code>#!/bin/sh\nfor i in 1 2 3 4 5\ndo\n  echo \"Looping ... number $i\"\ndone\n</code></pre>   <p>This is the result of a sample run.</p>   <pre><code>$ bash sample.sh\nLooping ... number 1\nLooping ... number 2\nLooping ... number 3\nLooping ... number 4\nLooping ... number 5\n</code></pre>   <p>This script also gives a same results. Check the difference.</p>   <pre><code>#!/bin/bash\nfor ((i=1;i&lt;=5;i+=1))\ndo\n  echo \"Looping ... number $i\"\ndone\n</code></pre>","title":"For loops"},{"location":"5._Loops/#while-loops","text":"<pre><code>while [ expression ]\ndo\n   Statement(s) to be executed\ndone\n</code></pre>   <p>Example:</p>   <pre><code>#!/bin/bash\nINPUT_STRING=hello\nwhile [ \"$INPUT_STRING\" != \"bye\" ]\ndo\n  echo \"Please type something in (bye to quit)\"\n  read INPUT_STRING\n  echo \"You typed: $INPUT_STRING\"\ndone\n</code></pre>   <p>This is the result of a sample run.</p>   <pre><code>$sh test.sh\nPlease type something in (bye to quit)\nhello\nYou typed: hello\nPlease type something in (bye to quit)\nhi\nYou typed: hi\nPlease type something in (bye to quit)\nbye\nYou typed: bye\n$\n</code></pre>","title":"While loops"},{"location":"ABySS/","text":"<p>ABySS is a de novo, parallel, paired-end sequence assembler. It can run as an MPI job in the HPCC cluster. The latest version currently installed on the HPCC is 2.1.5, which can be loaded by</p> <pre><code>module load ABySS/2.1.5\n</code></pre> <p>You can optionally load other tools as needed, provided that they have been installed under the same toolchain environment as ABySS/2.1.5. For example,</p> <pre><code>module load BEDTools/2.27.1 SAMtools/1.9 BWA/0.7.17\n</code></pre> <p>is valid after you've loaded ABySS.</p> <p>A sample SLURM script is below.</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=abyss_test  \n#SBATCH --nodes=4  \n#SBATCH --ntasks-per-node=2  \n#SBATCH --mem-per-cpu=5G  \n#SBATCH --time=1:00:00  \n#SBATCH --output=%x-%j.SLURMout\n\necho \"$SLURM_JOB_NODELIST\"\n\nmodule load ABySS/2.1.5\n\nexport OMPI_MCA_mpi_warn_on_fork=0  \nexport OMPI_MCA_mpi_cuda_support=0\n\nabyss-pe k=25 name=test in='/mnt/research/common-data/Bio/ABySS/test-data/reads1.fastq /mnt/research/common-data/Bio/ABySS/test-data/reads2.fastq' v=-v np=8 j=2\n</code></pre> <p>This script launches an MPI job by requesting 8 processes; they are distributed on 4 nodes (<code>--nodes=4</code>) with two processes each (<code>--ntasks-per-node=2</code>). Accordingly, in the <code>abyss-pe</code> command line, we specify <code>np=8</code>. Regarding parameter j, the manual states</p>  <p>The paired-end assembly stage is multithreaded, but must run on a single machine. The number of threads to use may be specified with the parameter j. The default value for j is the value of np.</p>  <p>So, rather than using np as the default value for j, we set j = 2 which is the number of CPUs per node as requested (in this case \"task\" is equivalent to CPU). To submit the job,</p> <p><code>sbatch --constraint=\"[intel16|intel18]\"</code></p> <p>While the job is running, you may look at the SLURM output file, in this example,\u00a0<code>abyss_test-&lt;job ID&gt;.SLURMout</code>, which has a lot of running log, including the following:</p> <p><code>Running on 8 processors</code> <code>6: Running on host lac-391</code> <code>0: Running on host lac-194</code> <code>2: Running on host lac-225</code> <code>4: Running on host lac-287</code> <code>7: Running on host lac-391</code> <code>3: Running on host lac-225</code> <code>1: Running on host lac-194</code> <code>5: Running on host lac-287</code></p>","title":"ABySS"},{"location":"AMD_Optimizing_CPU_Libraries_and_Compilers/","text":"<p>With the purchase of amd20 cluster, we've installed AOCC (AMD Optimizing C/C++ Compiler) compiler system and AOCL (AMD Optimizing CPU Libraries) on the HPCC system. Here we introduce how to use the installed compilers and libraries.</p> <p>Based on LLVM 10.0 release, AMD compilers use the commands clang, clang++ and flang to compile c, c++ and fortran codes respectively. Users can simply load an AOCC module and use the command directly. To find out the version of AOCC installed in HPCC, please run the following command\u00a0on a dev node. All modules should be able to load directly.</p> <pre><code> module spider AOCC\n</code></pre> <p>AOCL are a set of numerical libraries specifically tuned for AMD EPYC processor family. The Libraries can work with either AOCC or GCC compilers. Users need to load a version of GCC or AOCC module and AOCL can be loaded. All available versions of AOCL can be found by running the following command\u00a0on a dev node. Using AOCL on the new amd20 nodes is considered to provide better performance than using GCC compiled libraries (such as OpenBLAS, ScaLAPACK ...). User can find out how to use the libraries and the linkers from the  documentation.</p> <pre><code> module spider AOCL\n</code></pre> <p>Besides AOCC and AOCL, a version of OpenMPI compiled with AOCC (version 2.2.0) is also installed in case users would like to test MPI programs with AMD compilers. Users can simply load the <code>aompi</code> toolchain by running the following command\u00a0on a dev node to load both AOCC and OpenMPI (version 4.0.3).\u00a0</p> <pre><code>ml -* aompi\n</code></pre> <p>An\u00a0<code>aoacl</code> toolchain is also available and able to load the triple modules: AOCC, OpenMPI and AOCL at a time.</p> <p>To find out more information about AOCC or AOCL, Please check AMD Tools and SDKs.</p>","title":"AMD Optimizing CPU Libraries and Compilers"},{"location":"ANSYS/","text":"","title":"ANSYS"},{"location":"ANSYS/#draft-feb-2019","text":"<p>These instructions are for using Ansys on the current HPCC environment that uses Slurm scheduler.\u00a0 They may be helpful for those who are transitioning from our previous Moab/Torque/PBS scheduler.</p>","title":"DRAFT Feb 2019"},{"location":"ANSYS/#license-issues","text":"<p>We are facing a temporary issue with Ansys licensing due to changes made over the many versions HPCC has installed.\u00a0\u00a0 If you have license issues when Using Ansys, here is the work-around:</p> <p>1. Log in to webrdp.msu.edu (see Web Site Access to HPCC )\u00a0 or start an X11 terminal (with MobaXterm/Windows or XQuartz/Mac) (see Connect to HPCC System ).\u00a0</p> <p>2. SSH Connect to any development node, remembering to add -X options. \u00a0\u00a0</p> <p>3.\u00a0 Start this program on any dev node.\u00a0\u00a0 it launches a GUI  </p> <p>/opt/software/ANSYS/19.2/ansys_inc/shared_files/licensing/lic_admin/anslic_admin  </p> <ol> <li>On the left side of the window are three buttons. Click the button \"set License Preferences for User \\&lt;username&gt;\". A new window will open</li> </ol> <p>5. select Release 19.2 in that new window and click OK</p> <p>6. another window will open with tabs across the top and two options in the bottom that are the same for each tab. On the bottom, click the option for \"Use a seperate license for each application\". It doesn't matter which Tab you've slected (Solver/PrePost/etc). That setting should be the same for all tabs.</p> <p>7. click OK, which closes that window.</p> <p>8. In the original Ansys license utility, click File, and then \"exit\" to close it. This modifies the config file in your home directory.</p> <p>9. Close any current sessions in which you running Ansys and start it again on any method (dev node, in 'salloc' interactive job etc). You should now be able to use the features you needed before.</p>","title":"License Issues"},{"location":"ANSYS/#guidelines-for-scheduling-parallel-mpi-jobs","text":"<p>Since moving to Slurm,\u00a0 batch scripts that had worked under Torque are no longer working.\u00a0\u00a0 Here are some guidelines for requesting resources.\u00a0\u00a0\u00a0</p>","title":"Guidelines for scheduling parallel (MPI) jobs"},{"location":"ANSYS/#use-ntasks-instead-of-nodes","text":"<p>Note that <code>-N</code>\u00a0 or <code>-\u2013nodes=</code> will request that number of unique computers, but what most users want is the number of tasks across.\u00a0 Then in addition, use number of tasks instead of number of nodes for -t parameter to</p> <p><code>-t $SLURM_NTASKS</code></p>","title":"Use --ntasks instead of nodes"},{"location":"ANSYS/#dont-forget-to-request-memory","text":"<p>Request memory per task, and since the default is to have 1 cpu per task, you can request memory using <code>--mem-per-cpu=1gb</code></p>","title":"Don't forget to request memory"},{"location":"ANSYS/#create-a-temporary-file-for-node-list","text":"<p>Inside the job, Fluent requires a file of a particular format ,and the slurm node file doesn't work.\u00a0\u00a0 This seems to work</p> <pre><code># create and save a unique temporary file \nFLUENTNODEFILE=`mktemp`\n# fill that tmpfile with node list Fluent can use\nscontrol show hostnames &gt; $FLUENTNODEFILE\n# in your fluent command, use this parameter\n-cnf=$FLUENTNODEFILE\n</code></pre> <p>Example fluent Job script (using Intel compiler). \u00a0 Increase tasks and memory as needed</p> <p>Ansys/Fluent job</p> <pre><code>#!/usr/bin/bash --login\n# example 1 hour job with ntasks across any number of nodes\n# adjust the ram and tasks as needed\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=1\n#SBATCH --ntasks=10\n#SBATCH --mem-per-cpu=1gb \n\n\n# create host list\nNODEFILE=`mktemp`\nscontrol show hostnames &gt; $NODEFILE\n\n# Load the ansys/cfx v19.2 module\nmodule load ANSYS\n\n\n# The Input file\nDEF_FILE=baseline.def  # this file is something you have to provide!\n\n\ncfx5solve -def $DEF_FILE -parallel -par-dist $NODEFILE -start-method \"Platform MPI Distributed Parallel\"&gt; cfx5.log\n</code></pre> <p>After you have logged into a development nodes with an X11 terminal (or use the webrdp gateway as described above),\u00a0 You may run ANSYS tools in parallel and interactively as follows.</p> <pre><code># start a approximately 4 hour interactive job with 10 tasks.  Adjust tasks and memory as needed\n# you'll have 4 hours to work. You must be in a X11 terminal for this to work\n\n salloc --ntasks=10 --cpus-per-task=1 --mem-per-cpu=1gb --time=3:59:00 --x11 \n\n# wait for log-in and then...\n\n# load module\nml intel ansys\n\n# this creates a temporary file and fills it with node list Fluent can use\nNODEFILE=`mktemp`\nscontrol show hostnames &gt; $FLUENTNODEFILE\n\n#for example, run the workbench \nrunwb2\n\u00a0\n# after running workbench you can start fluent directly\n# note we are using Intel mpi\nfluent 3ddp -t $SLURM_NTASKS -mpi=intel -cnf=$FLUENTNODEFILE -ssh\n</code></pre>","title":"Create a temporary file for node list"},{"location":"ANSYS/#cfx5-solver","text":"<p>This solver uses a different hosts file format for the par-dist parameter.\u00a0 The following uses an example Definition file provided by Ansys 19.2.\u00a0</p> <p>The batch script will adapted the par-dist file depending on how you specify tasks and tasks-per-node (the example below does not specify tasks per node).\u00a0\u00a0\u00a0 Code is taken from https://secure.cci.rpi.edu/wiki/index.php?title=CFX.</p> <p>CFX5 Solver Example sbatch</p> <pre><code>#!/usr/bin/bash --login\n# example 1 hour job with ntasks across any number of nodes\n# adjust the ram and tasks as needed\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=1 \n#SBATCH --ntasks=10\n#SBATCH --mem-per-cpu=1gb \n\nmodule load ansys\n\n# codde adapts the hosts file depending on if you use multiple nodes and the tasks-per-node option.   \nsrun hostname -s &gt; /tmp//hosts.$SLURM_JOB_ID\nif [ \"x$SLURM_NPROCS\" = \"x\" ]; then\n    if [ \"x$SLURM_NTASKS_PER_NODE\" = \"x\" ];then\n     SLURM_NTASKS_PER_NODE=1\n     fi\n     SLURM_NPROCS=`expr $SLURM_JOB_NUM_NODES \\* $SLURM_NTASKS_PER_NODE`\nfi\n# use ssh instead of rsh\nexport CFX5RSH=ssh\n# format the host list for cfx\ncfxHosts=`tr '\\n' ',' &lt; /tmp//hosts.$SLURM_JOB_ID`\n\n# example file\nDEF=/opt/software/ANSYS/19.2/ansys_inc/v192/CFX/examples/StaticMixer.def\n# run the partitioner and solver\ncfx5solve -par -par-dist \"$cfxHosts\" -def $DEF -part $SLURM_NPROCS -start-method \"Platform MPI Distributed Parallel\"\n# cleanup\nrm /tmp/hosts.$SLURM_JOB_ID\n\n\n# output will be in a file named like StaticMixer_001.out and StaticMixer_001.res\n</code></pre>","title":"CFX5 Solver"},{"location":"Accessing_Repositories_with_SSH_Key-Based_Authentication/","text":"","title":"Accessing Repositories with SSH Key-Based Authentication"},{"location":"Accessing_Repositories_with_SSH_Key-Based_Authentication/#generating-a-ssh-keypair","text":"<p>Please consult\u00a0SSH Key-Based Authentication on how to generate a SSH keypair.</p>","title":"Generating a SSH Keypair"},{"location":"Accessing_Repositories_with_SSH_Key-Based_Authentication/#setting-up-your-ssh-agent","text":"<p>Please consult\u00a0SSH Key-Based Authentication on how to setup your SSH agent.</p>","title":"Setting up Your SSH Agent"},{"location":"Accessing_Repositories_with_SSH_Key-Based_Authentication/#other-configuration-tweaks","text":"<p>Currently, the HPCC default is to enable X11 forwarding when trying to connect to other machines via SSH. Your connections to <code>vcs.icer.msu.edu</code> can be sped up by overriding this default when connecting to it from HPCC machines. (Eventually, HPCC will override this at the system level, but for now you can do so on your own.)</p> <p>In your HPCC home directory, create a <code>.ssh</code> directory if it does not already exist. In that directory, place a file named <code>config</code> with the following contents:</p> <pre><code>Host gitlab.msu.edu\n    ForwardX11 no\n    ForwardX11Trusted no\n</code></pre>","title":"Other Configuration Tweaks"},{"location":"AlphaFold_installed_in_HPCC/","text":"<p>To use AlphaFold installed in HPCC, users can use the command:</p> <pre><code>[UserName@dev-amd20-v100 ~]$ ml spider AlphaFold\n</code></pre> <p>to find all versions installed in HPCC and use the command:</p> <pre><code>[UserName@dev-amd20-v100 ~]$ ml spider AlphaFold/&lt;version&gt;\n</code></pre> <p>to find how to load a specific AlphaFold version, where <code>&lt;version&gt;</code> is the version of AlphaFold to load. All AlphaFold versions use the same data structure and location\u00a0<code>/mnt/research/common-data/alphafold/database</code> as mentioned in\u00a0Alphafold via Singularity.</p> <p>Illegal memory address</p> <p>If CUDA_ERROR_ILLEGAL_ADDRESS or an illegal memory access was encountered while running AlphaFold, this is mostly due to not enough memory in GPU cards (from the python package \"jax\"). Please try to request the high memory GPU card A100 (79GB) to run your AlphaFold jobs. You can also set the environment variable:</p> <pre><code>export XLA_PYTHON_CLIENT_ALLOCATOR=platform\n</code></pre> <p>to see if the allocated memory is enough or not. Please see  GPU memory allocation for more information.</p>","title":"AlphaFold installed in HPCC"},{"location":"AlphaFold_installed_in_HPCC/#alphafold-example","text":"<p>Users can get an example of AlphaFold to run on HPCC nodes. After log into HPCC and ssh to a dev node with GPU cards, users can run the powertools command:</p> <pre><code>[UserName@dev-amd20-v100 ~]$ getexample AlphaFold\n</code></pre> <p>to copy the example directory <code>AlphaFold</code> in the current directory. After you cd to the directory, you should be able to see the files inside:</p> <pre><code>[UserName@dev-amd20-v100 ~]$ cd AlphaFold\n[UserName@dev-amd20-v100 AlphaFold]$ ls\ndata.fasta  README  slurm_script.sb\n[UserName@dev-amd20-v100 AlphaFold]$ cat slurm_script.sb\n#!/bin/bash\n#SBATCH --job-name AlphaFold\n#SBATCH --time=12:00:00\n#SBATCH --gres=gpu:4\n#SBATCH --cpus-per-task=24\n#SBATCH --mem=90GB\n#SBATCH --constraint=[intel18|amr|nvf|nal|nif]\n\nexport NVIDIA_VISIBLE_DEVICES=\"${CUDA_VISIBLE_DEVICES}\"\n\nml -* fosscuda/2020a AlphaFold/2.0.0\n\nalphafold --fasta_paths=$PWD/data.fasta --output_dir=$PWD --preset=casp14 --max_template_date=2020-05-14 --model_names=model_1\n\nscontrol show job ${SLURM_JOBID}\njs -j ${SLURM_JOBID}\n</code></pre> <p>The job script file <code>slurm_script.sb</code> shows how to load the <code>AlphaFold</code> module and run the command <code>alphafold</code>. Since most of the specifications and variables have been set in the <code>alphafold</code> command script and the module file, 5 options:</p> <pre><code>--fasta_paths\n--output_dir\n--preset\n--max_template_date\n--model_names\n</code></pre> <p>for the job are specified in the command line. Users can look into the AlphaFold documentation or use the commands:</p> <pre><code>[UserName@dev-amd20-v100 AlphaFold]$ alphafold --help\n</code></pre> <p>to find out how to use all options after you load the AlphaFold module.</p>","title":"AlphaFold example"},{"location":"Alphafold/","text":"","title":"Alphafold"},{"location":"Alphafold/#alphafold-via-singularity","text":"","title":"Alphafold via Singularity"},{"location":"Alphafold/#alphafold-installed-in-hpcc","text":"<p>.</p>","title":"AlphaFold installed in HPCC"},{"location":"Alphafold_via_Singularity/","text":"<p>Alphafold can be run via Singularity.</p> <p>Alphafold database is located in <code>/mnt/research/common-data/alphafold/database</code>.</p> <pre><code>database\n\u251c\u2500\u2500 bfd\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt_a3m.ffdata\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt.tar.gz\n\u251c\u2500\u2500 mgnify\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 mgy_clusters_2018_12.fa\n\u251c\u2500\u2500 params\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_1.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_1_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_2.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_2_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_3_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_4.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_4_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_5.npz\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 params_model_5_ptm.npz\n\u251c\u2500\u2500 pdb70\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 md5sum\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_a3m.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_a3m.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_clu.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_cs219.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_cs219.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_hhm.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_hhm.ffindex\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pdb_filter.dat\n\u251c\u2500\u2500 pdb_mmcif\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mmcif_files\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 obsolete.dat\n\u251c\u2500\u2500 small_bfd\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 bfd-first_non_consensus_sequences.fasta\n\u251c\u2500\u2500 uniclust30\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 uniclust30_2018_08\n\u2514\u2500\u2500 uniref90\n    \u251c\u2500\u2500 uniref90.fasta\n    \u2514\u2500\u2500 uniref90.fasta.1.gz\n</code></pre> <p>Before running Alphafold, you need to set</p> <pre><code>export ALPHAFOLD_DATA_PATH=\"/mnt/research/common-data/alphafold/database\"  \nexport ALPHAFOLD_MODELS=\"/mnt/research/common-data/alphafold/database/params\"\n</code></pre> <p>To run alphafold, please use the following template (for more information about options/flags, please refer to https://github.com/deepmind/alphafold.</p> <p>In the script, <code>input.fasta</code> is your input data, and\u00a0you need to set up output_dir. Since the command <code>/usr/bin/hhsearch</code> inside the container does not work on intel14 nodes (<code>Illegal instruction</code>), please use the <code>SBATCH</code> option\u00a0<code>--constraint</code> in the job script.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name alphafold-run\n#SBATCH --time=08:00:00\n#SBATCH --gres=gpu:1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=20G\n#SBATCH --constraint=\"[intel16|intel18|amd20]\"\n\nexport ALPHAFOLD_DATA_PATH=\"/mnt/research/common-data/alphafold/database\"\nexport ALPHAFOLD_MODELS=\"/mnt/research/common-data/alphafold/database/params\"\n\nsingularity run --nv \\\n-B $ALPHAFOLD_DATA_PATH:/data \\\n-B $ALPHAFOLD_MODELS \\\n-B .:/etc \\\n--pwd  /app/alphafold /opt/software/alphafold/2.0.0/alphafold.sif \\\n--data_dir=/data \\\n--output_dir=/mnt/gs18/scratch/users/my_id/alphafold/output \\\n--fasta_paths=/mnt/gs18/scratch/users/my_id/alphafold/input.fasta  \\\n--uniref90_database_path=/data/uniref90/uniref90.fasta  \\\n--mgnify_database_path=/data/mgnify/mgy_clusters_2018_12.fa   \\\n--bfd_database_path=/data/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\\n--uniclust30_database_path=/data/uniclust30/uniclust30_2018_08/uniclust30_2018_08 \\\n--pdb70_database_path=/data/pdb70/pdb70  \\\n--template_mmcif_dir=/data/pdb_mmcif/mmcif_files  \\\n--obsolete_pdbs_path=/data/pdb_mmcif/obsolete.dat \\\n--max_template_date=2020-05-14   \\\n--model_names=model_1 \\\n--preset=casp14\n</code></pre>","title":"Alphafold via Singularity"},{"location":"An_SSH_tunneling_via_multiple_hops/","text":"<p>In some cases, you want to access dev nodes directly from your local machine (technically, you have to get through the gateway, but you don't have to ssh to a dev node manually with a tunneling). There are two ways to do that.</p>","title":"An SSH tunneling via multiple hops"},{"location":"An_SSH_tunneling_via_multiple_hops/#using-proxyjump","text":"<p>Because we need to hop twice (your local machine -&gt; gateway -&gt; dev node), we need an ssh config file under .ssh directory of your local machine (the file name should be config).\u00a0The following is an example config file which defines intel18 and k80 dev node. With this file, you can connect a dev node from your local machine with 'ssh intel18' or 'ssh k80'.</p> <pre><code>Host gateway\n    HostName gateway.hpcc.msu.edu\n    User here_you_put_your_net_id\n\nHost intel18\n    HostName dev-intel18\n    User here_you_put_your_net_id\n    ProxyJump gateway\n\nHost k80\n    HostName dev-intel16-k80\n    User here_you_put_your_net_id\n    ProxyJump gateway\n</code></pre> <p>config example</p> <p>Now you can just type the name of host to connect.</p> <p></p> <p>Tip: With SSH Key-Based Authentication you don't have to type your password when you login.</p>","title":"Using ProxyJump"},{"location":"An_SSH_tunneling_via_multiple_hops/#using-port-forwarding","text":"<p>Instead of using ProxyJump, you can use port forwarding. For this method, you need to open two terminals on your local machine.</p> <p>1st terminal (left in the picture): type </p> <pre><code>ssh -L 1234:dev-intel18:22\u00a0&lt;your_net_id&gt;@gateway.hpcc.msu.edu\n</code></pre> <p>You can change 1234 to any number larger than 1024 (1234 here is a port number you are using). You can change dev-intel18 to any dev-node name, but 22 (port number of dev node) should be remained. For example,\u00a0</p> <pre><code>ssh -L 4321:dev-intel16:22 &lt;your_net_id&gt;@gateway.hpcc.msu.edu\n</code></pre> <p>is also working.</p> <p>2nd terminal (right in the picture): type </p> <pre><code>ssh -p 1234 &lt;your_net_id&gt;@localhost\n</code></pre> <p>If it is the first time, it would request connection confirmation. type yes. Then you will arrive a dev-node on the 2nd terminal.</p> <p></p>","title":"Using port forwarding"},{"location":"Application_Icons_on_Desktop/","text":"<p>It is much easier to execute your favorite apps by clicking icons on the desktop just like using your Windows or Mac PC. We can certainly do this through Open OnDemand. There are some app icons already created in the directory <code>/opt/software/OnDemand/Desktop-Icons</code>, where you can see them by listing the folder:</p> <pre><code>$ ls /opt/software/OnDemand/Desktop-Icons\nANSYS.desktop             Dolphin.desktop  GaussView.desktop  Maestro.desktop   rstudio.desktop  tecplot.desktop           VMD.desktop\nchromium-browser.desktop  firefox.desktop  GSEA.desktop       MATLAB.desktop    sas.desktop      Terminal.desktop\nCOMSOL.desktop            Fluent.desktop   Jupyter.desktop    Nautilus.desktop  Stata.desktop    User's Anaconda3.deskto\n</code></pre> <p>User can easily add the icons to their OnDemand interactive desktop by following the sections below.</p> <p>[ Use Command Lines ] [ Use Interactive Desktop ] [ Create App Icons ]</p> <p>A video instruction is also provided (click to start).</p>","title":"Application Icons on Desktop"},{"location":"Application_Icons_on_Desktop/#use-command-lines","text":"<p>You can simply copy them to your desktop directory <code>~/Desktop</code> . For example, if you would like to have MATLAB icon on your OnDemand desktop, you can run</p> <pre><code>$ mkdir -p ~/Desktop\n$ cp /opt/software/OnDemand/Desktop-Icons/MATLAB.desktop ~/Desktop/\n</code></pre> <p>Once the apps' desktop files are copied, request an  Interactive Desktop session as mentioned in the Open OnDemand page. You should see the apps' icons on your desktop once you launch it:</p> <p></p>","title":"Use Command Lines"},{"location":"Application_Icons_on_Desktop/#use-interactive-desktop","text":"<p>You can also request and start an  Interactive Desktop session to copy the app icons from\u00a0<code>/opt/software/OnDemand/Desktop-Icons</code>. Simply double-click the \"<code>Trash</code>\" or \"<code>Computer</code>\" icon on your desktop. It will pop out the file manager window. In the \"<code>Location:</code>\" place, please enter the directory <code>/opt/software/OnDemand/Desktop-Icons</code>. (If the \"<code>Location:</code>\" place does not allow any input, please click on\u00a0 ). It should show all app icons in the window. Right click on an app icon you would like to copy to your desktop. Choose \"Copy to\" and click \"Desktop\":</p> <p></p> <p>The icon you choose will be copied to your desktop.</p>","title":"Use Interactive Desktop"},{"location":"Application_Icons_on_Desktop/#create-app-icons","text":"<p>If your favorite app icons are not in the directory, you can try to use one them as an example</p> <pre><code>$ cat /opt/software/OnDemand/Desktop-Icons/MATLAB.desktop\n\n[Desktop Entry]\nType=Application\nName=MATLAB\nIcon=/opt/software/OnDemand/images/matlab.png\nExec=bash -c \"module load MATLAB/2018a; matlab -desktop\"\nTerminal=false\nGenericName=\n</code></pre> <p>and modify it. Change the following contents</p> <pre><code>Name=&lt;Software Name&gt;\nIcon=&lt;Location and File name of the Software Icon&gt;\nExec=&lt;Commands to Run the Software&gt;\n</code></pre> <p>to your app's. Save the file with your app's file name in <code>~/Desktop</code> directory. Every time you launch an Interactive Desktop session, the icon shows on your desktop. If you have any question, please let us know. We can help you to create one.</p>","title":"Create App Icons"},{"location":"Aspera_bulk_file_transfer/","text":"<p>The Aspera Connect application (ascp) is a useful file transfer tool for downloading or uploading large files in bulk between the HPCC and data repository sites such as those operated by NCBI. \u00a0In order to interact with a server via aspera, the remote host must be running the Aspera server. \u00a0This tutorial will demonstrate how to install and use the command line version of Aspera to download files from the NCBI ftp site.</p> <p>You can only execute Aspera file transfers from gateway. \u00a0Transfers on the dev-nodes will not work correctly.</p> <p>Go to https://www.ibm.com/products/aspera/downloads to download \"aspera connect\".</p> <ol> <li>Select the Linux OS and download     <code>aspera-connect-3.7.2.141527-linux-64.sh</code> (version may change over     time) to your home directory.</li> <li>Run <code>chmod u+x aspera-connect-3.7.2.141527-linux-64.sh</code></li> <li>Run <code>./aspera-connect-3.7.2.141527-linux-64.sh</code></li> </ol> <p>The installation will then be located in <code>~/.aspera/connect/</code>; and the command ascp is in <code>~/.aspera/connect/bin/</code></p> <p>Example use:</p> <pre><code>~/.aspera/connect/bin/ascp -T -k 1 -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh anonftp@ftp.ncbi.nlm.nih.gov:/refseq/uniprotkb ~/NCBI_data\n</code></pre> <p>More instructions and examples can be found here.</p>","title":"Aspera bulk file transfer"},{"location":"Assembly_of_PacBio_long_reads_with_Canu/","text":"","title":"Assembly of PacBio long reads with Canu"},{"location":"Assembly_of_PacBio_long_reads_with_Canu/#introduction","text":"<p>Canu is used for de novo assembly using long reads, as generated from PacBio or Oxford Nanopore technologies. It consists of three steps: read correction, read trimming and contig assembly.</p> <p>As of Sept 2021, we have the latest version 2.2 installed on the HPCC. You can load it by</p> <pre><code>module purge\nmodule load GCCcore/8.3.0 Java/11 Perl/5.30.0 gnuplot/5.2.8\nexport PATH=/opt/software/canu/canu-2.2/bin:$PATH\n</code></pre> <p>Then, simply running <code>canu</code> will give you a good amount of help information. For example, at the bottom of the help document, we learn that canu supports three types of raw input data:</p> <pre><code>[technology]\n-pacbio      &lt;files&gt;\n-nanopore    &lt;files&gt;\n-pacbio-hifi &lt;files&gt;\n</code></pre> <p>While canu can automate job submission using SLURM, we don't recommend this method. Therefore, please specify <code>useGrid=false</code> in the canu command to disable grid support. Users will write a job script manually, treating canu as an ordinary program.</p>","title":"Introduction"},{"location":"Assembly_of_PacBio_long_reads_with_Canu/#an-example-using-pacbio-reads","text":"<p>The PacBio reads data we will be assembling is the same as the one used in the canu tutorial, which can be downloaded using the following command:</p> <pre><code>curl -L -o pacbio.fastq http://gembox.cbcb.umd.edu/mhap/raw/ecoli_p6_25x.filtered.fastq\n</code></pre> <p>By default, the canu pipeline will correct the reads, trim the reads, and then assemble the reads to contigs. Minimally, you can run canu on a dev-node in the following way (we need to first load all necessary modules):</p> <pre><code>module purge\nmodule load GCCcore/8.3.0 Java/11 Perl/5.30.0 gnuplot/5.2.8\nexport PATH=/opt/software/canu/canu-2.2/bin:$PATH\n\n/bin/time -v canu -p ecoli -d ecoli-pacbio genomeSize=4.8m useGrid=false maxThreads=10 -pacbio pacbio.fastq &gt; runCanu_2021-09-14.log 2&gt;&amp;1 &amp;\n</code></pre> <p>Above,</p> <ul> <li><code>pacbio.fastq</code> is the input file, considered as raw and unprocessed     reads. Coupled with <code>-pacbio</code>, canu knows which technology has     generated these reads.</li> <li><code>-p</code>: set the file name prefix of intermediate and output files;     it's mandatory.</li> <li><code>-d</code>: set assembly directory name for canu to run in. If not     supplied, it'll run in the current directory. It is not possible to     run two different assemblies in the same directory.</li> <li><code>genomeSize</code>: in bases, with common prefixes allowed, such as 4.7m     or 2.8g. canu uses it to determine coverage in the input reads.</li> <li><code>useGrid=false</code>: make canu run on the local machine.</li> <li><code>maxThreads</code>: the maximum number of threads that each task can use.</li> <li>Finally, we put <code>time -v</code> in front of the canu command in order to     get resource usage, which will be shown at the end of the log file     <code>runCanu_2021-09-14.log</code>. For example,<ul> <li><code>Maximum resident set size (kbytes): 4113216</code> tells us that the     maximum memory used during the process is about 4G.</li> <li><code>Percent of CPU this job got: 476%</code> tells us we've used on     average 5 CPUs.</li> </ul> </li> </ul> <p>If we want to have canu run in the HPCC cluster, we can write a job script accordingly:</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=canu_ecoli\n#SBATCH --cpus-per-task=10\n#SBATCH --mem=10G\n#SBATCH --time=2:00:00\n#SBATCH --output=%x-%j.SLURMout\n\nmodule purge\nmodule load GCCcore/8.3.0 Java/11 Perl/5.30.0 gnuplot/5.2.8\nexport PATH=/opt/software/canu/canu-2.2/bin:$PATH\n\n/bin/time -v canu -p ecoli -d ecoli-pacbio genomeSize=4.8m useGrid=false maxThreads=10 -pacbio pacbio.fastq &gt; runCanu_2021-09-14.log 2&gt;&amp;1\n</code></pre> <p>The canu command is exactly the same as the one we run on the dev-node, except that the trailing <code>&amp;</code> sign should be removed when it is within a job script.</p> <p>The primary output file for most users is the assembled contigs. In this example, it is <code>ecoli-pacbio/ecoli.contigs.fasta</code> under your current working directory. Refer to this page when you want to learn more about the output, such as the various statistics of the reads analyzed, as reported in the <code>ecoli.report</code> file.</p>","title":"An example using PacBio reads"},{"location":"Assembly_of_PacBio_long_reads_with_Canu/#notes","text":"<ul> <li>To adjust default parameters, you need to consult     canu parameter reference.</li> <li>The three steps (error correction, trimming and assembly) can be     individually run. See this example.</li> <li>If your data is     PacBio HiFi reads (i.e.     CCS reads with predicted accuracy &gt;= Q20 or 99%), you may want to     use the option <code>-pacbio-hifi</code> rather than <code>-pacbio</code>. Canu will skip     read correction and trimming in this case.</li> </ul>","title":"Notes"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/","text":"","title":"BLAST/BLAST+ with Multiple Processors"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#overview","text":"<p>It is possible to run BLAST or BLAST+ on the HPCC in multi-threaded mode. \u00a0This is advantageous in that is allows users to leverage multiple processors to complete their BLAST searches, thereby decreasing compute time.</p> <p>To load BLAST or BLAST+ on the HPCC:</p> <pre><code># Loading BLAST\nmodule purge\nmodule load BLAST/2.2.26-Linux_x86_64\n\n# Loading BLAST+\nmodule purge\nmodule load icc/2017.4.196-GCC-6.4.0-2.28  impi/2017.3.196 BLAST+/2.8.1-Python-2.7.14\n</code></pre>","title":"Overview"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#multi-threading-vs-mpi","text":"<p>Multi-threaded BLAST runs enable the user to launch multiple worker threads on a single node. \u00a0However, because standard BLAST and BLAST+ do not use distributed memory, you cannot accomplish multi-threaded runs across multiple nodes. \u00a0Therefore, users executing multi-threaded BLAST or BLAST+ runs should not reserve more than one node, as this will reserve hardware resources that cannot be used.</p>","title":"Multi-Threading vs. MPI"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#job-submission-guidelines","text":"<p>First, we need to differentiate between traditional NCBI BLAST and BLAST+. \u00a0Traditional NCBI BLAST utilizes the \"-a #\" flag to specify the number of processors to use for the job (default is 1). \u00a0BLAST+ uses the \"-num_threads #\" flag to specify the number of worker threads to use. \u00a0Depending upon which type of BLAST you use, you will need to adjust your job submission script parameters accordingly.</p>","title":"Job Submission Guidelines"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#traditional-blast","text":"<p>Using the \"-a\" flag in BLAST will specify the number of\u00a0processors\u00a0to use. \u00a0To reserve the appropriate quantity of resources in your job submission script, you will need to reserve a number of cores equal to the value specified by the \"-a\" flag \u00a0For example, if you used a command like:</p> <pre><code>blastall -p blastp -d swissprot -i\u00a0prot.fasta\u00a0-o test1.blast -e 0.001 -a 4\n</code></pre> <p>You should specify something like the following in your SLURM job submission script:</p> <pre><code>#SBATCH --cpus-per-task=4\n</code></pre>","title":"Traditional BLAST"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#blast","text":"<p>In contrast, BLAST+ uses the \"-num_threads\" flag to specify the number of worker\u00a0threads\u00a0to create. \u00a0In order to specify the correct number of cores for the job, you will need to\u00a0ADD ONE\u00a0to the number of threads specified. \u00a0This is to account for the number of worker threads,\u00a0PLUS\u00a0the main process thread. \u00a0So if you used an equivalent BLAST+ command like:</p> <pre><code>blastn -task blastn -db swissprot -query prot.fasta -out test1.blast -evalue 0.001 -num_threads 4\n</code></pre> <p>You should use the following in your SLURM\u00a0script:</p> <pre><code>#SBATCH --cpus-per-task=5\n</code></pre>","title":"BLAST+"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#blastdb","text":"<p>The BLASTDB environmental variable tells BLAST or BLAST+ where to find your databases that can be searched. \u00a0On the HPCC, we offer select BLAST-ready data sets for this purpose in a common read-only area. \u00a0BLAST data sets can be accessed at:</p> <pre><code>/mnt/research/common-data/Bio/blastdb\n</code></pre> <p>If you are using the FASTA sequences instead of nucleotide data sets, you need to augment the path above as follows:</p> <pre><code>/mnt/research/common-data/Bio/blastdb/FASTA\n</code></pre> <p>For cluster jobs, you will need to set the value of BLASTDB in your job submission script, for example:</p> <pre><code>export BLASTDB=/mnt/research/common-data/Bio/blastdb:/mnt/research/common-data/Bio/blastdb/FASTA:$BLASTDB\n</code></pre>","title":"BLASTDB"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#a-word-about-memory","text":"<p>In either case (BLAST or BLAST+) your requested memory (in the examples above, 4gb) will be divided amongst all of your task threads. \u00a0Plan accordingly.</p>","title":"A Word About Memory"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#blast-data-preparation","text":"<p>Data downloaded from the NCBI website, or prepared by users can, in most cases, be easily converted for use with BLAST. \u00a0This brief tutorial is designed to illustrate a fairly basic scenario where the user wants to download a set of FASTA sequences from the NCBI website and prepare them for BLAST-ing.</p>","title":"BLAST data preparation"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#download","text":"<p>The simplest way to do this is to note the link of the FASTA file, and use either the \"wget\" or \"curl\" command. \u00a0For example:</p> <pre><code>wget ftp://ftp.ncbi.nih.gov/repository/UniGene/Triticum_aestivum/Ta.seq.all.gz\n</code></pre> <p>or</p> <pre><code>curl -O ftp://ftp.ncbi.nih.gov/repository/UniGene/Triticum_aestivum/Ta.seq.all.gz\n</code></pre> <p>This will download the file \"Ta.seq.all.gz\" into the current directory. Now unzip the file:</p> <pre><code>gunzip Ta.seq.all.gz\n</code></pre> <p>This will leave a file called \"Ta.seq.all\" in your directory.</p>","title":"Download"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#preparing-the-indices","text":"<p>To prepare the BLAST indices for nucleotides:</p> <pre><code>formatdb -i Ta.seq.all -p F\n</code></pre> <p>The command above will produce several files, such as:</p> <pre><code>Ta.seq.all.fa.nhr\nTa.seq.all.fa.nin\nTa.seq.all.fa.nsq\n</code></pre> <p>If you want to produce protein indices instead of, or in addition to nucleotides, run:</p> <pre><code>formatdb -i Ta.seq.all -p T\n</code></pre> <p>In this case, this will produce the files:</p> <pre><code>Ta.seq.all.fa.phr\nTa.seq.all.fa.pin\nTa.seq.all.fa.psq\n</code></pre> <p>You can verify whether your BLAST formatting was successful by looking at the \"formatdb.log\" file which should now be present in your directory.</p>","title":"Preparing the Indices"},{"location":"Basic_Mathematical_Library_Tests_on_AMD_EPYC_Processors/","text":"<p>This test runs many calculations of sine, cosine and\u00a0logarithm functions in parallel. Each of the calculations is independent from the others and finally they get summed up. The test is executed by a C program written with OpenMP multi-threading and compiled with different compilers and libraries. Three letters (A, G and I) followed by a digit (1 or 2) are used to specify different tests:</p>     Letters First Second Digit     A AMD Compiler AMD\u00a0basic mathematical Library 1: all threads running in one socket   G GNU Compiler GNU\u00a0basic mathematical Library (-lm) 2: threads evenly spread to two different sockets   I Intel Compiler Intel\u00a0basic mathematical Library (included in compiler)     <p>where the letter in the first and second position represents which compiler and basic mathematical library is in use respectively. The performance results are presented in the following figure:</p> <p></p> <p>where all timing values were derived by the average of running ten times. As you can see in the figure, the performance of the parallel scaling is almost linear for all compilers and the scaling efficiency1 is strong (about 61% for AA1 and AA2). From the comparison of the timing results, Intel compiler with its library shows the best performance. However, GCC and AMD compilers with AMD basic mathematical library also performs well. In the results of 128 threads, the elapsed time of AMD compiler with AMD library are very closed to the time of Intel's. In the tests of spreading threads. we also find out all threads running on one socket has no difference from spreading them on two different sockets.</p> <p>The same C program was also compiled and run on an intel18 and an intel16 node. The timing of amd20 node with 128 threads is about 3 times faster than the performance of\u00a0intel18 (with 40 threads) and 4.5 times faster than the performance of\u00a0intel16 (with 28 threads). The decrease in timing is well prorated with the increase on thread number.\u00a0</p>","title":"Basic Mathematical Library Tests on AMD EPYC Processors"},{"location":"Buy-In_Accounts_with_SLURM/","text":"<p>If you want to have priority access to our clusters, you can purchase buy-in nodes.  Users who run on buy-in nodes receive priority access to their nodes within 4 hours and are exempt from the 1 million CPU hour per year limit.</p>","title":"Buy-In Accounts with SLURM"},{"location":"Buy-In_Accounts_with_SLURM/#types-of-partitions","text":"<p>There are three types of partitions configured in SLURM.</p>    Name Purpose     Buyin Partition (names vary) A partition is created for each buy-in account. Each buy-in partition includes all non-buy-in nodes, allowing buy-in jobs to span buy-in and non-buy-in nodes. These jobs get equal consideration for scheduling on non-buy-in nodes as jobs in general-long. When jobs submitted to these partitions request a wall time of four hours or less, they are also submitted to the general-short, enabling them to use other available buy-in nodes and ensuring they are scheduled as fast as possible.   general-short This partition includes all nodes--buy-in and non-buy-in--and runs jobs that request a wall time of four hours or less. Jobs in this partition are considered for scheduling after jobs in buy-in partitions and the general-long partition. To prevent these jobs from being continuously bumped by general-long/buy-in jobs, they are also submitted to general-long. Jobs with a requested wall time of four hours or less are automatically submitted to this partition.   general-long This partition includes non-buy-in nodes and allows jobs to run for up to seven days. Jobs with a requested wall time of four hours or less are also submitted to the general-short partition.   general-long-bigmem This partition includes non-buy-in nodes with more memory and CPU cores than most nodes and allows jobs to run for up to seven days. Jobs requesting more than 256GB or 40 CPUs per node are automatically submitted to this partition. This partition ensures large jobs get priority access to large nodes over jobs that can run elsewhere.\u00a0Jobs with a requested wall time of four hours or less are also submitted to the general-short partition.   general-long-gpu This partition contains non-buy-in nodes with GPUs and allows jobs to run for up to seven days. Jobs requesting GPUs are automatically submitted to the partition. This partition ensures jobs requesting GPUs get priority access to nodes with GPUs over jobs not requesting GPUs. Jobs with a requested wall time of four hours or less are also submitted to the general-short partition.    <p>In most cases, you will not need to specify a partition when submitting a job. Given a specified (or default) account, SLURM's job submit plugin will assemble the partition list automatically.</p>","title":"Types of Partitions"},{"location":"Buy-In_Accounts_with_SLURM/#default-account","text":"<p>When submitting a job without specifying an account, your default account is used. You can check your default account using the <code>buyin_status</code> power tool.</p> <pre><code>$ module load powertools\n$ buyin_status -l\n\nUser:     fordste5\nAccounts: test1_classres test1 classres\nDefault:  classres\n</code></pre> <p>User <code>fordste5</code> has a default account of <code>classres</code>. If <code>fordste5</code> submits a job without specifying an account, it will be submitted to the <code>classres</code> partition. If the job requested four hours or less of wall time, it will be submitted to the <code>general-short</code> partition as well. Jobs submitted by users with a default account of <code>general</code> will be queued in the <code>general-long</code> and <code>general-short</code> partitions.</p>    Default Account Wall Time&lt;=4 Hours Wall Time&lt;=4 Hours and\u00a0&gt;=256GB or 40CPU Wall Time&lt;=4 Hours and\u00a0GPUs Reqested Wall Time&gt;4 Hours\u00a0 Wall Time&gt;4 Hours and\u00a0&gt;=256GB or 40CPU Wall Time&gt;4 Hours and\u00a0GPUs     general general-long,general-short general-long-bigmem,general-short general-long-gpu,general-short general-long general-long-bigmem general-long-gpu   <code>&lt;buyin&gt;</code> <code>&lt;buyin&gt;</code>, general-long, general-short <code>&lt;buyin&gt;</code>, general-long-bigmem, general-short <code>&lt;buyin&gt;</code>, general-long-gpu, general-short <code>&lt;buyin&gt;</code>, general-long <code>&lt;buyin&gt;</code>, general-long-bigmem <code>&lt;buyin&gt;</code>, general-long-gpu    <p>where <code>&lt;buyin&gt;</code> is the name of buyin account.</p>","title":"Default Account"},{"location":"Buy-In_Accounts_with_SLURM/#using-non-default-accounts","text":"<p>You can see what accounts you have access to using the <code>buyin_status</code> powertool:</p> <pre><code>$ module load powertools\n$ buyin_status -l\n\nUser:     fordste5\nAccounts: test1_classres test1 classres\nDefault:  classres\n</code></pre> <p>This output shows that <code>fordste5</code> is a member of the <code>test1_classres</code>, <code>test1</code>, <code>and\u00a0classres</code> buy-in accounts. Both <code>test1</code> and <code>classres</code> correspond to partitions with the same name.</p> <p>Because <code>fordste5</code> has a default account of <code>classres</code>, if they want\u00a0to submit jobs to <code>test1</code>, they will have to explicitly specify the <code>test1</code> account at submission. This is done by using either using the <code>-A test1</code> option for <code>srun/sbatch/salloc</code>, or by adding the <code>#SBATCH -A test1</code> directive to their batch script.</p>","title":"Using Non-Default Accounts"},{"location":"Buy-In_Accounts_with_SLURM/#checking-the-status-of-buy-ins","text":"<p>The status of buy-ins can be viewed with the <code>buyin_status</code> powertool.</p> <pre><code>$ module load powertools\n$ buyin_status -a classres\n\nUser:     fordste5\nAccounts: test1_classres test1 classres\nDefault:  classres\n\n\nBuyin: classres\n  JOBID      STATE      USER       CPUS PRIORITY  TIME_LIMIT           START_TIME\n  17401      RUNNING    fordste5   10   101          1:00:00  2018-08-09T12:55:17\n  17409      RUNNING    changc81   4    100          1:00:00  2018-08-09T12:57:54\n\n  Partition: classres\n    lac-421 (down*)\n    csn-020 (down*)\n    csp-018 (down*)\n    css-034 (down*)\n    css-035 (down*)\n    css-079 (down*)\n    css-080 (down*)\n    css-033 (allocated)\n      JOBID      ACCOUNT    USER       CPUS       TIME  TIME_LEFT\n      17402      general    fordste5   10         4:36      55:24\n      17401      classres   fordste5   10         4:36      55:24\n</code></pre> <p>This tool will list all jobs that are queued for a buy-in account, all nodes associated with the buy-in\u00a0account\u2013including node\u00a0status\u2013and a list of jobs running on each node. Because jobs with a wall time of four hours or less can run on any buy-in node, it is possible to see jobs running on buy-in nodes from other accounts.</p> <p>Buy-in partitions have the same name as a given buy-in account</p>","title":"Checking the Status of Buy-Ins"},{"location":"Buy-In_Accounts_with_SLURM/#managing-buy-in-account-membership","text":"<p>SLURM allows for the configuration of account coordinators. Account coordinators can add and remove users to accounts that they coordinate. Buy-in account owners are configured as coordinators of their buy-in accounts and can request that other users also be added as coordinators.</p> <p>If you had a buy-in account configured in the old Moab scheduler, account coordinators for your SLURM buy-in account were copied from the managers listed on the Moab account.</p> <p>View what users have access to your buy-in account:</p> <pre><code>$ sacctmgr show association where account=&lt;account&gt;\n</code></pre> <p>Add a user to your buy-in account:</p> <pre><code>$ sacctmgr add user name=&lt;userid&gt; account=&lt;account&gt;\n</code></pre> <p>Remove a user from your buy-in account:</p> <pre><code>$ sacctmgr delete user &lt;userid&gt; where account=&lt;account&gt;\n</code></pre>","title":"Managing Buy-In Account Membership"},{"location":"COMSOL%202/","text":"<p>To use COMSOL with SLURM you should use the slurm option '-mpibootstrap slurm' and omit using srun.</p> <p>Sample run line:\u00a0</p>   <pre><code>comsol batch -mpibootstrap slurm -inputfile batchin.mph -outputfile batchout.mph -batchlog batchlog.out\n</code></pre>","title":"COMSOL"},{"location":"COMSOL/","text":"<p>To use COMSOL with SLURM you should use the slurm option '-mpibootstrap slurm' and omit using srun.</p> <p>Sample run line:\u00a0</p>   <pre><code>comsol batch -mpibootstrap slurm -inputfile batchin.mph -outputfile batchout.mph -batchlog batchlog.out\n</code></pre>","title":"COMSOL"},{"location":"CUDA_Example/","tags":["how-to guide"],"text":"<p>There are several CUDA versions installed in HPCC. Users can use the command:</p> <pre><code>[username@dev-amd20-v100 ~]$ ml spider CUDA\n</code></pre> <p>to see all installed versions. In order to compile a CUDA code, users might also need to load a GNU or Intel compiler (for C and Fortran). A toolchain which combines CUDA, C and Fortran compilers can be found by using the command:</p> <pre><code>[username@dev-amd20-v100 ~]$ ml spider gcccuda\n</code></pre> <p>for GNU and CUDA compilers and</p> <pre><code>[username@dev-amd20-v100 ~]$ ml spider iccifortcuda\n</code></pre> <p>for Intel and CUDA compilers. You can load one of the versions directly to do your CUDA code compilation.</p> <p>GPU Capabilities</p> <p>Please be aware that GPU K80 and K20 cards are not able to run normally with CUDA versions higher than 11.0.</p>","title":"CUDA Example"},{"location":"CUDA_Example/#simple-cuda-example","tags":["how-to guide"],"text":"<p>Users can get a simple CUDA code to do the compilation. After log into HPCC and ssh to a dev node with GPU cards, users can run the powertools command:</p> <pre><code>[username@dev-amd20-v100 ~]$ getexample helloCUDA\n</code></pre> <p>to copy the CUDA example directory \"<code>helloCUDA</code>\" in the current directory. After you cd to the directory, you should be able to see the CUDA code file: \"<code>Hello.cu</code>\":</p> <pre><code>[username@dev-amd20-v100 ~]$ cd helloCUDA\n[username@dev-amd20-v100 ~]$ ls\nHello.cu  Hello-CUDA.sb  README  run.sh\n[username@dev-amd20-v100 ~]$ cat Hello.cu\n#include &lt;stdio.h&gt;\n\n#define NUM_BLOCKS 16\n#define BLOCK_WIDTH 1\n\n__global__ void hello()\n{\n    printf(\"Hello world! I'm a thread in block %d\\n\", blockIdx.x);\n}\n\n\nint main(int argc,char **argv)\n{\n    // launch the kernel\n    hello&lt;&lt;&lt;NUM_BLOCKS, BLOCK_WIDTH&gt;&gt;&gt;();\n\n    // force the printf()s to flush\n    cudaDeviceSynchronize();\n\n    printf(\"That's all!\\n\");\n\n    return 0;\n}\n</code></pre> <p>The CUDA code defines 16 blocks with one GPU core in each block. Each core runs one thread to print out \"<code>Hello world!</code>\" and the block number of the core. Totally 16 threads are executed in parallel. We can simply compile the code by \"<code>nvcc</code>\" command after a version of gcccuda toolchain is loaded:</p> <pre><code>[username@dev-amd20-v100 ~]$ ml -* gcccuda/2019b\n[username@dev-amd20-v100 ~]$ nvcc Hello.cu -o Hello_CUDA\n[username@dev-amd20-v100 ~]$ ./Hello_CUDA\nHello world! I'm a thread in block 5\nHello world! I'm a thread in block 12\nHello world! I'm a thread in block 11\nHello world! I'm a thread in block 0\nHello world! I'm a thread in block 6\nHello world! I'm a thread in block 1\nHello world! I'm a thread in block 7\nHello world! I'm a thread in block 13\nHello world! I'm a thread in block 14\nHello world! I'm a thread in block 2\nHello world! I'm a thread in block 8\nHello world! I'm a thread in block 4\nHello world! I'm a thread in block 10\nHello world! I'm a thread in block 3\nHello world! I'm a thread in block 9\nHello world! I'm a thread in block 15\nThat's all!\n</code></pre> <p>Once it is compiled successfully, the execution of the binary \"<code>Hello_CUDA</code>\" on a GPU node should give the output as above.</p>","title":"Simple CUDA example"},{"location":"Change_Primary_Group/","text":"<p>Generally, HPCC users need to request\u00a0root privilege through the ticket system to change their primary group permanently. However, a user can also use <code>newgrp</code> command to change his primary group temporarily to any one of his groups. In this page, we will introduce</p>","title":"Change primary group"},{"location":"Change_Primary_Group/#primary-group-change-on-command-line","text":"<p>To change a user's primary group with a command line, simply run</p> <pre><code>[username@dev-intel18 CurrentPath]$ newgrp &lt;Group Name&gt;\n</code></pre> <p>where <code>&lt;Group Name&gt;</code> has to be one of the your group names. To find out\u00a0all your group names, please run <code>groups</code> command:</p> <pre><code>[username@dev-intel18 CurrentPath]$ groups\nchemistry VMD g09 BiCEP education-data ParaView\n</code></pre> <p>where the primary group is \"<code>chemistry</code>\", the first group name in the results. After the execution of <code>newgrp</code> command, a new shell session is created and\u00a0the current environment, including the current working directory remains the same.</p> <pre><code>[username@dev-intel18 CurrentPath]$ newgrp g09\n[username@dev-intel18 CurrentPath]$ groups\ng09 chemistry VMD BiCEP education-data ParaView\n</code></pre> <p>where the primary group is changed to g09. To leave the session, just run \"<code>exit</code>\" command. It will go back to the previous session with the original primary group (<code>chemistry</code>).</p> <p>If a user would like to start the shell session with a new primary group as though the user just logged in. The optional \"-\" flag can be used</p> <pre><code>[username@dev-intel18 CurrentPath]$ newgrp - g09\n[username@dev-intel18 ~]$ groups\ng09 chemistry VMD BiCEP education-data ParaView\n</code></pre> <p>where the user's environment is reinitialized and the shell session starts from his home directory.</p> <p>Some users might have the issue to lose the header in front of prompt after executing the command:</p> <pre><code>[username@dev-intel14-phi CurrentPath]$ newgrp g09\nbash-4.2$ groups\ng09 chemistry VMD BiCEP education-data ParaView\n</code></pre> <p>Please try <code>source</code> the default bashrc file to get it back.</p> <pre><code>bash-4.2$ source /etc/bashrc\n[changc81@dev-intel14-phi MyCurrentPath]$ \n</code></pre> <p>Please notice! If a job is submitted in the session of a new primary group, the environment with the new primary group will be used in the job running by default instead of the original one. For more information, please check the row of <code>--export</code> in the table of List of Job Specifications page.</p>","title":"Primary Group Change on Command Line"},{"location":"Change_Primary_Group/#primary-group-change-in-job-script","text":"<p>Many HPCC users have more than one research spaces. In a job running, the user might need his primary group set to be the group of the research space where the output files are. In order to do this, the job script can use <code>newgrp</code> from the beginning of the command lines. For example, an original job script may look like:</p> <pre><code>#!/bin/bash --login\n\n#SBATCH --time=00:10:00             # limit of wall clock time - how long the job will run (same as -t)\n#SBATCH --ntasks=5                  # number of tasks - how many tasks (nodes) that you require (same as -n)\n#SBATCH --mem-per-cpu=2G            # memory required per allocated CPU (or core) - amount of memory (in bytes)\n\nmodule purge\nmodule load GCC/6.4.0-2.28 OpenMPI  ### load necessary modules, e.g. \n\nsrun -n 5 &lt;executable&gt;              ### call your executable (similar to mpirun)\n\nscontrol show job $SLURM_JOB_ID     ### write job information to output file\n</code></pre> <p>If the job script is submitted in a research space, adding a few lines can make the primary group the same as the group of the research space:</p> <pre><code>#!/bin/bash --login\n\n#SBATCH --time=00:10:00             # limit of wall clock time - how long the job will run (same as -t)\n#SBATCH --ntasks=5                  # number of tasks - how many tasks (nodes) that you require (same as -n)\n#SBATCH --mem-per-cpu=2G            # memory required per allocated CPU (or core) - amount of memory (in bytes)\n\nGroupName=$(readlink -f $PWD|awk -F \"/\" '{ if ($2==\"mnt\" &amp;&amp; $3==\"research\" &amp;&amp; $4!=\"\") system(\"ls -ld /mnt/research/\"$4\"|cut -d \\\" \\\" -f 4\") }')\nnewgrp ${GroupName} &lt;&lt; EOS\n\nmodule purge\nmodule load GCC/6.4.0-2.28 OpenMPI  ### load necessary modules, e.g. \n\nsrun -n 5 &lt;executable&gt;              ### call your executable (similar to mpirun)\n\nscontrol show job $SLURM_JOB_ID     ### write job information to output file\nEOS\n</code></pre> <p>where the group name of the research space for job running is automatically determined in line 7. You can also directly set the variable \"<code>GroupName</code>\" to be the group name of your research space:</p> <pre><code>GroupName=&lt;group name of your research space&gt;\n</code></pre> <p>in line 7 if you are sure about jobs running in the research space. After the group name of the research space is given, the command lines between line 9 and 16 (inside the two \"<code>EOS</code>\") will be run under the environment of the new primary group (<code>${GroupName}</code>).</p>","title":"Primary Group Change in Job Script"},{"location":"Change_Primary_Group/#automatic-primary-group-change-after-login","text":"<p>If users need to change their primary group constantly, they can use the powertools script PrimaryGroup. Users can add a command line \"source /opt/software/powertools/doc/PrimaryGroup \" in the beginning of the file \\~/.bashrc:</p> <p>~/.bashrc</p> <pre><code>#!/bin/bash\n\nsource /opt/software/powertools/doc/PrimaryGroup &lt;Group Name&gt;          # Add this line to change primary group automatically\n\n# Source global definitions\nif [ -f /etc/bashrc ]; then\n        . /etc/bashrc\nfi\n\n...\n...\n</code></pre> <p>where <code>&lt;Group Name&gt;</code> is one of his group names. This also provides a flexibility that user's primary group can be changed to any of his groups at any time without requesting root privilege.</p> <p>Please notice! The\u00a0powertools script PrimaryGroup only works on dev nodes. It does not work on gateway or compute nodes. Users need to ssh to a dev node to see the effect. After this command line is added in the <code>~/.bashrc</code> file, the command <code>newgrp</code> with the optional \"-\" flag will not work. Users can still use\u00a0<code>newgrp</code> without \"<code>-</code>\" flag.</p>","title":"Automatic Primary Group Change after Login"},{"location":"Checkpoint_with_DMTCP/","text":"<p>Note</p> <p>To run DMTCP correctly, the limit of stack size could not be \"unlimited\". To set it to a number n, run \"ulimit -s n\". For example, \"ulimit -s 8192\" would set the stack size as 8192. User could also add it into .bashrc file.\u00a0 It should also be set in the job batch script, as seen below.</p>  <p>DMTCP (Distributed MultiThreaded Checkpointing) transparently checkpoints a single-host or distributed computation in user-space. It supports MPI (various implementations), OpenMP, MATLAB, Python, Perl, R, and many programming languages and shell scripting languages.\u00a0DMTCP works under Linux with no modifications to the application binaries. It can be used by users (no root privilege needed). DMTCP allows one to checkpoint to disk a distributed computation and later restart from a checkpoint, or even migrate the processes by moving the checkpoint files to another host prior to restarting.</p> <p>There is one DMTCP coordinator for each computation that you wish to checkpoint. A DMTCP coordinator process is started on one host. Application binaries are started under the dmtcp_launch command, causing them to connect to the coordinator upon startup. As threads are spawned, child processes are forked, remote processes are spawned via ssh, libraries are dynamically loaded, DMTCP transparently and automatically tracks them. To checkpoint, use dmtcp_coordinator command to start checkpointing. To restart from a checkpoint, use dmtcp_restart .</p> <p>By default, DMTCP uses gzip to compress the checkpoint images. This can be turned off. This will be faster, and if your memory is dominated by incompressible data, this can be helpful. gzip can add seconds for large checkpoint images. Typically, checkpoint and restart is less than one second without gzip.</p> <p>To run a program with checkpointing usually involves following 4 steps (may need more option settings for special cases) :</p> <ol> <li>Start dmtcp_coordinator\u00a0<ul> <li>$ dmtcp_coordinator --daemon --exit-on-last $@ 1&gt;/dev/null     2&gt;&amp;1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0#run coordinator as daemon in background</li> </ul> </li> <li>Launch program with dmtcp_launch<ul> <li>$ dmtcp_launch ./a.out\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0     \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0# launch ./a.out</li> </ul> </li> <li>To checkpoint, run dmtcp_command with checkpointing option. It will     generate a set of checkpointing image files (file type:\u00a0.dmtcp)\u00a0and     a shell script for restart.<ul> <li>$ dmtcp_command\u00a0--bcheckpoint\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0     \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # checkpointing</li> </ul> </li> <li>Restart: Creating a checkpoint causes the dmtcp_coordinator to write     a script, dmtcp_restart_script.sh, along with a checkpoint file     (file type: .dmtcp) for each client process. The simplest way to     restart a previously checkpointed computation is:<ul> <li>$ ./dmtcp_restart_script.sh\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0# restart     using script</li> <li>$ dmtcp_restart ckpt_*.dmtcp\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 #     Alternatively, if all processes were on the same processor, and     there were no .dmtcp files prior to this checkpoint:</li> </ul> </li> </ol> <p>Following is the sample script longjob.sb that using DMTCP for checkpointing a long job so that the job could be run as a sequence of short walltime jobs. To obtain the complete example, run \"module load powertools; getexample dmtcp_longjob\".</p> <pre><code>#!/bin/bash -login\n\n## resource requests for task:\n\n#SBATCH -J count-longjob                  # Job Name\n\n#SBATCH --time=00:06:00                   # Note that 6 min is not enough to complete the job. It enough for checkpointing and resubmit job\n\n#SBATCH -N 1 -c 1 --mem=20MB              # requested resource\n\n#SBATCH --constraint=lac                  # user could add other requests as usual.\n\n# set a limited stack size so DMTCP could work\nulimit -s 8192\n\n# current working directory shuld have source code dmtcp1.c\ncd ${SLURM_SUBMIT_DIR}\n\n# this script file name. This script may be resubmit multiple times until job completed\nexport SLURM_JOBSCRIPT=\"longjob.sb\"\n\n\n######################## start dmtcp_coordinator #######################\n\nfname=port.$SLURM_JOBID                                                                 # to store port number\n\ndmtcp_coordinator --daemon --exit-on-last -p 0 --port-file $fname $@ 1&gt;/dev/null 2&gt;&amp;1   # start coordinater\n\nh=`hostname`                                                                            # get coordinator's host name\n\np=`cat $fname`                                                                          # get coordinator's port number\n\nexport DMTCP_COORD_HOST=$h                                                  # save coordinators host info in an environment variable\n\nexport DMTCP_COORD_PORT=$p                                                  # save coordinators port info in an environment variable\n\n#rm $fname\n\n\n\n\n# uncommand following lines to print out some information if user wish\n\n#echo \"coordinator is on host $DMTCP_COORD_HOST \"\n\n#echo \"port number is $DMTCP_COORD_PORT \"\n\n#echo \" working directory: ${SLURM_SUBMIT_DIR} \"\n\n#echo \" job script is $SLURM_JOBSCRIPT \"\n\n\n\n\n####################### BODY of the JOB ######################\n\n# prepare work environment of the job\n\nmodule swap GNU/6.4.0-2.28 GCC/4.9.2\n\n\n# build the program if executable file does not exist\n\nif [ ! -f count.exe ] \n\nthen\n\n    cc count.c -o count.exe\n\nfi\n\n\n\n\n# run the program count.exe. \n\n# To run interactively: \n\n#    $ ./count.exe n num.odd 1&gt; num.even \n\n# it will count to number n and generate 2 files: \n\n# num.odd contains all the odd number;\n\n# num.even contains all the even number.\n\n\n\n# To run with DMTCP, use dmtcp commamds.\n\n# if first time launch, use \"dmtcp_launch\"\n\n# otherwise use \"dmtcp_restart\"\n\n\n\n\n# set checkpoint interval. This script would wait after dmtcp_launch\n\n# the job for the interval (in seconds), then do start the checkpoint. \n\nexport CKPT_WAIT_SEC=$(( 3 * 60 ))            # checkpointing when program runs for 3 min\n\n\n\n\n# Launch or restart the execution\n\nif [ ! -f ckpt_*.dmtcp ]                      # if no ckpt file exists, it is first time run, use dmtcp_launch\n\nthen\n\n  # first time run, use dmtcp_launch to start the job and run on background */\n\n  dmtcp_launch -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --rm --ckpt-open-files ./count.exe 800 num.odd 1&gt; num.even 10&gt;&amp;- 11&gt;&amp;- &amp;\n\n\n\n\n  #wait for an inverval of checkpoint seconds to start checkpointing\n  sleep $CKPT_WAIT_SEC\n\n\n\n  # start checkpointing\n  dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --ckpt-open-files --bcheckpoint\n\n\n  # kill the running job after checkpointing\n  dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --quit\n\n\n  # resubmit the job\n  sbatch $SLURM_JOBSCRIPT\n\n\nelse            # it is a restart run\n\n  # restart job with checkpoint files ckpt_*.dmtcp and run in background\n  dmtcp_restart -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT ckpt_*.dmtcp 1&gt; num.even &amp;\n\n\n  # wait for a checkpoint interval to start checkpointing\n  sleep $CKPT_WAIT_SEC\n\n\n  # if program is still running, do the checkpoint and resubmit\n\n  if dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT -s 1&gt;/dev/null 2&gt;&amp;1\n  then   \n    # clean up old ckpt files before start checkpointing\n    rm -r ckpt_*.dmtcp\n\n    # checkpointing the job\n    dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --ckpt-open-files -bc\n\n    # kill the running program and quit\n    dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --quit\n\n    # resubmit this script to slurm\n    sbatch $SLURM_JOBSCRIPT\n\n  else\n\n    echo \"job finished\"\n\n  fi\n\nfi\n\n# show the job status info\nscontrol show job $SLURM_JOB_ID\n</code></pre>","title":"Checkpoint with DMTCP"},{"location":"Cluster_amd20_with_AMD_CPUs/","text":"<p>In 2020, the HPCC purchased a new cluster amd20 powered by AMD EPYC processors. Each amd20 node has total 128 cores and at least 0.5 TB RAM. Each core has a base clock speed 2.6 GHz, up to 3.3 GHz.\u00a0</p> <p>Please check the following sections for how to use the test nodes and the node performance from our test results for user reference.</p>","title":"Cluster amd20 with AMD CPUs"},{"location":"Cluster_amd20_with_AMD_CPUs/#running-jobs-on-amd20-test-nodes","text":"","title":"Running Jobs on amd20 Test Nodes"},{"location":"Cluster_amd20_with_AMD_CPUs/#basic-mathematical-library-tests-on-amd-epyc-processors","text":"","title":"Basic Mathematical Library Tests on AMD EPYC Processors"},{"location":"Cluster_amd20_with_AMD_CPUs/#amd-optimizing-cpu-libraries-and-compilers","text":"","title":"AMD Optimizing CPU Libraries and Compilers"},{"location":"Cluster_amd20_with_AMD_CPUs/#basic-architecture-information","text":"<p>The AMD CPU contains 2 sockets with CPU packages with 4 NUMA nodes in each socket, and 16 cores in each NUMA node. Each NUMA core contains two \"Core Complex Dies\", which have two four-core \"Core-Complex\" modules. Each four-core Core-Complex shares a 16 MB L3 cache.</p> <p></p> <p>A 7002 series processor (via AMD)</p> <p></p> <p>A Core-Complex (via AMD) logical diagram.</p> <p></p> <p>A 1 TB amd20 node.</p> <p>Generally, cores within the same L3 cache have the lowest latency, followed by in-NUMA cores, other cores on the same socket, and cores on the other socket are slower. The SLURM job scheduler on the HPCC will try to keep within the same NUMA node by default. One option to try during testing is to use OpenMP within the L3 node, and MPI for everything else. When using newer versions of OpenMPI, you can use the following argument with OMP_NUM_THREADS=4 to distribute one 4-thread rank per L3:</p> <pre><code>mpirun -map-by ppr:1:l3cache:pe=4 --bind-to core\n</code></pre> <p>In testing, the Intel Compiler and MKL toolchain works well if you \u201cexport MKL_DEBUG_CPU_TYPE=5\u201d and compile for AVX2 instead of AVX512.</p> <p>If you\u2019re doing single-node scaling work, be aware of memory bandwidth; on these nodes, HPL scales from 1-96 cores linearly but only 3.5 -&gt; 4 TF from 96-&gt;128 when going from 3 cores per L3 to 4.</p> <p>Each node has a 100 gigabit HDR100 connection. There are 52-56 nodes per switch:</p> <p></p>","title":"Basic architecture information"},{"location":"Cluster_amd20_with_AMD_CPUs/#amd20-network-topology","text":"<p>You can figure out by using scontrol show node and looking at</p> <pre><code>$ sinfo -n amr-[000-209] -o %n,%b\nHOSTNAMES,ACTIVE_FEATURES\namr-188,amr,gbe,amd20,ib,rack-S21\namr-186,amr,gbe,amd20,ib,rack-S21\namr-000,amr,gbe,amd20,ib,rack-Z21\namr-001,amr,gbe,amd20,ib,rack-Z21\namr-002,amr,gbe,amd20,ib,rack-Z21\n</code></pre> <p>Z21/Y21 are on the same switch, etc.</p> <p>Resources:</p> <p>High Performance Computing: Tuning Guide for AMD EPYC\u2122 7002 Series Processors</p> <p>Compiler Options Quick Ref Guide for AMD EPYC 7xx2 Series Processors.pdf</p>","title":"AMD20 network topology."},{"location":"Collaborative_Research/","text":"","title":"Collaborative research"},{"location":"Collaborative_Research/#research-space","text":"<p>To support collaborative research on campus, a PI may request a shared research space where files and softwares can be shared. </p>","title":"Research space"},{"location":"Collaborative_Research/#software-installation-request","text":"<p>We suggest the following two steps when you are in need of a particular software package.</p> <ol> <li>Go to <code>/opt/software/</code> on a dev-node. This is where all software packages are installed. Check if your desired software and specific versions are there. Please note there could be some name discrepancy caused by switch between lower and upper case letters. Take BBMap for example, all the versions available can be viewed by \"<code>ls -l /opt/software/BBMap</code>\". Send us a ticket if the software is totally absent in <code>/opt/software</code> or your desired version is missing. If the software and the version are both present in <code>/opt/software</code>, go to step 2.</li> <li>Follow the instruction to try loading software/version. If you can't find the software using <code>module spider</code>, send us a ticket. If you can find the software but not the desired version, send us a ticket too.</li> </ol>","title":"Software installation request"},{"location":"Collaborative_Research/#using-our-academic-research-consulting-services","text":"<p>ICER Academic Research Consulting service (ARCS) is to meet the computationally complex needs of the MSU research community. Research groups in need of software and computational workflow development services, dedicated training programs, as well as those that require additional scientific expertise in areas such as bioinformatics, time-series analysis, and computational modeling may request long-term collaborations with ARCS research consultants. ICER ARCS consultants are PhD-level researchers with domain-specific computational expertise and experience at R1 institutions. ARCS collaborations require funding and may be funded internally through startup grants or unit-level support or externally by grant support or industrial partnerships. More information can be found here.</p>","title":"Using our Academic Research Consulting Services"},{"location":"Common_Module_Commands/","text":"<p>A module manages environment variables needed to load a particular piece of software. The login process loads a few modules by default for HPCC users. The modules include the GNU compilers, the OpenMPI communication library, MATLAB and R.</p> <p>To see a list of modules that are currently loaded:</p>   <pre><code>module list\n</code></pre>   <p>To search what modules are available to be loaded:</p>   <pre><code>module avail\n</code></pre>   <p>To see the entire list of all modules:</p>   <pre><code>module spider\n</code></pre>   <p>To find out if a particular software is in the list of all modules:</p>   <pre><code>module spider &lt;software_name&gt;\n</code></pre>   <p>To check how to load a particular version of a software</p>   <pre><code>module spider &lt;software_name&gt;/&lt;version&gt;\n</code></pre>   <p>To load a module:</p>   <pre><code>module load &lt;module_name&gt;\n</code></pre>   <p>To unload a module:</p>   <pre><code>module unload &lt;module_name&gt;\n</code></pre>   <p>To swap a module:</p>   <pre><code>module swap GNU Intel\n</code></pre>   <p>To see what environment variables would be set/changed if you load a specific module:</p>   <pre><code>module show &lt;module_name&gt;\n</code></pre>   <p>To load a modulefile in a non-standard directory:</p>   <pre><code>module use &lt;path_to_module&gt;\nmodule load &lt;modulename&gt;\n</code></pre>   <p>For more information about Lmod module system, please refer to the documentation web site.</p>","title":"Common Module Commands"},{"location":"Compilers_and_Libraries/","text":"<ul> <li>Compilers</li> <li>MPI Libraries</li> <li>Math Libraries</li> <li>Build Commands</li> </ul>","title":"Compilers and Libraries"},{"location":"Compilers_and_Libraries/#compilers","text":"<p>HPCC has various compilers installed, which can be used to create OpenMP, pthreads, MPI, hybrid, and serial programs. To view the version of compilers available, please use the module avail command. We presently offer GNU (default), Intel, PGI, CUDA, g95.</p> <pre><code>[ongbw@dev-amd09 ~]$ module avail GNU\n\n------------------------------------------------- /opt/software/modulefiles/compilers --------------------------------------------------\n  GNU/4.4.5 (default)\n\n[ongbw@dev-amd09 ~]$ module avail Intel\n\n------------------------------------------------- /opt/software/modulefiles/compilers --------------------------------------------------\n  Intel/12.0.0.084 (default)    Intel/12.1.0.233    Intel/12.1.2.273\n\n[ongbw@dev-amd09 ~]$ module avail PGI\n\n------------------------------------------------- /opt/software/modulefiles/compilers --------------------------------------------------\n  PGI/11.10 (default)\n\n[ongbw@dev-amd09 ~]$ module avail g95\n\n------------------------------------------------- /opt/software/modulefiles/compilers --------------------------------------------------\n  g95/0.93\n\n[ongbw@dev-amd09 ~]$ module avail CUDA\n\n------------------------------------------------- /opt/software/modulefiles/compilers --------------------------------------------------\n  CUDA/4.0 (default)\n</code></pre>","title":"Compilers"},{"location":"Compilers_and_Libraries/#mpi-libraries","text":"<p>HPCC also has various MPI libraries installed. These include: OpenMPI (default), MVAPICH, IMPI, MPICH, MPICH2.</p> <pre><code>[ongbw@dev-amd09 ~]$ module avail OpenMPI\n\n------------------------------------------- /opt/software/modulefiles/GNU/4.4.5/modulefiles --------------------------------------------\n  OpenMPI/1.4.3 (default)    OpenMPI/1.4.4    OpenMPI/1.5.3\n\n[ongbw@dev-amd09 ~]$ module avail MVAPICH\n\n------------------------------------------- /opt/software/modulefiles/GNU/4.4.5/modulefiles --------------------------------------------\n  MVAPICH/1.1 (default)    MVAPICH2/1.7    MVAPICH2/1.7-r5078 (default)    MVAPICH2/1.8-a1p1\n\n[ongbw@dev-amd09 ~]$ module avail IMPI\n\n------------------------------------------- /opt/software/modulefiles/GNU/4.4.5/modulefiles --------------------------------------------\n  IMPI/4.0.1.007 (default)\n\n[ongbw@dev-amd09 ~]$ module avail MPICH\n\n------------------------------------------- /opt/software/modulefiles/GNU/4.4.5/modulefiles --------------------------------------------\n  MPICH2/1.4.1p1 (default)\n\n\n[ongbw@dev-amd09 ~]$ module avail MPICH2\n\n------------------------------------------- /opt/software/modulefiles/GNU/4.4.5/modulefiles --------------------------------------------\n  MPICH2/1.4.1p1 (default)\n</code></pre>","title":"MPI Libraries"},{"location":"Compilers_and_Libraries/#math-libraries","text":"<p>HPCC has many common libraries available, including MKL, FFTW, ACML, BLAS, ScaLaPACK, LaPACK, PETSc. The module names and available link flags are summarized in the following table. Additionally, some code samples are available as indicated by typing:</p> <pre><code>module load powertools\ngetexample &lt;example name&gt;\n</code></pre>    Library Name Module Name Link Flags Powertool Examples     ACML ACML -lamcl -lacml_mp     BLAS BLAS -lblas -lcblas -lfblas -lxblas     FFTW FFTW -lfftw3 -lfftw3_omp -lfftw3_mpi -lfftw3_threads fftw   LaPACK LAPACK -llapack -ltmglib     MKL MKL -lmkl_blacs -lfftw3xc_intel -lfftw2xc_intel -lmkl_blas95_lp64 -lmkl_core -lmkl_lapack95_lp64 -lmkl_scalapack_lp64 -lmkl_solver_lp64 MKL_Example   PETSc PETSc -lfblas -lflapack -lpetsc libtriangle.a     ScaLaPACK ScaLAPACK -lscalapack","title":"Math Libraries"},{"location":"Compilers_and_Libraries/#build-commands","text":"<p>The commands and flags for the various compilers are summarized in the table below:</p>    Compiler Module Name Serial OpenMP Pthreads MPI Hybrid     Intel Fortran Intel ifort myserial.f ifort -fopenmp myopenmp.f ifort -pthread mypthread.f mpif90 mympi.f mpif90 -fopenmp myhybrid.f   Intel C Intel icc myserial.c icc -fopenmp myopenmp.c icc -pthread mypthread.c mpicc mympi.c mpicc -fopenmp myhybrid.c   Intel C++ Intel icpc myserial.cpp icpc -fopenmp myopenmp.cpp icpc -pthread mypthread.cpp mpic++ mympi.cpp mpic++ -fopenmp myhybrid.cpp   GNU Fortran GNU gfortran myserial.f90 gfortran -openmp myopenmp.f90 gfortran -pthread mypthread.f90 mpif90 mympi.f mpif90 -openmp myhybrid.f   GNU C GNU gcc myserial.c gcc -fopenmp myopenmp.c gcc -pthread mypthread.c mpicc mympi.c mpicc -openmp myhybrid.c   GNU C++ GNU g++ myserial.cpp g++ -fopenmp myopenmp.cpp g++ -pthread mypthread.cpp mpic++ mympi.cpp mpic++ -openmp myhybrid.cpp   PGI Fortran PGI pgif77 myserial.f pgf77 -mp myopenmp.f pg77 -pthread mypthread.f mpif77 mympi.f mpif77 -mp myhybrid.f       pgf90 myserial.f90 pgf90 -mp myopenmp.f90 pgf90 -pthread mypthread.f90 mpif90 mympi.f mpif90 -mp myhybrid.f   PGI C PGI pgcc myserial.c pgcc -mp myopenmp.c pgcc -pthread mypthread.c mpicc mympi.c mpicc -mp myhybrid.c   PGI C++ PGI pgCC myserial.cpp pgCC -mp myopenmp.cpp pgCC -pthreads mypthread.cpp mpic++ mympi.cpp mpic++ -mp myhybrid.cpp   g95 g95 g95 myserial.f90 not supported g95 -pthread mypthread.f90 mpif90 mympi.f not available","title":"Build Commands"},{"location":"Connect_over_SSH_with_VS_Code/","text":"<p>VS Code allows you to work on remote servers from your local machine. </p>","title":"Connect over SSH with VS Code"},{"location":"Connect_over_SSH_with_VS_Code/#prerequistes","text":"<ul> <li>Install VS Code,  click here</li> <li>~/.ssh/config</li> </ul>","title":"Prerequistes"},{"location":"Connect_over_SSH_with_VS_Code/#connecting-to-hppc-with-vs-code","text":"<p>Because we need to hop twice (your local machine -&gt; gateway -&gt; dev node), we need an ssh config file. The following is an example config which defines intel18 and k80 dev node. With this file, you can connect dev node from your local machine with 'ssh intel18' or 'ssh k80'.</p> <pre><code>Host gateway\n  HostName gateway.hpcc.msu.edu\n  User your_net_id\n\nHost intel18\n  HostName dev-intel18\n  User your_net_id\n  ProxyJump gateway\n\nHost k80\n  HostName dev-intel16-k80\n  User your_net_id\n  ProxyJump gateway\n</code></pre> <p>Next, we need an extension program for VS Code, which can be obtained here</p> <p></p> <p>When the extension is installed, press F1, and select 'Remote-SSH: Connect to Host...' </p> <p>Then select intel18 or k80. </p> <p>A new VS Code window will pop up. When you are successfully connected, bottom left of the VS Code will show server information such as 'SSH: intel18'. </p>","title":"Connecting to HPPC with VS Code"},{"location":"Connect_over_SSH_with_VS_Code/#working-on-hpcc-with-vs-code","text":"<p>Now that you are connected, you can run commands and codes from VS Code. When you are connected to the HPC, all files created through VS Code will be saved to the HPC, not on your local machine.</p> <p>To test the ability to run remote code, let's create a Python file 'hello.py' with VS Code. Click 'New file' and create a file with the following contest to the file: </p> <pre><code>print('hello!')\n</code></pre> <p>Now, open a terminal from 'Terminal' menu from VS Code (or by pressing the key sequence CTRL+Shift+`  and run the code.</p> <pre><code>$ python hello.py\n</code></pre> <p>Once you have connected the server, click the Remote Explorer to see the list of servers. Click the icon to connect to host in a new window. Right click the name to see the option.  </p> <p></p>","title":"Working on HPCC with VS Code"},{"location":"Connect_over_SSH_with_atom/","text":"<p>Atom does not support ProxyJump or port forwarding as VS Code. However, you can use rsync gateway, because Atom downloads the file from the server and edits on a local machine, and uploads it. For remote file editing, you need to install \"remote-ftp\" package.</p> <p>File \u2192 Setting</p> <p></p> <p>Install \u2192 searching \"Remote-FTP\", and Install</p> <p></p> <p>Configuration: Package \u2192 Remote FTP \u2192 Create SFTP config file</p> <p></p> <p>Edit config files (You need to change, host, user, pass and maybe remote. host: rsync.hpcc.msu.edu, user, pass, and remote: your net it and password, and your home dir path)</p> <pre><code>{\n  \"protocol\": \"sftp\",\n  \"host\": \"rsync.hpcc.msu.edu\",   //\n  \"port\": 22,\n  \"user\": \"your_net_id\",          // your net_id is here\n  \"pass\": \"your_pass_word\",       // your password is here\n  \"promptForPass\": false,\n  \"remote\": \"/\",                  // default is root (/). You would like to change it to your home dir path such as /mnt/home/your_net_id or research path\n  \"local\": \"\",\n  \"agent\": \"\",\n  \"privatekey\": \"\",\n  \"passphrase\": \"\",\n  \"hosthash\": \"\",\n  \"ignorehost\": true,\n  \"connTimeout\": 10000,\n  \"keepalive\": 10000,\n  \"keyboardInteractive\": false,\n  \"keyboardInteractiveForPass\": false,\n  \"remoteCommand\": \"\",\n  \"remoteShell\": \"\",\n  \"watch\": [],\n  \"watchTimeout\": 500\n}\n</code></pre> <p>Menu activation: Package \u2192 Remote FTP \u2192 Toggle</p> <p></p> <p>Click remote, and connect </p> <p>Connected. </p>","title":"Connet over SSH with Atom"},{"location":"Connect_over_SSH_with_editors/","text":"<ul> <li>Connect over SSH with VS Code</li> <li>Connect over SSH with Atom</li> </ul>","title":"Connect over SSH with editors"},{"location":"Connect_to_HPCC_System/","tags":["tutorial"],"text":"","title":"Connect to the HPCC"},{"location":"Connect_to_HPCC_System/#connect-to-the-gateway","tags":["tutorial"],"text":"<p>Open the SSH client in your local computer and run <code>ssh -XY &lt;username&gt;@hpcc.msu.edu</code>, where <code>&lt;username&gt;</code> should be your HPCC account name. SSH will prompt  for your password. Please type the password of your MSU NetID (the password will be invisible). Upon a successful login, you will see a welcome message and current usage load for each development/dev-node. </p> <p>Our gateway is for entrance to the HPCC system only. Connect further to a dev-node for your program running and other tasks. See the next section.</p> <p>NOTE: If you have difficulty in X11 forwarding (e.g. receiving an error message \"Warning: untrusted X11 forwarding setup failed: xauth key data not generated\"), please check your local machine's \"config\" file (<code>$HOME/.ssh/config</code>) setting. It should look like</p> <pre><code>Host *\nForwardAgent yes\nForwardX11 yes\nForwardX11Trusted yes\nXAuthLocation /opt/X11/bin/xauth\n</code></pre>","title":"Connect to the gateway"},{"location":"Connect_to_HPCC_System/#ssh-to-a-dev-node","tags":["tutorial"],"text":"<p>To access a dev-node from gateway, pick one and type \"<code>ssh -X node-name</code>\" (if you don't need to launch a GUI program, omit  <code>-X</code>).   See here for dev-node usage and policy.</p>","title":"SSH to a dev-node"},{"location":"Connect_to_HPCC_System/#test-x-windows","tags":["tutorial"],"text":"<p>If your local computer has  a X-Windows client installed (such as Xquatz, or mobaxterm ...) and you log into the nodes with the  <code>-X</code> option, you can test if GUI will work by running <code>xeyes</code> on the command line. If everything is set correctly, you should see a pop-up window containing a pair of eyes.</p>","title":"Test X-Windows"},{"location":"Containers_Docker_and_Singularity_/","text":"<p>Docker</p> <p>Singularity: I. Introduction</p> <p>Singularity: II. Running a container on HPC</p> <p>Singularity: III. Advanced skills</p>","title":"Containers (Docker and Singularity)"},{"location":"Display_Compute_Nodes_and_Job_Partitions_by_sinfo_command/","text":"","title":"Display Compute Nodes and Job Partitions by sinfo command"},{"location":"Display_Compute_Nodes_and_Job_Partitions_by_sinfo_command/#information-of-compute-nodes","text":"<p>If you would like to run a job with a lot of resources, it is a good idea to check available resources, such as which nodes are available as well as how many cores and how much memory is availabe on those nodes,  so the job will not wait for too much time. Users can use SLURM command sinfo to get a list of nodes controlled by the job scheduler. Such as, running the command sinfo -N -r -l, where the specifications -N for showing nodes, -r for showing nodes only responsive to SLURM and -l for long description are used.</p> <p>However, for each node, sinfo displays all possible partitions and causes repetitive information. Here, the powertools command node_status can be used to display much better results:</p> <pre><code>$ node_status                       # powertools command\n\nWed Apr 22 11:14:40 EDT 2020\n\nNodeName       Account         State     CPU(Load:Aloc Idl:Tot)    Mem(Aval:Tot)Mb   GPU(I:T)   Reason\n----------------------------------------------------------------------------------------------------------\ncsm-001        general       ALLOCATED      13.61: 20    0: 20       45186: 246640      N/A\ncsm-002       albrecht         MIXED        10.14: 15    5: 20        1072: 246640      N/A\ncsm-003         colej        ALLOCATED       7.45: 20    0: 20       50032: 246640      N/A\n......\ncsn-005        general         MIXED         9.92: 12    8: 20       16160: 118012    k20(0:2)\n......\ncs*      =&gt;   33.3%(buyin)   91.4%(162)     43.6%: 59.5%( 3240)      69.9%(17.0Tb)    97%( 78)   Usage%(Total)\n......\n......\nlac-078        general         MIXED        11.38:  8   20: 28       69884: 118012      N/A\nlac-079          ptg         ALLOCATED      22.37: 28    0: 28       15612: 118012      N/A\nlac-080       merzjrke         MIXED         2.48: 16   12: 28       50032: 246640    k80(0:8)\n......\n......\nvim-002          ccg           MIXED        66.14: 63   81:144     5427008:6145856      N/A\n\nintel14  =&gt;   34.5%(buyin)   91.7%(168)     47.8%: 62.7%( 3576)      60.1%(31.1Tb)    97%( 78)   Usage%(Total)\nintel16  =&gt;   69.0%(buyin)   98.8%(429)     55.2%: 65.1%(12200)      76.6%(79.9Tb)    70%(384)   Usage%(Total)\nintel18  =&gt;   63.6%(buyin)   99.4%(176)     45.8%: 55.8%( 7040)      77.1%(31.3Tb)    55%( 64)   Usage%(Total)\n\nSummary  =&gt;   60.3%(buyin)   97.4%(773)     51.2%: 61.9%(22816)      73.1%( 142Tb)    72%(526)   Usage%(Total\n</code></pre> <p>The result of node_status is a good reference to find out how many nodes available for your  jobs as it displays important information including node names, buyin accounts, node states,  CPU cores, memory, GPU, and the reason the node is unavailable.</p> <p>If you need more complete details of a particular node, you can use scontrol show node -a  command: <pre><code>$ scontrol show node -a skl-166\nNodeName=skl-166 Arch=x86_64 CoresPerSocket=20\n   CPUAlloc=0 CPUTot=40 CPULoad=0.01\n   AvailableFeatures=skl,gbe,intel18,ib,edr18\n   ActiveFeatures=skl,gbe,intel18,ib,edr18\n   Gres=(null)\n   NodeAddr=skl-166 NodeHostName=skl-166 Version=18.08\n   OS=Linux 3.10.0-693.21.1.el7.x86_64 #1 SMP Wed Mar 7 19:03:37 UTC 2018\n   RealMemory=376162 AllocMem=0 FreeMem=382562 Sockets=2 Boards=1\n   State=DOWN ThreadsPerCore=1 TmpDisk=174080 Weight=103 Owner=N/A MCS_label=N/A\n   Partitions=general-short,general-short-18,general-long,general-long-18,qian-18,nvl-benchmark-18,piermaro-18,vmante-18,liulab-18,devolab-18,tsangm-18,plzbuyin-18,chenlab-18,shadeash-colej-18,allenmc-18,cmse-18,seiswei-18,niederhu-18,daylab-18,junlin-18,mitchmcg-18,pollyhsu-18,davidroy-18,yueqibuyin-18,eisenlohr-18\n   BootTime=2019-02-11T15:07:38 SlurmdStartTime=2019-02-11T15:08:44\n   CfgTRES=cpu=40,mem=376162M,billing=57176\n   AllocTRES=\n   CapWatts=n/a\n   CurrentWatts=0 LowestJoules=0 ConsumedJoules=0\n   ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s\n   Reason=Currently being imaged [fordste5@2019-02-11T09:49:30]\n</code></pre>","title":"Information of Compute Nodes"},{"location":"Display_Compute_Nodes_and_Job_Partitions_by_sinfo_command/#slurm-partitions-for-jobs","text":"<p>One of the important details about a node is what kind of jobs can run on it. For example, if a node is a buy-in node, only jobs with walltime equal to or less than 4 hours can run for a non-buyin users. We can check the summary of all partitions using sinfo with  the -s specification:</p> <pre><code>$ sinfo -s\nPARTITION           AVAIL  TIMELIMIT   NODES(A/I/O/T)  NODELIST\ngeneral-short          up    4:00:00    729/26/16/771  csm-[001-005,007-010,017-022],csn-[001-039],csp-[006-007,016-020,025-026],css-[001-003,007-012,014,016-020,023,032-036,038-045,047-050,052-067,071-072,074-076,079-085,087-095,097-103,106-109,111-127],lac-[000-225,228-247,250-261,276-369,372,374-445],nvl-[000-007],qml-[000-005],skl-[000-167],vim-[000-002]\ngeneral-long           up 7-00:00:00      269/0/8/277  csm-001,csn-020,csp-[006-007,016-018,020,025],css-[008-012,014,016-019,023,032,034-036,038-045,047-050,052-066,071,075-076,079-080,083,087-089,092-095,097-099,107,118,121,124,126],lac-[038-044,078,123,209,217,225,228,230-235,246-247,276-284,300-301,336-339,353-360,363-364,372,374-399,401-420,422-445],skl-[023,026-112]\ngeneral-long-bigmem    up 7-00:00:00        17/0/0/17  lac-[252-253,306],qml-[000,005],skl-[143-147,162-167],vim-001\ngeneral-long-gpu       up 7-00:00:00       46/12/0/58  csn-[001-019,021-036],lac-[030,087,137,143,192-199,287-290,292-293,342,348],nvl-[005-007]\n</code></pre> <p>where the list of job partitions and their setup for walltime limit and nodes are shown. More detailed information for each job partition can also be found by -p specification:</p> <pre><code>$ sinfo -p general-long -r -l\nMon Jul 13 12:22:16 2020\nPARTITION    AVAIL  TIMELIMIT   JOB_SIZE ROOT OVERSUBS     GROUPS  NODES       STATE NODELIST\ngeneral-long    up 7-00:00:00 1-infinite   no       NO        all      2    draining lac-[231,247]\ngeneral-long    up 7-00:00:00 1-infinite   no       NO        all      1     drained css-053\ngeneral-long    up 7-00:00:00 1-infinite   no       NO        all    217       mixed csm-001,csp-[006,017-018,020,025],css-[010,018-019,023,032,034-035,038,044,047-049,052,055-056,061-066,075,088-089,098-099,107,118,126],lac-[038-044,078,123,209,217,225,228,230,232,234-235,276-280,282-284,300-301,336-337,339,353-360,363,372,374-382,384-399,401-420,423,427-445],skl-[023,026,028-029,031,033-034,036-042,044-046,048,050-067,069-079,081-094,096-106,108-112]\ngeneral-long    up 7-00:00:00 1-infinite   no       NO        all     50   allocated csn-020,csp-016,css-[008-009,011,016-017,036,039-043,045,050,054,057-060,083,087,092-095,097,121,124],lac-[233,246,281,338,364,383,422,424-426],skl-[027,030,032,035,043,047,049,068,080,095,107]\n</code></pre> <p>Users can also show nodes only allowed for specific job partitions by using -N and -p:</p> <pre><code>$ sinfo -N -l -r -p general-short,general-long\nMon Jul 13 12:25:58 2020\nNODELIST   NODES     PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON\ncsm-001        1 general-short       mixed   20   2:10:1 246640   174080    101 gbe,ib,i none\ncsm-001        1  general-long       mixed   20   2:10:1 246640   174080    101 gbe,ib,i none\ncsm-002        1 general-short       mixed   20   2:10:1 246640   174080    101 gbe,ib,i none\ncsm-003        1 general-short       mixed   20   2:10:1 246640   174080    101 gbe,ib,i none\ncsm-004        1 general-short       mixed   20   2:10:1 246640   174080    101 gbe,ib,i none\ncsm-005        1 general-short       mixed   20   2:10:1 246640   174080    101 gbe,ib,i none\n...\n...\nskl-166        1 general-short       mixed   40   2:20:1 376162   174080    103 skl,gbe, none\nskl-167        1 general-short       mixed   40   2:20:1 376162   174080    103 skl,gbe, none\nvim-000        1 general-short       mixed   64   4:16:1 306780   174080    102 gbe,inte none\nvim-001        1 general-short       mixed   64   4:16:1 306780   174080    102 gbe,inte none\nvim-002        1 general-short   allocated  144   8:18:1 614585   174080    102 gbe,inte none\n</code></pre> <p>For a complete instruction of sinfo, please refer to the SLURM web page.</p>","title":"SLURM Partitions for Jobs"},{"location":"Docker/","tags":["how-to guide"],"text":"<p>What is Docker? Docker is a tool to make it easier to create, deploy and run applications by using containers. Containers allow developers to package up an application with all of the dependencies such as libraries and tools, and deploy it as one package. The application will run on most OS machines (Mac/Windows/Linux) regardless of any customized settings. This page covers how you can running on the development environments using Docker containers.</p> <ul> <li>Docker installation</li> <li>Testing Docker installation</li> <li>Running Docker containers from prebuilt     images</li> <li>Build Docker images which contain your own     code<ul> <li>Docker images</li> <li>Building your first Docker     image<ul> <li>Creating working     directory</li> <li>Python script</li> <li>Dockerfile</li> <li>Build the image</li> <li>Run your image</li> </ul> </li> </ul> </li> <li>Docker and Jupyter notebook</li> </ul>","title":"Docker"},{"location":"Docker/#docker-installation","tags":["how-to guide"],"text":"<p>Docker supports various OS (Mac/Windows/Linux) and installation of Docker and running on your OS is very easy.</p> <p>For installation, click here:\u00a0Mac/Windows/Linux.</p>  <p>Note</p> <ul> <li>If you are a Windows user, please make sure you     have\u00a0shared your drive.</li> <li>You have to log into Docker to use images from Docker repository.</li> </ul>","title":"Docker installation"},{"location":"Docker/#testing-docker-installation","tags":["how-to guide"],"text":"<p>When you installed Docker, test your Docker installation by running the following command:</p> <pre><code>$ docker --version\nDocker version 19.03.8, build afacb8b\n</code></pre> <p>When you run the <code>docker</code> command without <code>--version</code>, you will see the options available with docker. Alternatively, you can test your installation by running the following (you have to log into Docker to use this test):</p> <pre><code>$ docker run hello-world\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\n0e03bdcc26d7: Pull complete\nDigest: sha256:6a65f928fb91fcfbc963f7aa6d57c8eeb426ad9a20c7ee045538ef34847f44f1\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n1. The Docker client contacted the Docker daemon.\n2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n(amd64)\n3. The Docker daemon created a new container from that image which runs the\nexecutable that produces the output you are currently reading.\n4. The Docker daemon streamed that output to the Docker client, which sent it\nto your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n$ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\nhttps://hub.docker.com/\n\nFor more examples and ideas, visit:\nhttps://docs.docker.com/get-started/\n</code></pre>","title":"Testing Docker installation"},{"location":"Docker/#running-docker-containers-from-prebuilt-images","tags":["how-to guide"],"text":"<p>Now, you have setup everything, and it is time to use Docker seriously. You will run a container from the Alpine Linux image on your system and will learn the\u00a0run docker\u00a0command. However, you should first know what containers and images are, and the difference between containers and images.</p> <p>Images: The file system and configuration of applications which are created and distributed by developers. Of course, you can create and distribute images.</p> <p>Containers: Running instances of Docker images. You can have many containers for the same image.\u00a0</p> <p>Now you know what containers and images are, and let's run the <code>docker run alpine ls -l</code> command in your terminal.</p> <pre><code>$ docker run alpine ls -l\nUnable to find image 'alpine:latest' locally\nlatest: Pulling from library/alpine\ncbdbe7a5bc2a: Pull complete\nDigest: sha256:9a839e63dad54c3a6d1834e29692c8492d93f90c59c978c1ed79109ea4fb9a54\nStatus: Downloaded newer image for alpine:latest\ntotal 56\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 bin\ndrwxr-xr-x    5 root     root           340 May 26 17:11 dev\ndrwxr-xr-x    1 root     root          4096 May 26 17:11 etc\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 home\ndrwxr-xr-x    5 root     root          4096 Apr 23 06:25 lib\ndrwxr-xr-x    5 root     root          4096 Apr 23 06:25 media\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 mnt\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 opt\ndr-xr-xr-x  187 root     root             0 May 26 17:11 proc\ndrwx------    2 root     root          4096 Apr 23 06:25 root\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 run\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 sbin\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 srv\ndr-xr-xr-x   12 root     root             0 May 26 17:11 sys\ndrwxrwxrwt    2 root     root          4096 Apr 23 06:25 tmp\ndrwxr-xr-x    7 root     root          4096 Apr 23 06:25 usr\ndrwxr-xr-x   12 root     root          4096 Apr 23 06:25 var\n(base) Hyperion:~ yongjunchoi$\n</code></pre> <p>When you run the <code>docker run alpine ls -l</code> command, it searches for the <code>alpine:latest</code> image from your system first. If your system has it (i.e. if you downloaded it previously), Docker uses that image. If your system does not have that image, then Docker\u00a0fetches the <code>alpine:latest</code> image from the Docker repository first, saves it in\u00a0your system, then runs a container from the saved image.\u00a0</p> <p><code>docker run alpine</code> starts a container, and then <code>ls -l</code> will be a command which is fed to the container, so Docker starts the given command and results show up.</p> <p>To see a list of all images on your system, you can use the <code>docker images</code> command.</p> <pre><code>$ docker images\nalpine                     latest              f70734b6a266        4 weeks ago         5.61MB\nhello-world                latest              bf756fb1ae65        4 months ago        13.3kB\n</code></pre> <p>Next, let's try another command.</p> <pre><code>$ docker run alpine echo \"Hello world\"\nHello world\n</code></pre> <p>In this case, Docker ran the <code>echo</code> command in your <code>alpine</code> container, and then exited it. Exit means the container is terminated after running the command.</p> <p>Let's try another command.</p> <pre><code>$ docker run alpine sh\n</code></pre> <p>It seems nothing happened. In fact, docker ran the <code>sh</code> command in your alpine container, and exited it. If you want to be inside the container shell, you need to use <code>docker run -it alpine sh</code>. <code>-it</code> means interactive and allocating a pseudo-TTY. You can find more help on the <code>run</code> command\u00a0with *docker run --help`.</p> <p>Let's run <code>docker run -it alpine sh</code>.</p> <pre><code>$ docker run -it alpine sh\n/ # ls\nbin    dev    etc    home   lib    media  mnt    opt    proc   root   run    sbin   srv    sys    tmp    usr    var\n/ # uname -a\nLinux c1552c9b6cf0 4.19.76-linuxkit #1 SMP Fri Apr 3 15:53:26 UTC 2020 x86_64 Linux\n/ # exit\n</code></pre> <p>You are inside of the container shell and you can try out a few commands like <code>ls</code> and <code>uname -a</code> and others. To quit the container, type <code>exit</code> on the terminal. If you use the <code>exit</code> command, the container is terminated. If you want to keep the container active, then you can use keys <code>ctrl p</code>, and then <code>ctrl q</code> (You don't have to press these key combinations simultaneously). If you want to go back into the container, you can type <code>docker attach \\&lt;container_id\\&gt;</code>, such as <code>docker attach\u00a0c1552c9b6cf0</code>. You can find container id with <code>docker ps -all</code>. This command will be explained next.</p> <p>Now, let's learn about the <code>docker ps</code> command which shows you all containers that are currently running.</p> <pre><code>$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS\n</code></pre> <p>You cannot see any container because no containers are running. To see a list of all containers that you ran, use <code>docker ps --all</code>. You can see that STATUS says that all containers exited. \u00a0</p> <pre><code>$ docker ps --all\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES\nc1552c9b6cf0        alpine              \"sh\"                     6 minutes ago       Exited (0) 2 minutes ago                        wonderful_cori\n5de22ab86f2a        alpine              \"echo 'Hello world'\"     18 minutes ago      Exited (0) 18 minutes ago                       goofy_visvesvaraya\ndf35ee7df7e3        alpine              \"ls -l\"                  31 minutes ago      Exited (0) 31 minutes ago                       fervent_gould\n6dbe999044b4        hello-world         \"/hello\"                 3 hours ago         Exited (0) 3 hours ago\n</code></pre> <p>When Docker containers are created, the Docker system automatically assign a universally unique identifier (UUID) number to each container to avoid any naming conflicts. CONTAINER ID is a shortform of the UUID. You can assign names to your Docker containers when you run them, using <code>--name</code> flags. In addition, you can rename your Docker container's name \u00a0with <code>rename</code> command. For example, let's rename \"wonderful_cori\" to \"my_container\" with\u00a0<code>docker rename</code> command.</p> <pre><code>$ docker rename wonderful_cori my_container\n$ docker ps --all\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES\nc1552c9b6cf0        alpine              \"sh\"                     10 minutes ago      Exited (0) 6 minutes ago                        my_container\n5de22ab86f2a        alpine              \"echo 'Hello world'\"     22 minutes ago      Exited (0) 22 minutes ago                       goofy_visvesvaraya\ndf35ee7df7e3        alpine              \"ls -l\"                  35 minutes ago      Exited (0) 35 minutes ago                       fervent_gould\n</code></pre>","title":"Running Docker containers from prebuilt images"},{"location":"Docker/#build-docker-images-which-contain-your-own-code","tags":["how-to guide"],"text":"<p>Now you are ready to use Docker to create your own applications! First, you will learn more about Docker images. Then you will build your own image and use that image to run an application on your local machine.</p>","title":"Build Docker images which contain your own code"},{"location":"Docker/#docker-images","tags":["how-to guide"],"text":"<p>Docker images are basis of containers. In the above example, you pulled the alpine image from the repository and ran a container based on that image. To see the list of images that are available on your local machine, run the <code>docker images</code> command.</p> <pre><code>$ docker images\nREPOSITORY                 TAG                 IMAGE ID            CREATED             SIZE\nalpine                     latest              f70734b6a266        4 weeks ago         5.61MB\nhello-world                latest              bf756fb1ae65        4 months ago        13.3kB\ncentos/python-36-centos7   latest              070f320fe348        2 weeks ago         698MB\ncentos                     7                   b5b4d78bc90c        2 weeks ago         203MB\n...\n</code></pre> <p>The TAG refers to a particular snapshot of the image and the ID is the corresponding UUID of the image. Images can have multiple versions. When you do not assign a specific version number, the client defaults to latest. If you want a specific version of the image, you can use docker pull command as follows:</p> <pre><code>$ docker pull centos:7\n</code></pre> <p>You can search for images from a repository (for example, the following is the Docker hub for CentOS) or directly from the command line using <code>docker search</code>.</p> <pre><code>$ docker search centos\nNAME                               DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED\ncentos                             The official build of CentOS.                   6014                [OK]\nansible/centos7-ansible            Ansible on Centos7                              129                                     [OK]\nconsol/centos-xfce-vnc             Centos container with \"headless\" VNC session\u2026   115                                     [OK]\njdeathe/centos-ssh                 OpenSSH / Supervisor / EPEL/IUS/SCL Repos - \u2026   114                                     [OK]\ncentos/mysql-57-centos7            MySQL 5.7 SQL database server                   76\nimagine10255/centos6-lnmp-php56    centos6-lnmp-php56                              58                                      [OK]\ntutum/centos                       Simple CentOS docker image with SSH access      46\ncentos/postgresql-96-centos7       PostgreSQL is an advanced Object-Relational \u2026   44\nkinogmt/centos-ssh                 CentOS with SSH                                 29                                      [OK]\npivotaldata/centos-gpdb-dev        CentOS image for GPDB development. Tag names\u2026   12\nguyton/centos6                     From official centos6 container with full up\u2026   10                                      [OK]\ncentos/tools                       Docker image that has systems administration\u2026   6                                       [OK]\ndrecom/centos-ruby                 centos ruby                                     6                                       [OK]\npivotaldata/centos                 Base centos, freshened up a little with a Do\u2026   4\npivotaldata/centos-mingw           Using the mingw toolchain to cross-compile t\u2026   3\ndarksheer/centos                   Base Centos Image -- Updated hourly             3                                       [OK]\nmamohr/centos-java                 Oracle Java 8 Docker image based on Centos 7    3                                       [OK]\npivotaldata/centos-gcc-toolchain   CentOS with a toolchain, but unaffiliated wi\u2026   3\nmiko2u/centos6                     CentOS6 \u65e5\u672c\u8a9e\u74b0\u5883                                   2                                       [OK]\nblacklabelops/centos               CentOS Base Image! Built and Updates Daily!     1                                       [OK]\nindigo/centos-maven                Vanilla CentOS 7 with Oracle Java Developmen\u2026   1                                       [OK]\nmcnaughton/centos-base             centos base image                               1                                       [OK]\npivotaldata/centos7-dev            CentosOS 7 image for GPDB development           0\nsmartentry/centos                  centos with smartentry                          0                                       [OK]\npivotaldata/centos6.8-dev          CentosOS 6.8 image for GPDB development         0\n</code></pre>","title":"Docker images"},{"location":"Docker/#building-your-first-docker-image","tags":["how-to guide"],"text":"<p>In this section, you will build a simple Docker image with writing a Dockerfile, and run it. For this purpose, we will create a Python script, and a Dockerfile.</p>","title":"Building your first Docker image"},{"location":"Docker/#creating-working-directory","tags":["how-to guide"],"text":"<p>Let's create a working directory where you will make the following files: <code>hello.py</code>, <code>Dockerfile</code></p> <pre><code>$ cd ~\n$ mkdir my_first_Docker_image\n$ cd my_first_Docker_image\n</code></pre>","title":"Creating working directory"},{"location":"Docker/#python-script","tags":["how-to guide"],"text":"<p>Create the <code>hello.py</code> file with the following content.</p> <pre><code>print(\"Hello world!\")\nprint(\"This is my 1st Docker image!\")\n</code></pre>","title":"Python script"},{"location":"Docker/#dockerfile","tags":["how-to guide"],"text":"<p>A Dockerfile is a text file which has a list of commands that  Docker calls while creating an image. The Dockerfile is similar to a job batch file, and contains all information that Docker needs to know to to run the application package.</p> <p>In the <code>my_first_Docker_image</code> directory, create a file, called Dockerfile, which has the content below.</p> <pre><code># our base image. The latest version will be pulled.\nFROM alpine\n\n# install python and pip\nRUN apk add --update py3-pip\n\n# copy files required to run\nCOPY hello.py /usr/src/my_app/\n\n# run the application\nCMD python3 /usr/src/my_app/hello.py\n</code></pre> <p>Now, let's learn the meaning of the each line.</p> <p>The first line means that we will use alpine Linux as a base image. No version is specified, so the latest version will be pulled. Use the <code>FROM</code> keyword.</p> <pre><code>FROM alpine\n</code></pre> <p>Next, the Python <code>pip</code> package is installed. Use the <code>RUN</code> keyword.</p> <pre><code>RUN apk add --update py3-pip\n</code></pre> <p>Next, copy the file to the image. <code>/usr/src/my_app</code> will be created while the file is copied. Use the <code>COPY</code> keyword.</p> <pre><code>COPY hello.py /usr/src/my_app/\n</code></pre> <p>The last step is run the application with the <code>CMD</code> keyword.\u00a0<code>CMD</code>\u00a0tells the container what the container should do by default when it is started.</p> <pre><code>CMD python3 /usr/src/my_app/hello.py\n</code></pre>","title":"Dockerfile"},{"location":"Docker/#build-the-image","tags":["how-to guide"],"text":"<p>Now you are ready to build your first Docker image. The <code>docker build</code> command will do most of the work. However, before you run the <code>docker build</code> command, check again if you are logged in to Docker.\u00a0</p> <p>To build the image, use the following command.</p> <pre><code>$ docker build -t my_first_image .\n</code></pre> <p>The client will pull all necessary images and create your image. If everything goes well, your image is ready to be used! Run the <code>docker images</code> command to see if your image <code>my_first_image</code> is shown.</p>","title":"Build the image"},{"location":"Docker/#run-your-image","tags":["how-to guide"],"text":"<p>When you successfully create your Docker image, test it by starting a new container from the image.</p> <pre><code>$ docker run my_first_image\n</code></pre> <p>If everything went well, you will see this message.</p> <pre><code>Hello world!\nThis is my 1st Docker image!\n</code></pre>","title":"Run your image"},{"location":"Docker/#docker-and-jupyter-notebook","tags":["how-to guide"],"text":"<p>Now, we will build a Docker image which deploys a Jupyter notebook.</p> <p>First, let's check Jupyter images on Docker Hub. We will use minimal-notebook.</p> <pre><code>$docker search jupyter\nNAME                                    DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED\njupyter/datascience-notebook            Jupyter Notebook Data Science Stack from htt\u2026   666\njupyter/all-spark-notebook              Jupyter Notebook Python, Scala, R, Spark, Me\u2026   301\njupyterhub/jupyterhub                   JupyterHub: multi-user Jupyter notebook serv\u2026   248                                     [OK]\njupyter/scipy-notebook                  Jupyter Notebook Scientific Python Stack fro\u2026   241\njupyter/tensorflow-notebook             Jupyter Notebook Scientific Python Stack w/ \u2026   218\njupyter/pyspark-notebook                Jupyter Notebook Python, Spark, Mesos Stack \u2026   157\njupyter/base-notebook                   Small base image for Jupyter Notebook stacks\u2026   106\njupyter/minimal-notebook                Minimal Jupyter Notebook Stack from https://\u2026   105\n...\n</code></pre> <p>Let's start by creating a directory <code>my_note_book</code>. Copy <code>hello.py</code>, which we used for the Python image to the <code>my_note_book</code> directory. Then\u00a0create a Dockerfile under the <code>my_note_book</code> directory and add the following content:</p> <pre><code># base image\nFROM jupyter/base-notebook\n\n# copy files\nCOPY hello.py /home/my_note_book/\n\n# the port number the container should expose\nEXPOSE 8888\n</code></pre> <p>The last part specifies the port number which needs to be exposed. The default port for Jupyter is 8888, and therefore, we will expose that port.</p> <p>Now, build the image using the following command:</p> <pre><code>$ docker build -t mynotebook .\nSending build context to Docker daemon  4.096kB\nStep 1/3 : FROM jupyter/base-notebook\n ---&gt; 7ea955290e01\nStep 2/3 : COPY model.py /home/my_note_book_dir/\n ---&gt; 9a1dd638a667\nStep 3/3 : EXPOSE 8888\n ---&gt; Running in a955eb421dde\nRemoving intermediate container a955eb421dde\n ---&gt; 8d3f7447955c\nSuccessfully built 8d3f7447955c\nSuccessfully tagged mynotebook:latest\n</code></pre> <p>Now, everything is ready. You can run the image using the <code>docker run</code> command.</p> <pre><code>$ docker run -p 8888:8888 mynotebook\nExecuting the command: jupyter notebook\n[I 20:26:03.538 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret\n[I 20:26:04.144 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.7/site-packages/jupyterlab\n[I 20:26:04.144 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab\n[I 20:26:04.146 NotebookApp] Serving notebooks from local directory: /home/jovyan\n[I 20:26:04.147 NotebookApp] The Jupyter Notebook is running at:\n[I 20:26:04.147 NotebookApp] http://c47b22cd65be:8888/?token=a53b2303380fa4a2e92cc50441ad121a08d6e39dea726fd9\n[I 20:26:04.147 NotebookApp]  or http://127.0.0.1:8888/?token=a53b2303380fa4a2e92cc50441ad121a08d6e39dea726fd9\n[I 20:26:04.147 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 20:26:04.151 NotebookApp]\n\n    To access the notebook, open this file in a browser:\n        file:///home/.local/share/jupyter/runtime/nbserver-6-open.html\n    Or copy and paste one of these URLs:\n        http://c47b22cd65be:8888/?token=a53b2303380fa4a2e92cc50441ad121a08d6e39dea726fd9\n     or http://127.0.0.1:8888/?token=a53b2303380fa4a2e92cc50441ad121a08d6e39dea726fd9\n[I 20:26:11.151 NotebookApp] 302 GET /?token=a53b2303380fa4a2e92cc50441ad121a08d6e39dea726fd9 (172.17.0.1) 0.65ms\n</code></pre>","title":"Docker and Jupyter notebook"},{"location":"Environment_Variables/","text":"<p>The following is a list of correspondence between Torque and SLURM environment variables:</p>           Description Torque SLURM Variable   The ID of the job PBS_JOBID SLURM_JOB_ID   Job array ID (index) number PBS_ARRAYID SLURM_ARRAY_TASK_ID   Directory where the submission command  was executed PBS_O_WORKDIR SLURM_SUBMIT_DIR   Name of the job PBS_JOBNAME SLURM_JOB_NAME   List of nodes allocated to the job PBS_NODEFILE SLURM_JOB_NODELIST   Number of Processors  Per Node requeste PBS_NUM_PPN SLURM_JOB_CPUS_PER_NODE   Total number of cores requested PBS_NP SLURM_NTASKS * SLURM_CPUS_PER_TASK   Total number of nodes requested PBS_NUM_NODES SLURM_NTASKS   Current Host of PBS job PBS_O_HOST SLURM_SUBMIT_HOST","title":"Environment Variables"},{"location":"Example_SLURM_scripts/","text":"<p>Three examples are given in this pdf tutorial</p> <ul> <li>Example 1: single node, single core</li> <li>Example 2: single node, multiple threads</li> <li>Example 3: multiple nodes (e.g., a MPI program)</li> </ul>","title":"Example SLURM scripts"},{"location":"File-Permission-in-Research-Space_34963746.html/","text":"","title":"File Permission in Research Space 34963746.html"},{"location":"File-Permission-in-Research-Space_34963746.html/#teaching-file-permission-in-research-space","text":"<p>A user with account name <code>User1</code> is not able to access a directory <code>Dirct</code> in his research space <code>Group1</code>. The following is the result of the <code>ls</code> command:</p> <pre><code>[User1@dev-intel14-k20 ~]$ ls -la /mnt/research/Group1\ntotal 98\ndrwxrwS---   3 ProjInvs Group1 8192 Aug  6 08:53 .\ndrwxr-xr-x 391 root     root      0 Sep  9 07:34 ..\n-rwx------   1 User2    Group2 4299 Jul  2  2018 file1\n-rwx------   1 User3    Group3 2452 Jul  2  2018 file2\ndrwxrwS---   2 User2    Group2 8192 May 22 11:31 Dirct\n-rw-rw-r--   1 User1    Group1  263 Aug  6 08:54 file3\n</code></pre> <p>Q: How to make <code>User1</code> able to access the directory <code>Dirct</code>?</p> <p>A: Since <code>User2</code> is the owner of the directory, <code>User2</code> can run a\u00a0 command to change the group ownership:</p> <pre><code>[User2@dev-node ~]$ chgrp -R Group1 /mnt/research/Group1/Dirct\n</code></pre> <p>Q: In order for all group users able to access files and directories in the research space, what should they do?</p> <p>A: They should run the following commands:</p> <ol> <li> <p>Change the group ownership of all files and directories to the research group <code>Group1</code>:</p> <pre><code>[UserID@dev-node ~]$ chgrp -R Group1 /mnt/research/Group1/ 2&gt;/dev/null\n</code></pre> </li> <li> <p>Open the permissions of all files and directories to be readable (<code>r</code>) and writable (<code>w</code>) to group users:</p> <pre><code>[UserID@dev-node ~]$ chmod -R g+rw /mnt/research/Group1/ 2&gt;/dev/null\n</code></pre> </li> <li> <p>Make all directories sticky to the group ownership for any file generated inside (turn on group sticky bits):</p> <pre><code>[UserID@dev-node ~]$ chmod g+s $(find /mnt/research/Group1/ -type d -user $USER 2&gt;/dev/null)\n</code></pre> </li> <li> <p>Group users should not copy files to their research space with preserving the ownership, such as using command \"<code>cp -p ...</code> \".</p> </li> </ol>","title":"Teaching : File Permission in Research Space"},{"location":"File_Permissions_on_HPCC/","text":"<p>The HPCC offers several different types of storage for users. All of these filesystems make use of standard UNIX file permissions. Understanding how standard UNIX permissions and ownership works is an important way to control access to your files.</p>","title":"File Permissions on HPCC"},{"location":"File_Permissions_on_HPCC/#unix-users-and-groups","text":"<p>Every user has a unique username on HPCC systems. This is typically your MSU NetID. Every user is also a member of at least one group. This group is typically the department the user is in (such as cse or plb). An user can be a member of additional groups. To see what groups you are a member of, run the <code>groups</code> command. If you feel you are in the wrong group, please contact HPCC staff.</p>","title":"UNIX users and groups"},{"location":"File_Permissions_on_HPCC/#unix-file-ownership","text":"<p>Every file and directory has two sets of ownership, the user and the group. The user owner is normally set to the user that created the file. Normally, the user owner of a file or directory is the only user that is able to change permissions or group ownership.</p> <p>The group owner of a file or directory allows a user owner to grant permissions to a group of users for a particular file or directory. The user owner of a file can change the group ownership of a file to any group that they are a member of. Any file created by a user normally defaults to group owner being set to the user's primary group, unless the user or directory owner has changed the behavior (using procedures described below.)</p>","title":"UNIX file ownership"},{"location":"File_Permissions_on_HPCC/#the-three-types-of-basic-unix-permissions","text":"","title":"The three types of basic UNIX permissions"},{"location":"File_Permissions_on_HPCC/#read","text":"<p>Read permission on a file allows the contents of a file to be read. The read permission, when applied to a directory, allows the contents of a directory to be listed. Referred to as \"r\" in the output of the <code>ls -l</code> command.</p>","title":"Read"},{"location":"File_Permissions_on_HPCC/#write","text":"<p>Write permission on a file allows the file to be modified or deleted. Write permissions in a directory allow the creation of additional files in that directory. Referred to as \"w\" in the output of the <code>ls -l</code> command.</p>","title":"Write"},{"location":"File_Permissions_on_HPCC/#execute","text":"<p>The execute permission allows a file to be run as an executable. When applied to a directory it allows traversal of that directory: the ablility to access files or subdirectories in that directory. Referred to as \"x\" in the output of the <code>ls -l</code> command.</p>","title":"Execute"},{"location":"File_Permissions_on_HPCC/#displaying-permissions-of-files-and-directories","text":"<p>To display permissions in the current directory, run:</p> <pre><code>ls -l\n</code></pre> <p></p> <p>You can also display the permissions of an individual file or directory by running:</p> <pre><code>ls -ld filename\n</code></pre> <p>For example, you can check the permissions of your home directory:</p> <pre><code>ls -ld ~\n</code></pre>","title":"Displaying permissions of files and directories"},{"location":"File_Permissions_on_HPCC/#applying-these-to-the-three-types-of-users","text":"<p>In the normal UNIX security model, there are three levels that are evaluated when considering file or directory access: user owner, group owner, and everyone else on the system. These types are typically referred to as user (<code>u</code>), group (<code>g</code>) and other (<code>o</code>). Only the owner of a file or a directory is allowed to change its permissions or the group name (to one of his groups).</p>","title":"Applying these to the three types of users"},{"location":"File_Permissions_on_HPCC/#to-change-user-permissions-in-this-case-add-all-permissions-run-the-following-command","text":"<pre><code>chmod u+rwx FileName\n</code></pre> <p>Note that any file you create will already have the \"rw\" permission for your user account.\u00a0\u00a0 However to have a program script able to be run from the command line, you need to change the 'execute' permission</p> <pre><code>chmod u+x FileName\n</code></pre>","title":"To change user permissions (in this case, add all permissions), run the following command:"},{"location":"File_Permissions_on_HPCC/#group-and-other-permissions-can-also-be-altered","text":"<p>To allow anyone in the group that owns the file to be able to read that file, change\u00a0 the group read permission:</p> <pre><code>chmod g+r FileName\n</code></pre> <p>To allow anyone in the group to read and write the file, you can change the read and write permission</p> <pre><code>chmod g+wr FileName\n</code></pre> <p>If you have a file that is currently read and writeable by the group (g+wr) and you want to make it private, remove those permissions:</p> <pre><code>chmod g-rw FileName\n</code></pre> <p>To add the ability for other users to write to a file or directory (this allows all users on the HPC to see and read this file if it's in a shared folder which we don't recommend).</p> <pre><code>chmod o+w FileName\n</code></pre>","title":"Group and other permissions can also be altered:"},{"location":"File_Permissions_on_HPCC/#change-group-name","text":"<p>To change the group ownership of a file or a directory, simply run</p> <pre><code>chgrp &lt;GroupName&gt; &lt;FileName&gt;\n</code></pre> <p>where <code>&lt;GroupName&gt;</code> is the group name which you would like to change to and <code>&lt;FileName&gt;</code> is the name and path of the file or directory.</p>","title":"Change group name:"},{"location":"File_Permissions_on_HPCC/#working-with-non-primary-groups-and-permissions","text":"<p>If you have more than one group associated with your account, you can switch group owns the files created by default with the <code>newgrp</code> command: <code>newgrp myothergroup</code>. If you need to do this frequently, you can contact HPCC staff to change your primary group. You can also change the default group for new files created in a directory by setting the set-group-ID setting. The /mnt/research HPCC Research file share spaces have this setting set by default. To set the set-group-ID bit on a directory:</p> <pre><code>chmod g+s DirectoryName\n</code></pre> <p>To remove the set-group-ID bit on a directory:</p> <pre><code>chmod g-s DirectoryName\n</code></pre>","title":"Working with non-primary groups and permissions"},{"location":"File_Permissions_on_HPCC/#other-special-permissions","text":"<p>There are other group permissions beyond the scope of this document, primarily the set-user-ID bit and the \"sticky\" bit. For more information about special permissions, please review the GNU documentation, available on any HPCC system:</p> <pre><code>info chmod\n</code></pre>","title":"Other special permissions"},{"location":"File_Permissions_on_HPCC/#filesystem-specific-differences","text":"","title":"Filesystem-specific differences"},{"location":"File_Permissions_on_HPCC/#home","text":"<p>Your home directory has default permissions that allow only you to have access. Other users, whether they are in your primary group or not, are not allowed access to the contents of your home directory by default. If you wish to allow other users access to your home directory, you will need to change permissions on it.  </p> <p>To allow every member of a group access to read your home directory, use:</p> <pre><code>chmod g+rx ~\n</code></pre> <p>To allow every user outside your UNIX group to read your home directory, use:</p> <pre><code>chmod o+rx ~\n</code></pre> <p>To allow world-wide read access to your home directory</p> <pre><code>chmod a+rx ~\n</code></pre>","title":"Home"},{"location":"File_Permissions_on_HPCC/#scratch","text":"<p>Directories are created as private to you by default. If you do not wish this to be the case, you can use the technique for sharing directories (see below).</p>","title":"Scratch"},{"location":"File_Permissions_on_HPCC/#tmpdir-space","text":"<p>Directories are creates as world-readable by default, but the scheduler deletes the contents of $TMPDIR after a job exits. If you require additional security for this temporary space, manually setting the permissions of $TMPDIR is necessary. Here is an example to mimic the security of home directory space:</p> <pre><code>chmod go-rwx $TMPDIR\n</code></pre>","title":"TMPDIR space"},{"location":"File_Permissions_on_HPCC/#sharing-a-single-directory-inside-your-home-directory","text":"<p>If you wish to share only a single directory in your home directory and keep all other contents private,\u00a0 you can use the following techinque:</p> <pre><code># create the shared folder\ncd ~\nmkdir shared\nchmod o+rwx shared\n# create a shared file in the shared folder\necho \"hello, iCER\" &gt; shared/sharefile.txt\nchmod o+rw shared/sharefile.txt\n# anyone can read this file using\ncat /mnt/home/&lt;netid&gt;/shared/sharefile.txt\n</code></pre> <p>You can use the same technique for your $SCRATCH folder to share folders on that.\u00a0\u00a0 Note if there are other directories above your shared directory (e.g. it's a sub-sub-directory like \\~/project/data/shared), then every directory in the path will need the execute bit set for everyone.</p>","title":"Sharing a single directory inside your home directory"},{"location":"File_Permissions_on_HPCC/#other-resources","text":"<p>This just covers the basics of UNIX file permissions. Here are some other resources for more in-depth information: Software Carpentry - Permissions The Linux Cookbook, 2nd ed., Chapter 9 https://www.computerhope.com/unix/uumask.htm</p>","title":"Other resources"},{"location":"File_Systems/","tags":["explanation"],"text":"<p>HPCC provides six types of file storage. They are referred to here as HOME, RESEARCH, SCRATCH, FLASH (ffs17), LOCAL and RAMDISK. This article addresses the differences between these file storage systems from a hardware and software point of view. The primary usage, allocation and application cases for each storage type are also detailed to help users select the appropriate file systems for running their job.</p>","title":"File Systems"},{"location":"File_Systems/#home-space","tags":["explanation"],"text":"","title":"Home Space"},{"location":"File_Systems/#research-space","tags":["explanation"],"text":"","title":"Research Space"},{"location":"File_Systems/#scratch-file-systems","tags":["explanation"],"text":"","title":"Scratch File Systems"},{"location":"File_Systems/#local-file-systems","tags":["explanation"],"text":"","title":"Local File Systems"},{"location":"File_Systems/#guidelines-for-choosing-file-storage-and-io","tags":["explanation"],"text":"","title":"Guidelines for Choosing File Storage and I/O"},{"location":"File_Systems/#sensitive-data-on-the-hpcc","tags":["explanation"],"text":"## Attachments:     [Picture1.jpg](attachments/11895667/11895686.jpg) (image/jpeg)","title":"Sensitive Data on the HPCC"},{"location":"File_and_Data_Transfer/","text":"","title":"File and Data Transfer"},{"location":"File_and_Data_Transfer/#file-transfer","text":"","title":"File transfer"},{"location":"File_and_Data_Transfer/#mapping-hpc-drives-with-samba","text":"","title":"Mapping HPC drives with Samba"},{"location":"File_and_Data_Transfer/#mapping-hpc-drives-with-sshfs","text":"","title":"Mapping HPC drives with SSHFS"},{"location":"File_and_Data_Transfer/#sftp-mapping-on-hpcc-file-systems","text":"","title":"SFTP Mapping on HPCC file systems"},{"location":"File_and_Data_Transfer/#transferring-data-with-globus","text":"","title":"Transferring data with Globus"},{"location":"File_and_Data_Transfer/#science-dmz","text":"","title":"Science DMZ"},{"location":"File_and_Data_Transfer/#rclone-rsync-for-cloud-storage","text":"","title":"Rclone - rsync for cloud storage"},{"location":"File_transfer/","text":"<p>This document highlights several simple methods to transfer files to the HPCC home and research directories. There are two main gateway systems for copying files.\u00a0</p> <ol> <li> <p><code>hpcc.msu.edu</code>: This is our login gateway. While it can be used for file transfer, it's not intended for high volumes of files. More importantly, the scratch space is not mounted there and so you can't access your files on scratch.</p> </li> <li> <p><code>rsync.hpcc.msu.edu</code>: It has access to scratch, and is dedicated to file transfer. Although this gateway is named by the popular Linux \"rsync\" command, it can be used for \"sftp\" or \"scp\" as well. Starting in October 2022, login to the rsync gateway will accept SSH keys as the ONLY authentication method. Username/password won't work. Please refer to the SSH key tutorial for setting up your keypair.</p> </li> </ol>","title":"File transfer"},{"location":"File_transfer/#using-filezilla-for-mac-and-windows","text":"","title":"Using FileZilla for Mac and Windows"},{"location":"File_transfer/#general-password-based-setup","text":"<p>FileZilla is a GUI application for copying files between a remote host and your computer.</p> <ol> <li> <p>Download and install the appropriate (free) FileZilla client from     https://filezilla-project.org/download.php?show_all=1     and select your operating system version. Mac users will have to     'unzip' the file and move the application into your Applications     folder.</p> </li> <li> <p>To use, launch the program.</p> </li> <li> <p>In the top dialog boxes, enter:</p> <ul> <li>(Host) <code>rsync.hpcc.msu.edu</code></li> <li>(Username) <code>&lt;your HPCC username&gt;</code></li> <li>(Password) <code>&lt;your password&gt;</code></li> <li>(Port) 22  </li> </ul> <p>Then click connect or quickconnect. The first time you use this,     you will have to accept the host certificate.</p> </li> <li> <p>Once connected, the left column displays files on your local     computer, the right column displays files on HPCC.</p> </li> <li> <p>You can select the appropriate directories by double clicking     through each tree. Files can be dragged and dropped from one column     to the next. By dragging files from the left column to the right,     you are uploading files to HPCC from your local computer. By     dragging files from the right column to the left, you can download     files from HPCC to your local computer.</p> </li> </ol>","title":"General password-based setup"},{"location":"File_transfer/#key-based-setup","text":"<p>As mentioned above, password authentication will be disabled in October 2022. See the screenshot below for how to set up your FileZilla using SSH keys. If you haven't generated them, please refer to the SSH key tutorial for how. </p> <p></p> <p>Fore more detailed steps, check out this FileZilla doc: How to Use SSH Private Keys for SFTP</p>","title":"Key-based setup"},{"location":"File_transfer/#using-linux-commands","text":"<p>A number of different command-line utilities are available to OS X and Linux users. Each of them has its own advantages.</p> <ol> <li> <p>Basic file copy (scp)</p> <p>A simple command for transferring files between the cluster and another host is scp. To copy a file from a local directory to file space on the cluster, use a line like</p> <pre><code>scp example.txt username@rsync.hpcc.msu.edu:example_copy.txt\n</code></pre> <p>This will copy the file named example.txt in the local host's home directory to the user's home directory on the cluster, with the copy having the name example_copy.txt. Leaving the space after the colon blank gives the new file the same name as the original.\u00a0 Note: To transfer a file name with spaces you must put a backslash before each space in your file name, i.e. <code>scp \"My File Name\" username@hpcc.msu.edu:\"My\\ File\\ Name\"</code>.</p> <p>To copy a file from the cluster to your local directory,</p> <pre><code>scp username@rsync.hpcc.msu.edu:example.txt ./example_copy.txt\n</code></pre> <p>will copy the file named example.txt from the user's home directory on the cluster to the home directory of the local host, naming the new file example_copy.txt. Leaving the space after the slash blank gives the new file the same name as the original. The -r option can be used to copy entire directories recursively.\u00a0</p> </li> <li> <p>Synchronize directories (rsync)</p> <p>If you are an advanced LINUX/Mac user, there is a wonderful little utility that makes mirroring directories simple. The syntax looks very similar to scp.</p> <ul> <li> <p>To mirror <code>&lt;local_dir&gt;</code> on my local computer to <code>&lt;hpcc_dir&gt;</code> on hpcc, the following command can be issued.</p> <pre><code>rsync -ave ssh &lt;local_dir&gt; username@rsync.hpcc.msu.edu:&lt;hpcc_dir&gt;\n</code></pre> <p>In the above command, rsync will scan through both directories. If any files in the <code>&lt;local_dir&gt;</code> are newer, they will be uploaded to <code>&lt;hpcc_dir&gt;</code>. (It is also possible to get rsync to upload ALL different files, regardless of which is newer).</p> </li> <li> <p>To mirror the HPCC directory to your local system, call</p> <pre><code>rsync -ave ssh username@rsync.hpcc.msu.edu:&lt;hpcc_dir&gt; &lt;local_dir&gt;\n</code></pre> </li> <li> <p>Please use rsync command with the option --chmod=Dg+s to transfer files from a local computer to your research space.     See the following example:</p> <pre><code>rsync -ave ssh TestDir --chmod=Dg+s &lt;username&gt;@rsync.hpcc.msu.edu:/mnt/research/&lt;GroupName&gt;/\n</code></pre> </li> </ul> <p>!!! Note:     the first time you use rsync, you might want to add the -n flag to     do a dry run before any files are copied.</p> </li> <li> <p>Interactive file copy (sftp)</p> <p>When preforming several data transfers between hosts, the sftp command may be preferable, as it allows the user to work interactively. Running</p> <pre><code>sftp username@rsync.hpcc.msu.edu\n</code></pre> <p>from a local host establishes a connection between that host and the cluster. Both hosts can be navigated. For the local file system, lcd changes to the specified directory, lpwd prints the working directory, and lls prints a list of files in the current directory. For the remote file system, the same three commands are available, minus the leading \"l.\" Also available are commands to change permissions, rename files, and manipulate directories on the remote host. The two key commands are <code>get &lt;file&gt;</code>, which copies the file in the remote working directory to the local working directory, and <code>put &lt;file&gt;</code>, which copies the file in the local working directory to the remote working directory. The quit command closes the connection between hosts.</p> </li> <li> <p>Copy file from Internet (wget)</p> <p>Wget is a simple command useful for copying files from the Internet to a user's file space on the cluster.\u00a0 Submitting the line</p> <pre><code>wget http://www.examplesite.com/examplefile.txt\n</code></pre> <p>downloads examplefile.txt to the user's working directory.</p> </li> </ol>","title":"Using Linux commands"},{"location":"Frequently_Asked_Questions_FAQ_/","text":"","title":"Frequently Asked Questions (FAQ)"},{"location":"Frequently_Asked_Questions_FAQ_/#what-is-my-hpcc-user-namepassword","text":"<p>If you are affiliated with MSU, then your MSU NetID is your user name, and your NetID password is your HPCC password. This is the same as those for all the MSU online services. An HPCC account must be requested by an MSU faculty member at\u00a0https://contact.icer.msu.edu/account</p>","title":"What is my HPCC user name/password?"},{"location":"Frequently_Asked_Questions_FAQ_/#can-i-reset-my-password-on-the-hpcc-because-my-login-got-denied-after-multiple-failed-attempts","text":"<p>No. The authentication on the HPCC is directly tied to MSU. You will need to request a password reset at https://netid.msu.edu/netid/password/index.html</p>","title":"Can I reset my password on the HPCC because my login got denied after multiple failed attempts?"},{"location":"Frequently_Asked_Questions_FAQ_/#i-used-to-be-able-to-connect-to-the-hpcc-server-but-now-i-cant-why","text":"<p>There can be multiple reasons for this, such as system downtime (so please check the ICER blog first). Another common reason is account expiry. The HPCC periodically disables users who are no longer affiliated with the university or registered with a class for which the instructor has created temporary student accounts. To re-activate your HPCC account, please have your PI submit a sponsoring form at https://contact.icer.msu.edu/sponsoredrenewal</p>","title":"I used to be able to connect to the HPCC server, but now I can't. Why?"},{"location":"Frequently_Asked_Questions_FAQ_/#can-you-keep-me-posted-on-the-current-status-of-the-hpcc","text":"<p>Yes. Users are encouraged to follow the HPCC Announcements blog  to keep updated on the status of HPCC (such as scheduled downtimes and urgent notices).</p>","title":"Can you keep me posted on the current status of the HPCC?"},{"location":"Frequently_Asked_Questions_FAQ_/#i-am-looking-for-help-to-troubleshoot-my-problem-how-do-i-share-my-codefiles-with-you","text":"<p>We do not go to your directory to view files or test your code for that matter. Please send your files along with your reply to the ticket email.</p>","title":"I am looking for help to troubleshoot my problem. How do I share my code/files with you?"},{"location":"Frequently_Asked_Questions_FAQ_/#is-there-any-limit-per-user-on-using-the-hpcc-resources","text":"<p>Limit on running a program on a dev-node: 2 CPU hours. If you are running a multi-threaded program, the wall time limit would be (roughly) 2 hours divided by the number of threads.</p> <p>Limit on file counts: 1 million files for each of the home, research and scratch spaces.</p> <p>Limit on storage size: each user has up to 1 TB of storage for free (for each of the home and research directories); beyond 1 TB, the cost is $125 per TB per year. For scratch space (i.e. <code>/mnt/scratch/&lt;your_user_name&gt;</code>), 50 TB is the maximum and cannot be increased further (users will need to archive/delete files when this limit has been exceeded).</p> <p>Limit on cluster usage: 1) the longest wall time you can request is 7 days; 2) the maximal number of CPU cores you can use is 1040\u00a0at any one time; 3) the maximal number of jobs that can be queued or running is 1000 (except in the scavenger queue); 4) non-buyin users have a maximum of 1 million CPU hours per year.</p>","title":"Is there any limit per user on using the HPCC resources?"},{"location":"Frequently_Asked_Questions_FAQ_/#i-would-like-to-know-more-about-the-dev-node-limit","text":"<p>When you connect to any\u00a0of the HPCC's dev-nodes, you will see the following\u00a0message:</p> <p><code>processes on development nodes are limited to two hours of CPU time.</code></p> <p>The two hour\u00a0CPU time limit is for each process you run on that dev-node.\u00a0If one process uses CPU time greater\u00a0than 2 hours, then only that\u00a0process will be killed. You can, however, still connect to that dev-node, and run another process.\u00a0Additionally, if your process uses 100% CPU (1 core), it will be terminated in two hours. If your process uses 200% CPU (2 cores), it will be terminated in one hour, and so on.</p>","title":"I would like to know more about the dev-node limit."},{"location":"Frequently_Asked_Questions_FAQ_/#how-do-i-check-my-cpu-time-usage","text":"<p>Run the command <code>cputime</code>. You can also run <code>sreport</code> to get the information with date such as <code>sreport user top user= start=2021-01-06 -t hour</code>.\u00a0</p>","title":"How do I check my cpu time usage?"},{"location":"Frequently_Asked_Questions_FAQ_/#how-do-i-get-my-storage-usage-data","text":"<p>Run command \"quota\". You can't write new files if your quota has been used up.</p>","title":"How do I get my storage usage data?"},{"location":"Frequently_Asked_Questions_FAQ_/#i-have-a-buyin-account-do-i-need-to-specify-it-when-i-submit-jobs","text":"<p>No. When submitting a job without specifying an account, your default account is used. You can check your default account using the \"buyin_status -l\" command; buyin user's default is their buyin account. We recommend you read this if you have purchased buyin nodes.</p>","title":"I have a buyin account, do I need to specify it when I submit jobs?"},{"location":"Frequently_Asked_Questions_FAQ_/#what-is-hpccs-data-backup-policy","text":"<p>We back up data in users' home and research directories, not in their scratch spaces.</p> <p>You will have 24 hourly backups for the previous 24 hour period. For previous days however, we will provide daily backups only. Daily backups are performed at 12 AM Eastern Time and retained for 60 days.</p>","title":"What is HPCC's data backup policy?"},{"location":"Frequently_Asked_Questions_FAQ_/#my-files-in-the-scratch-space-are-gone","text":"<p>Files in scratch are automatically purged if the last modification time is older than 45 days. Note that the scratch spaces are not intended for long-term storage. Files saved in scratch have no back-up.</p>","title":"My files in the scratch space are gone."},{"location":"Frequently_Asked_Questions_FAQ_/#i-cant-transfer-files-fromto-my-scratch-space","text":"<p>If you use  as the hostname, you are connecting to the gateway (login) node which has no scratch mounted. You should connect to  in this case. It is a dedicated server for file transfer.","title":"I can't transfer files from/to my scratch space."},{"location":"Frequently_Asked_Questions_FAQ_/#do-you-support-running-gpu-jobs","text":"<p>Yes. There are three GPU dev-nodes and a series of compute nodes in the cluster; see Cluster resources.</p>","title":"Do you support running GPU jobs?"},{"location":"Frequently_Asked_Questions_FAQ_/#why-did-i-get-an-illegal-instruction-error","text":"<p>This is usually because a program was compiled on a newer CPU architecture (e.g., intel18) but then run on an older one (e.g., intel14). Our system has a range of CPUs, and the newest versions support new instructions not available on the older CPUs. One short-term fix is to run programs on the same CPU that they were compiled on. Based on our experience, this error has occurred only on intel14 nodes and therefore you need to avoid them. That is, for dev-node testing, pick one from dev-intel16, dev-intel16-k80 and dev-intel18. For job submission, add <code>#SBATCH--constraint=\"[intel16|intel18]\"</code>in your SLURM script.</p>","title":"Why did I get an \"Illegal Instruction\" error?"},{"location":"Frequently_Asked_Questions_FAQ_/#how-do-i-use-python-on-the-hpcc","text":"<p>There are two methods: users can install their own version of Python with Anaconda or use the versions of Python installed on the HPCC system. See here.</p>","title":"How do I use Python on the HPCC?"},{"location":"Frequently_Asked_Questions_FAQ_/#i-tried-to-use-python-matplotlib-to-plot-but-got-an-error-of-no-module-named-_tkinter","text":"<p>If you use the default python module (<code>/opt/software/Python/3.6.4-foss-2018a/bin/python</code>) on a dev-node, you need to load the Tkinter module before using python in order to proceed without errors. Run: <code>module load Tkinter/3.6.4-Python-3.6.4</code></p>","title":"I tried to use python matplotlib to plot, but got an error of \"No module named '_tkinter'\""},{"location":"Frequently_Asked_Questions_FAQ_/#i-have-a-python-conflict-what-should-i-do-to-resolve-it","text":"<p>Upon login to a dev-node, a default module list will load automatically. Since Python/3.6.4 is included in the list, it can interfere with a user's conda environment. As a consequence, your program may not be able to find packages installed in your conda environment even if it has been activated. In other words, the program still picks up Python/3.6.4 in the module system. The solution is to run <code>module unload Python</code> before activating the conda environment.</p>","title":"I have a Python conflict. What should I do to resolve it?"},{"location":"Frequently_Asked_Questions_FAQ_/#how-do-i-deactivate-conda-base-environment","text":"<p>Many users have reported that after a local installation of Anaconda on the HPCC, their login prompt changes\u00a0to something starting with <code>(base) -bash-4.2$</code>. This is because conda activates the default environment, <code>base</code>, upon startup. To disable this behavior, which often results in conflicts with system defaults, users can run the following command:</p> <pre><code>conda config --set auto_activate_base False\n</code></pre>","title":"How do I deactivate Conda base environment?"},{"location":"Frequently_Asked_Questions_FAQ_/#why-did-my-module-load-command-output-errors","text":"<p>There are many reasons that errors occur when you try loading a module. However, the most common cause is that you have forgotten to run <code>module purge</code>. Sometimes, <code>module spider</code> can also fail to find the module. Most likely it's because your personal module cache is out of date. To clear it, run <code>rm -r ~/.lmod.d/.cache</code>.</p>","title":"Why did my \"module load\" command output errors?"},{"location":"Frequently_Asked_Questions_FAQ_/#i-want-to-install-software-packages-what-should-i-do","text":"<p>See here for instructions. Please note that we encourage users to install software on their own, if possible. The HPCC has provided numerous versions of compilers and libraries which should accommodate the vast majority of software across different fields.</p> <p>If you are thinking of requesting the system-wide installation of a piece of software, we strongly recommend you check the following factors when submitting a request for software installation:\u00a0</p> <p>(1) How popular is the software? If it is not a popular software, are there other users on HPCC who would also be using it? If you are the only one using it, we would recommend it be installed in your home directory.\u00a0</p> <p>(2) What type of license agreement does the software have? Some software licenses may restrict use even when they are free. Examples include software with export control, specific end-user license agreement, etc. When software licenses restrict use, we typically recommend the user directly make an agreement with the software provider to obtain and install it in their home directory. If it will be used by a group of people, HPCC system administrators can help with setting up the group access in compliance with the license agreement.\u00a0</p> <p>(3) Is the software well maintained and up-to-date? If the software you wish to install is legacy software or is not being well maintained, chances are its installation will require an older version of its dependencies as well. The effort to install this software may then be greater than the effort required to find an up-to-date software with the same, similar, or even better functionality. It may be time to consider transitioning to using a newer software.\u00a0\u00a0</p>","title":"I want to install software packages, what should I do?"},{"location":"Frequently_Asked_Questions_FAQ_/#what-does-the-message-nodes-required-for-job-are-down-drained-or-reserved-for-jobs-in-higher-priority-partitions-mean-after-my-job-is-submitted","text":"<p>Once a job is submitted the scheduler adds it to the calculations and continues to update the status of the job as the system works. The status for a job will reflect the current state of the scheduler, so you will see this message update once the scheduler has found a place to put the job. There are always some nodes which are down or drained in the cluster due to normal maintenance, but the \"reserved for jobs in higher priority partitions\" is the important part, and simply indicates that the scheduler has not yet found a time to schedule the job. This will update as the scheduler continues to function.</p>","title":"What does the message \"Nodes required for job are DOWN, DRAINED or reserved for jobs in higher priority partitions\" mean after my job is submitted?"},{"location":"Frequently_Asked_Questions_FAQ_/#can-i-use-hpc-through-web-browsers","text":"<p>Yes, we provide Open OnDemand, a web portal for easy web access to the HPCC. Check out this tutorial.</p>","title":"Can I use HPC through web browsers?"},{"location":"Frequently_Asked_Questions_FAQ_/#what-should-i-do-when-i-cannot-load-modules","text":"<p>See How to find and load software modules.</p>","title":"What should I do when I cannot load modules?"},{"location":"Frequently_Asked_Questions_FAQ_/#i-have-issues-with-copying-files-to-my-hpcc-research-space","text":"<p>Many users have reported problems copying or transferring files to their research space. Although their research space still has plenty of space, they still get the following error message:</p>  <p>failed to ... Disk quota exceeded</p>  <p>This problem may occur because the folders which you copy or transfer files to have\u00a0incorrect group ownership or no set-group-ID. Please read this for more instructions.</p>","title":"I have issues with copying files to my HPCC research space."},{"location":"Frequently_Asked_Questions_FAQ_/#what-is-powertools","text":"<p>The powertools module is a collection of software tools and examples that allows researchers to better utilize HPC systems. Powertools was created to help advanced users use the HPCC more effectively. To learn more about powertools, run the command <code>powertools</code>.</p>","title":"What is powertools?"},{"location":"Frequently_Asked_Questions_FAQ_/#i-want-copy-files-fromto-my-ms-one-drivegoogle-drive","text":"<p>Rclone is currently installed on the HPCC. This software supports research in the cloud and helps HPCC users to sync files and directories between MSU\u2019s HPCC and their cloud storage, including OneDrive and Google Drive. Please refer to Rclone</p>","title":"I want copy files from/to my MS One Drive/Google Drive."},{"location":"Frequently_Asked_Questions_FAQ_/#how-to-check-the-hpcc-node-usage","text":"<p>Users can see this information by simply running the <code>node_status</code> command on any dev node. We also offer a web-based dashboard at https://icer.msu.edu/dashboard.</p>","title":"How to check the HPCC node usage?"},{"location":"Frequently_Asked_Questions_FAQ_/#does-hpcc-offer-a-cheaper-long-term-archiving-plan","text":"<p>We do not. However, MSU offers the Data Storage Finder (https://data-storage-finder.tech.msu.edu, on-campus only). There are several possible options for data archiving.</p>","title":"Does HPCC offer a cheaper long-term archiving plan?"},{"location":"GATK4/","text":"<p>Be sure to read this Quick Start before using GATK4. In particular, note the following statement from the developers:</p>  <p>Once you have downloaded and unzipped the package (named <code>gatk-[version]</code>), you will find four files inside the resulting directory:</p>  <p><code>gatk</code> <code>gatk-package-[version]-local.jar</code> <code>gatk-package-[version]-spark.jar</code> <code>README.md</code></p>  <p>Now you may ask, why are there two jars? As the names suggest, <code>gatk-package-[version]-spark.jar</code> is the jar for running Spark tools on a Spark cluster, while <code>gatk-package-[version]-local.jar</code> is the jar that is used for everything else (including running Spark tools \"locally\", i.e. on a regular server or cluster).</p> <p>So does that mean you have to specify which one you want to run each time? Nope! See the gatk file in there? That's an executable wrapper script that you invoke and that will choose the appropriate jar for you based on the rest of your command line. You could still invoke a specific jar if you wanted, but using gatk is easier, and it will also take care of setting some parameters that you would otherwise have to specify manually.</p>  <p>On the HPCC, after login to a dev-node, run: <code>module load GATK/4.0.5.1-Python-3.6.4</code>. As a tip, if you happen to run a <code>module purge</code> command in the middle of your work, and want to go back to the original login environment, please type the command: <code>exec bash -l</code></p> <p>A simple test on the HPCC is provided below.</p> <pre><code>module load GATK/4.0.5.1-Python-3.6.4\ngatk --java-options \"-Xmx8G\" HaplotypeCaller -R /opt/software/GATK/3.3-0-Java-1.7.0_80/resources/exampleFASTA.fasta -I /opt/software/GATK/3.3-0-Java-1.7.0_80/resources/exampleBAM.bam -O gatk_test.vcf\n</code></pre>","title":"GATK4"},{"location":"Gaussian_Access/","text":"<p>To obtain access to this software on HPCC complete\u00a0and sign the appropriate\u00a0Gaussian Confidentiality Agreement. Please ensure that all required sections are filled out legibly or else the form will be returned to you.</p> <ul> <li>Research Group Leader Form\u00a0\u2014\u00a0Only one form needs     to be completed per research group.</li> <li>User/Researcher Form\u00a0\u2014 Each user of the software     needs to complete this form.\u00a0Please ensure that your group leader     has completed the Research Group Leader form before submitting     this.\u00a0</li> <li>Coursework Form\u00a0\u2014 This form is to be completed by     the each student enrolled in an MSU course. Please note that the     course instructor should have completed the Research Group Leader     Form above. Access ends after the class is completed.\u00a0</li> </ul> <p>Email the completed form to\u00a0general@rt.hpcc.msu.edu.\u00a0 Please make the subject of your email, \"Gaussian Confidentiality Agreement.\" Additionally, the original form\u00a0must\u00a0be mailed to the ICER office:\u00a0Biomedical &amp; Physical Sciences Building, Room\u00a01440, 567 Wilson Rd, East Lansing, MI 48824.</p> <p>Your completed agreement will then be sent to Gaussian for the final approval. Once approved, the HPCC System Admin team will act upon the request and grant access.</p> <p>Note: Typically this process typically takes about two weeks from the date of signing the Confidentiality Agreement to Gaussian giving approval of the software so we ask that you please be patient. Without a personally signed Confidentiality Agreement, you will not be granted access to Gaussian.</p>","title":"Gaussian Access"},{"location":"Gaussian_Job_Script/","text":"<p>Here is a simple job script <code>g16.sb</code> for running Gaussian job <code>g16.com</code>:</p> <p>g16.sb</p> <pre><code>#!/bin/bash --login\n\n#SBATCH \u2013-job-name=GaussianJob\n#SBATCH \u2013-ntasks=1\n#SBATCH \u2013-cpus-per-task=4\n#SBATCH --mem=7G\n#SBATCH \u2013-time=00:10:00 \n\nInputFile=g16.com\nOutputFile=g16.log\n\nmodule load Gaussian/g16 powertools\n# GAUSS_SCRDIR=&lt;your preferred Gaussian scratch space&gt;\n# mkdir -p ${GAUSS_SCRDIR}\n\ng16 &lt; ${InputFile} &gt; ${OutputFile}\n\n### write job information to SLURM output file\nscontrol show job $SLURM_JOB_ID \n\n# Print out resource usage  \njs -j $SLURM_JOB_ID           ### powetools command\n</code></pre> <p>where the Gaussian input file <code>g16.com</code> can be found from the\u00a0previous section Running Gaussian by Command Lines.</p> <p>For the resource request (<code>#SBATCH</code> lines) above, since Gaussian can only run parallely with shared memory in HPCC system, only 1 task (with 1 node) is requested in line 2. The number of CPUs requested in line 3 is the same as the setting of \"<code>%NProcShared</code>\" (<code>=4</code>) in the Gaussian input file. The memory request in line 4 should be larger than the setting of \"<code>%Mem</code>\" (<code>=5GB</code>) in the Gaussian input file in case the job runs out of memory. Please also make sure the walltime request in line 5 is longer enough to finish the job.</p> <p>In the command lines, you need to make sure Gaussian/g16 is loaded as in line 10. If you would like to use scratch directory other than <code>/mnt/scratch/$USER</code> for the Gaussian scratch files, you could set up a different one with line 11 and 12. The calculation of the Gaussian job is executed in line 14 with input file <code>g16.com</code> and output file <code>g16.log</code>. Once the calculation is done, line 17 and 20 will be executed to print out the job information and resource usage respectively to the SLURM output file ( with file name: <code>slurm-&lt;JobID&gt;.out</code>).</p>","title":"Gaussian Job Script"},{"location":"Gaussian_Job_with_Checkpointing_Run/","text":"<p>For running a large system with Gaussian, it usually takes a long time and many resources to complete. It is a good idea to set up checkpointing so the calculation can keep going in case of any interruption due to walltime limit or possible system malfunction. Checkpointing function can save a snapshot of a Gaussian running state so it can restart from the previous calculation. Users can also divide a long-time job into many 4-hour short jobs since jobs with walltime less than or equal to 4 hours can use the buy-in nodes (55% of all nodes) in HPCC.</p> <p>In order to have an appropriate checkpointing run with Gaussian, an unified read-write file setting (<code>%RWF</code>) should be in the Link 0 section of the input file. An example <code>water.gjf</code> is in the following:</p> <p>water.gjf</p> <pre><code>%NProcShared=2\n%Mem=3GB\n%RWF=water.rwf\n%NoSave\n%chk=water.chk\n#P opt b3lyp/aug-cc-pVTZ\n\nwater molecules\n\n0 1\nO   -2.12123400  1.99409800 -1.27381200\nH    1.52438600  0.53672100  0.67508800\nH    1.76493000 -0.81527300 -0.18137000\nO   -1.12977500 -0.31430400 -0.37860700\nH   -1.76492800 -0.81528500  0.18137200\nH   -1.52439700  0.53670800 -0.67510100\nO    2.89125300 -1.69896600 -1.06351900\nO    1.12976700 -0.31428900  0.37859300\nH    2.99568600 -1.73945400 -2.01677200\nH    3.39746100 -2.42787400 -0.69708600\nO   -2.89123000 -1.69896400  1.06353700\nH   -2.99563400 -1.73945600  2.01679300\nH   -2.43456700  2.07972500 -2.17761600\nH   -2.58174600  2.66131900 -0.75942800\nH   -3.39743400 -2.42788000  0.69711700\n</code></pre> <p>The input file requests geometry optimization of 5 water molecules with a very large basis set aug-cc-pVTZ. It will take about 25 CPU hours to finish the whole calculation. We have the setting on <code>%RWF</code> which specifies <code>water.rwf</code> file for checkpointing function besides <code>water.chk</code> file. Since the specification <code>%RWF</code> is placed before <code>%NoSave</code> line, the rwf file will be deleted if the calculation is normally completed without any error.</p> <p>In order to have several restart running after first running stops, we can build a restart Gaussian input file <code>restart.gjf</code> simply as</p> <p>restart.gjf</p> <pre><code>%NProcShared=2\n%Mem=3GB\n%RWF=waters.rwf\n%NoSave\n%chk=waters.chk\n#P Restart\n</code></pre> <p>Since all information about the calculation is recorded in the rwf file, a line with \"Restart\" is enough for Gaussian to restart from the previous job. This restart input file can also be created by the commands:</p> <pre><code>$ grep '^%' waters.gjf &gt; restart.gjf\n$ echo -e '#P Restart\\n' &gt;&gt; restart.gjf\n</code></pre> <p>where we simply \"<code>grep</code>\" the lines starting with \"<code>%</code>\" sign in <code>water.gjf</code> and put them to the Gaussian restart file with \"<code>#P Restart</code>\" line in the end.</p> <p>Now we need a job script to submit the Gaussian calculation. The script needs to keep submitting jobs to restart the previous calculation until it is completed. Here is a job script <code>water.sb</code> which can do the work:</p> <p>water.sb</p> <pre><code>#SBATCH \u2013-job-name=LongJob\n#SBATCH \u2013-ntasks=1\n#SBATCH \u2013-cpus-per-task=2\n#SBATCH --mem=5G\n#SBATCH \u2013-time=04:00:00\n\nmodule load Gaussian/g16 powertools\nOutputFile=\"water-${SLURM_JOBID}.log\"             # Gaussian output file name for each job\n\n# How many seconds before end of job to submit another\nBeforeEnd=300                                       # 5 minutes\n\n# The background script to keep job submission until calculation is completed\n(sleep $((4*60*60 - BeforeEnd))                     # sleep until the time before end of job\njs -j ${SLURM_JOBID}                                # print out resource usage\ncat ${OutputFile} &gt;&gt; water.log                      # collect Gaussian outputs into one file\necho -e \"\\n\\n====== Gaussian calculation on job ${SLURM_JOBID} stops. ======\\n\\n\" &gt;&gt; water.log\necho \"The Gaussian calculation has not completed. Submit another job to keep doing it.\"\nsbatch water.sb                                     # submit another job\nscancel ${SLURM_JOBID}  )&amp;                          # job stops if g16 command is not finished\n\n# Whether this is a restart job or not\nif [ -f water.rwf ] &amp;&amp; [ -f water.chk ]; then\n   InputFile=\"restart.gjf\"\nelse\n   InputFile=\"water.gjf\"\nfi\n\ng16 &lt; ${InputFile} &gt; ${OutputFile}\n\n# The following commands are not executed unless g16 command is completed.\n# Print out resource usage \njs -j $SLURM_JOB_ID           ### powetools command\n\ncat ${OutputFile} &gt;&gt; water.log \necho -e \"\\n\\n====== Gaussian calculation is completed on job ${SLURM_JOBID}. ======\\n\\n\" &gt;&gt; water.log\n</code></pre> <p>where a background script in <code>(---)&amp;</code> from line 14 to 20 is added to keep submitting job<code>s</code>.</p> <p>Once the job is started, the background script is running at the same time as the foreground script. The background script is in sleep for 3 hours and 55 minutes first. During this time, the foreground script runs the Gaussian calculation or restart the previous calculation if the checkpointing files (<code>water.rwf</code> and <code>water.chk</code>) exist. After 5 minutes before the end of the job, the background is awake to print out the resource usage and Gaussian output. It submits another job and stop the current running job in line 19 and 20 if the g16 command in line 29 is not completed. If the g16 command is finished before the background is awake, the job will keep executing all command lines after line 30 and finish. There will be no more job submitted.</p> <p>Since the rwf file usually takes a lot of file space, it is suggested to run checkpointing jobs in scratch space in case your home or research space is over quota. Users can create a directory in their scratch space. Copy all files (<code>water,gjf</code>, <code>restart.gjf</code> and <code>water.sb</code>) and submit their job script there. Please check your job running frequently. Make sure to copy necessary files back to home or research directory from time to time since files on scratch will be purged if they have not been modified for 45 days.</p>","title":"Gaussian Job with Checkpointing Run"},{"location":"Gaussian_on_HPCC/","text":"<p>Here is the slides pdf for the Gaussian workshop.</p>","title":"Gaussian on HPCC"},{"location":"Guidelines_for_Choosing_File_Storage_and_I_O/","text":"<p>HOME, RESEARCH and SCRATCH are referred to as networked file systems.\u00a0Each node must go through the network switch to access these spaces. <code>/tmp</code> and <code>/mnt/local</code> are locally accessible in the hard drive of each node. The space is not affected by the network and has larger size compared with the RAMDISK <code>/dev/shm</code> which is located inside the node\u2019s RAM. However, <code>/dev/shm</code> is the closest storage location for files. Files stored here take up some of the node\u2019s memory space.</p> <p>The table below provides detailed information about each type of storage\u00a0on HPCC. ($USER is your login username and GROUP is your research group name). Please use the table below to choose which file system is best for your job.\u00a0The two columns from the left are the locations with system automatic backup. The three columns from the right are the locations with system automatic purge. The column in between shows the location where it is often considered and treated by users who requested it as the same as HOME or RESEARCH space,\u00a0 but it is NOT automatically backuped!\u00a0</p>      HOME RESEARCH nodr portion of HOME/RESEARCH SCRATCH LOCAL RAMDISK     Primary Private files or data storage for each user Shared files or data storage for group users same as the standard HOME/RESEARCH Temporary large files or data storage for users and groups Temporary small files or data usage for job running same as LOCAL with very fast I/O   Access location Automatic login $HOME\u00a0 or\u00a0/mnt/home/$USER /mnt/research/GROUP /mnt/ufs18/nodr $SCRATCH or /mnt/scratch/$USER /mnt/local or /tmp (at each node) $TMPDIR (used in a job as /tmp/local/$SLURM_JOBID) /dev/shm (at each node)   Size 50GB upto 1TB, 1 million files, ($125/year for each additional TB) upto 1TB\u00a0and 1 million files ($125/year for each additional TB) as a portion of HOME or RESEARCH by user's request. No limit on the number of files 50TB and 1 million files ~400GB for intel14, ~170GB for intel16, ~400GB for intel18 \u00bd of RAM   Command to check quota quota quota quota quota #SBATCH --tmp=20gb to reserve 20gb in $TMPDIR. \u00bd of the memory requested by job   Backup Yes Yes No No No No   Purge policy No No No Yes. (Files not accessed or modified for more than 45 days may be removed) Yes (at completion of job) Yes (RAM may be reused by other jobs)   I/O Best Practice low I/O using\u00a0 single or multiple nodes Same as HOME same as HOME or RESEARCH heavy I/O on files of large size using single or multiple nodes frequent I/O operations on many files in one node frequent and fast I/O operations on small files in one node   Careful with Watch for quota. Avoid heavy parallel I/O. Same as HOME. In addition, need to set umask or file permission so files can be shared in group. Be aware of no automatic backup. May need to do backup manually by user.\u00a0 Avoid frequent I/O on many small files(&lt; 1MB), such as untarring a tar file to create many small files in a short time. Move files to HOME or RESEARCH before purge period elapses. Need to copy or move files to HOME or RESEARCH before job completes.Only local access available. Users are not able to store files in one node and gain I/O access to them from other nodes. Same as LOCAL. Request extra memory in your job script so you'll have enough space for file storage.","title":"Guidelines for Choosing File Storage and I/O"},{"location":"HPCC-Job-Submission-Workflow_40337480.html/","text":"","title":"HPCC Job Submission Workflow 40337480.html"},{"location":"HPCC-Job-Submission-Workflow_40337480.html/#teaching-hpcc-job-submission-workflow","text":"<p>Goal: Understand basic workflow for submitting jobs to the clusters.</p> <p>Task: Run hello.c on one core and one compute node.</p> <ul> <li>Login to HPCC</li> <li>Load the powertools module.</li> <li>Create a copy the helloworld getexample in your current directory.</li> <li>Change into the helloworld directory created.</li> <li>View the submission script in the helloworld directory.</li> <li>Compile the hello C program on the development node.</li> <li>Submit the script to the SLURM scheduler.</li> <li>Check the queue for your job.</li> <li>Examine your job</li> <li>Check the output of your job</li> <li>View the output of your job</li> </ul> <p>Answer \u00a0Expand source</p> <pre><code>#Login to HPCC\nssh -XY msu_netid@hpcc.msu.edu\nssh dev-intel18\n\n#Load powertools module\nmodule load powertools\n\n#Copy the helloworld getexample\ngetexample helloworld\nls -l\n\n#Change\u00a0into the helloworld directory\ncd helloworld\nls -l\n\n#View the submission script in the helloworld directory\ncat hello.sb\n\n#Compile the C program &lt;hello&gt; on the development node\ngcc hello.c -o hello\n\n#Submit the jobscript hello.sb to the SLURM scheduler\nsbatch hello.sb\n\n#Check the queue for your job\nqstat \u2013u $USER\n\n#Examine your job\nscontrol &lt;job_id&gt;\n\n#View the output of your job\nless &lt;slurm-jobid.out&gt;\n</code></pre>","title":"Teaching : HPCC Job Submission Workflow"},{"location":"HPCC_File_Systems_Overview/","text":"<p>HPCC provides a variety of secure\u00a0file storage options\u00a0for research data and fast connections for high-speed file communication (I/O). Users have access to\u00a0replicated high capacity storage\u00a0that can be shared among group members or remain private to each user. Additionally,\u00a0high performance storage is\u00a0available for temporarily staging data that needs to be access quickly. Moreover, each compute node also has an attached local disk for data intensive jobs. Storage options include:</p>","title":"HPCC File Systems Overview"},{"location":"HPCC_File_Systems_Overview/#ufs18-home-directory","text":"<p>All users are given a home directory to store your research located /mnt/home/$USER on every node, where the environment variable $USER stores the login account name. \u00a0 Each new user starts with a 50 GB limit for file storage space and each space can contains less than 1 million files. You can request and increase this up to 1TB by completing the Quota Increase Request form. Storage space greater than 1TB is available for an annual fee, paid through a MSU financial account.</p> <p>Home directories are automatically backed-up except the files saved in <code>nodr</code> space.\u00a0 To access file backup, please submit a ticket. Please mention the paths of the folders or filenames with the time frame you would like to be restored. For more information about checking quota and increasing limit on number of files, please see ufs18 file system.</p>","title":"ufs18: Home Directory"},{"location":"HPCC_File_Systems_Overview/#ufs18-research-space","text":"<p>A 1 TB block of space to be shared among members of a research group may be obtained if a MSU PI can provide adequate justification. Additional space may also be purchased.  Files on this system, located at /mnt/research/[groupname], are also backed up (except the files saved in <code>nodr</code> space). Both /mnt/home and /mnt/research are stored in /mnt/ufs18. To understand the quota setting on research space, please refer to ufs18 file system.</p>","title":"ufs18: Research Space"},{"location":"HPCC_File_Systems_Overview/#local-disk-tmp-mntlocal-devshm","text":"<p>Each cluster node has it's own disk drive, called the \"local\" disk, and between 100-300 GB of \"scratch\" space is available.\u00a0\u00a0 This is local to each node, not networked and hence is not accessible from other nodes (e.g. for multi-node jobs). This space is transient, volatile storage optimized for smaller-scale I/O. \u00a0It is available as a folder /mnt/local in which you can create folders during your job, but also as a special folder created for the job and in the $TMPDIR environment variable.\u00a0 Files in $TMPDIR are only available during the job, so your script must copy files before the job ends.\u00a0 Files may be stored for a maximum of 8\u00a0days on\u00a0/mnt/local.\u00a0 This space is regularly and routinely erased to ensure a maximum amount of free space for users.</p>","title":"Local Disk: /tmp, /mnt/local &amp; /dev/shm"},{"location":"HPCC_File_Systems_Overview/#gs18-ls15-scratch-space","text":"<p>In addition to home and research files, we have file systems for temporary work that we call \"scratch\" available to users.\u00a0 Two Scratch File Systems are installed in the HPC system. gs18 is based on the IBM General Parallel File System (GPFS) and located at /mnt/gs18 with a capacity of 834 TB. ls15 uses Lustre technology and locates at /mnt/ls15 with a capacity of 1.9 PB. Research groups can also have their scratch folders available. If you would like a folder for your research group please request one. \u00a0</p> <p>The parallel file systems are designed for heavy reading and writing of large files (I/O). Both of them are connected to all compute nodes with infiniband and can sustain very high data transfer rates.\u00a0\u00a0 For many types of jobs, it's much faster than using your home or research folders.\u00a0\u00a0 However, jobs that read and write many small files are not suitable for ls15 system.\u00a0\u00a0 For those type of jobs you can explore using $TMPDIR (/mnt/local) or /mnt/ffs17 (Flash Drive).\u00a0 Please contact us for more information if you find the Scratch file system is too slow for your work.</p> <p>Unlike the home and research\u00a0directories, the scratch spaces are temporary file systems and not intended for long-term storage so there is no back-up. Files are automatically purged after 45 days if they are not modified. The quota of the space for each user is limited to total 50TB and less than 1 Million files.</p> <p>Because the scratch space is intended for job running, it cannot be accessed from the gateway node, only the development nodes and file transfer gateway. \u00a0That is, after you log in to the gateway (ssh hpcc.msu.edu ) and then try to access scratch you will receive an error \u00a0\"No such file or directory.\" \u00a0 To access scratch, please connect to a development node (e.g. ssh dev-intel16). \u00a0Your scratch folder is also available to all of the compute nodes and hence your jobs. \u00a0 \u00a0 To be able to transfer files on your scratch folder to/from hpcc, you may use our file transfer gateway, which has the host name\u00a0**rsync.hpcc.msu.edu. See\u00a0Transferring Files to the HPCC for more information.</p> <p>Exceeding your quota will\u00a0'lock\u00a0out' your directory\u00a0and you will not be able to\u00a0delete or create new files in your\u00a0folder.\u00a0To request additional space visit:\u00a0https://contact.icer.msu.edu/contact.</p>","title":"gs18 &amp; ls15: Scratch Space"},{"location":"HPCC_File_Systems_Overview/#fs-07-optsoftware-optmodules","text":"<p>This is where HPCC installs application software (binaries and code) and module files. It is mostly readable and executable for all users.</p>","title":"fs-07: /opt/software &amp; /opt/modules"},{"location":"HPCC_File_Systems_Overview/#additional-storage-options","text":"<p>For MSU researchers, up to 1TB of secure storage can be allocated per PI for free. Backup of data files are stored off-site. Additional storage can be purchased at an annual rate of\u00a0$125/TB. To make a storage increase request greater &gt; 1TB, please complete the Increase Digital File Storage Form.</p> <p>To find out how to use the above file systems and which one to choose. please refer to the file systems page.</p>","title":"Additional Storage Options"},{"location":"HPC_Concept/","text":"","title":"HPC Concept"},{"location":"HPC_Concept/#why-use-hpcc","text":"<p>A comparison between your personal computer (PC) and ICER's HPCC</p>","title":"Why Use HPCC"},{"location":"HPC_Concept/#parallel-computing","text":"<p>An introduction to three basic parallel models: Shared Memory, Distributed Memory and Hybrid</p>","title":"Parallel Computing"},{"location":"HPC_Concept/#hpc-glossary","text":"<p>Definitions for commonly used HPC terminology</p>","title":"HPC Glossary"},{"location":"HPC_Glossary/","text":"<p>Program \u2013 code stored on a computer intended to fulfill a certain task</p> <ul> <li>There are many types of programs:<ul> <li>Part of the operating system and help computer function</li> <li>Fulfill a particular job are called applications</li> </ul> </li> <li>Typically stored on disk (g., hard drive)</li> <li>A program needs memory and various operating system resources     (g., peripheral interfaces) to run</li> </ul> <p>Operating System \u2013 manages all resources needed for a program (e.g., macOS)</p> <p>Process \u2013 program with all necessary resources loaded into memory</p> <ul> <li>When a program is run, it is loaded into memory which makes it     accessible for processing by the computer\u2019s central processing unit     (CPU)</li> <li>There can be multiple instances of a single program, and each     instance of that running program is a process</li> <li>Each process has a separate memory address space, which means that a     process runs independently and is isolated from other processes</li> <li>This independence of processes is valuable so that a problem with     one process cannot cause havoc with another process</li> </ul> <p>Central processing unit (CPU) - logical hardware unit capable of executing a single process (i.e., gets instructions then performs calculations)</p> <ul> <li>Made up of:<ul> <li>Processor is a device that processes program instructions to     manipulate data</li> <li>Socket is an array of pins that connect processor to     motherboard</li> </ul> </li> <li>Individual CPU processors now contain multiple cores for more     efficient multi-tasking and parallel computing<ul> <li>Core is the smallest hardware unit capable of performing a     processing task</li> <li>Ex: dual-core processor has two cores</li> </ul> </li> </ul> <p></p> <p></p> <p>Thread (of execution) is the smallest set of programmed instructions that can be managed independently by an operation system. In general, one thread is handled by one core.</p> <p>As video gaming popularity increased, so did the need for more computing power. To accomplish this a CPU can work with a graphics processing unit (GPU), usually found on a graphics card docked into the motherboard, to quickly render high-resolution images and video concurrently. A GPU gets its intense computing power from hundreds of smaller cores capable of crunch application data in parallel. Multiple GPUs can be installed on one graphics card or multiple graphics card can be installed in one node to further improve computation power through parallelism. After GPUs became popular for gaming, they were made fully programmable to be useful in processing big science data. The resulting general-purpose graphics processing unit (GPGPU) is used extensively in supercomputing to increase speed and improve analysis of scientific data.</p> <p>Node is a single computer comprised of 1+ CPUs, memory, network interfaces, etc.</p> <p>Cluster is a group of nodes networked together so that a program can run on them in parallel</p> <p>Parallel computing is an umbrella term describing the use of multiple computers or computers made up of multiple processors in combination to solve a single problem</p> <p>Within HPCC there are different types of nodes:</p> <ul> <li>Gateway nodes are nodes used to enter the computer system:<ul> <li>Login - login and non-intensive compute tasks (e., moving     files)</li> <li>rsync - data transfer to/from HPCC</li> </ul> </li> <li>Development (dev) nodes are nodes used to navigate file systems     and compile, test, and schedule heavy computational tasks (e.,     jobs)</li> <li>Compute nodes are clusters that perform scheduled jobs</li> <li>Accelerator nodes are nodes equipped with accelerated cards     (g., GPU or phi nodes)</li> </ul> <p>Secure Shell (SSH) - network protocols and implementing suite of utilities that provide a secure way to access and execute commands on a remote computer over an unsecured network</p> <p>Remote synchronization (rsync) - software utility for Linux systems that efficiently sync files and directories between two hosts or machines making ideal for transferring large files</p> <p>File system \u2013 tree-like directory organization for storing many files</p>","title":"HPC Glossary"},{"location":"HPC_Glossary/#for-more-information-on-these-terms-check-out-the-following-videos","text":"<p>What is ICER\u2019s HPCC (11min)</p> <p>HPCC System Layout (7min)</p>","title":"For more information on these terms, check out the following videos:"},{"location":"HPC_Tutorial_Series/","text":"","title":"HPC Tutorial Series"},{"location":"HPC_Tutorial_Series/#connect-over-ssh-with-editors","text":"","title":"Connect over SSH with editors"},{"location":"HPC_Tutorial_Series/#containers-docker-and-singularity","text":"","title":"Containers (Docker and Singularity)"},{"location":"HPC_Tutorial_Series/#learning-the-shell","text":"","title":"Learning the Shell"},{"location":"HPC_Tutorial_Series/#linux-command-line-interface-for-beginners-i","text":"","title":"Linux Command Line Interface for Beginners I"},{"location":"HPC_Tutorial_Series/#linux-command-line-interface-for-beginners-ii","text":"","title":"Linux Command Line Interface for Beginners II"},{"location":"HPC_Tutorial_Series/#makefile","text":"","title":"Makefile"},{"location":"HPC_Tutorial_Series/#python-on-hpc","text":"","title":"Python on HPC"},{"location":"HPC_Tutorial_Series/#regular-expressions","text":"","title":"Regular Expressions"},{"location":"HPC_Tutorial_Series/#using-ddd","text":"","title":"Using DDD"},{"location":"HPC_s_entire_layout_at_ICER/","text":"<ul> <li>Users can use MSU's HPCC resources by first connecting to     gateway\u00a0<code>gateway.hpcc.msu.edu</code></li> <li>gateway and rsync\u00a0are the only two nodes directly accessible     to the internet.\u00a0<ul> <li>\"gateway\" is not meant for running software, connecting to     scratch space or compute nodes.\u00a0</li> <li>\"rsync\" is mainly for file transfer and is able to connect to     scratch but is not able to access to compute nodes.</li> </ul> </li> <li>From the gateway node, users can connect to any development node to     compile their jobs or and run short tests.</li> <li>Each development node runs the same operating system and uses the     AMD64/Intel64 instruction set. Compiled software should be \"binary     compatible\" across the cluster, if you do not compile with     architecture-specific tuning (e.g. -march or -x.) For example, if     you compile your code on dev-intel14, it can mostly run on the     intel16 cluster.</li> <li>A detailed description of our hardware can be     found\u00a0on the page of Cluster Resources.</li> </ul> <p></p>","title":"HPC's entire layout at ICER"},{"location":"HTSeq%202/","text":"<p>HTSeq is a Python package for analysis of high-throughput sequencing data. It's available upon a fresh login to a dev-node. For example, you can call command <code>htseq-count</code> directly without needing to load specific modules. This is because HTSeq is installed to the default Python distribution (you can run module list\u00a0to see the list of modules loaded by default).</p> <p>If you happen to run a module purge\u00a0command in the middle of your work, you need to load the default Python back again in order to use HTSeq. To do so:</p> <pre><code>module purge\nmodule load GCC/6.4.0-2.28 OpenMPI/2.1.2 Python/3.6.4\n</code></pre> <p>Tip: if you want to go back to the original shell environment immediately after your login, you can run: <code>exec bash -l</code></p>","title":"HTSeq"},{"location":"HTSeq/","text":"<p>HTSeq is a Python package for analysis of high-throughput sequencing data. It's available upon a fresh login to a dev-node. For example, you can call command <code>htseq-count</code> directly without needing to load specific modules. This is because HTSeq is installed to the default Python distribution (you can run module list\u00a0to see the list of modules loaded by default).</p> <p>If you happen to run a module purge\u00a0command in the middle of your work, you need to load the default Python back again in order to use HTSeq. To do so:</p> <pre><code>module purge\nmodule load GCC/6.4.0-2.28 OpenMPI/2.1.2 Python/3.6.4\n</code></pre> <p>Tip: if you want to go back to the original shell environment immediately after your login, you can run: <code>exec bash -l</code></p>","title":"HTSeq"},{"location":"Home_Space/","text":"<p>Each user account is given a home space for personal file storage. By default,\u00a0It is set only accessible to the user and located at <code>/mnt/home/$USER</code> for every node, where <code>$USER</code> is the environment variable of the user's login name. It is often referred as \u201chome directory\u201d since this is the beginning directory after login of any HPCC node.</p> <p>Every home space starts with a 50 GB limit for file storage space and can not contain more than 1 million files. You can request and increase this up to 1TB by completing Quota Increase Request form. Storage space greater than 1TB is available for an annual fee paid through a MSU financial account. Users can find the fee and submit their request by Large Quota Increase Request form. To find out the quota and used space of your home directory, please check Space quota section. If you would like to store more than 1 million files in your home space, please refer to the following section Limit on number of files.</p> <p>All home directories are stored in the IBM General Parallel File System (GPFS) mounted on /mnt/ufs18. It is automatically backed-up except files saved in <code>nodr</code> space. To restore any file from backup, please submit a ticket and let us know the paths to the files or the directory with the time frame you would like them restored. To learn more information, please check the documentation of ufs18 file system.</p> <p>For the system security and user data privacy, we recommend that users do NOT open HOME directory access permission to others. When you report an issue about files saved in HOME, please attach them to your message for reference. ICER staffs do not access any files or directories in your home directory.</p>","title":"Home Space"},{"location":"Home_Space/#incorrect-used-space","text":"<p>Currently our home file system check quota function will sometimes cause a user's directory\u00a0over the quota due to incorrect calculation of used space .\u00a0 If you see this please open a ticket and we will work with you to temporarily increase your quota.\u00a0 We continue to work with our vendor to correct this issue.</p>","title":"Incorrect Used Space"},{"location":"Home_Space/#space-quota","text":"<p>The only way to get quota information of home space is to run the command <code>quota</code>\u00a0:</p> <pre><code>$ quota\nhome directory: Space    Space   Space     Space     Files     Files     Files     Files        \n                 Quota    Used    Available % Used    Quota     Used      Available % Used\n-----------------------------------------------------------------------------------------------\n/mnt/home/$USER  50G      32G     18G       64%       1048576   432525    616051    59%\n</code></pre> <p>where all file spaces accessible to the user are listed, including home, research, scratch and ffs17. In each space, the information of quota, usage and availablility on space size and number of files can be found. If \"Free\" or \"Available\" is a negative value (such as the \"Space Available\" column in the above example), the usage is over the quota, please remove, transfer or compress some files so the used space or the file count can be lower than the \"Quota\" value. Since GPFS uses a different compression algorithm, you may notice higher space size after files are copied to the ufs18 file system.</p>","title":"Space quota"},{"location":"Home_Space/#actual-disk-usage-du-different-from-quota-results","text":"<p>The new file system has a smallest file block size of 64k. This means that files between 2K and 64K will occupy 64K of space. This causes space usage inflated greatly for users with large amounts of small files. One suggested solution would be to compress many small files into one large file (larger than 64K). in this way, the number of files can also be reduced. If you still have any difficulty, a temporarily larger quota can be requested if your quota is at 1T with many small files.</p>","title":"Actual disk usage (du) different from quota results"},{"location":"Home_Space/#limit-on-number-of-files","text":"<p>Besides the quota on the size of space, users are also limited to 1 million files in their home or research directory. We need to set this limit because, with a great number of files, the file system will spend too much time on back-up to be able to function normally. If possible, users can compress many files into one to reduce the file number.</p> <p>If users do not wish to have the limit, they can request to have part of their home (or research space) moved under <code>nodr</code> space (<code>/mnt/ufs18/nodr/</code> or <code>/mnt/ufs18/nodr/research/</code>) where there is no limit on the file count yet no back-up on the files either. Users will be responsible for their own back-up in the <code>nodr</code> space.</p> <p>By default, one half of the space quota is assigned to the requested <code>nodr</code> space. The quota of the original space under <code>/mnt/home/</code> (or <code>/mnt/research/</code>) is then donwsized to its half so the total space quota remains the same. A different size of <code>nodr</code> space can also be assigned according to user's request. Once this space is created, the path and the quota information can be found by the <code>quota</code> command mentioned above.</p> <p>All home space files are periodically, automatically backed up (except  those files that a user has opted to store in a specially requested  <code>nodr</code> space).\u00a0To access file backups, please submit a  help ticket containing the file paths and the period i.e., the time frame, from  which the files should be restored.  </p>","title":"Limit on number of files"},{"location":"How_Jobs_are_Scheduled/","text":"<p>SLURM schedules jobs in two ways: the main scheduler and the backfill scheduler. The main scheduler constantly tries to start high priority jobs. The backfill scheduler considers all jobs, and starts any jobs that won't defer the start time of a higher priority job.</p>    Scheduler Function When it Runs Run Time     Main Launches high priority jobs that can start immediately. Stops evaluating jobs once it encounters a job that cannot be started.\u00a0 About every 2 seconds 0.08-2 seconds   Backfill Evaluates the entire queue. Launches jobs that won't interfere with the start time of a higher priority job. Sets jobs'\u00a0StartTime and SchedNodeList. 20 seconds after the last backfill cycle completes 2-15+ minutes","title":"How Jobs are Scheduled"},{"location":"How_Jobs_are_Scheduled/#starttime-and-schednodelist","text":"<p>The backfill scheduler sets the StartTime and SchedNodeList parameters on jobs that can start within the next 7 days. These parameters can be viewed in the output of \"scontrol show job <code>&lt;jobid&gt;</code>\". StartTime estimates when a job will start and SchedNodeList shows the nodes this job might start on. StartTime is only an estimate. These values are updated every time the backfill scheduler runs and may change as running jobs complete and new jobs are submitted.</p>","title":"StartTime and SchedNodeList"},{"location":"How_Jobs_are_Scheduled/#minimum-job-requirements-to-avoid-deferment","text":"<p>Jobs must meet certain criteria before the backfill scheduler will avoid potentially deferring them through starting lower priority jobs. These thresholds allow the backfill scheduler to cycle faster and maintain high system utilization.</p>    Criteria Minimum Description     Priority 3000 Jobs require a minimum priority of 3000 is require to avoid potential deferment in scheduling. Buy-in account jobs are never below this threshold.   Age 30 minutes Jobs must be queued for at least 30 minutes to avoid potential deferment in scheduling. This applies to all jobs.","title":"Minimum Job Requirements to Avoid Deferment"},{"location":"How_Jobs_are_Scheduled/#job-priority-factors","text":"<p>A jobs priority is determined by a combination of several priority factors. Age, size, fairshare, and whether it was submitted to a buy-in account all contribute to the job\u2019s priority.</p>    Priority Factor Description Maximum Contribution to Priority     Age Starts at zero at job submission, then increases linearly to a maximum of 60000 after 30 days 60000 after 30 days   Fairshare Starts at 60000 and decreases and users' recent usage goes up. Usage for this calculation is decayed 50% each day 60000 for no recent cluster usage   Size Scales linearly with the amount of CPU and memory requested by a job. 100 per CPU, 20 per GB. 52000+ depending on memory requested   QOS Adds 3000 to buy-in jobs to ensure they are always above backfill schedulers minimum priority for reserving resources 3000","title":"Job Priority Factors"},{"location":"How_to_find_and_load_software_modules/","text":"","title":"How to find and load software modules"},{"location":"How_to_find_and_load_software_modules/#general-search-using-module-spider","text":"<p>To search for a particular software module (say ABC), you would run</p> <pre><code>module spider ABC # can also be abc, ABc... since the name is case-insensitive\n</code></pre> <p>Once you find it, and want to load a specific version (say 1.1.1), run</p> <pre><code>module spider ABC/1.1.1 # should only be ABC\n</code></pre> <p>The resulting output information will tell you what prerequisites modules are needed before loading your <code>ABC/1.1.1</code>. You don't need to know the full name of the software. See below (note the third case about PCRE). </p> <pre><code>$ module spider sam\n\n--------------------------\n  SAMtools:\n--------------------------\n    Description:\n      SAM Tools provide various utilities for manipulating alignments in\n      the SAM format, including sorting, merging, indexing and generating\n      alignments in a per-position format.\n\n     Versions:\n        SAMtools/0.1.19\n        SAMtools/1.5\n        SAMtools/1.7\n        SAMtools/1.8\n        SAMtools/1.9\n\n--------------------------\n  For detailed information about a specific \"SAMtools\" module (including how to load the modules) use the module's full name.\n  For example:\n\n     $ module spider SAMtools/1.7\n--------------------------\n\n\n\n$ module spider amtool\n\n--------------------------\n  BamTools:\n--------------------------\n\n    Description:\n      BamTools provides both a programmer's API and an end-user's toolkit\n      for handling BAM files.\n\n     Versions:\n        BamTools/2.4.1\n        BamTools/2.5.1\n\n\n--------------------------\n  SAMtools:\n--------------------------\n\n    Description:\n      SAM Tools provide various utilities for manipulating alignments in\n      the SAM format, including sorting, merging, indexing and generating\n      alignments in a per-position format.\n\n     Versions:\n        SAMtools/0.1.19\n        SAMtools/1.5\n        SAMtools/1.7\n        SAMtools/1.8\n        SAMtools/1.9\n\n\n$ module spider PCRE\n\n--------------------------\n  PCRE:\n--------------------------\n    Description:\n      The PCRE library is a set of functions that implement regular\n      expression pattern matching using the same syntax and semantics as\n      Perl 5.\n\n     Versions:\n        PCRE/8.38\n        PCRE/8.39\n        PCRE/8.40\n        PCRE/8.41\n     Other possible modules matches:\n        PCRE2\n\n--------------------------\n  To find other possible module matches execute:\n\n      $ module -r spider '.*PCRE.*'\n\n--------------------------\n</code></pre>","title":"General search using module spider"},{"location":"How_to_find_and_load_software_modules/#example-of-loading-r","text":"<p>In this example, we are looking for available versions of R. </p> <pre><code>$ ssh dev-intel18\n$ module spider R\n-----------\n  R:\n-----------\n    Description:\n      R is a free software environment for statistical computing and graphics.\n\n     Versions:\n        R/3.3.1\n        ...\n        ...\n        R/4.0.2\n        R/4.0.3\n        R/4.1.0\n        R/4.1.2\n</code></pre> <p>Suppose we want to load R/4.1.2, and in order to know how, we run</p> <pre><code>$ module spider R/4.1.2\n\n    Description:\n      R is a free software environment for statistical computing and graphics.\n\n\n    You will need to load all module(s) on any one of the lines below before the \"R/4.1.2\" module is available to load.\n\n      GCC/11.2.0  OpenMPI/4.1.1\n</code></pre> <p>Above output tells us to load two prerequisites (GCC/11.2.0 and OpenMPI/4.1.1) before loading R. Therefore, we'll run</p> <pre><code>module purge # a MUST-HAVE\nmodule load GCC/11.2.0  OpenMPI/4.1.1\nmodule load R/4.1.2\n</code></pre> <p>You are all set. Note that \"module purge\" is always needed before you start loading a different software module.</p>","title":"Example of loading R"},{"location":"How_to_find_and_load_software_modules/#advanced-skill-searching-for-modules-in-module-file-hierarchy-using-module-avail","text":"<p>If you start with a minimum set of loaded modules (most commonly a compiler-MPI pair, or a compiler alone), and want to know what software packages are available to load in the current <code>MODULEPATH</code> (run <code>echo $MODULEPATH</code>to see the paths), you can run <code>module avail</code>. <code>module avail</code> is different from <code>module spider</code> above (which lists all possible modules, not just the modules that can be seen in the current <code>MODULEPATH</code>). To check availability of a particular module, use <code>module avail keyword.</code> If the keyword (such as \"openmpi\") is long and distinct, the search result would normally be clean. However, for \"R\" for example, a single letter that can appear in almost any module names, we need to use regular expression to fully specify the pattern when running module avail. See below.</p> <pre><code>$ module purge\n$ module load GCC/8.3.0 OpenMPI/3.1.4\n$ module avail # a large amount of output will be printed on your screen and they are omitted here.\n\n# Now we search for available R versions under the current GCC-OpenMPI pair.\n$ module -r avail '^R$'\n--------------------------------------- /opt/modules/MPI/GCC/8.3.0/OpenMPI/3.1.4 ----------------------------\n   R/3.6.2    R/3.6.3    R/4.0.2.bak    R/4.0.2.test    R/4.0.2    R/4.1.0 (D)\n\n  Where:\n   D:  Default Module\n</code></pre>","title":"Advanced skill: Searching for modules in module file hierarchy using \"module avail\""},{"location":"How_to_find_and_load_software_modules/#troubleshooting","text":"<p>Sometimes, \"<code>module spider</code>\" doesn't work because your personal module cache is out of date. To clear it, do <code>rm -r ~/.lmod.d/.cache</code></p>","title":"Troubleshooting"},{"location":"ICER_HPC_Classroom_Support/","text":"<p>This document provides a defined pathway for instructors at MSU to utilize ICER/HPCC resources for classroom education.</p>","title":"Classroom Support"},{"location":"ICER_HPC_Classroom_Support/#services-available","text":"<p>ICER will work with instructors to provide the following services during the class:</p> <ul> <li> <p>Training workshops on using the HPCC, via our asynchronous Desire2Learn-based training modules.</p> </li> <li> <p>HPCC student accounts and research space for use in classes and for sharing data files</p> </li> <li> <p>Access to OnDemand, a web-based portal to use Python via Jupyter notebooks,  RStudio, Matlab, Stata, and an interactive Linux desktop. OnDemand has access to all of the HPCC's file systems.</p> </li> <li> <p>Software installation for teaching purpose</p> </li> <li> <p>Reservations for computing resources in the cluster so that the students can instantly have access to the resources to finish the assignments</p> </li> <li> <p>Helpdesk tickets (submitting your questions via our online form)</p> </li> </ul>","title":"Services Available"},{"location":"ICER_HPC_Classroom_Support/#request-classroom-support","text":"<p>Requests should be submitted two weeks in advance, to allow for time for account creation, specialized software installation, etc.</p>","title":"Request Classroom Support"},{"location":"ICER_HPC_Classroom_Support/#request-individual-student-user-accounts","text":"<p>Instructors need to submit to New Account Request online form for all students enrolled in the course. Please note that instructors are also responsible for requesting the accounts be closed when students are no longer enrolled in the course or when the course is finished. The following information should also be included in the request.</p> <ul> <li>MSU NetIDs of students enrolled in the course</li> <li>Course name for the group name of the student accounts</li> <li>Software installation, CPU/GPU core reservation (as needed)</li> <li>Start and end dates of the course</li> </ul>","title":"Request individual student user accounts"},{"location":"ICER_HPC_Classroom_Support/#request-a-research-space-for-course-use","text":"<p>Instructors can also submit a Research request to create a research space for the course group and add all students enrolled in the request form.</p>","title":"Request a research space for course use"},{"location":"ICER_HPC_Classroom_Support/#note-on-storage-quota","text":"<p>Both the home directory of a user and the research directory of a group are initialized with 50 GB storage quota. Additional space up to 1 TB may be requested for home directory or research space. Beyond 1 TB, ICER will need to charge the additional storage.</p>","title":"Note on storage quota"},{"location":"ICER_HPC_Classroom_Support/#class-account-agreement","text":"<p>Instructors are expected to make good faith efforts towards implementing the following policies:</p> <ul> <li> <p>Awareness of ICER and MSU policies -- Instructors using ICER     resources are expected to have an understanding of ICER and MSU IT     policies. Broadly, they should be cognizant of MSU's data sharing     policies, wait times on queued jobs and the possibility of     unscheduled outages. These factors should be taken into     consideration while designing assignments and projects that utilize     ICER resources. ICER/HPCC policies are emailed to users upon     account creation.</p> </li> <li> <p>Contacting ICER -- Instructors should provide clear policies in     their syllabus about when students should contact ICER staff. \u00a0ICER     will provide account, hardware and system software support. Very limited applications software support for the course     TAs and instructors is available. However, so as not to overwhelm     the ticketing system with course-specific questions, we ask that all     student questions be routed through the TA or course instructor who     will then determine whether to forward these to ICER's ticketing     system. While students are encouraged to visit ICER research     consultants during office hours, these hours are meant for research     support and are not designed to be used as a means of TA support. In     particular, students should be aware that submitting queries to ICER     that seek answers to homework problems will be considered cheating     and a violation of the Honor Code.</p> </li> <li> <p>Planning for outages -- ICER resources may become unavailable as     a result of an unscheduled system outage. Instructors are advised     not to depend on ICER resources for final exams and/or projects that     require a short turnaround time.</p> </li> <li> <p>Storing data -- Instructors should advise students against     storing data on the \"scratch\" space. Files are typically purged on     scratch after 45 days if no modification has been made, and cannot be recovered. Instructors may     request a research space for students to store their class related     data for the duration of the semester. Students may also store their     data in their home directory. Accounts for course work are typically     limited to 50 GB.</p> </li> <li> <p>Terminating educational accounts -- Student HPCC accounts, linked     home folders, and group folders created for courses will be removed     30 days after the end of the class. This applies only to     education-sponsored accounts and NOT research-sponsored student     accounts. However, students who wish to convert their     educated-sponsored account to a research-sponsored account after the     course is completed must have their research supervisor submit a     request for membership change no later than 30 days after the     semester in which the course was completed in order to retain the     data saved in the education-sponsored student account.</p> </li> <li> <p>Acknowledging ICER -- Instructors and students are encouraged to     acknowledge the use of ICER/HPCC upon publication of data related to the     resources used during the course.</p> </li> </ul> <p>Last updated: Aug 2022</p>","title":"Class Account Agreement"},{"location":"Information_for_Central_Michigan_University_and_Western_Michigan_University_Users/","text":"<p>Note</p> <p>This information is for users at Central Michigan University and Western Michigan University. Users with MSU NetIDs should disregard this information.</p>","title":"Information for Central Michigan University and Western Michigan University Users"},{"location":"Information_for_Central_Michigan_University_and_Western_Michigan_University_Users/#contacting-the-hpcc-for-help","text":"<p>CMU Users: Please email cmichhelp@hpcc.msu.edu for help with the HPCC systems.</p> <p>WMU Users: Please email\u00a0wmichhelp@hpcc.msu.edu for help with the HPCC systems.</p>","title":"Contacting the HPCC for Help"},{"location":"Information_for_Central_Michigan_University_and_Western_Michigan_University_Users/#changing-your-password","text":"<p>First, connect to the HPCC using your credentials (for more information, see Connect to the HPCC):</p> <pre><code>ssh userid@hpcc.msu.edu\n</code></pre> <p>After logging into HPCC gateway, ssh to one of the development nodes. Use the \"passwd\" command on a dev node:</p> <pre><code>login as: cmichtestCMICH\nUsing keyboard-interactive authentication.\nPassword:\n                /_// ___// ___// _  ;    / /_/ // _  ;/ ___// ___/\n               / // /__ / _/_ / _,.'    / __  // _,.'/ /__ / /__\n              /_//____//____//_//_!    /_/ /_//_/   /____//____/\n________________________________________________________________________________\n\n          Welcome to Michigan State's High Performance Computing Center\n                    ** Unauthorized access is prohibited **\n________________________________________________________________________________\n\n    For GPU development please use green nodes.\n\n          Development Nodes (usage)\n      --------------------------------\n    dev-amd20-v100 (low) dev-amd20 (low)\n    dev-intel14-k20 (low) dev-intel14 (high)\n    dev-intel16-k80 (low) dev-intel16 (high)\n    dev-intel18 (low)\n________________________________________________________________________________\n[cmichtestCMICH@gateway-01 ~]$ ssh dev-amd20\n===\nPlease note that processes on development nodes are limited to two hours of\nCPU time; for longer-running jobs, please submit to the queue.\n\nDevelopment nodes are a shared system; for information about performance\nconsiderations please see: https://wiki.hpcc.msu.edu/x/N4JnAg\n===\n[cmichtestCMICH@dev-amd20 ~]$ passwd\nChanging password for cmichtestCMICH.\nEnter login(LDAP) password: # enter your old password\nNew Password: # enter your new password\n</code></pre> <p>Password limitations: External user passwords are limited to eight characters. The system will check the password for common problems and reject any insecure choices.</p> <pre><code>Reenter New Password:\n</code></pre> <p>Reenter your password.</p> <pre><code>LDAP password information changed for cmichtestCMICH\n</code></pre>  <p>Note</p> <p>It may take up to a hour for the password change to be propagated to all LDAP servers.</p>","title":"Changing your password"},{"location":"Information_for_Central_Michigan_University_and_Western_Michigan_University_Users/#using-cmu-nodes","text":"<p>Your account automatically has access to CMU nodes. The actual nodes assigned may vary depending on hardware availability.</p>","title":"Using CMU nodes"},{"location":"Installing_Local_Perl_Modules_with_CPAN/","text":"<p>CPAN\u00a0is a convenient way to build and install perl modules, but many people have difficulty knowing how to do this if they lack \"root\" permissions. \u00a0This tutorial will demonstrate how to install Perl modules to a local user space using CPAN.</p>","title":"Installing Local Perl Modules with CPAN"},{"location":"Installing_Local_Perl_Modules_with_CPAN/#procedure","text":"<p>First start the CPAN shell:</p> <pre><code>$ cpan\n\nTerminal does not support AddHistory.\n\ncpan shell -- CPAN exploration and modules installation (v1.9402)\n\nEnter 'h' for help.\n\ncpan[1]&gt;\n</code></pre> <p>Next determine where you want to install your local Perl modules. Let's assume we are going to place them in: /mnt/home/myUid/perlmods</p> <p>Now from within the CPAN shell, enter the following three (3) commands:</p> <pre><code>cpan[1]&gt; o conf mbuildpl_arg \"--install_base /mnt/home/myUid/perlmods\"\n\n     mbuildpl_arg       [--install_base /mnt/home/myUid/perlmods]\n\n   Please use 'o conf commit' to make the config permanent!\n\n\ncpan[2]&gt; o conf makepl_arg \"PREFIX=/mnt/home/myUid/perlmods\"\n\n    makepl_arg         [PREFIX=/mnt/home/myUid/perlmods]\n\n  Please use 'o conf commit' to make the config permanent!\n\n\ncpan[3]&gt; o conf prefs_dir \"/mnt/home/myUid/.cpan/prefs\"\n\n    prefs_dir          [/mnt/home/myUid/.cpan/prefs]\n\n  Please use 'o conf commit' to make the config permanent!\n</code></pre> <p>If you want to make the settings above permanent, enter \"o conf commit\". \u00a0Otherwise, bear in-mind you will need to reset this value every time you restart CPAN. If you do make the setting permanent, you can always change it later and re-commit as shown above.</p> <p>Now to install a module (lets assume we want to build \"Math::GMP\") simply enter:</p> <pre><code>cpan[4]&gt; install Math::GMP\n</code></pre> <p>Respond to any prompts for information that might be requested. When you are finished, enter:</p> <pre><code>cpan[5]&gt; quit\n</code></pre> <p>Setting the PERL5LIB Path</p> <p>Now that you've successfully installed a local Perl module, you will need to tell Perl where to find them. \u00a0This can be easily accomplished by setting the environmental path variable \"PERL5LIB\". For example:</p> <pre><code>$ export PERL5LIB=/mnt/home/myUid/perlmods:$PERL5LIB\n</code></pre> <p>You can add this export to your .bashrc file if you'd like to ensure it is always loaded upon login. In addition, for any scripts that you write that utilize these local Perl modules that you run on the HPCC cluster, you should add this export statement to your job script, or create a\u00a0custom user module\u00a0that does that for you, and which can be loaded from within your job script.</p>","title":"Procedure"},{"location":"Installing_TensorFlow_1.x_with_virtualenv/","text":"","title":"Installing TensorFlow 1.x with virtualenv"},{"location":"Installing_TensorFlow_1.x_with_virtualenv/#installation","text":"<p>To install TensorFlow (the latest stable release) in a python virtual environment, follow the steps below. </p> <p>Install TF 1.13.1</p> <pre><code>module purge\nmodule load GCC/6.4.0-2.28  OpenMPI/2.1.2\nmodule load CUDA/10.0.130 cuDNN/7.5.0.56-CUDA-10.0.130 \nmodule load Python/3.6.4\n\nvirtualenv -p python3 tf-1.13.1-env\nsource ~/tf-1.13.1-env/bin/activate\npip install tensorflow-gpu\n</code></pre> <p>In the above pip install command, specifying tensorflow-gpu as the package name will install the latest stable release with GPU support. As of May 2019, the latest version is 1.13.1.</p> <p>Notes:</p> <ol> <li>Prior to version 1.13, installation of TF requires CUDA 9.</li> <li>For installation of version 2.0 alpha, refer to another wiki in the TensorFlow menu.</li> <li>In October 2019, TF 2.0 was released. If you still want to install  an older version 1.x, please specify the version explicitly when  running pip install, such as <code>pip instal tensorflow-gpu==1.14.</code> Directly writing \"tensorflow-gpu\" will install the latest TF 2.0  which has fundamental updates/differences as compared with 1.x, and can produce errors.</li> </ol>","title":"Installation"},{"location":"Installing_TensorFlow_1.x_with_virtualenv/#testing-example-1","text":"<p>We will test TF 1.13.1 with the following commands (using this cifar10 estimator tutorial).\u00a0</p> <pre><code>module purge\nmodule load GCC/6.4.0-2.28  OpenMPI/2.1.2\nmodule load CUDA/10.0.130 cuDNN/7.5.0.56-CUDA-10.0.130 \nmodule load Python/3.6.4\nsource ~/tf-1.13.1-env/bin/activate\nexport TF_CPP_MIN_LOG_LEVEL=2 # disables the warning, doesn't enable AVX/FMA.\n\ncd &lt;your working dir&gt;\ngit clone https://github.com/tensorflow/models.git\ncd models/tutorials/image/cifar10_estimator\npython generate_cifar10_tfrecords.py --data-dir=${PWD}/cifar-10-data\npython cifar10_main.py --data-dir=${PWD}/cifar-10-data  --job-dir=${PWD}/TMP_cifar10  --num-gpus=4  --train-steps=1000\n</code></pre> <p>If you want to submit above commands through a SLURM job script, you will need add a directive line of</p> <p><code>#SBATCH --gres=gpu:4</code></p> <p>in your script to request 4 GPUs. Additionally, you may want to make \"<code>echo $CUDA_VISIBLE_DEVICES</code>\"\u00a0your first command before running the main part. See tutorial page \"Submitting a TensorFlow job\" for a job script example.</p>","title":"Testing example 1"},{"location":"Installing_TensorFlow_1.x_with_virtualenv/#testing-example-2-tfkeras","text":"<p>We will run the following the python code <code>test_keras.py</code> (adopted from https://www.tensorflow.org/guide/keras):</p> <p>test_keras.py</p> <pre><code>import tensorflow as tf\nfrom tensorflow.keras import layers\nimport numpy as np\n\nprint(tf.VERSION)\nprint(tf.keras.__version__)\n\nmodel = tf.keras.Sequential([\n# Adds a densely-connected layer with 64 units to the model:\nlayers.Dense(64, activation='relu', input_shape=(32,)),\n# Add another:\nlayers.Dense(64, activation='relu'),\n# Add a softmax layer with 10 output units:\nlayers.Dense(10, activation='softmax')])\n\nmodel.compile(optimizer=tf.train.AdamOptimizer(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\ndata = np.random.random((1000, 32))\nlabels = np.random.random((1000, 10))\n\nval_data = np.random.random((100, 32))\nval_labels = np.random.random((100, 10))\n\nmodel.fit(data, labels, epochs=10, batch_size=32, validation_data=(val_data, val_labels))\n</code></pre> <p>To run this code:</p> <pre><code>module purge\nmodule load GCC/6.4.0-2.28  OpenMPI/2.1.2\nmodule load CUDA/10.0.130 cuDNN/7.5.0.56-CUDA-10.0.130\nmodule load Python/3.6.4\nsource ~/tf-1.13.1-env/bin/activate\npython test_keras.py\n</code></pre>","title":"Testing example 2 (tf.keras)"},{"location":"Installing_TensorFlow_using_anaconda/","text":"<p>An alternative to using virtual environment for installing TF is to use conda.  You will need to first log into our GPU-equipped dev-node dev-intel16-k80.</p>","title":"Installing TensorFlow using anaconda"},{"location":"Installing_TensorFlow_using_anaconda/#install-tf-cpu-only","text":"<p>Installation</p> <pre><code>export PATH=[your top conda dir]/bin:$PATH\nconda create --name tf\nsource activate tf\nconda install -c conda-forge tensorflow \nsource deactivate\n</code></pre> <p>Test</p> <pre><code>export PATH=[your top conda dir]/bin:$PATH\nsource activate tf\npython\n&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; print (tf.__version__)\n&gt;&gt;&gt; tf_session = tf.Session()\n&gt;&gt;&gt; x = tf.constant(1)\n&gt;&gt;&gt; y = tf.constant(1)\n&gt;&gt;&gt; print(tf_session.run(x + y))\nsource deactivate\n</code></pre>","title":"Install TF CPU-only"},{"location":"Installing_TensorFlow_using_anaconda/#install-tf-with-gpu-support","text":"<p>Installation</p> <pre><code>export PATH=[your top conda dir]/bin:$PATH\nconda create --name tf_gpu\nsource activate tf_gpu\nconda install tensorflow-gpu  \nsource deactivate\n</code></pre> <p>Test</p> <pre><code>export PATH=[your top conda dir]/bin:$PATH\nsource activate tf_gpu\npython\n&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\nsource deactivate\n</code></pre>","title":"Install TF with GPU support"},{"location":"Interactive_Job/","text":"<p>It is helpful to run your work and see the response of the commands right away to check if there is any error in your work flow. To use the interactive mode with resources more than the limit imposed on the dev nodes, HPCC users can submit an interactive job using salloc or srun command with options of resource requests.</p> <p>salloc command</p> <p>salloc and GPUs</p> <p>srun command</p> <p>Job with graphical application</p>","title":"Interactive Job"},{"location":"Interactive_Job/#salloc-command","text":"<p>For salloc, the command line</p> <pre><code>salloc -N 1 -c 2 --time=1:00:00\n</code></pre> <p>will allocate a job with resources of 1 node, 2 cores and walltime 1 hour. The execution will first wait until the job controller can provide the resources.</p> <pre><code>[username@dev-intel18 WorkDir]$ salloc -N 1 -c 2 --time=1:00:00\nsalloc: Required node not available (down, drained or reserved)\nsalloc: job 7625 queued and waiting for resources\n</code></pre> <p>Once that happens, the terminal will be transported to a command prompt on a compute node assigned to the job.</p> <pre><code>[username@dev-intel18 WorkDir]$ salloc -N 1 -c 2 --time=1:00:00\nsalloc: Required node not available (down, drained or reserved)\nsalloc: job 7625 queued and waiting for resources\nsalloc: job 7625 has been allocated resources\n[username@test-skl-000 WorkDir]$\n</code></pre> <p>where \"test-skl-000\" after the symbol\u00a0@ is the name of the assigned compute node.</p>","title":"salloc command"},{"location":"Interactive_Job/#salloc-and-gpus","text":"<p>GPUs requested for an interactive job can now be used without submitting an additional srun.</p> <pre><code>[username@dev-intel18 ~]$ salloc --gres=gpu:1 --time=1:00:00\nsalloc: Pending job allocation 14884030\nsalloc: job 14884030 queued and waiting for resources\nsalloc: job 14884030 has been allocated resources\nsalloc: Granted job allocation 14884030\nsalloc: Waiting for resource configuration\nsalloc: Nodes csn-003 are ready for job\n[username@csn-003 ~]$ nvidia-smi\nMon Apr  8 10:26:41 2019       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K20m          On   | 00000000:03:00.0 Off |                    0 |\n| N/A   30C    P8    26W / 225W |      0MiB /  4743MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>","title":"salloc and GPUs"},{"location":"Interactive_Job/#srun-command","text":"<p>A similar way can also be used with srun command:</p> <pre><code>[username@dev-intel18 WorkDir]$ srun -N 4 --ntasks-per-node=2 -t 00:60:00 --mem-per-cpu=1000M --pty /bin/bash\nsrun: Required node not available (down, drained or reserved)\nsrun: job 7636 queued and waiting for resources\nsrun: Granted job allocation 7636\n[username@test-skl-000 WorkDir]$ \n</code></pre> <p>As we can see, the specification \"--pty /bin/bash\" is required for srun command to request an interactive mode. Any command executed in this kind of interactive jobs will be launched parallelly with the number of task requested. srun can also be used in a command line without the specification \"--pty /bin/bash\". You may refer to the srun web site for more details.</p>","title":"srun command"},{"location":"Interactive_Job/#job-with-graphical-application","text":"<p>To scheduling an interactive job able to use graphical user interface (GUI) software, the specification --x11 for X11 forwarding needs to be specified at the command (salloc or srun) line.\u00a0\u00a0 If you are using Mac Terminal, you must have Xquartz installed, and you must use the \"-X\" parameter to allow x11 forwarding when connecting to a dev node prior to running the salloc command.\u00a0\u00a0\u00a0 The other option is to first log into our web-based remote desktop, and run the terminal there.\u00a0\u00a0\u00a0 See Web Site Access to HPCC for GUI software.\u00a0\u00a0 If you are on Windows and using Moba Xterm to log in, these instructions will work with our with the \"-X\" parameter.\u00a0 Putty does not support X11 and so this will not work with putty.\u00a0</p> <pre><code>[username@gateway-03 ~]$ ssh -X dev-intel18\n[username@dev-intel18 ~]$ cd WorkdDir  # this is optional, but you may want to select your work directory, for example\n[username@dev-intel18 WorkDir]$ salloc --ntasks=1 --cpus-per-task 2 --time 00:30:00 --x11\nsalloc: Granted job allocation 7708\nsalloc: Waiting for resource configuration\nsalloc: Nodes css-076 are ready for job\n[username@css-076 WorkDir]$ module load MATLAB\n[username@css-076 WorkDir]$ matlab\nMATLAB is selecting SOFTWARE OPENGL rendering.\nOpening log file:  /mnt/home/username/java.log.7159\n</code></pre> <p></p>","title":"Job with graphical application"},{"location":"Job-Arrays---Run-multiple-similar-jobs-simultaneously_40337504.html/","text":"","title":"Job Arrays   Run multiple similar jobs simultaneously 40337504.html"},{"location":"Job-Arrays---Run-multiple-similar-jobs-simultaneously_40337504.html/#teaching-job-arrays-run-multiple-similar-jobs-simultaneously","text":"<p>Job arrays are an efficient way to submit and manage a collection of jobs that differ from each other by only a single index parameter. All sub-jobs should have the same initial options (ex. size, time, etc.). For example, in the task below we wish to run python application, python_script.py ten times, each with a different input parameter for name. Instead of creating 10 separate Slurm job scripts and submitting them separately, we can create an array job.</p> <ul> <li> <p>Make a copy of the job script hello.sb and name it array_job.sb</p> <p>Answer </p> </li> </ul> <pre><code>    cp hello.sb array_job.sb\n    gedit array_job.sb\n</code></pre> <ul> <li>Edit array_job.sb to request an array job consisting of 10     sub-jobs, with index parameters 1-10. We can do this by adding the     following command to the slurm job script.</li> </ul> <pre><code>    #SBATCH --array=1:10\n</code></pre> <ul> <li>Specify the job\u2019s stdin</li> </ul> <pre><code>    cd test_$SLURM_ARRAY_JOB_ID\n</code></pre> <ul> <li>Edit array_job.sb to simultaneously run     python_script.py</li> </ul> <pre><code>    python3 python_script.py\n</code></pre> <p>This example was modified from:\u00a0https://crc.ku.edu/hpc/how-to/arrays</p>","title":"Teaching : Job Arrays - Run multiple similar jobs simultaneously"},{"location":"Job_Constraints/","text":"<p>Constraints are set to restrict which node features are required for a given job. Use\u00a0-C\u00a0or\u00a0--constraint = &lt;list&gt;\u00a0when submitting your job to specify a constraint.\u00a0</p>","title":"Job Constraints"},{"location":"Job_Constraints/#operators","text":"<p>The following operators can be used to combine multiple features when specifying constraints.</p>            Operator Function Description Example   feature&amp;feature AND Nodes allocated for the job must have both features lac&amp;ib   feature|feature OR Each node allocated for the job must have one feature or the other. lac|vim   [feature|feature] XOR All nodes allocated for the job must have either one feature or the other. Useful for multi-node shared memory jobs. [intel16|intel18]    <p>Multi-Node Jobs</p> <p>Multi-node jobs that use constraints to control which cluster hardware they run on must use the XOR syntax to specify multiple clusters, e.g. '[intel18|intel16]', and not the OR syntax, e.g. 'intel18|intel16'. Multi-node jobs using OR instead of XOR may experience a considerable performance decrease or may be killed by HPCC staff without warning.</p> <p>In most cases where OR is used, the job would be better served by XOR. For this reason, constraints using OR are automatically re-written to use XOR. If an OR constraint is required, prepend the constraint request with 'NOAUTO:'.</p>","title":"Operators"},{"location":"Job_Constraints/#automatic-job-constraints","text":"<p>When no constraints are specified, a job will get a default constraint of\u00a0[intel14|intel16|intel18]. This\u00a0ensures the job runs on only one type of cluster hardware. User specified constraints that don't use the feature|feature or [feature|feature] syntax are automatically combined with\u00a0[intel14|intel16|intel18]. If a more complex constraint is required, 'NOAUTO:' must be prepended to the constraint.</p>            User Specified Constraint Literal Constraint Effective Constraint Result   None [intel14|intel16|intel18] [intel14|intel16|intel18] Run this job on nodes that are all the same cluster type   --constraint=lac lac&amp;[intel14|intel16|intel18] lac Run this job on only Laconia nodes   --constraint=\"lac&amp;ib\" lac&amp;ib&amp;[intel14|intel16|intel18] lac&amp;ib Run this job on only nodes with both lac and ib features   --constraint=\"intel16|intel18\" [intel16|intel18] [intel16|intel18] Run this job on nodes that are either all intel16 or all intel18   --constraint=\"NOAUTO:lac|vim\" lac|vim lac|vim Run this job on nodes the each have either the lac or vim feature   --constraint=\"[intel16&amp;ib|intel18&amp;ib]\" [intel16&amp;ib|intel18&amp;ib] [intel16&amp;ib|intel18&amp;ib] Run this job on nodes that are either all intel16&amp;ib or all intel18&amp;ib","title":"Automatic Job Constraints"},{"location":"Job_Script_and_Job_Submission/","text":"<p>The <code>sbatch</code> command is used for submitting batch jobs to the cluster (same as Torque's <code>qsub</code>). <code>sbatch</code> accepts a number of options either from the command line, or (more typically) from a batch job script. In this section, we will show you a simple job script and how to submit it to SLURM.  Note the <code>sbatch</code> command only runs on development and compute nodes - it will not work on any gateway node.  </p>","title":"General rules for writing a job script"},{"location":"Job_Script_and_Job_Submission/#job-script","text":"<p>A job script contains two parts: #SBATCH lines for resource request and command lines for job running. The script should be in plain text format.  Below is an example used to explain what each lines means. </p> <p>Note that the job script below is not intended to be used as your job template (more on that later)</p> <pre><code>#!/bin/bash --login\n########## SBATCH Lines for Resource Request ##########\n\n#SBATCH --time=00:10:00             # limit of wall clock time - how long the job will run (same as -t)\n#SBATCH --nodes=1-5                 # number of different nodes - could be an exact number or a range of nodes (same as -N)\n#SBATCH --ntasks=5                  # number of tasks - how many tasks (nodes) that you require (same as -n)\n#SBATCH --cpus-per-task=2           # number of CPUs (or cores) per task (same as -c)\n#SBATCH --mem-per-cpu=2G            # memory required per allocated CPU (or core)\n#SBATCH --job-name Name_of_Job      # you can give your job a name for easier identification (same as -J)\n\n########## Command Lines for Job Running ##########\n\nmodule load GCC/6.4.0-2.28 OpenMPI  ### load necessary modules.\n\ncd &lt;path_to_code&gt;                   ### change to the directory where your code is located.\n\nsrun -n 5 &lt;executable&gt;              ### call your executable. (use srun instead of mpirun.)\n\nscontrol show job $SLURM_JOB_ID     ### write job information to SLURM output file.\njs -j $SLURM_JOB_ID                 ### write resource usage to SLURM output file (powertools command).\n</code></pre> <p>As was previously said, the above job script is not intended to be used as your job template. Your job is most likely going to use much simpler resource specifications than shown above. For purpose of illustration, this job requests 10 minutes walltime, at least 1 at most 5 different nodes, a total of 5 parallel tasks (processes) in distributed memory, 2 cores per task for parallel threads in shared memory and 2 GB memory per core (total 2 GB x 5 tasks x 2 cpus-per-task = 20 GB) with job name \"Name_of_Job\". After this job starts, it first loads two modules: <code>GCC/6.4.0-2.28</code> and the default version of OpenMPI. Then, change the directory to the path of the code and run the specified executable in 5 parallel tasks. After running the program, it outputs the job information and quits.</p> <p>By default, SLURM will try to use the settings: <code>--nodes=1, --tasks-per-node=1, --cpus-per-task=1, --time=00:01:00</code> and <code>--mem-per-cpu=750</code> for each job if any of them can not be acquired from the job specifications. Also, the job script must begin with a specification of a shell type or an interpreter on the first line, such as <code>#!/bin/bash</code> or '#!/usr/bin/python' . All lines starting with <code>#SBATCH</code> need to be placed above the first command line in the script. If they are below the line, the job controller will not execute them and lead to unexpected results.</p>","title":"Job script"},{"location":"Job_Script_and_Job_Submission/#batch-job-submission","text":"<p>Once the job script has been created, the job can be submitted using the <code>sbatch</code> command. If the command has been submitted successfully, the job controller will issue a job ID on the screen:</p> <pre><code>$ sbatch myjob.sb\nSubmitted batch job 8929\n</code></pre> <p>Optionally, any job specification (by #SBATCH line) can also be requested by <code>sbatch</code> command line with an equivalent option. For instance, the #SBATCH\u00a0 --nodes=1-5 line could be removed from the job script,  and instead be specified from the command line:</p> <p><pre><code>$ sbatch --nodes=1-5 myjob.sb\n</code></pre> Command line specifications take precedence over those in the job script.</p>","title":"Batch job submission"},{"location":"Job_Script_and_Job_Submission/#advanced-job-with-multiple-tasks","text":"<p>In the SLURM job options, the number of tasks (<code>--ntasks</code>) is used for jobs running multiple tasks with  distributed memory. It has the same meaning as the number of processes (-np) used in\u00a0mpirun application. This is similar to number of nodes (<code>nodes=</code>) used in a Torque (#PBS) job script.</p> <p>Each task could also use more than 1 core in shared memory (controlled by --cpus-per-task similar to ppn= in\u00a0#PBS job script) and each node could run more than 1 task (controlled by --tasks-per-node). By default, SLURM will use 1 core per task if --cpus-per-task (or -c) is not specified.</p> <p>It is better for multiple-task jobs to use srun command instead of mpirun to launch a software application. More details about srun, check out the SLURM web site.</p>","title":"Advanced: job with multiple tasks"},{"location":"Job_with_Checkpointing_Run/","text":"<p>Checkpointing is a function to save a snapshot of an application's running state, so it can restart from the saved point in case job running fails or reaches the time limit. Some applications might already have this feature for long-term computation. If users develop their own program, it is encouraged to implement checkpointing as a part of their codes. They can develop a function to write result variables to file systems at regular intervals and a function to read those variables in when restart.</p> <p>However if the program you used does not and can not include the feature, you may consider using \"Distributed MultiThreaded CheckPointing\" (or DMTCP) installed on HPCC nodes. DMTCP is a tool for transparently checkpointing the state of a distributed program spread across many machines without modifying the user's program or the operating system kernel.\u00a0For more details about DMTCP, please refer to their web site.</p> <p>Below we show examples of using DMTCP in HPCC system:</p>","title":"Job with Checkpointing Run"},{"location":"Job_with_Checkpointing_Run/#checkpoint-with-dmtcp","text":"<p>It shows an example job script to do checkpointing by DMTCP commands.</p>","title":"Checkpoint with DMTCP"},{"location":"Job_with_Checkpointing_Run/#powertools-longjob-by-dmtcp","text":"<p>It introduces how to use the longjob powertool for checkpointing on HPCC.</p>","title":"Powertools longjob by DMTCP"},{"location":"Kettering_Users/","text":"","title":"Kettering Users"},{"location":"Kettering_Users/#user-specific-information","text":"<p>This information is for users at Kettering University. Users with MSU NetIDs should disregard this information.</p> <p>1) Request a Community ID</p> <p>You will need to request a Community ID at the following link: https://community.idm.msu.edu/selfservice/</p>  <p>2) Click on link sent in Community ID verification email and verify the account</p> <p>3) Go to the following link and specify who your PI is and you will receive a confirmation email with your username in the footer of the email https://contact.icer.msu.edu/community_id</p>","title":"User-specific information"},{"location":"LabNotebook_AntiSmash/","tags":["lab notebook","conda","antismash"],"text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","title":"Lab Notebook: AntiSMASH"},{"location":"LabNotebook_AntiSmash/#lab-notebook-installing-antismash-on-hpcc-using-conda","tags":["lab notebook","conda","antismash"],"text":"<p>AntiSMASH (The antibiotics &amp; Secondary Metabolite Analysis Shell) is bioinformatics program for identifying genes belonging to secondary metabolite pathways, particularly in plant, fungi and bacteria species. Documentation for AntiSMASH can be found at https://docs.antismash.secondarymetabolites.org, but you will need to follow the instruction below to install AntiSMASH in your home directory on HPCC.</p> <p>Small analyses can be done through their webportal (see https://docs.antismash.secondarymetabolites.org/website_submission/), however if your analysis requires running AntiSMASH on the HPCC, please see the instruction below for how to install this program localy using Anaconda</p> <p>If you have not installed Anaconda in you home directory, see https://docs.icer.msu.edu/Using_conda/</p> <p>Installing AnitSMASH</p> <pre><code># Clear modules and load Anaconda\nmodule purge\nmodule load Conda/3\n\n# Add biconda to your Anaconda install\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\n\n# Create a cona environemnt for antimash and its depdencies\nconda create -n antismash\nconda activate antismash \n\n# As of version 6.1, you need to install each of these \n# programs first before install antismash or it never resolves\n# Seems to be an issue with the conda recipie, not AntiSMASH itself\nconda install hmmer2\nconda install hmmer\nconda install diamond\nconda install fasttree\nconda install prodigal\nconda install blast\nconda install muscle\nconda install glimmerhmm\n\n# Once the above installs have complete, this command will\n# install antismash and update the versions of the above programs # as needed. \n# \n# You may be asked to 'DOWNGRADE' diamond, hmmer, muslce, \n# and some perl libraries; this is normal \nconda install antismash\n\n# Test your install\nantismash --check-prereqs\nantismash --help\n\n# Download databases\ndownload-antismash-databases\n</code></pre> <p>Once you have completed the above steps, to run AntiSMASH in the future, do:</p> <pre><code>module purge\nmodule load Conda/3\n\nconda activate antismash\n\n# check\nantismash --help\n</code></pre>","title":"Lab Notebook --- Installing AntiSMASH on HPCC using Conda"},{"location":"LabNotebook_Bactopia/","tags":["lab notebook","bactopia"],"text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","title":"LabNotebook Bactopia"},{"location":"LabNotebook_Bactopia/#installing-bactopia-on-the-hpcc","tags":["lab notebook","bactopia"],"text":"<p>Bactopia is a bacteria genome analysis tool. It can be found at https://bactopia.github.io/v2.1.1/ and has extensive documentation. However, if you follow the instructions for installation on the MSU HPCC you may run into issues with solving the Python environment with Conda.</p>","title":"Installing Bactopia on the HPCC"},{"location":"LabNotebook_Bactopia/#installation-steps","tags":["lab notebook","bactopia"],"text":"<pre><code>module load Conda/3\n</code></pre> <p>Download miniconda https://docs.conda.io/en/latest/miniconda.html</p> <p>Activate the miniconda environment</p> <p>Install mamba</p> <pre><code>conda install -c conda-forge -c bioconda mamba\n</code></pre> <p>Update mamba: </p> <pre><code>conda install -c conda-forge 'mamba&gt;=0.24.*'\n</code></pre> <p>Downloaded and activate bactopia: </p> <pre><code>mamba create -n bactopia -c conda-forge -c bioconda bactopia\nconda activate bactopia\n</code></pre> <p>For further information, see this github issue: https://github.com/bactopia/bactopia/issues/355</p>","title":"Installation steps:"},{"location":"LabNotebook_Pymesh/","tags":["lab notebook","conda","pymesh"],"text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","title":"LabNotebook Pymesh"},{"location":"LabNotebook_Pymesh/#lab-notebook-installing-antismash-on-hpcc-using-conda","tags":["lab notebook","conda","pymesh"],"text":"<p>PyMesh is a code base developed by Qingnan Zhou for his PhD research at New York University. It is a rapid prototyping platform focused on geometry processing. PyMesh is written with both C++ and Python, where computational intensive functionalities are realized in C++, and Python is used for creating minimalistic and easy to use interfaces. Documentation for AntiSMASH can be found at https://github.com/PyMesh/PyMesh, but you will need to follow the instruction below to install AntiSMASH in your home directory on HPCC.</p> <p>If you have not installed Anaconda in you home directory, see https://docs.icer.msu.edu/Using_conda/</p> <p>Installing Pymesh</p> <pre><code># Clear modules and load Anaconda\nmodule purge\nmodule load Conda/3\n\n# You need to create a conda environment with an older version of python\nconda create -n pymesh python=3.6\n\n# Activate the environment\nconda activate pymesh\n\n# Use the the 0.2.1 version because the 0.3 version on conda forge required\n# GLIBC 2.18 which is incompatible with the version (2.17) on HPCC (Centos7 thing I think)\nconda install pymesh2=0.2.1\n\n# Once done, test by trying to do 'import pymesh' in python\n</code></pre> <p>Why not Pymesh 0.30</p> <p>The version of Pymesh 0.3 on conda forge produces the following error when I try to import pymesh</p> <pre><code>ImportError: /lib64/libc.so.6: version `GLIBC_2.18' not found (required by /mnt/home/panchyni/anaconda3/envs/pymesh/lib/python3.6/site-packages/pymesh/lib/libstdc++.so.6)\n</code></pre> <p>As far as I understand it, the GLIBC version is tied to the OS, so its not a s simple as importing another library. If for some reason version 0.3 is required, there are a couple of options, though they may be time consuming:</p> <ul> <li>Build PyMesh from source using the files from the github. There are a number of dependencies, so you might want to see if you can use a conda environment to install the dependencies</li> <li>Create a conda environment within an Singularity container of Centos8</li> </ul>","title":"Lab Notebook --- Installing AntiSMASH on HPCC using Conda"},{"location":"LabNotebook_ROS/","tags":["lab notebook","ROS"],"text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","title":"LabNotebook ROS"},{"location":"LabNotebook_ROS/#the-problem","tags":["lab notebook","ROS"],"text":"<p>The latest version of the Robot Operating System, Humble Hawksbill https://www.openrobotics.org/blog/2022/5/24/ros-2-humble-hawksbill-release, uses a version of <code>libQtCore.so.5</code> that is not compatible with the HPCC Linux kernel version (3.10 as of writing). Specifically, it requires kernel version 3.15+. When the ROS is installed into a Docker container, and used inside a Singularity image with e.g. <code>singularity pull docker://morris2001/humble</code> and <code>singularity run humble_latest.sif</code>, the <code>rqt</code> command inside the Singularity image will result in the error:</p> <pre><code>ImportError: libQt5Core.so.5: cannot open shared object file: No such file or directory\n</code></pre>","title":"The problem"},{"location":"LabNotebook_ROS/#the-solution","tags":["lab notebook","ROS"],"text":"<p>Inside your Dockerfile where you build the ROS container with Ubuntu 22.04, add the command</p> <pre><code>RUN /usr/bin/strip --remove-section=.note.ABI-tag /usr/lib/x86_64-linux-gnu/libQt5Core.so.5\n</code></pre> <p>This is based on discussion here: https://github.com/dnschneid/crouton/wiki/Fix-error-while-loading-shared-libraries:-libQt5Core.so.5</p> <p>This command removes the offending piece of code that is incompatible with the HPCC Linux kernel, and allows <code>rqt</code> to launch successfully.</p>","title":"The solution"},{"location":"LabNotebook_Using_an_alternate_OS_Singularity/","tags":["lab notebook","singularity","centos8"],"text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","title":"LabNotebook Using an alternate OS Singularity"},{"location":"LabNotebook_Using_an_alternate_OS_Singularity/#lab-notebook-using-an-alternate-os-via-singularity","tags":["lab notebook","singularity","centos8"],"text":"<p>Occasionally, I run into a peice of software which needs a newer version of GLIBC (&gt; 2.17) to be installed. Rather than try to upgrade GLIBC (which seems complicated and can go very poorly, see this thread for a general idea), its alot easier to run a different OS image in a container and run the software there. For this, we will use Singularity, which functions similarly to Docker, but is available on HPCC because unlike Docker it doesn't give you root privledges on the host system. Futhremore, its alot easier to work with than a virtual box (used them in the past, they can be fun but a lot of overhead + issues sharing files). Finally, you can run environment managers like Conda with Singularity to layer package management on top of the alternate OS, or do things like work with a downgraded python, all without affecting any system level install.</p> <p>Below, I will go through obtaining a CentOS8 image which has an updated GLIBC, building an overlay, and installing conda in that overlay, and building a conda environment for pymesh (version 0.3) as an example of this process</p>  <p>Note</p> <p>This is very brief and direct explanation of installing something in a Singularity image. A more general explanation of Singularity images, overlay, etc will be added in the future (probably as  another Notebook)</p>","title":"Lab Notebook --- Using an alternate OS via Singularity"},{"location":"LabNotebook_Using_an_alternate_OS_Singularity/#getting-a-centos8-image","tags":["lab notebook","singularity","centos8"],"text":"<p>You can pull a Docker built image directly using Singularity. The code below will grab a CentOS8 image with a new GLIBC which is needed from the pymesh 0.3 version on conda.</p> <pre><code># This works for now but\n# Probably want to update/find different OS since centos8 hit end of life at the end of 2021, probably centOS9\n# As of current, most similar OS with GLIBC &gt; 2.17\nsingularity pull centos8.sif docker://centos:8.3.2011\n</code></pre> <p>I keep my images in a folder called \"singularity_pull_images\" so, below replace \"../singularity_pull_images\" with whever your images are kept</p>","title":"Getting a CentOS8 image"},{"location":"LabNotebook_Using_an_alternate_OS_Singularity/#pymesh-03-in-centos8-container","tags":["lab notebook","singularity","centos8"],"text":"<p>Now what we have an image, we can make an 'overlay' which is basically a 'filesystem inside a singe file'. This both cuts down on the number of files used on your home directory AND help silo off environments/installs/etc that are associated with a particular Singularity image (although this last step isn't really necessary, but it helps). Also, you generally have more acess to the filesystem in the overlay, meaning you can install things outside of your home directory in the overlay.</p> <p>For now, we will make an overlay, install conda in that overlay and then use the overlay to contain all the files for pymesh and its dependencies</p>","title":"Pymesh 0.3 in CentOS8 Container"},{"location":"LabNotebook_Using_an_alternate_OS_Singularity/#setup-singularity-container","tags":["lab notebook","singularity","centos8"],"text":"<pre><code># The following commands are in powertools, which should be loaded by default; otherwise you will need to run 'module load powertools'\noverlay_build 5 overlay.img\noverlay_install_conda overlay.img https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh ../singularity_pull_images/centos8.sif \n\n# Now we access our overlay within our CentOS8 image\noverlay_start overlay.img ../singularity_pull_images/centos8.sif \n\n# After this, your prompt should be 'Singularity&gt;'\n</code></pre>","title":"Setup Singularity Container"},{"location":"LabNotebook_Using_an_alternate_OS_Singularity/#install-pymesh-within-the-container","tags":["lab notebook","singularity","centos8"],"text":"<p>Now that we are inside of our container, we can install pymesh. Using the CentOS8 image, we shouldn't get a GLIBC error, but we still need an older version of python (CentOS8 uses 3.9, pymesh needs 3.6). So, we will make conda enviroment with an older python (which is alot easier that trying to downgrade the system python of the Singularity image)</p> <pre><code># Need an older version of python for pymesh 0.3\nconda create -n pymesh2 python=3.6 pymesh2=0.3    \n\n# Now, activate our new environmet\nconda init\nsource ~/.bashrc                                  # Need to manually update shell post conda init\nconda activate pymesh2\n\n# After this your prompt should be '(pymesh2) Singularity&gt;'\n</code></pre>","title":"Install Pymesh within the container"},{"location":"LabNotebook_Using_an_alternate_OS_Singularity/#check-install-using-nose","tags":["lab notebook","singularity","centos8"],"text":"<p>Now, within our conda environment, within our Singualrity container, lets install the nose package (unit testing in python) and run the test from the pymesh github (https://github.com/PyMesh/PyMesh)</p> <pre><code># From within the pymesh2 environment\nconda install nose\npython -c \"import pymesh; pymesh.test()\"\n\n# You should get a couple warning and a couple S (skipped) tests, pretty sure this is fine\n</code></pre>","title":"Check install using nose"},{"location":"LabNotebook_Using_an_alternate_OS_Singularity/#endnote","tags":["lab notebook","singularity","centos8"],"text":"<p>This solution might seem (overly) complicated, because at the end we are running a conda environment inside of Singularity container. However, this allows us to use an older python version (via conda) and an alternative OS/GLIBC (via Singularity), either of which is an issue which is difficult to solve (downgrade python/upgrade GLIBC) and which it is not difficult to really screw things up by attempting</p>","title":"Endnote"},{"location":"LabNotebook_VisIt/","tags":["lab notebook","VisIt"],"text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","title":"LabNotebook VisIt"},{"location":"LabNotebook_VisIt/#lab-notebook-using-visit-on-hpcc","tags":["lab notebook","VisIt"],"text":"<p>VisIt is an Open Source, interactive, scalable, visualization, animation and analysis tool (see the  VisIt Docs for more detail)</p> <p>Currently, there are two challenges using VisIt on HPCC (1) failed installation due to conflicts with GLIBC and (2) running the GUI without it crashing immediately after loading.</p>","title":"Lab Notebook --- Using VisIt on HPCC"},{"location":"LabNotebook_VisIt/#easy-solution-using-an-older-visit","tags":["lab notebook","VisIt"],"text":"<p>To address the install issues, the easiest solution is to use an older (before 3.x) version of VisIt which can be found here. Use the version titled:</p> <pre><code>Linux - x86_64 64 bit\nRedhat Enterprise Linux 7.5, 4.18.9-1.el7.elrepo.x86_64 #1 SMP, gcc 4.8.5\n</code></pre> <p>Or use the following command on HPCC:</p> <pre><code>wget https://github.com/visit-dav/visit/releases/download/v2.13.3/visit2_13_3.linux-x86_64-rhel7.tar.gz\n</code></pre> <p>Once this file is donwloaded, unpack using tar -xzf and then begin and then begin an Interactive Desktop session via OnDemand. This step is necessary because running VisIt from the command line will cause the GUI interface to crash immediately after loading (tested with XQuartz on Mac and MobXterm on Windows). Instead, after creating an Interactive Desktop,navigate to the unpacked VisIt folder, enter the bin folder within, and click on the fie named 'visit'. This should start VisIt within the Interactive Desktop.</p>","title":"Easy Solution, using an older VisIt"},{"location":"LabNotebook_VisIt/#intermediate-solution-using-rhel7-visit-33-with-modules","tags":["lab notebook","VisIt"],"text":"<p>Following from the above solution, I realized that the new (as of writing) version of RHEL7 should work on HPCC without the GLIBC issue, but it doesn't out of the box. However, the fix wasn't difficult once I figured out that right module to load on HPCC, though the combinaton with the need to use OnDemand makes things a little clunky.</p> <p>First, get the RHEL7 3.3 version of VisIt with Mesa from their website, or use the following command:</p> <pre><code>wget https://github.com/visit-dav/visit/releases/download/v3.3.0/visit3_3_0.linux-x86_64-rhel7-wmesa.tar.gz\n</code></pre> <p>Once this file is downloaded, unpack using tar -xzf and then begin an Interactive Desktop session via OnDemand. This step is STILL necessary because running this version of VisIt from the command line ALSO will cause the GUI interface to crash though this time it waits until you try to draw something (tested with XQuartz on Mac).</p> <p>Because we need to use some module, once the Interactive Desktop is started, open a terminal and navigate to the folder where VisIt was unpacked and go to the 'bin' directory. There, run the following commands:</p> <pre><code>module load GCCcore/11.2.0\nmodule load PCRE2/10.37\n</code></pre> <p>Then, if you are in the bin folder, you can start Visit using:</p> <pre><code>./visit\n</code></pre> <p>I think this need to be done in the same terminal window for VisIt to start in the right environment (i.e. with the modules loaded)</p>","title":"Intermediate Solution, using RHEL7 Visit 3.3 with modules"},{"location":"LabNotebook_VisIt/#future-getting-centos8-visit-to-run-on-hpcc","tags":["lab notebook","VisIt"],"text":"<p>I've encountered several problem trying to getting any version VisIt 3.x to work on HPCC (tested 3.0 and the current as of now 3.3), though it turns out this was because I was using the CentOS8 version and RHEL7 is close enough to what we use that the workaround is not bad (see above). Using the prebuilt CentOS8 binary files will not work because they are looking for a version of GLIBC later than version 2.17 which is built into CentOS7. </p> <p>I am looking into this becaue I have encountered this issue before with other easily accessible installs of user requested software (see Pymesh on Conda) and might be a useful method to test software prior to a future OS update on HPCC. As such, I have been using Visit to explore ways to try to install CentOS8 built software on HPCC. Currently, the idea is to use a prebuilt binary on a Singualirty container of Centos8, though when I do this it cannot find the Qt5 library that comes with VisIt or, sometimes, an X11 library(might be able to fix this by messing aroubd with library paths within Singularity)</p>","title":"Future: Getting CentOS8 Visit to run on HPCC"},{"location":"LabNotebook_VisIt/#why-no-just-build-visit-from-source-wouldnt-that-be-easier","tags":["lab notebook","VisIt"],"text":"<p>Short answer, no.</p> <p>Long answer, when I try to build VisIt 3.x from source, it fails to build its own version of Qt5 (cannot find the xkbcommon library on the system even though its there) and will not recognize Qt5 on the system. Might be related to the library issues trying to work in Singularity, not sure.</p>","title":"Why no just build Visit from source, wouldn't that be easier?"},{"location":"LabNotebook_template/","tags":["lab notebook"],"text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","title":"LabNotebook template"},{"location":"Lab_Notebooks/","tags":["lab notebook"],"text":"<p>Warning</p> <p>The following Lab Notebooks are intended as a record of how particular problems were solved at a particular time and not updated or maintained in any way to reflect current system setting or versions of installed software. If you are having a problem with software/topic addressed by one of these notebooks, they may provide a solution or at least a starting point, but there is no garuntee that worked here will work again.</p>","title":"Lab Notebooks"},{"location":"Lab_Notebooks/#notebooks","tags":["lab notebook"],"text":"<ul> <li>Installing AntiSMASH</li> <li>Installing PyMesh</li> <li>Using VisIt on HPCC</li> <li>Using an alternate OS via Singularity</li> </ul>","title":"Notebooks"},{"location":"Learning_the_Shell/","text":"<ol> <li> <p>Variables - Part I</p> </li> <li> <p>Variables - Part II</p> </li> <li> <p>Expansion</p> </li> <li> <p>Conditional statements</p> </li> <li> <p>Loops</p> </li> </ol>","title":"Learning the Shell"},{"location":"Linux_Command_Line_Interface_for_Beginners_I/","text":"<p>The OS of MSU HPC is CentOS which is a brach of Linux. So if you want to use our system, it is essential to equip some basic knowledge on Linux. Even though Linux supports GUI, most works are done on a dummy terminal via texts. The Linux command line is a text interface to Linux.\u00a0</p> <p>We will walk through some practical exercises to become familiar with a few basic commands and concept.</p> <ul> <li>Navigation<ul> <li>Relative and absolute Paths</li> </ul> </li> <li>Creating and removing directories</li> <li>Creating and removing files</li> <li>Copying moving/renaming directories and files</li> <li>Listing directories and files</li> <li>Exercise</li> </ul>","title":"Linux Command Line Interface for Beginners I"},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#navigation","text":"<p>Let's run the first command. Type pwd and pressing the Enter or Return key to run it (From now, I'll not mention pressing Enter/Return key part to run command).</p> <pre><code>$ pwd\n/mnt/home/iamsam/\n</code></pre> <p>You will see a path such as /mnt/home/user/your_id</p> <p>pwd\u00a0is an abbreviation of 'print working directory'. It print out the shell's current working directory. You can change the working directory using the cd command, an abbereviation for 'change directory'.</p> <pre><code>$ cd /\n$ pwd\n/\n</code></pre> <p>Now your working directory is '/' which is the root directory.\u00a0There is nothing much you can do on the root directory, so let's go to your 'home' directory.</p> <pre><code>$ cd \n$ pwd\n/mnt/home/iamsam/\n</code></pre> <p>Regardless of your location, when you just type 'cd', you will be home. You can also type 'cd \\~' instead of 'cd' to be back your home.</p> <p>To go the previous directory, type 'cd -'</p> <pre><code>$ cd -\n$ pwd\n/\n</code></pre> <p>The root directory has many subdirectories including your home directory. Let's go to 'bin' directory.</p> <pre><code>$ cd bin\n$ pwd\n/bin\n</code></pre> <p>To go up to the parent directory (it is / for us now), use the special syntax of two dots with cd such as</p> <pre><code>$ cd ..\n$ pwd\n/\n</code></pre> <p>To go up to the previous directory, use - with cd such as</p> <pre><code>$ cd -\n$ pwd\n/bin\n</code></pre> <p>You can use .. more than once if you have to move up multiple levels of parent directories.</p> <pre><code>$ cd ../..\n$ pwd\n</code></pre>","title":"Navigation"},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#relative-and-absolute-paths","text":"<p>A path is an address of a directory. Most of the examples we've looked at so far uses relative paths. So you final location is decides based on your current working directory. However, sometimes you want to use an absolute path than relative one. Your home's absolute path at HPC is /mnt/home/your_id. See the example to find how to use a relative and absolute paths.</p> <pre><code>$ cd /\n$ pwd\n/\n$ cd -\n$ pwd\n/mnt/home/iamsam\n$ cd /\n$ cd /mnt/home/iamsam\n$ pwd\n/mnt/home/iamsam\n</code></pre>","title":"Relative and absolute Paths"},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#creating-and-removing-directories","text":"<p>To make a directory, use mkdir (short for 'make directory') such as</p> <pre><code>$ mkdir temp01\n$ls\ntemp01\n</code></pre> <p>ls is a command to list files and folder. We will learn it in a minute. You can create multiple directories as well.</p> <pre><code>$ mkdir temp02 temp03 temp04\n$ ls\ntemp01 temp02 temp03 temp04\n</code></pre> <p>To make a subdirectory, use with p option such as</p> <pre><code>$ mkdir -p temp04/temp05/temp06\n$ ls temp04\ntemp05\n</code></pre> <p>To remove directory use rm command with r. Without r option, rm will not delete directories (but you can delete files). r means recursive.</p> <pre><code>$ rm temp04\nrm: cannot remove 'temp04': Is a directory\n$ rm -r temp04\n</code></pre>","title":"Creating and removing directories"},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#creating-and-removing-files","text":"<p>You can use editors to create files, but it is out of scope of this tutorial. Let's use ls and a pipe &gt;\u00a0(we will explain pipes later).</p> <pre><code>$ ls\ntemp01 temp02 temp03\n$ ls &gt; list.txt\n$ ls\nlist.txt temp01 temp02 temp03\n</code></pre> <p>Now we have three directories (temp01, temp02, temp03) and one file (list.txt). We already know how to delete directories. To delete files, we use command rm such as</p> <pre><code>$ rm list.txt\n$ ls\ntemp01 temp02 temp03\n</code></pre>","title":"Creating and removing files"},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#copying-and-renaming-directories-and-files","text":"<p>To copy files or directories, use cp such as</p> <pre><code>$ cp list.txt list2.txt\n$ ls\nlist.txt list2.txt temp01 temp02 temp03\n</code></pre> <p>With r option, you can copy files and directories recursively. i.e., copy subdirectories and files.</p> <p>mv command mv files/directories or rename them.</p> <pre><code>$ mv list2.txt temp01\n$ ls\nlist.txt temp01 temp02 temp03\n\n$ ls temp01\nlist2.txt\n\n$ cd temp01\n$ mv list2.txt list3.txt\n$ ls\nlist3.txt\n</code></pre>","title":"Copying and renaming directories and files"},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#listing-directories-and-files","text":"<p>We already used ls. To list directories and files, us\u00a0ls (short for 'list').\u00a0</p> <pre><code>$ ls\ntemp01 temp02 temp03\n</code></pre> <p>There are many options for ls. Most frequently used options are</p> <ul> <li>a: list all files and directories including hidden contents</li> <li>h: print sizes in human readable format (e.g.: 1K, 2.4M, 3.1G)</li> <li>l: list with a long listing format</li> <li>t: sort my modification time</li> </ul> <p>You can use options separately like ls -l -a -t or together like ls -lat.</p> <pre><code>$ ls -l -a -t\ntotal 8\n-rw-r--r--    1 iamsam  staff    30 Mar 23 15:37 list.txt\ndrwxr-xr-x    6 iamsam  staff   192 Mar 23 15:37 .\ndrwxr-xr-x    2 iamsam  staff    64 Mar 23 15:37 temp03\ndrwxr-xr-x    2 iamsam  staff    64 Mar 23 15:37 temp02\ndrwxr-xr-x    2 iamsam  staff    64 Mar 23 15:37 temp01\ndrwxr-xr-x+ 113 iamsam  staff  3616 Mar 23 15:21 ..\n\n$ ls -lat\ntotal 8\n-rw-r--r--    1 iamsam  staff    30 Mar 23 15:37 list.txt\ndrwxr-xr-x    6 iamsam  staff   192 Mar 23 15:37 .\ndrwxr-xr-x    2 iamsam  staff    64 Mar 23 15:37 temp03\ndrwxr-xr-x    2 iamsam  staff    64 Mar 23 15:37 temp02\ndrwxr-xr-x    2 iamsam  staff    64 Mar 23 15:37 temp01\ndrwxr-xr-x+ 113 iamsam  staff  3616 Mar 23 15:21 ..\n</code></pre>","title":"Listing directories and files"},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#exercise","text":"<p>Now let's do some exercise.\u00a0</p> <ul> <li>Log in your MSU HPC account and go to any dev-node.\u00a0</li> <li>Create linux_tutorial\u00a0dir on your home.</li> <li>Copy a folder and contents for this tutorial from\u00a0</li> <li>/mnt/research/common-data/workshops/intro2Linux_iamsam to     linux_tutorial\u00a0dir on your home</li> <li>Go to linux_tutorial</li> <li>Find a hidden directory and rename it to not_hidden</li> <li>Check the contents of not_hidden</li> <li>Create a new directory called new_dir</li> <li>Copy the file youfoundit.txt into new_dir</li> <li>remove garbage dir</li> </ul> <p>This is an answer (I do not include the login process). \u00a0Expand source</p> <pre><code>$ mkdir linux_workshop\n$ cp -r /mnt/research/common-data/workshops/intro2Linux_iamsam linux_tutorial\n$ cp linux_tutorial\n$ ls -a\n$ mv .hidden not_hidden\n$ ls not_hidden\n$mkdir new_dir\n$ cp not_hidden/youfoundit.txt new_dir\n$ rm -r garbage\n</code></pre>","title":"Exercise"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/","text":"<ul> <li>Files and     folders</li> <li>File permissions</li> <li>Concatenate files</li> <li>Redirection</li> <li>Wildcards</li> <li>Manuals</li> <li>Commands of monitoring files</li> <li>Archiving and compression</li> <li>Environment variables</li> </ul>","title":"Linux Command Line Interface for Beginners II"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#files-and-folders","text":"<p>Linux has a single directory 'tree', separated by slash, the top is the 'root'. All additional disks are connected on /mnt ('mounted'). In Linux, there is no concept of driver letters.</p> <p>tree is a recursive directory listing program which prints a depth indented listing of files. With no arguments, tree lists the files in the current directory. You can change the depth with -L argument.</p> <p>The example shows the structure of the 'linux_tutorial' directory which we used for an exercise.</p> <pre><code>$ tree\n.\n\u251c\u2500\u2500 000.txt\n\u251c\u2500\u2500 001.txt\n\u251c\u2500\u2500 002.txt\n\u251c\u2500\u2500 009.txt\n\u251c\u2500\u2500 010.txt\n\u251c\u2500\u2500 a.txt\n\u251c\u2500\u2500 A.txt\n\u251c\u2500\u2500 b.txt\n\u251c\u2500\u2500 B.txt\n\u251c\u2500\u2500 c.txt\n\u251c\u2500\u2500 C.txt\n\u251c\u2500\u2500 garbage\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 garbage_01\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 garbage_02\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 garbage_03\n\u251c\u2500\u2500 my_data.dat\n\u251c\u2500\u2500 my-data.txt\n\u251c\u2500\u2500 z.txt\n\u2514\u2500\u2500 Z.txt\n\n$ tree -L 1\n.\n\u251c\u2500\u2500 000.txt\n\u251c\u2500\u2500 001.txt\n\u251c\u2500\u2500 002.txt\n\u251c\u2500\u2500 009.txt\n\u251c\u2500\u2500 010.txt\n\u251c\u2500\u2500 a.txt\n\u251c\u2500\u2500 A.txt\n\u251c\u2500\u2500 b.txt\n\u251c\u2500\u2500 B.txt\n\u251c\u2500\u2500 c.txt\n\u251c\u2500\u2500 C.txt\n\u251c\u2500\u2500 garbage\n\u251c\u2500\u2500 my_data.dat\n\u251c\u2500\u2500 my-data.txt\n\u251c\u2500\u2500 z.txt\n\u2514\u2500\u2500 Z.txt\n</code></pre> <p>tree command does not show hidden files/directory by default. To see hidden files/directories, use -a argument.</p> <pre><code>$ tree -a\n.\n\u251c\u2500\u2500 000.txt\n\u251c\u2500\u2500 001.txt\n\u251c\u2500\u2500 002.txt\n\u251c\u2500\u2500 009.txt\n\u251c\u2500\u2500 010.txt\n\u251c\u2500\u2500 a.txt\n\u251c\u2500\u2500 A.txt\n\u251c\u2500\u2500 b.txt\n\u251c\u2500\u2500 B.txt\n\u251c\u2500\u2500 c.txt\n\u251c\u2500\u2500 C.txt\n\u251c\u2500\u2500 garbage\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 garbage_01\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 garbage_02\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 garbage_03\n\u251c\u2500\u2500 .hidden\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 youfoundit.txt\n\u251c\u2500\u2500 my_data.dat\n\u251c\u2500\u2500 my-data.txt\n\u251c\u2500\u2500 z.txt\n\u2514\u2500\u2500 Z.txt\n\n2 directories, 19 files\n</code></pre>","title":"Files and folders"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#file-permissions","text":"<p>Linux has a method to keep files private and safe. When you type\u00a0ls -a you will get a lot of information of your directory including files such as</p> <p></p> <p>The first letter shows a type of a file.</p> <p>-: normal file</p> <p>d: directory</p> <p>l: symbolic link</p> <p>Next nine characters shows permission; first three is for you, next three is for your group members, and last three is for a whole world. Let's see the first file in the picture which shows <code>d rwxr-xr-x</code>.</p> <p>The first character is 'd', and therefore, even the name is 'hello.txt', it is not a file, but a directory. Next nine charcters represent the settings for the three sets of permissions.</p> <ul> <li>The first three characters show the permissions for the user who     owns the file (user permissions).</li> <li>The middle three characters show the permissions for members of the     file\u2019s group (group permissions).</li> <li>The last three characters show the permissions for anyone not in the     first two categories (other permissions).</li> </ul> <p>The letters represent:</p> <ul> <li>r: Read permissions. The file can be opened, and its content viewed.</li> <li>w: Write permissions. The file can be edited, modified, and deleted.</li> <li>x: Execute permissions. If the file is a script or a program, it can     be run (executed).</li> </ul> <p>In our screenshot, the first three characters are 'rwx' which means you(owner) can read, write/modify or execute (it means you can go inside the directory here). Next\u00a0three characters are 'r_x' which means your group members can read, or execute, but can not modify the contents. Last\u00a0three characters are 'r_x' which means everyone else can read, and execute.</p> <p>Another examples:</p> <ul> <li>---------: it means no permissions have been granted at all.</li> <li>rwxrwxrwx: it means full permissions have been granted to everyone.</li> </ul> <p>Permissions can be set with chmod command, and ownership set with chown. You can only change these for files you are the owner of. To use chmod, we need to tell it 'who' we are setting permissions for, 'what' change we are making, 'which' permissions we are setting.</p> <p>The \u201cwho\u201d values can be:</p> <ul> <li>u: User - the owner of the file.</li> <li>g: Group - members of the group the file belongs to.</li> <li>o: Others - people not governed by the u and g permissions.</li> <li>a: All - all of the above.</li> </ul> <p>If none of these are used, chmod behaves as if \u201ca\u201d had been used.</p> <p>The \u201cwhat\u201d values can be:</p> <ul> <li>\u2013: Minus sign. Removes the permission.</li> <li>+: Plus sign. Grants the permission. The permission is added to the     existing permissions. If you want to have this permission and only     this permission set, use the = option, described below.</li> <li>=: Equals sign. Set a permission and remove others.</li> </ul> <p>The \u201cwhich \u201d values can be:</p> <ul> <li>r:\u00a0 The read permission.</li> <li>w: The write permission.</li> <li>x: The execute permission.</li> </ul> <p>You can also use a three three-digit numbers (total nine) to provide the permission with\u00a0chmod. The leftmost digit represents the permissions for the owner. The middle digit represents the permissions for the group members. The rightmost digit represents the permissions for the others. x is 1, w is 2, and r is 4. and the some of these three numbers are the permission.</p> <p>The digits you can use and what they represent are listed here:</p> <ul> <li>0: (000) No permission.</li> <li>1: (001) Execute permission.</li> <li>2: (010) Write permission.</li> <li>3: (011) Write and execute permissions.</li> <li>4: (100) Read permission.</li> <li>5: (101) Read and execute permissions.</li> <li>6: (110) Read and write permissions.</li> <li>7: (111) Read, write, and execute permissions.</li> </ul> <p>The tables shows a summary of the chmod command.</p>    User Type Permission     u - \u00a0user + add r - read (4)   g - group - delete w - write (2)   o - others = set exactly x - execute (1)   a - all      <p>Examples:</p> <p><code>chmod u=rx, og=r my_file1.txt</code> : set the user has read, and executable permissions; group/other have read permission only for my_file1.txt</p> <p><code>chmod 544 my_file1.txt</code>: same as\u00a0chmod u=rx, og=r my_file1.txt.\u00a0</p> <p><code>chmod 750 my_file1.txt</code>:\u00a0set the user has read, write, and executable permissions; group has read/executable permission; others no permission.</p>","title":"File permissions"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#concatenate-files","text":"<p>The cat (short for \u201cconcatenate\u201c) command is one of the most frequently used command in Linux. cat command allows us to create single or multiple files, view contain of file, concatenate files and redirect output in terminal or files.</p> <ul> <li>To display contents of a file: cat filename</li> <li>To view contents of multiple files: cat file1 file2</li> <li>To create a file: cat\u00a0&gt;file2.txt</li> </ul> <p>If file content is large, and won't fit in a terminal screen, you can use more and less with cat command such as (we are using pipes\u00a0|, which will be covered later)</p> <ul> <li>cat file2.txt |more</li> <li>cat file2.txt |less</li> </ul> <p>more, less, and most\u00a0(most has more features than more and less commands) are commands to open a given file for interactive reading.</p>","title":"Concatenate files"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#redirection","text":"<p>Redirection is a feature such that when you execute a command with it, you can change the standard input/output devices. The standard input (stdin) device is the keyboard, and the standard output (stdout) device is the screen. With redirection, stdin/stdout can be changed.</p> <p>&gt;: Output redirection (overwriting)</p> <p>eg. ls -la\u00a0&gt; list.txt (ls -la results save as list.txt instead of standard output (screen))</p> <p>&gt;&gt;: Output redirection (appending)</p> <p>eg.\u00a0ls -l\u00a0&gt;&gt; list.txt (ls -l result will be appended at the end of the list.txt. If there is no list.txt, it works as ls -la\u00a0&gt; list.txt)</p> <p>&lt;: Input redirection</p> <p>&gt;&amp;: writing the output from one file to the input of another file.</p> <p>eg. matlab\u00a0&gt; outfile 2&gt;&amp;1 : send stdout and stderr to 'outfile'. Here 1 and 2 are file descriptors. File descriptor 1 is the standard output (stdout), and\u00a02 is the standard error (stderr).\u00a0&gt; is redirection, and &amp; indicates that what follows and precedes is a file descriptor and not a filename.</p>","title":"Redirection"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#wildcards","text":"<p>Wildcards are symbols or special characters that represent other characters. You can use them with any command such as ls/rm/cp etc.</p> <p>*: anything or nothing</p> <p>?: single character</p> <p>[ ]: any character or range of characters</p> <p>[! ]: inverse the match of [ ]</p> <p>ls *.txt\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0# list all txt files</p> <p>ls *-?.txt\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0# list all files with \u2018-\u2018 and with one cha in front of \u2018.txt\u2019</p> <p>ls [0-9]*.txt\u00a0 \u00a0 \u00a0# list all files start with a number.</p> <p>ls [A-Z]*.txt\u00a0 \u00a0 \u00a0# list all files start with a capital letter?\u00a0[A-Z] can be different based on LC_COLLATE value. For further discussion, check here. In HPC at MSU, the default of\u00a0[A-Z] is\u00a0\u00a0a, A, b, B, c, C, ....y, Y, z, Z, which is standard collations (en_US).\u00a0</p> <p>Try ls [[:lower:]].txt;\u00a0 ls [[:upper:]].txt;\u00a0 ls [[:lower:][:upper:]].txt</p> <p>ls [!a-Z]*.txt \u00a0 \u00a0 # list txt files that don\u2019t begin with any letter.</p> <p>For more informaiton, refer to\u00a0Regular Expressions.</p>","title":"Wildcards"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#manuals","text":"<p>Linux includes a built in manual for nearly all commands. Type \u2018man\u2019 followed by the commands.</p> <p>e.g.\u00a0man man</p> <p>To navigate the man pages use the arrow keys to scroll up and down or use the enter key to advance al ine, space bar to advance a page, letter u to go back a page. Use the q key to quit out of the display.</p> <p>The manual pages often include these sections:</p> <ul> <li>Name: a one line desctiption of what it does</li> <li>Synopsis: basic syntax for the command line.</li> <li>Description: describes the program\u2019s functionalities.</li> <li>Options: lists commnand line options available for this program.</li> <li>Example: examples of some of the options available.</li> <li>See Also: list of related commands.</li> </ul> <p>Note that options can be with single dash \u2018-\u2018 or double dash \u2018\u2014\u2019</p>","title":"Manuals"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#commands-of-monitoring-files","text":"<p>Following commands are used for monitering files. </p> <p>wc: count words, lines or characters</p> <p>eg. who | wc -l \u00a0 \u00a0 # number of users logged in</p> <p>grep: find patterns in files or data, returns just matching lines</p> <p>eg. who |grep $USER</p> <p>sort: given a list of items, sort in various ways</p> <p>eg. who | sort</p> <p>head: list only top n lines of file</p> <p>who &gt; who.txt; head who.txt</p> <p>tail: list only last n lines of file</p>","title":"Commands of monitoring files"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#archiving-and-compression","text":"<p>Following commands are used for archiving and compression: zip, unzip, tar</p> <p>unzip: unzip</p> <p>tar: create (tar -c) or extract (tar -x) \u2018tape archive\u2019 file.</p> <p>Exercise :\u00a0</p> <p>Using the man pages, find out what the default number of lines that head and tail will display, and how to limit those to just one line</p> <p>Can you tar all files and folders in workshop folder? Question? Use man.\u00a0</p>","title":"Archiving and compression"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#environment-variables","text":"<p>Shell maintains and you can set \u2018variables\u2019 that the shell uses for configuration and in your script.\u00a0Variables start with $, and can be seen with echo $VARNAME</p> <p>Explore common variables with the echo command and list what they are\u00a0$HOME, $USER, $SHELL, $PATH.</p> <p>For more information, please refer to\u00a0</p> <p>1. Variables - Part I</p>","title":"Environment variables"},{"location":"Linux_Shell/","tags":["reference"],"text":"<p>A Unix/Linux shell is a command-line interpreter which provides a user interface for the Unix/Linux operating system. Users control the operation of a computer by submitting single commands or by submitting one or more commands via a shell script. Several common shell choices are available on HPCC:</p> <ul> <li><code>bash</code>: a Bourne-shell (sh) compatible shell with many newer advanced features as well </li> <li><code>tcsh</code>: an advanced variant on <code>csh</code> with all the features of modern shells</li> <li><code>zsh</code>: an advanced shell which incorprates all the functionality of <code>bash</code>, <code>tcsh</code>, and <code>ksh</code> combined</li> <li><code>csh</code>: the original C-style shell</li> </ul> <p>The default shell provided to HPCC users is the <code>bash</code> shell. To change your shell, please contact HPCC support. To find out your current shell run <code>echo $SHELL</code>.</p>  <p>Note</p> <p><code>bash</code> is the the only officially supported shell for the HPCC </p>","title":"Linux shells"},{"location":"Linux_Shell/#environment-variables","tags":["reference"],"text":"<p>Environment variables are a set of dynamically named values which can control the way running processes will behave on a computer. Many of the Unix commands and tools require certain environment variables to be set. Many of these are set automatically for the users when they log in or load applications via the module command. </p> <p>To view your current set of environment variables run <code>env</code>. </p> <p>To assign a new value to an environment variable in either <code>bash</code> or <code>zsh</code>: <code>export &lt;name&gt;=&lt;value&gt;</code></p> <p>To assign a new value to an environment variable in either <code>tcsh</code> or <code>csh</code>: <code>setenv &lt;name&gt; &lt;value&gt;</code></p> <p>To print the value of a variable: <code>echo $&lt;name&gt;</code></p>","title":"Environment variables"},{"location":"Linux_Shell/#commonly-used-environment-variables","tags":["reference"],"text":"<p>Bash variables are preceded with $ and optionally enclosed in brackets when used e.g. <code>$USER</code> or <code>${USER}</code>.</p> <ul> <li><code>$HOSTNAME</code>:  Name of the computer currently running the script. This should be one of the nodes listed in the variable <code>$SLURM_JOB_NODELIST</code> </li> <li><code>$USER</code>: User Name (NetID). Useful if you would like to dynamically generate a directory on some scratch space.</li> <li><code>$HOME</code>: User's home directory. Can also use <code>~/</code> symbol.</li> <li><code>$SCRATCH</code>: Your folder on the scratch disk.                                                   </li> <li><code>$TMPDIR</code>: Temporary working folder of a running job at <code>/mnt/local/$SLURM_JOBID</code>. </li> </ul> <p>To see all variables in the context of your job, add this line to your job script, which will list all variables that contain the word 'SLURM'</p> <pre><code>$ env | grep SLURM\n</code></pre>","title":"Commonly used environment variables"},{"location":"Linux_Shell/#startup-scripts","tags":["reference"],"text":"<p>A startup script is a shell script which the login process executes. It provides an opportunity to alter your environment. You are free to setup your own startup scripts but be careful to make sure they are set up correctly for both interactive and batch access or it may negatively affect your ability to log in or run batch jobs on the system:</p> <ul> <li>bash  <code>~/.bashrc</code></li> <li>tcsh  <code>~/.chsrc</code> </li> <li>zsh   <code>~/.zshrc</code> </li> <li>csh   <code>~/.cshrc</code> </li> </ul>","title":"Startup scripts"},{"location":"List_Jobs_by_squeue_sview/","text":"","title":"List Jobs by squeue &amp; sview"},{"location":"List_Jobs_by_squeue_sview/#squeue-command","text":"<p>After you submit jobs, you can check their information and status in the queue. The simplest way is to use squeue command to list all of your jobs:</p> <pre><code>$ squeue -l -u $USER\nThu Aug  2 14:45:57 2018\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n              9405 general-s     Job1   nobody  RUNNING       0:32  01:00:00      1 lac-198\n              9406 general-s Rmpi_tes   nobody  RUNNING       0:11     30:00      2 lac-[386-387]\n              9290 general-l  LongJob   nobody  PENDING       0:00  36:00:00     40 (Resources)\n</code></pre> <p>where the argument <code>-l</code> reports more of the available information (i.e. <code>l</code> for long format) and <code>-u</code> specifies which user's jobs to show. You may find a complete squeue specifications from the SLURM web site.</p> <p>HPCC staff also wrote some powertools commands so users can see their jobs conveniently. Before using\u00a0powertools commands, please make sure powertools module is loaded by using moudle list (By default,  the powertools module should be loaded unless the user has purged modules  with module purge)</p> <pre><code>$ module list\n\nCurrently Loaded Modules:\n  1) powertools/1.2\n</code></pre> <p>One of the commands sq works the same as the above squeue command:</p> <pre><code>$ sq                         # powertools command\nThu Aug  2 14:48:51 2018\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n              9405 general-s     Job1 username  RUNNING       0:35  01:00:00      1 lac-198\n              9406 general-s Rmpi_tes username  RUNNING       0:14     30:00      2 lac-[386-387]\n              9290 general-l  LongJob username  PENDING       0:00  36:00:00     40 (Resources)\n</code></pre> <p>where it shows the job IDs, job partition, job name, username, job state, elapsed time, walltime limit, total number of nodes and the node list or the waiting reason for the user's each jobs. Users can also use qs command to see more information:</p> <pre><code>$ qs                         # powertools command\nThu Aug  2 14:49:27 2019\n                                                                                        Start_Time/\n     JobID         User     Account     Name   Node CPUs  TotMem  Tres   WallTime  ST  Elapsed_Time  NodeList(Reason)\n-----------------------------------------------------------------------------------------------------------------------\n    60889405    MyHPCCAcc   general       Job1    1   16      2G  gpu:1    3:55:00  R      1:49:24   lac-198\n    60889496    MyHPCCAcc   general  Rmpi_test    2    8   1500M   N/A       30:00  R        29:46   lac-[386-387]         \n    60889290    MyHPCCAcc  classres    LongJob   40   80    750M  k80:1 3-00:00:00 PD 08-03T10:29:41 (Resources)\n</code></pre> <p>where more items, such as, total number of CPUs, memory, gpu per node and job start time (for pending jobs) or elapsed time (for running jobs) are shown. For a complete usage of squeue command, please refer to the SLURM web site.</p>","title":"squeue command"},{"location":"List_Jobs_by_squeue_sview/#sview-command","text":"<p>Besides the text listing of the jobs, SLURM also offer a command to show the squeue information with a graphical interface. Use the command sview:</p> <pre><code>$ sview\n</code></pre> <p>You will see an image of a job list displaying all jobs in the queue:</p> <p></p> <p>Click on each job, it will pop out another window and show the detailed information. For a complete usage of sview command, please refer to the SLURM web site.</p>","title":"sview command"},{"location":"List_of_Job_Specifications/","text":"<p>The following is a list of basic #SBATCH specifications. To see the complete options of SBATCH, please refer to the SLURM sbatch command page.</p>    #SBATCH Options Description Examples     <code>-a</code>, <code>--array=&lt;indexes&gt;</code> Submit a job array, with multiple jobs to be executed. The indexes specification   identifies what array ID values should be used.  Each job has the same   job ID (<code>$SLURM_JOB_ID</code>) but different array ID (<code>$SLURM_ARRAY_TASK_ID</code>   variable). <code>#SBATCH -a 0-15</code>     <code>#SBATCH --array=0,6,16-32</code>    Can use a step function with the <code>:</code> separator. <code>#SBATCH --a 0-15:4</code> (same as <code>#SBATCH \u2013a 0,4,8,12</code>)    A maximum number of simultaneously running jobs may be specified with the <code>%</code> separator. <code>#SBATCH --array=0-15%4</code> (4 jobs running simultaneously)   <code>-A</code>, <code>--account=&lt;account&gt;</code> This option tells SLURM to use the specified buy-in account. Unless you are an   authorized user of the account, your job will not run. <code>#SBATCH   -A</code>   <code>--begin=&lt;time&gt;</code> Submit the batch script to the Slurm controller immediately, like normal, but tell the controller to defer the allocation of the job until the specified time.   Time may be of the form HH:MM:SS to run a job at a specific time of day (seconds are optional). <code>#SBATCH   --begin=16:00</code>   <code>-C</code>, <code>--constraint=&lt;list&gt;</code> Request node feature. May be   specified with symbol <code>&amp;</code> for and, <code>\\|</code> for or, etc.   Constraints using <code>\\|</code> must be prepended with <code>NOAUTO:</code>. Click   here for more information about constraints. <code>#SBATCH   -C NOAUTO:intel16\\|intel14</code>   <code>-c</code>, <code>--cpus-per-task=&lt;ncpus&gt;</code> Require <code>&lt;ncpus&gt;</code> number of processors per task <code>#SBATCH   -c 3</code> (3 cores per node)   <code>-d</code>, <code>--dependency=&lt;dependency_list&gt;</code> Defer the start of this job until the specified dependencies have been satisfied completed. <code>&lt;dependency_list&gt;</code> Is of the form: <code>#SBATCH   -d after:&lt;JobID1&gt;:&lt;JobID2&gt;,afterok:&lt;JobID3&gt;</code>    <code>after:job_id[:jobid...]</code> This job can begin execution after the specified jobs have begun execution. <code>afterany:job_id[:jobid...]</code> This job can begin execution after the specified jobs have terminated.<code>afterburstbuffer:job_id[:jobid...]</code> This job can begin   execution after the specified jobs have terminated and any associated burst   buffer stage out operations have completed.<code>aftercorr:job_id[:jobid...]</code> A   task of this job array can begin execution after the corresponding task ID in   the specified job has completed successfully (ran to completion with an exit   code of zero).<code>afternotok:job_id[:jobid...]</code> This job can begin execution   after the specified jobs have terminated in some failed state (non-zero exit   code, node failure, timed out, etc).<code>afterok:job_id[:jobid...]</code> This job can   begin execution after the specified jobs have successfully executed (ran to   completion with an exit code of zero).<code>expand:job_id</code> Resources allocated to   this job should be used to expand the specified job. The job to expand must   share the same QOS (Quality of Service) and partition. Gang scheduling of   resources in the partition is also not supported. singleton This job can   begin execution after any previously launched jobs sharing the same job name   and user have terminated.    <code>-D</code>, <code>--chdir=&lt;directory&gt;</code> Set the working directory of the batch script to <code>&lt;directory&gt;</code> before it is executed. The path can be specified as full   path or relative path to the directory where the command is executed. <code>#SBATCH   -D /mnt/scratch/username</code>   <code>-e</code>, <code>--error=&lt;filename&gt;</code> Instruct   Slurm to connect the batch script's standard error directly to the file name   specified. By default both standard output and standard error are directed to   the same file. See <code>-o</code>, <code>--output</code> for the default file   name. <code>#SBATCH   -e /home/username/myerrorfile</code>   <code>--export=&lt;environment variables [ALL] \\| NONE&gt;</code> Identify   which environment variables are propagated to the launched application, by   default all are propagated. Multiple environment variable names should be   comma separated. <code>#SBATCH   --export=EDITOR=/bin/emacs,ALL</code>   <code>--gres=&lt;list&gt;</code> Specifies   a comma delimited list of generic consumable resources. The format of each   entry on the list is <code>name[[:type]:count]</code>, where name is that of   the consumable resource. To request for GPU, <code>--gres=gpu:k20:1</code> is an example   to request one k20 GPU. Valid GPU types are k20, k80 and v100. Note that type   is optional, but the number of GPUs is necessary. <code>#SBATCH   --gres=gpu:2</code> (request 2 GPUs per node)     <code>#SBATCH --gres=gpu:k80:2</code> (request 2 K80 GPUs per node)   <code>--gres-flags=enforce-binding</code> This   option ensures that CPUs available to the job will be those bound to   allocated GPUs. The enforce-binding type may increase the performance of some   GPU jobs. NOTE: The number of available CPUs bound with GPUs on a node is   smaller than the total number of CPUs on the node. Configuration details   can be seen in <code>/etc/slurm/gres.conf</code> . <code>#SBATCH   --gres-flags=enforce-binding</code>   <code>-G</code>, <code>--gpus=[&lt;type&gt;:]&lt;number&gt;</code> Specify   the total number of GPUs required for the job. An optional GPU type   specification can be supplied. Valid GPU types are k20, k80 and v100.   Note that type is optional, but the number of GPUs is necessary. <code>#SBATCH   --gpus=k80:2</code> (request 2 k80 GPUs for entire job)     <code>#SBATCH --gpus=2</code> (request 2 GPUs for entire job)   <code>--gpus-per-node=[&lt;type&gt;:]&lt;number&gt;</code> Specify   the number of GPUs required for the job on each node included in the job's   resource allocation. An optional GPU type specification can be   supplied. Valid GPU types are k20, k80 and v100. Note that type is   optional, but the number of GPUs is necessary. <code>#SBATCH   --gpus-per-node=v100:8</code> (request 8 v100 GPUs for each node requested by job)     <code>#SBATCH --gpus-per-node=8</code> (request 8 GPUs for each node requested by job)   <code>--gpus-per-task=[&lt;type&gt;:]&lt;number&gt;</code> Specify   the number of GPUs required for the job on each task to be spawned in the   job's resource allocation. An optional GPU type specification can be   supplied. Valid GPU types are k20, k80 and v100. Note that type is   optional, but the number of GPUs is necessary. <code>#SBATCH   --gpus-per-task=k80:2</code> (request 2 k80 GPUs for each task requested by job)     <code>#SBATCH --gpus-per-task=2</code> (request 2 GPUs for each task requested by job)   <code>-H</code>, <code>--hold</code> Specify   the job is to be submitted in a held state (priority of zero). A held job can   now be released using scontrol to reset its priority (e.g. <code>scontrol release &lt;job_id&gt;</code>).    <code>-I</code>, <code>--immediate</code> The   batch script will only be submitted to the controller if the resources   necessary to grant its job allocation are immediately available. If the job   allocation will have to wait in a queue of pending jobs, the batch script   will not be submitted.    <code>-i</code>, <code>--input=&lt;filename pattern&gt;</code> Instruct   Slurm to connect the batch script's standard input directly to the file name   specified in the \"filename pattern\".    <code>-J</code>, <code>--job-name=&lt;jobname&gt;</code> Specify   a name for the job allocation. <code>#SBATCH   -J MySuperComputing</code>   <code>--jobid=&lt;jobid&gt;</code> Allocate   resources as the specified job id.    <code>-L</code>, <code>--licenses=&lt;license&gt;</code> Specification   of licenses (or other resources available on all nodes of the cluster) which   must be allocated to this job. <code>#SBATCH   -L comsol@1718@lm-01.i</code>   <code>--mail-type=&lt;type&gt;</code> Notify   user by email when certain event types occur. Valid type values are NONE, BEGIN, END, FAIL, REQUEUE, ALL   (equivalent to BEGIN, END, FAIL, REQUEUE, and STAGE_OUT), STAGE_OUT (burst   buffer stage out and teardown completed), TIME_LIMIT, TIME_LIMIT_90 (reached   90 percent of time limit), TIME_LIMIT_80 (reached 80 percent of time limit),   TIME_LIMIT_50 (reached 50 percent of time limit) and ARRAY_TASKS (send emails   for each array task). <code>#SBATCH   --mail-type=BEGIN,END</code>   <code>--mail-user=&lt;user&gt;</code> User   to receive email notification of state changes as defined by <code>--mail-type</code>. The default value is   the submitting user. <code>#SBATCH   --mail-user=user@msu.edu</code>   <code>--mem=&lt;size[units]&gt;</code> Specify   the real memory required per node. <code>#SBATCH   --mem=2G</code> (M or G bytes)   <code>--mem-per-cpu=&lt;size[units]&gt;</code> Minimum   memory required per allocated CPU <code>#SBATCH   --mem-per-cpu=2G</code> (M or G bytes)   <code>-N</code>, <code>--nodes=&lt;minnodes[-maxnodes]&gt;</code> Request   that a minimum of <code>minnodes</code> nodes   be allocated to this job. A maximum node count may also be specified   with <code>maxnodes</code>. If   only one number is specified, this is used as both the minimum and maximum   node count. <code>#SBATCH   --nodes=2-4</code> (Request 2 to 4 different nodes)   <code>--no-requeue</code> Request   that a job not be requeued under any circumstances. Jobs are requeued by   default if a node they are running on fails. This options may be useful for   jobs that will not run properly after having run partially and failing. <code>#SBATCH   --no-requeue</code>   <code>-n</code>, <code>--ntasks=&lt;number&gt;</code> Request   total <code>&lt;number&gt;</code> of   tasks. The default is one task per node, but note that the <code>--cpus-per-task</code> option will   change this default. <code>#SBATCH   -n 4</code> (All tasks could be in 1 to 4 different nodes)   <code>--ntasks-per-node=&lt;ntasks&gt;</code> Request   that <code>&lt;ntasks&gt;</code> be   invoked on each node. This is related to <code>--cpus-per-task=ncpus</code>,   but does not require knowledge of the actual number of cpus on each node.    <code>--tasks-per-node=&lt;ntasks&gt;</code>     <code>-o</code>, <code>--output=&lt;filename pattern&gt;</code> Instruct   Slurm to connect the batch script's standard output directly to the file name   specified in the <code>&lt;filename pattern&gt;</code>. <code>#SBATCH   -o /home/username/output-file</code>    The default file name is <code>slurm-%j.out</code>, where the   <code>%j</code> is replaced by the job ID. For job arrays, the default file   name is <code>slurm-%A_%a.out</code>, <code>%A</code> is replaced by the job ID   and <code>%a</code> with the array index. Need a file name or filename pattern not just a directory.   <code>-t</code>, <code>--time=&lt;time&gt;</code> Set   a limit on the total run time of the job allocation. The total run time in   the form: HH:MM:SS or DD-HH:MM:SS <code>#SBATCH   -t 00:20:00</code>   <code>--tmp=&lt;size[units]&gt;</code> Specify   a minimum amount of temporary disk space per node. <code>#SBATCH   --tmp=2G</code>   <code>-v</code>, <code>--verbose</code> Increase   the verbosity of sbatch's informational messages. Multiple <code>v</code> will further increase sbatch's   verbosity. By default only errors will be displayed.    <code>-w</code>, <code>--nodelist=&lt;node name list&gt;</code> Request   a specific list of your buy-in nodes. The job will contain all of these hosts and possibly   additional hosts as needed to satisfy resource requirements. <code>#SBATCH   --nodelist=host1,host2,host3,...</code>    The list may be specified as a comma-separated list of hosts, a range of   hosts, or a filename. The host list will be assumed to be a filename if it   contains a <code>/</code> character. <code>#SBATCH -w host[1-5,7,...]</code>     <code>#SBATCH -w /mnt/home/userid/nodelist</code>   <code>-x</code>, <code>--exclude=&lt;node name list&gt;</code> Explicitly   exclude certain nodes from the resources granted to the job.","title":"List of Job Specifications"},{"location":"Load_the_software/","text":"<p>When running <code>module spider orthomcl</code> on a dev-node, you will see the following:</p> <pre><code>----------\nOrthoMCL:\n----------\n    Description:\n      OrthoMCL is a genome-scale algorithm for grouping orthologous protein\n      sequences.\n\n     Versions:\n        OrthoMCL/2.0.9-custom-Perl-5.24.0\n        OrthoMCL/2.0.9-Perl-5.24.0\n</code></pre> <p>They are the same version of OrthoMCL, except that the custom one has fixed a possible error due to sequence identifier. See the\u00a0issue\u00a0reported in GitHub.</p> <p>Since OrthoMCL uses BLAST, it needs to be loaded as well. An example of loading OrthoMCL would be</p> <pre><code>module purge\nmodule load icc/2016.3.210-GCC-5.4.0-2.26  impi/5.1.3.181\nmodule load OrthoMCL/2.0.9-custom-Perl-5.24.0\nmodule load BLAST/2.2.26-Linux_x86_64\n</code></pre> <p>The next thing you need to do is to request and set up your MySQL configuration file, see\u00a0https://docs.icer.msu.edu/MySQL_configuration/</p> <p>After you have your config file (say <code>orthomcl.config</code>) ready, you need to run <code>orthomclInstallSchema</code> to install the required schema into the database:</p> <pre><code>orthomclInstallSchema orthomcl.config install_schema.log\n</code></pre> <p>For the rest of your analysis steps, please read\u00a0http://orthomcl.org/common/downloads/software/v2.0/UserGuide.txt</p>","title":"Load the software"},{"location":"Local_File_Systems/","text":"<p>The local file systems are available on each cluster compute node and dev-node. However, the files saved in each node are different from the others. Their introductions are below.</p>","title":"Local File Systems"},{"location":"Local_File_Systems/#tmp-or-mntlocal","text":"<p>LOCAL space is a hard drive on a node. The files on this space can be accessed locally on each node without going through network. This space is a good choice for jobs using a single node or multiple nodes where I/O is processed only on each node's local file. \u00a0When network traffic is high, using this space will likely allow your program to run faster than running on HOME, RESEARCH or SCRATCH space. Please note that LOCAL space is\u00a0shared with all processes running on the same node\u00a0and there is\u00a0no direct I/O from other nodes. The space also has\u00a0no auto backup. It should be used as temporary storage space. When the execution of programs in a job is completed, any useful files in this space should be saved back to HOME or RESEARCH space.</p>","title":"/tmp or /mnt/local"},{"location":"Local_File_Systems/#tmpdir","text":"<p>TMPDIR directory is automatically created as<code>/tmp/local/$SLURM_JOBID</code>\u00a0when your job starts on the local node and is deleted after it finishes.</p>","title":"$TMPDIR"},{"location":"Local_File_Systems/#devshm","text":"<p>RAMDISK space is a \u201clogical\u201d storage space; it sits inside a node\u2019s RAM, not disk. Linux supports a system tool that provides an interface for users to intercept the I/O requests to <code>/dev/shm</code> with memory operations. \u00a0We may think of it as a virtual disk in memory. Due to this nature, access to this space is actually access to RAM. Since the bandwidth of the access is much higher, \u00a0the I/O operations are considered faster than LOCAL space. However, since programs take up some of the node's memory, the usable RAM space for program execution becomes less. This space is good for programs that do not require large memory and perform very frequent I/O on small files.</p>  <p>Warning</p> <p>Please limit the use of the local spaces. It is also used for MPI runtime to implement fast communication between MPI processes. Users are advised to clean up the space after use. The files that over 2 weeks old will be removed without notice. In the situation that the space is over 90% full, we may clean up those not currently used files without notice.</p>","title":"/dev/shm"},{"location":"Makefile/","text":"<p>Makeifles are a simple way to organize code compilation. This tutorial offers a very basic idea of what is possible using make. For this tutorial, please download\u00a0three files,  a main program hello.c,  a functional code hellofunc.c,  and an include file hello.h under the 'hello' directory.</p>  <p>To compile these codes, you would use the following command:</p> <pre><code>gcc -o hello hello.c hellofunc.c -I.\n</code></pre> <p>This command compiles the two c files, and names the executable hello. With the '-I.' flag, gcc will look in the current directory for the include file 'hello.h'. With only two .c files, it is easy to compile with the above approach, but with more files, it is more likely having typos. In addition,\u00a0if you are only making changes to one .c file, the above approach\u00a0recompiles all of .c files every time which is time-consuming and inefficient.</p> <p>So it is time to learn how makefile will be helpful for such cases. First, create a file which has the following two lines. (The filename should be\u00a0makefile or Makefile), and put it under the 'hello' directory.</p> <pre><code>hello: hello.c hellofunc.c\n\u00a0 \u00a0 \u00a0 gcc -o hello hello.c hellofunc.c -I.\n</code></pre> <p>Now, type make on the terminal and check if the executable is created. The make\u00a0command will execute the compile command as you have written it in the makefile. Note that make with no arguments executes the first rule in the file. Furthermore, by putting the list of files on which the command depends on the first line after the ':', make knows that the rule hello needs to be executed if any of those files change.\u00a0One very important thing to note is that there should be a tab before the gcc command in the makefile (multiple spaces do not work!). There must be a tab at the beginning of any command, otherwise, you will get a lot of errors.</p> <p>Can we make it a little bit more efficient? Let's modify our makefile as following:</p> <pre><code>CC=gcc \nCFLAGS=-I.\nhello: hello.o hellofunc.o\n    $(CC) -o hello hello.o hellofunc.o\n</code></pre> <p>In this makefile,\u00a0we define CC and CFLAGS, which are special macros communicating to make how we want to compile the files hello.c and hellofunc.c. In particular, CC is for the C compiler, and CFLAGS is the list of flags to pass to C compiler. By putting the object files (hello.o and hellofunc.o) in the dependency list and in the rule, make knows it must first compile the .c files\u00a0individually, and then build the executable hello.</p> <p>If your project is small, like consisting of a few separate codes, this form of makefile is enough to handle the set of codes. However, this makefile misses include files. For example, if you made a change to 'hello.h', make would not recompile the .c files, even though they needed to be. In order to fix this problem, we need to tell make that all .c files depend on certain .h files. It can be done by writing a simple rule and adding it to the makefile.</p> <pre><code>CC=gcc\nCFLAGS=-I.\nDEPS = hello.h\n\n%.o: %.c $(DEPS)\n    $(CC) -c -o $@ $&lt; $(CFLAGS)\n\nhello: hello.o hellofunc.o \n    $(CC) -o hello hello.o hellofunc.o\n</code></pre> <p>This addition first creates the macro DEPS (the macro name does not have to be DEPS. You can use any name.), which is the set of .h files on which the .c files depend. Then we define a rule for all .o files. The rule says that the .o file depends on the .c files, and the .h files which are included in the DEPS. Next, the rule says that to generate the .o file, make needs to compile the .c file using the compiler defined in the CC. The -c flag says to generate the object file, the -o $@ says to put the output of the compilation in the file named on the left side of the :, the $&lt; is the first item in the dependencies list, and the CFLAGS macro is defined on the 2nd line.</p> <p>For the simplification, you can use special macros $@ and $^, which are the left and right sides of the :, respectively, to make the overall compilation rule more general. In the example below, all of the include files should be listed as part of the macro DEPS, and all of the object files should be listed as part of the macro OBJ.</p> <pre><code>CC=gcc\nCFLAGS=-I.\nDEPS = hello.h\nOBJ = hello.o hellofunc.o \n\n%.o: %.c $(DEPS)\n    $(CC) -c -o $@ $&lt; $(CFLAGS)\n\nhello: $(OBJ)\n    $(CC) -o $@ $^ $(CFLAGS)\n</code></pre> <p>Now you have a good sense of makefile. For more information on makefiles and the make function, check out the GNU Make Manual, which will tell you everything on makefile.</p> <p>You can download some makefile examples using getexample in our HPCC.</p>","title":"Makefile"},{"location":"Managing_Perl_modules/","text":"<p>To manage your Perl environment, we recommend using perlbrew,\u00a0http://metacpan.org/module/perlbrew. This allows you to create one or more self-contained Perl installations which live in your local $HOME directory, and allows you to easily and reliably install Perl modules. Instructions are as follows:</p>","title":"Managing Perl modules"},{"location":"Managing_Perl_modules/#installation","text":"<ol> <li> <p>It is the simplest to use the perlbrew installer, just paste this     statement to your terminal:</p> <pre><code>[ongbw@dev-intel10 ~]$ curl -kL http://install.perlbrew.pl | bash\n</code></pre> </li> <li> <p>This installs perlbrew to $HOME/perl5/perlbrew</p> </li> <li> <p>Append \"source \\~/perl5/perlbrew/etc/bashrc\" to the end of your     \\~/.bash_profile</p> <pre><code>[ongbw@dev-intel10 ~]$ echo \"source $HOME/perl5/perlbrew/etc/bashrc\" &gt;&gt; ~/.bash_profile\n</code></pre> <p>Log out of that development node, and log back in. You should now have a fully functional perlbrew installation. You can check this by checking that the environment variables are setup properly</p> <pre><code>[ongbw@dev-intel10 ~]$ env |grep -i perl\nPERL5LIB=/usr/local/lib/perl5\nPERLBREW_BASHRC_VERSION=0.63\nPERLBREW_ROOT=/mnt/home/ongbw/perl5/perlbrew\nPATH=/mnt/home/ongbw/perl5/perlbrew/bin:/opt/software/Stata/12.0:/opt/software/R/2.15.1--GCC-4.4.5/bin:/opt/software/MATLAB/R2011b/bin:/opt/software/Python/2.7.2--GCC-4.4.5/bin:/opt/software/cmake/2.8.5--GCC-4.4.5/bin:/opt/software/OpenMPI/1.4.3--GCC-4.4.5/bin:/usr/lib64/qt-3.3/bin:/opt/software/lmod/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/usr/local/hpcc/bin:/mnt/home/ongbw/bin\nPERLBREW_HOME=/mnt/home/ongbw/.perlbrew\n</code></pre> </li> <li> <p>Next, we will install     cpanminus\u00a0http://metacpan.org/module/App::cpanminus, a     script to get, unpack, build and install modules from CPAN,</p> <pre><code>$ perlbrew install-cpanm\n</code></pre> </li> </ol>","title":"Installation"},{"location":"Managing_Perl_modules/#brief-tutorial","text":"<ul> <li>Now that we have a working perlbrew installation, there are various     useful commands. perlbrew available shows the version of Perl     available for install</li> </ul> <pre><code>    [ongbw@dev-intel10 ~]$ perlbrew available\n    perl-5.19.0\n    perl-5.18.0\n    perl-5.16.3\n    perl-5.14.4\n    perl-5.12.5\n    perl-5.10.1\n    perl-5.8.9\n    perl-5.6.2\n    perl5.005_04\n    perl5.004_05\n    perl5.003_07\n</code></pre> <ul> <li>perlbrew list shows the list of Perl installations managed by     perlbrew. It is an empty list since I haven't installed anything     yet:</li> </ul> <pre><code>    [ongbw@dev-intel10 ~]$ perlbrew list\n    [ongbw@dev-intel10 ~]$ \n</code></pre> <ul> <li>To install a new version of perl using perlbrew, type perlbrew     install \\&lt;version&gt;. It might take a while.</li> </ul> <pre><code>    [ongbw@dev-intel10 ~]$ perlbrew install perl-5.18.0\n</code></pre> <p>we can now check that it is indeed installed:</p> <pre><code>    [ongbw@dev-intel10 ~]$ perlbrew list\n    perl-5.18.0 \n</code></pre> <ul> <li>To use this version of perl, type perlbrew use</li> </ul> <pre><code>    [ongbw@dev-intel10 ~]$ perlbrew use perl-5.18.0\n    [ongbw@dev-intel10 ~]$ perl --version\n\n    This is perl 5, version 18, subversion 0 (v5.18.0) built for x86_64-linux\n\n    Copyright 1987-2013, Larry Wall\n\n    Perl may be copied only under the terms of either the Artistic License or the GNU General Public License, which may be found in the Perl 5 source kit.\n\n    Complete documentation for Perl, including FAQ lists, should be found on this system using \"man perl\" or \"perldoc perl\". If you have access to the Internet, point your browser at http://www.perl.org/, the Perl Home Page.\n</code></pre> <ul> <li>To install a module for loaded version of perl, use cpanm. It will     pull     all the dependencies nicely for you</li> </ul> <pre><code>    [ongbw@dev-intel10 CPAN]$ cpanm Pod::WikiDoc\n    --&gt; Working on Pod::WikiDoc\n    Fetching http://www.cpan.org/authors/id/D/DA/DAGOLDEN/Pod-WikiDoc-0.20.tar.gz ... OK\n    Configuring Pod-WikiDoc-0.20 ... OK\n    ==&gt; Found dependencies: File::pushd, Probe::Perl, IPC::Run3, IO::String, Getopt::Lucid, Parse::RecDescent\n    --&gt; Working on File::pushd\n    Fetching http://www.cpan.org/authors/id/D/DA/DAGOLDEN/File-pushd-1.005.tar.gz ... OK\n    Configuring File-pushd-1.005 ... OK\n    Building and testing File-pushd-1.005 ... OK\n    Successfully installed File-pushd-1.005\n    --&gt; Working on Probe::Perl\n    Fetching http://www.cpan.org/authors/id/K/KW/KWILLIAMS/Probe-Perl-0.02.tar.gz ... OK\n    Configuring Probe-Perl-0.02 ... OK\n    Building and testing Probe-Perl-0.02 ... OK\n    Successfully installed Probe-Perl-0.02\n    --&gt; Working on IPC::Run3\n    Fetching http://www.cpan.org/authors/id/R/RJ/RJBS/IPC-Run3-0.045.tar.gz ... OK\n    Configuring IPC-Run3-0.045 ... OK\n    Building and testing IPC-Run3-0.045 ... OK\n    Successfully installed IPC-Run3-0.045\n    --&gt; Working on IO::String\n    Fetching http://www.cpan.org/authors/id/G/GA/GAAS/IO-String-1.08.tar.gz ... OK\n    Configuring IO-String-1.08 ... OK\n    Building and testing IO-String-1.08 ... OK\n    Successfully installed IO-String-1.08\n    --&gt; Working on Getopt::Lucid\n    Fetching http://www.cpan.org/authors/id/D/DA/DAGOLDEN/Getopt-Lucid-1.05.tar.gz ... OK\n    Configuring Getopt-Lucid-1.05 ... OK\n    ==&gt; Found dependencies: Exception::Class::TryCatch, Exception::Class\n    --&gt; Working on Exception::Class::TryCatch\n    Fetching http://www.cpan.org/authors/id/D/DA/DAGOLDEN/Exception-Class-TryCatch-1.12.tar.gz ... OK\n    Configuring Exception-Class-TryCatch-1.12 ... OK\n    ==&gt; Found dependencies: Exception::Class\n    --&gt; Working on Exception::Class\n    Fetching http://www.cpan.org/authors/id/D/DR/DROLSKY/Exception-Class-1.37.tar.gz ... OK\n    Configuring Exception-Class-1.37 ... OK\n    ==&gt; Found dependencies: Class::Data::Inheritable, Devel::StackTrace\n    --&gt; Working on Class::Data::Inheritable\n    Fetching http://www.cpan.org/authors/id/T/TM/TMTM/Class-Data-Inheritable-0.08.tar.gz ... OK\n    Configuring Class-Data-Inheritable-0.08 ... OK\n    Building and testing Class-Data-Inheritable-0.08 ... OK\n    Successfully installed Class-Data-Inheritable-0.08\n    --&gt; Working on Devel::StackTrace\n    Fetching http://www.cpan.org/authors/id/D/DR/DROLSKY/Devel-StackTrace-1.30.tar.gz ... OK\n    Configuring Devel-StackTrace-1.30 ... OK\n    Building and testing Devel-StackTrace-1.30 ... OK\n    Successfully installed Devel-StackTrace-1.30\n    Building and testing Exception-Class-1.37 ... OK\n    Successfully installed Exception-Class-1.37\n    Building and testing Exception-Class-TryCatch-1.12 ... OK\n    Successfully installed Exception-Class-TryCatch-1.12\n    Building and testing Getopt-Lucid-1.05 ... OK\n    Successfully installed Getopt-Lucid-1.05\n    --&gt; Working on Parse::RecDescent\n    Fetching http://www.cpan.org/authors/id/J/JT/JTBRAUN/Parse-RecDescent-1.967009.tar.gz ... OK\n    Configuring Parse-RecDescent-1.967009 ... OK\n    Building and testing Parse-RecDescent-1.967009 ... OK\n    Successfully installed Parse-RecDescent-1.967009\n    Building and testing Pod-WikiDoc-0.20 ... OK\n    Successfully installed Pod-WikiDoc-0.20\n    11 distributions installed\n</code></pre>","title":"Brief tutorial"},{"location":"Managing_Perl_modules/#local-libraries","text":"<p>If you want to use a local lib/ for perl modules with PERL5LIB (recommended), you'll have to make a few changes.</p> <ol> <li> <p>First remove ~/.cpan (or modify     ~/.cpan/CPAN/MyConfig.pm appropriately.</p> <pre><code>$ rm -rf ~/.cpan\n</code></pre> </li> <li> <p>Then, issue the command</p> <pre><code>$ cpanm --local-lib ~/lib App::local::lib::helper\n</code></pre> </li> </ol>","title":"Local Libraries"},{"location":"Managing_Perl_modules/#additional-documentation","text":"<ul> <li>perlbrew: http://metacpan.org/module/perlbrew</li> <li>cpanm:\u00a0http://metacpan.org/module/App::cpanminus</li> <li>local-lib: http://metacpan.org/module/local::lib</li> <li>local::lib::helper: https://metacpan.org/module/App::local::lib::helper</li> </ul>","title":"Additional Documentation"},{"location":"Mapping_HPC_drives_with_SSHFS/","text":"<p>Besides mapping HPCC drives with SMB,  SSHFS can also enable mounting of HPCC file systems on a local computer. Different from SMB mapping, which can only map to home or research space via the MSU campus network, this method can also work on your scratch space and uses any internet network.</p> <p>If users would like to use SSHFS without entering their password, they can generate an authentication key by following the direction of SSH Key-Based Authentication. After connecting to the HPCC with a terminal and executing the command <code>ssh-keygen</code>, two files,\u00a0<code>id_rsa</code> and <code>id_rsa.pub</code>, are generated\u00a0in the hidden directory <code>~/.ssh</code>. Besides copying the file\u00a0<code>id_rsa.pub</code> to the file <code>authorized_keys</code>, you also need to copy (transfer) the file <code>id_rsa</code> to your local computer. Make sure you know the path of the file in the local computer. You will need the file location when you set up SSHFS.</p> <ul> <li>On Mac OSX</li> <li>On Linux Systems</li> <li>On Windows OS</li> <li>Other Mapping Software</li> </ul>","title":"Mapping HPC drives with SSHFS"},{"location":"Mapping_HPC_drives_with_SSHFS/#on-mac-osx","text":"<ol> <li> <p>Download and install (or upgrade) the most recent versions of the following packages: FUSE for macOS and SSHFS from https://osxfuse.github.io.</p> </li> <li> <p>Reboot (not required)</p> </li> <li> <p>Using the Terminal, create a directory (as <code>&lt;local_mount_point&gt;</code> in step 4) for each filesystem you wish to mount. \u00a0If you are creating the folder outside of your home directory, you may need to use <code>sudo</code> before each command (sudo = superuser do ).\u00a0</p> <pre><code>[MacBook-Pro:~ icer2]$ mkdir &lt;local_mount_point&gt;\n/* begin example home directory */\n[MacBook-Pro:~ icer2]$ mkdir /Users/icer2/hpcc_home\n/* end example home directory */\n/* begin example scratch directory in the Mac /Volumes folder where drives are mounted */\n[MacBook-Pro:~ icer2]$ mkdir /Volumes/scratch\n/* end example scratch directory */\n</code></pre> </li> <li> <p>Mount the directory using  the <code>sshfs</code> command. We suggest you add     these additional flags to the command to make it be more \"Mac-like\"     :\u00a0<code>-ocache=no</code> , \u00a0<code>-onolocalcaches</code> \u00a0and <code>-o volname=hpcc_home</code> .\u00a0     For the last option, '<code>-o volname</code>' is the name that displays     in the Finder title bar, so change it for difference file     folders (e.g. use\u00a0<code>-o volname=hpcc_scratch</code> for your scratch     folder). After running the command, enter the password for logging     into HPCC and the FUSE drive icon will show on the desktop of     your local Mac computer.</p> <pre><code>[MacBook-Pro:~ icer2]$ sshfs &lt;user_id&gt;@rsync.hpcc.msu.edu:&lt;remote_directory_to_mount&gt; &lt;local_mount_point&gt; -ovolname=hpcc -o allow_other,defer_permissions,follow_symlinks,reconnect -ocache=no -onolocalcaches\n&lt;net_id&gt;@rsync.hpcc.msu.edu's password:\n/* begin example hpc's home directory, using /mnt/home/hpc/ */\n[MacBook-Pro:~ icer2]$ sshfs hpc@rsync.hpcc.msu.edu:/mnt/home/hpc/ /Users/icer2/hpcc_home -o volname=hpcc_home -o allow_other,defer_permissions,follow_symlinks,reconnect -ocache=no -onolocalcaches\nhpc@rsync.hpcc.msu.edu's password:\n/* end example home directory */\n/* begin example hpc's scratch directory with authorized key file ~/.ssh/id_rsa, using /mnt/gs18/scratch/users/hpc */\n[MacBook-Pro:~ icer2]$ mkdir /Volumes/scratch\n[MacBook-Pro:~ icer2]$ sshfs hpc@rsync.hpcc.msu.edu:/mnt/gs18/scratch/users/hpc /Volumes/scratch -o volname=hpcc_scratch -o allow_other,defer_permissions,follow_symlinks,reconnect,IdentityFile=~/.ssh/id_rsa -ocache=no -onolocalcaches\n(No password input)\n/* end example scratch directory */\n</code></pre> <p>If <code>&lt;remote_directory_to_mount&gt;</code> is a static link, please make sure to put <code>/</code> at the end of the directory path:</p> <ul> <li>For your home space, please use <code>/mnt/home/&lt;user_id&gt;/</code> instead\u00a0 of <code>/mnt/home/&lt;user_id&gt;</code>.</li> <li>For your research space, please use <code>/mnt/research/&lt;group_name&gt;/</code> instead of <code>/mnt/research/&lt;group_name&gt;</code>.</li> </ul> <p>As the above example (starting from line 5), home space <code>/mnt/home/hpc/</code> is used on line 7 instead of <code>/mnt/home/hpc</code>.</p> </li> <li> <p>To unmount a filesystem, use the <code>umount</code> command. </p>  <p>Note</p> <p>It's just the letter u before mount, NOT <code>unmount</code>.</p>  <pre><code>    umount &lt;local_mount_point&gt;\n    /* begin example */\n    umount /Users/icer2/hpcc_home\n    /* end example */\n</code></pre> <p>You'll see these folders in the finder if you use the Go menu, but you won't see them listed in the left side with the other mounted drives. \u00a0You must use the terminal and <code>umount</code> command to disconnect.</p> </li> </ol> <p>References:</p> <p>https://www.digitalocean.com/community/tutorials/how-to-use-sshfs-to-mount-remote-file-systems-over-ssh</p>","title":"On Mac OSX"},{"location":"Mapping_HPC_drives_with_SSHFS/#on-linux-systems","text":"<p>Please refer to this web site:</p> <p>https://tecadmin.net/install-sshfs-on-linux-and-mount-remote-filesystem/</p> <p>for how to mount remote filesystem over SSH on Linux.</p>","title":"On Linux Systems"},{"location":"Mapping_HPC_drives_with_SSHFS/#on-windows-os","text":"<ol> <li>Install the latest stable release of <code>winfsp</code> (version 1.11.22176 as of 8/11/22) WinFsp installer </li> <li>Full upstream documentation: WinFsp Documentation</li> </ol>  <p>Note</p> <p>Administrative access is required</p>  <ol> <li>Install the latest stable release of SSHFS-Win (version 3.5.20357 as of 8/11/22) SSHFS-Win installer </li> <li>Full upstream documentation: SSHFS-Win Documentation</li> </ol>  <p>Note</p> <p>Administrative access is required</p>  <ol> <li> <p>Mounting of Home and Scratch can be done through the command prompt or with the windows file explorer once WinFsp and SSHFS-Win are installed.</p> <ol> <li>Mounting Home directory with the command prompt<ul> <li>DriveLetter (H: or Z: etc.) is optional, but username is required </li> <li>Command structure <code>C:\\&gt;net use [DriveLetter&gt;:] \\\\sshfs\\&lt;username&gt;@rsync.hpcc.msu.edu</code></li> <li>example <code>C:\\&gt; net use F: \\\\sshfs\\ryanjos2@rsync.hpcc.msu.edu</code></li> </ul> </li> <li>Mounting Scratch directory with the command prompt requires a slightly different command.  Note the addition of '.r' to sshfs (<code>\\\\sshfs.r\\</code>) that designates to start from the root directory.<ul> <li>DriveLetter (H: or Z: etc.) is optional, but username is required </li> <li>Command structure <code>C:\\&gt;net use [DriveLetter:] \\\\sshfs.r\\&lt;username&gt;@rsync.hpcc.msu.edu\\mnt\\scratch\\&lt;username&gt;</code></li> <li>example <code>C:\\&gt;net use R: \\\\sshfs.r\\ryanjos2@rsync.hpcc.msu.edu\\mnt\\scratch\\ryanjos2</code></li> </ul> </li> <li>Mounting Home or Scratch directory with Microsoft file browser:<ul> <li>Both Home and Scratch use the same steps, just with different network paths<ul> <li>Home: <code>\\\\sshfs\\username@rsync.hpcc.msu.edu</code></li> <li>Scratch:  <code>\\\\sshfs.r\\username@rsync.hpcc.msu.edu\\mnt\\scratch\\username</code></li> </ul> </li> <li>In file browser on the 'This PC' page, select 'Map Network Drive' </li> <li>Fill in the appropriate network path (<code>\\\\sshfs\\username@rsync.hpcc.msu.edu</code>), select a drive letter and any desired options, and click Finish. </li> </ul> </li> </ol> </li> <li> <p>Mounting a research space requires an additional tool, the SSHFS-Win Manager GUI.  This Tool can also be used to mount Home and scratch space as well using the correct directory.  These filesystems do not need any advanced settings.  For home directory, leave the path blank.  For Scratch space, use \\mnt\\scratch\\hpcc_username\\ for the path with the trailing slash.</p> <ol> <li>Install latest stable release of sshfs-win-manager (1.3.1 as of 8/11/22)<ul> <li>SSHFS-Win Manager installer</li> <li>SSHFS-Win Manager documentation</li> <li>Administrative privileges are not required</li> </ul> </li> <li>open the SSHFS-Win Manager application      If the SSHFS-Win Manager application is minimized, it can be opened using the task bar     </li> <li>Create a new mount for the research space <ol> <li>Basics tab<ul> <li>Name: HPCC-groupname (descriptive name for mounted drive)</li> <li>Host: rsync.hpcc.msu.edu</li> <li>Auth Method: password (ask on connect)</li> <li>Path: /mnt/research/research_groupname/  (Note: ending slash)</li> <li>Drive letter can be left as auto or you can select a specific Drive letter </li> </ul> </li> <li>Select the Advanced tab and add the following options (connect on startup and reconnect on connection loss are optional)<ul> <li>Custom Command Line Params: On</li> <li>idmap -&gt; user</li> <li>create_file_umask -&gt; 000</li> <li>umask -&gt; 000</li> </ul> </li> <li>sftp_server -&gt; <code>sg research_groupname -c /usr/libexec/openssh/sftp-server</code> </li> </ol> </li> <li>After Saving the new drive mapping, click the connect icon </li> <li>After successful authentication, the drive should show connected (green) and the folder is now available in the file browser </li> </ol> </li> </ol>","title":"On Windows OS"},{"location":"Mapping_HPC_drives_with_SSHFS/#other-mapping-software","text":"<ul> <li>DirectNet Drive: http://www.directnet-drive.net/ for\u00a0Windows only</li> </ul>","title":"Other Mapping Software"},{"location":"Mapping_HPC_drives_with_Samba/","text":"<p>Warning</p> <ol> <li> <p>Samba now uses campus AD for user authentication, if you are unable to login and have not updated your netid password recently please try updating your netid password before opening a ticket</p> </li> <li> <p>This will only work if your computer has a university IP address. If you are off campus, you can use the MSU VPN  to obtain an MSU IP, which is available to all graduate students, staff and faculty.\u00a0</p> </li> <li> <p>If file transfer speed is a concern please use the sftp protocol for transferring large data sets, or use our Globus endpoint.</p> </li> </ol>  <p>The following tutorial will show you how to map your HPC home or research directory using SMB or CIFS File Sharing.</p> <ul> <li>Determining your Network Path</li> <li>Windows 10<ul> <li>Command-line Windows NetBIOS Commands</li> </ul> </li> <li>MacOS Example</li> <li>Linux</li> <li>Ubuntu Mount Example</li> <li>Ubuntu Example (Older Versions)</li> <li>More Information</li> </ul>","title":"Mapping HPC drives with Samba"},{"location":"Mapping_HPC_drives_with_Samba/#determining-your-network-path","text":"<p>We have the powertools command <code>show-samba-paths</code> to show all paths of your home and research space:</p> <pre><code>$ ml powertools                    # if powertools is not loaded\n$ show-samba-paths\n\n  HOME      |        Samba Path\n===================================================================\n  username  |  \\\\ufs.hpcc.msu.edu\\home-021\\username      (Windows)\n            |  smb://ufs.hpcc.msu.edu/home-021/username      (Mac)\n\n\n  RESEARCH        |        Samba Path\n===============================================================================\n  helpdesk        |  \\\\ufs.hpcc.msu.edu\\rs-011\\helpdesk              (Windows)\n                  |  smb://ufs.hpcc.msu.edu/rs-011/helpdesk              (Mac)\n------------------+------------------------------------------------------------\n  common-data     |  \\\\ufs-12-b.hpcc.msu.edu\\common-data             (Windows)\n                  |  smb://ufs-12-b.hpcc.msu.edu/common-data             (Mac)\n------------------+------------------------------------------------------------\n  BiCEP           |  \\\\ufs.hpcc.msu.edu\\rs-001\\BiCEP                 (Windows)\n                  |  smb://ufs.hpcc.msu.edu/rs-001/BiCEP                 (Mac)\n------------------+------------------------------------------------------------\n  education-data  |  \\\\ufs-12-b.hpcc.msu.edu\\education-data          (Windows)\n                  |  smb://ufs-12-b.hpcc.msu.edu/education-data          (Mac)\n</code></pre> <p>where the paths are the same for\u00a0Mac and Window computers but with different formats.</p>","title":"Determining your Network Path"},{"location":"Mapping_HPC_drives_with_Samba/#windows-10","text":"","title":"Windows 10"},{"location":"Mapping_HPC_drives_with_Samba/#step-1-enable-netbios-over-tcpip-on-windows","text":"<ul> <li>Click on Desktop icon on your Windows 8 screen</li> <li>Right click on Network icon on start bar at right hand side and     click on open network and sharing center  </li> <li>Click on Change adapter settings</li> <li>Right click on your Network interface and click on Properties</li> <li>Select the Internet Protocol version 4 (TCP/IPv4)</li> <li>Click the Advanced button under the General tab.</li> <li>Click the WINS tab.  </li> <li>Click the Enable NetBIOS Over TCP/IP button.     Click Ok</li> </ul>","title":"Step 1. Enable NetBIOS over TCP/IP on Windows:"},{"location":"Mapping_HPC_drives_with_Samba/#step-2-disable-smb1","text":"","title":"Step 2. Disable SMB1"},{"location":"Mapping_HPC_drives_with_Samba/#disable-samba-v1-protocol-with-powershell","text":"This step must be completed or your client will not be able to map the drive. If you have other mounts on the HPC cluster and they are using samba V1 they will stop working.    <ol> <li> <p>Press the Windows start button</p> </li> <li> <p>In the search box type \"power shell\" </p> </li> <li> <p>Right click on the \"Windows PowerShell\" icon and select \"Run as     Administrator\" </p> </li> <li> <p>Select Ok when security warning appears </p> </li> <li> <p>Disable Samba V1 by entering the following command into the windows     power shell. </p> <pre><code>Set-ItemProperty -Path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\LanmanServer\\Parameters\" SMB1 -Type DWORD -Value 0 -Force\n</code></pre> </li> <li> <p>Ensure SMB V2 and SMB V3 are enabled by entering the follwoing     command.\u00a0 In the past, on some versions of windows and for some file     systems we recommended the opposite of this setting.\u00a0 Running this     ensures it's enabled again.\u00a0</p> <pre><code>Set-ItemProperty -Path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\LanmanServer\\Parameters\" SMB2 -Type DWORD -Value 1 -Force\n</code></pre> </li> <li> <p>Navigate to \"Computer\" and click on the text labeled \"Map Network     Drive\" at the top of the screen.</p> <p></p> <p>From this menu you need to type your Network Path. Please see\u00a0#Determining your Network Path\u00a0for help</p> </li> <li> <p>Once you have typed in your Network Path you need to click on     the box \"Connect using different credentials.\" This will open a     window where you type in your MSU netid and     password:     </p> </li> </ol>  <p>Warning</p> <p>If you aren't able to sign in,  You will need to add \"CAMPUSAD\\\" to the beginning of your username. An indicator of this issue is if Windows displays the error \"The specified network password is not correct\" in the username dialog window.</p> <p>For example: substitute \"CAMPUSAD\\sparty\" for username \"sparty\" in the username field. The slash character is a backslash. A forward slash character will not work.</p>  <ol> <li> <p>Finally, select \"Finish\" and you will see your system trying to     connect</p> <p></p> </li> </ol>","title":"Disable Samba V1 protocol with PowerShell"},{"location":"Mapping_HPC_drives_with_Samba/#command-line-windows-netbios-commands","text":"<p>If you're working in Windows, you can use command line tools to manage your drive mapping.\u00a0 These commands also work in .bat files, if you're so inclined to connect/disconnect drives in that manner.\u00a0 Note you may also have to Disable SMBV1 and Enable SMB2 per instructions above.\u00a0</p> <ol> <li>From the Start Menu -&gt; Run -&gt; type 'cmd' in the box and hit enter,     the command shell should open.\u00a0 You can then use the following     commands to diagnose, disconnect and connect drives.</li> </ol>   <pre><code># to show all mapped connections/drives\n# use 'net use'\n\nC:\\Documents and Settings\\Administrator&gt;net use\nNew connections will not be remembered.\n\n\nStatus       Local     Remote                    Network\n\n-------------------------------------------------------------------------------\nUnavailable  J:        \\\\MachineA\\consult$    Microsoft Windows Network\nUnavailable  P:        \\\\MachineB\\everyone$    Microsoft Windows Network\n             Z:        \\\\vmware-host\\Shared Folders\n                                                 VMware Shared Folders\nThe command completed successfully.\n\n# to map a drive use:\n# net use &lt;drive_letter&gt;: \\\\&lt;hostname&gt;\\&lt;mount&gt; /user:hpcc\\&lt;net_id&gt;\n# where &lt;hostname&gt; and &lt;mount&gt; were determined from above 'Determining Your Network Path' and &lt;net_id&gt; is your MSU Net ID\n\nC:\\Documents and Settings\\Administrator&gt;net use m: \\\\ufs-10-a.hpcc.msu.edu\\jal /user:hpcc\\jal\nThe command completed successfully.\n\n# You can use 'net use' again to show the drive mapping status.\n\nC:\\Documents and Settings\\Administrator&gt;net use\nNew connections will not be remembered.\n\n\nStatus       Local     Remote                    Network\n\n-------------------------------------------------------------------------------\nUnavailable  J:        \\\\MachineA\\consult$    Microsoft Windows Network\nOK           M:        \\\\ufs-10-a.hpcc.msu.edu\\jal\n                                                 Microsoft Windows Network\nUnavailable  P:        \\\\MachineB\\everyone$    Microsoft Windows Network\n             Z:        \\\\vmware-host\\Shared Folders\n                                                 VMware Shared Folders\nThe command completed successfully.\n\n# To disconnect a drive use:\n# net use &lt;drive_letter&gt;: /delete\n## NOTE: this will only drop the connection, not delete the share or any data on the share ##\n\nC:\\Documents and Settings\\Administrator&gt;net use m: /delete\nm: was deleted successfully.\n\n# you can now use 'net use' again to show your mapping status after the delete to ensure it's gone.\n\nC:\\Documents and Settings\\Administrator&gt;net use\nNew connections will not be remembered.\n\n\nStatus       Local     Remote                    Network\n\n-------------------------------------------------------------------------------\nUnavailable  J:        \\\\MachineA\\consult$    Microsoft Windows Network\nUnavailable  P:        \\\\MachineB\\everyone$    Microsoft Windows Network\n             Z:        \\\\vmware-host\\Shared Folders\n                                                 VMware Shared Folders\nThe command completed successfully.\n\n\nC:\\Documents and Settings\\Administrator&gt;\n</code></pre>","title":"Command-line Windows NetBIOS Commands"},{"location":"Mapping_HPC_drives_with_Samba/#macos-example","text":"<p>Video Tutorial - Map Home directory using MacOS</p> <ol> <li>Open the Finder. </li> <li>Under \"GO\" click on \"Connect to Server\" </li> <li>From this menu you need to type your Network Path. Please     see\u00a0#Determining your Network     Path\u00a0for     help. </li> <li>Enter your MSU NeID and password for authentication and click     \"Connect\". </li> </ol>","title":"MacOS Example"},{"location":"Mapping_HPC_drives_with_Samba/#linux","text":"<ol> <li> <p>Install smb-client</p> <p>Ubuntu / Debian</p> <pre><code>apt install smbclient\n</code></pre> <p>Red Hat / Fedora</p> <pre><code>yum install samba-client\n</code></pre> </li> <li> <p>Edit /etc/samba/smb.conf</p> <pre><code>sudo vi  /etc/samba/smb.conf\n</code></pre> </li> <li> <p>Add the following lines to disable samba V1</p> <p>This step must be completed or your client will not be able to map the drive. If you have other mounts on on the HPC cluster and they are using samba V1 they will stop working. In this case please use SSHFS .</p> <pre><code>client min protocol = SMB2\nclient max protocol = SMB3\n</code></pre> </li> </ol>","title":"Linux"},{"location":"Mapping_HPC_drives_with_Samba/#ubuntu-mount-example","text":"<ol> <li>Open a File Browser window. In the \"File\" menu, select \"Connect to     Server...\" </li> <li> <p>Type your network path in the server address box. \u00a0 (Format is the     same as the Mac format)  </p> <p> 3.  Enter your userid and password and click connect.  4.  If connected properly the drive should appear in the file manager screen. </p> </li> </ol>","title":"Ubuntu Mount Example"},{"location":"Mapping_HPC_drives_with_Samba/#ubuntu-example-older-versions","text":"<ol> <li>Open a File Browser window. In the \"File\" menu, select \"Connect to     Server...\" </li> <li>In the window that appears, select \"SSH\" from the drop-down menu     next to \"Service type,\" enter \"hpcc.msu.edu\" for \"Server,\" enter     \"/mnt/home/username\" (where username is your NetID) for \"Folder,\"     and type your username next to \"User Name.\" For quick access to the     drive in future sessions, check the \"Add bookmark\" box and enter a     descriptive label for \"Bookmark name.\" Once all of this information     has been entered, click \"Connect.\" </li> <li>After a brief delay, a new window will appear, asking for your     password. Enter it, choose whether or not your password should be     saved using the radio buttons, and click \"Connect.\" </li> <li>The password window will then close, giving way to a File Browser     window displaying the contents of your home directory. During future     sessions, access the bookmark you added to reconnect. This can be     done from the \"Bookmarks\" menu in a File Browser window. </li> </ol>","title":"Ubuntu Example (Older Versions)"},{"location":"Mapping_HPC_drives_with_Samba/#more-information","text":"<ul> <li>http://us3.samba.org/samba/</li> </ul>","title":"More Information:"},{"location":"Matlab/","text":"","title":"Matlab"},{"location":"Matlab/#about-matlab","text":"<ul> <li> <p>There is a Matlab portal for Michigan State University users. Users are encouraged to visit it for information and support provided by Mathworks.</p> </li> <li> <p>Several versions of\u00a0MATLAB are installed on the cluster. By default, MATLAB/2018a is loaded. Other available version of MATLAB can be discovered by typing</p> </li> </ul> <pre><code>    hpc@dev-intel18:~&gt; module spider MATLAB\n</code></pre> <p>and then switching to a different version, for example, to switch from 2018a to 2019a, one can directly load the the version as</p> <pre><code>hpc@dev-intel18:~&gt; module load MATLAB/2019a\n</code></pre> <ul> <li>MATLAB's many\u00a0built-in functions\u00a0have multi-threaded capability. On     your personal computer, when a MATLAB\u00a0function with multi-threads is     called,\u00a0MATLAB\u00a0will automatically spawn as many threads as the     number of cores on the machine. To avoid over utilizing compute     nodes on HPCC, user should set the max number of threads by using     <code>maxNumCompThreads(N)</code> in matlab where N is the maximum number of     threads matlab would use in the session. User could also use option     <code>-SingleCompThread</code> to launch\u00a0matlab session that would only use a     single thread. Without these option, matlab\u00a0session will potentially     spawn as many as 28 (on intel16 nodes) or 20 (on intel14 nodes)     threads when a built-in multi-threaded function is called. To allow     the multi-thread functions in matlab, users need to do the     following:<ol> <li>Specify the maximum number of compute threads to be used with     \u00a0<code>maxNumCompThreads(N)</code>\u00a0at the beginning of the matlab     program where N is the maximum number of threads in the program.     For example,\u00a0<code>maxNumCompThreads(4)</code>\u00a0will set the maximum     number of threads used in the program to four. \u00a0</li> <li>If submitting to run as batch job, specify <code>--cpus-per-task=N</code>     in your job script where N should match the maximum number of     compute threads set in <code>maxNumCompThreads(N)</code>.</li> </ol> </li> </ul>  <p>Warning</p> <p>Starting Nov. 1, 2018 on new HPCC system, the matlab default setting of using a single compute thread is changed. <code>matlab-mt</code> and <code>matlab</code> commands would be the same for 2018 and older versions and <code>matlab-mt</code> will no longer exist from later versions starting 2019.</p>  <ul> <li>HPCC has many toolboxes installed. To see a list of installed     toolboxes and licenses, type</li> </ul> <pre><code>    hpc@dev-intel18:~&gt; module load powertools\n    hpc@dev-intel18:~&gt; licensecheck matlab\n    Checking Licenses for matlab\n    lmstat -a -c 27000@lm-02.i\n\n    Users of MATLAB:  (Total of 10000 licenses issued;  Total of 44 licenses in use)\n    Users of SIMULINK:  (Total of 10000 licenses issued;  Total of 1 license in use)\n    Users of Bioinformatics_Toolbox:  (Total of 10000 licenses issued;  Total of 0 licenses in use)\n    Users of Communication_Toolbox:  (Total of 10000 licenses issued;  Total of 0 licenses in use)\n\n    ......\n</code></pre>","title":"About MATLAB"},{"location":"Matlab/#running-matlab-on-scratch-space","text":"<p>It is strongly recommended that iCER users run programs on the scratch space. This may improve the speed of job execution as well as the whole system's performance. MATLAB users should carefully check whether any temporary files involved in the program execution need to be stored in scratch space. For example, if you use the Matlab compiler to make a Matlab program into a standalone program, you need to set the environment variable <code>MCR_CACHE_ROOT</code> in the scratch directory with: <code>export MCR_CACHE_ROOT=$SCRATCH</code> before starting the execution. This line should be added to the your job script before the line where you specify the task you want to run. This setting will override the default setting by MATLAB Compiler Runtime. By default, a directory for temporary data cache used by the MATLAB Compiler Runtime is created at user's home directory <code>$HOME</code>. Without this setting, users may run into the situation that the program running on scratch space\u00a0frequently accesses its cache space in the home space, which will greatly slow down the execution of the code, and may potentially slow down the whole system or even cause system instability. \u00a0</p>","title":"Running MATLAB on Scratch Space"},{"location":"Matlab/#running-matlab-interactively","text":"<p>In this document, we refer to an interactive session as one that involves a user typing commands into the MATLAB command windows.</p>","title":"Running MATLAB Interactively"},{"location":"Matlab/#short-sessions-two-hours","text":"<ul> <li><code>ssh</code> to one of the dev nodes and run Matlab:</li> </ul> <pre><code>    hpc@gateway:~&gt; ssh dev-intel16\n    Last login: Mon Dec  4 12:54:44 2017 from gateway\n    ===\n    This front-end node is not meant for running big or long-running jobs.  Jobs\n    that need to run longer than a few minutes should be submitted to the queue.\n    Long-running jobs on front-end nodes will be killed without warning.\n    ===\n    hpc@dev-intel16:~&gt; matlab -nodisplay\n</code></pre> <p>More information about running jobs interactively on compute nodes can be reviewed at\u00a0Running Programs Interactively</p> <ul> <li>If you require graphics, please ensure that you have an Xserver     running. For Linux and MAC users,</li> </ul> <pre><code>    ssh -X username@hpcc.msu.edu\n</code></pre> <p>If you want graphics, but don't want the desktop, type</p> <pre><code>    hpc@dev-intel16:~&gt; matlab -nodesktop\n</code></pre> <p>Interactive MATLAB jobs running on development nodes are limited to a two hour wall time limit, and will be killed automatically afte two hours.</p>","title":"Short Sessions (&lt; two hours)"},{"location":"Matlab/#long-sessions-two-hours","text":"<p>Longer interactive sessions are possible, but are not recommended. Modify the following commands to suit your requirements.</p> <p>If graphics are not required,</p> <pre><code>    hpc@dev-intel18:-&gt; salloc --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2gb --time=04:00:00 \n\n\n    salloc: Granted job allocation 310982\n\n    salloc: Waiting for resource configuration\n\n    salloc: Nodes lac-376 are ready for job\n\n    hpc@dev-intel18:-&gt; matlab -nodisplay \n</code></pre> <p>If graphics are required, add the option <code>\u2013x11</code> to the <code>salloc</code> command:</p> <pre><code>    hpc@dev-intel18:-&gt; salloc --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2gb --time=04:00:00 --x11\n</code></pre>  <p>Warning</p> <p>The above commands submit a job to the cluster. If the resources are not immediately available, you will have to wait till the requested resources are available. Requesting a job for four hours or less will typically be scheduled relatively quickly. Users may need to adjust the resources accordingly with the usage of the MATLAB program.</p>","title":"Long Sessions (&gt; two hours)"},{"location":"Matlab/#running-matlab-non-interactively","text":"","title":"Running MATLAB Non-Interactively"},{"location":"Matlab/#short-jobs-two-hours","text":"<p>A short job could be run on develop node without open matlab command window. From a development node, type</p> <pre><code>hpc@dev-intel16:~&gt; matlab -nodisplay -r \"myMatlab\" &amp;\n</code></pre> <p><code>myMatlab.m</code> will start running in the background.</p>","title":"Short Jobs (&lt; two hours)"},{"location":"Matlab/#long-jobs-two-hours","text":"","title":"Long Jobs (&gt; two hours)"},{"location":"Matlab/#single-matlab-job","text":"<p>To submit jobs to the cluster, a job script needs to be written. And submitted to the queue. The following sample job script file can be modified to suit your needs:</p> <p><code>myJob.sbatch</code></p> <pre><code>#!/bin/bash -login\n#SBATCH --job-name=myJobName             # specify a job name\n#SBATCH --nodes=1 --ntasks=1 --cpus-per-task=1 --time=02:05:00 --mem=550mb      # specify the resources needed\n#SBATCH --license=matlab@27000@lm-02.i          # specify the license request\n\ncd $SLURM_SUBMIT_DIR                            # go to the directory where this job is submitted\nmatlab -nodisplay -r \"myMatlab\"\n</code></pre> <ul> <li><code>myJobName</code> is a string to make your job easier to identify it when     managing or monitoring your jobs.</li> <li><code>--nodes=1</code> because you are running one matlab client in the job. If     you want put multiple matlab run in a single job script, you may     request more nodes for the job.</li> <li><code>--cpus-per-task=1</code> here because the matlab script <code>MyMatlab.m</code> does     not use multiple threads. If you are using multiple threads, you may     need to request more cpus.</li> <li><code>02:05:00</code> is the number of <code>hours:minutes:seconds</code> your job needs to     run. If it runs longer than this, it will be killed. If you request     more time than you need, your job may be delayed while the scheduler     finds a time to run it. If you don't know how long your job needs,     you will have to make a guess and use the real running time to     improve this number on future runs. The maximum walltime that can be     requested is <code>168:00:00</code>.</li> <li><code>--mem=550mb</code> reserves 550 megabytes of total memory for the job. We     recommend user to serve at least 550 MB for each session plus the     total size of data variables used in the computation. User could     click here to see the recommendation of the matlab     requirement by Mathworks.</li> <li><code>myMatlab</code> is the name of your matlab script without the .m extension</li> </ul> <p>Submit your job with:</p> <pre><code>hpc@dev-intel16:~&gt; sbatch myJob.sbatch\n</code></pre> <p><code>myJob.sbatch</code> is the name of your job script as shown above.</p>","title":"Single MATLAB Job"},{"location":"Matlab/#using-the-matlab-parallel-computing-toolbox","text":"<p>The MATLAB Parallel Computing Toolbox (PCT) provides users several parallel computing features.\u00a0</p> <ul> <li> <p>Parallel for-loops (parfor) . \u00a0(User could run \"module load     powertools; getexample MATLAB_parfor\" to down load a directory     \"MATLAB_parfor\" which contains an example of using parpool and     parfor with the \"local\" profile)</p> </li> <li> <p>Support for GPU computing</p> </li> <li> <p>Offload computing from your laptop to HPCC cluster (with MATLAB     Distributed Computing Server)\u00a0</p> </li> <li> <p>Distributed arrays and <code>spmd</code> (single-program-multiple-data) for large     dataset handling and data-parallel algorithms</p> <p>(1) One GPU card will be used for each worker. In order to use multiple GPUs, user need to use <code>spmd</code> capability that each instance of the program will use one card and multiple instances of the program take multiple cards.</p> <p>(2) If you use GPU capability, you need to have matlab run on a node with GPU. dev-intel14-k20, dev-intel16-k80, and dev-amd20-v100 are the develop nodes with GPUs. To request GPUs, use <code>\u2013gres=gpu:&lt;type&gt;:&lt;number&gt;</code> to request a number and type of GPU. <code>#SBATCH --gres=gpu:k20:1</code> is an example to request one k20 GPU. Valid GPU type are k20, k80 and v100. Note that type is optional, but the number of GPU is necessary.</p> </li> </ul>","title":"Using the MATLAB Parallel Computing Toolbox"},{"location":"Matlab/#using-the-matlab-parallel-server","text":"<p>The MATLAB Parallel Server lets users solve computationally and data-intensive problems by executing MATLAB and Simulink based applications on the HPCC cluster and clouds. (see the document for more information). HPCC cluster has this produce installed.</p> <p>We recommend that users prototype their applications using the Parallel Computing Toolbox, and then scale up to a cluster using MATLAB Parallel Server. To scale up to cluster, user does not need to recode the program. User only need to change the profile of the cluster.\u00a0</p>","title":"Using the MATLAB Parallel Server"},{"location":"Matlab/#setup-and-validate-your-cluster-profile","text":"<p>In this step you define a cluster profile to be used in subsequent steps.</p> <ol> <li> <p>Start the Cluster Profile Manager from the MATLAB desktop by     selecting on the\u00a0Home\u00a0tab in the\u00a0Environment\u00a0area\u00a0Parallel\u00a0&gt; Create     and\u00a0Manage Clusters.\u00a0</p> </li> <li> <p>Create a new profile in the Cluster Profile Manager by selecting Add     Cluster Profile\u00a0&gt;\u00a0Slurm.</p> </li> <li> <p>With the new profile selected in the list, click\u00a0Rename\u00a0to edit the     profile name,\u00a0Press\u00a0Enter.</p> </li> <li> <p>Select a profile in the list, click Edit to edit the profile     accordingly. After finishing editing, click Done to save the     profile.</p> </li> <li> <p>Click validation to validate the profile. The profile could be used     when it pass all the validation tests.</p> </li> </ol> <p>Starting from version 2018a, MATLAB supports the SLURM scheduler. Please refer to Mathworks' document for how to configure the cluster with the SLURM scheduler here. Previous versions of MATLAB are not recommended for use with the HPCC scheduler.</p>","title":"Setup and validate your cluster profile"},{"location":"Matlab/#using-the-matlab-thread-based-worker-pool","text":"<p>Started from Matlab version R2020a, thread-based worker pool is introduced. Please refer to this document\u00a0for more details. On HPCC, we provide our users an example showing how to use thread-base pool with <code>parfor</code>, as well as the comparison between process-based and thread-based pools. To obtain the example, module load <code>powertools</code> and <code>MATLAB/2020a</code>, then run <code>getexample MATLAB_threadPool</code>.\u00a0</p>","title":"Using the MATLAB thread-based worker pool"},{"location":"Matlab/#running-the-matlab2020a-on-amd-nodes","text":"<p>There is a bug in MATLAB/2020a that will lead to a \"segmentation fault\" on AMD node associated with the Java virtual machine. The patch may be introduced in the next release. If you find that the code works on other version but crashes in 2020a version, you may try the workaround that launch the matlab session without java virtual machine as the following:</p> <pre><code>hpc@eval-epyc19 ~]$ matlab -nodisplay -nojvm -r \"myExample\"\n</code></pre>","title":"Running the MATLAB/2020a on AMD nodes"},{"location":"Matlab/#running-matlab-on-intel14-nodes","text":"<p>There is an existing problem of running some functions on Intel14 nodes.\u00a0If you run into 'Illegal instruction detected' error, please use a node of other type.\u00a0Please add constrain to exclude intel14 type of compute nodes when submit to SLURM. For example: <pre><code>#SBATCH --constraint=[intel16|intel18|amd20]\n</code></pre></p>","title":"Running MATLAB on intel14 nodes"},{"location":"Matlab/#using-matlab-with-python","text":"<p>Here is the link to the cheat sheets provided by the Mathworks for users' reference.</p>","title":"Using MATLAB with Python"},{"location":"Module_System_and_Software_Installation/","text":"","title":"Module System and Software Installation"},{"location":"Module_System_and_Software_Installation/#the-module-system","text":"<p>There are many software packages installed on HPCC. This can make it very difficult to find a particular one and use it with correct environment setup without a module system. In order to manage the software and their dependencies systematically, we use the Lua-based Lmod module system which provides a convenient way to dynamically change the users' environment through module files. In the following section, we introduce the module commands for how to find and load (or unload) software on HPCC.</p>","title":"The Module System"},{"location":"Module_System_and_Software_Installation/#common-module-commands","text":"","title":"Common Module Commands"},{"location":"Module_System_and_Software_Installation/#installing-software","text":"<p>Although many software packages are available in our system, users might still want to install more and build their own software. Here, we recommend using  EasyBuild. The program is helpful to build software and module system under user's  home or research space. The system built by users can also work together with HPCC  oftware and module system.</p>","title":"Installing Software"},{"location":"Module_System_and_Software_Installation/#software-installation-by-easybuild","text":"","title":"Software Installation by EasyBuild"},{"location":"Module_System_and_Software_Installation/#user-created-modules","text":"","title":"User Created Modules"},{"location":"Module_System_and_Software_Installation/#compilers-and-libraries","text":"","title":"Compilers and Libraries"},{"location":"Module_System_and_Software_Installation/#using-version-control-systems","text":"","title":"Using Version Control Systems"},{"location":"Mothur/","text":"","title":"Mothur"},{"location":"Mothur/#loading-mothur","text":"<p>Mothur version 1.40.3 on the HPCC has two running modes, with and without MPI functionality. You can load either one by:</p> <pre><code>module purge\nmodule load icc/2017.1.132-GCC-6.3.0-2.27 impi/2017.1.132 Mothur/1.40.3-nonMPI-Python-2.7.13        # Mothur non-MPI version\nmodule load icc/2017.1.132-GCC-6.3.0-2.27 impi/2017.1.132 Mothur/1.40.3-Python-2.7.13               # Mothur MPI version\n</code></pre> <ul> <li> <p>As of Feb 7, 2019, the highest version is\u00a0Mothur/1.41.3 (in MPI mode only).</p> </li> <li> <p>As reported by some Mothur users, when using Mothur and vsearch together, the only compatible version of vsearch is 1.8. So after loading Mothur, you would add a line of \"<code>module load vsearch/1.8.0</code>\".</p> </li> </ul>","title":"Loading Mothur"},{"location":"Mothur/#running-mothur","text":"<p>Take a look at this example code<code>batch.m</code>:</p> <p><code>/mnt/research/common-data/Examples/mothur/batch.m</code>:</p> <pre><code>set.current(fasta=ex.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.subsample.fasta, count=ex.trim.contigs.good.unique.good.filter.unique.precluster.uchime.pick.pick.subsample.count_table,  processors=1)\ndist.seqs(fasta=current, cutoff=0.2, processors=8)\n</code></pre> <p>where we specified processors=8 in line 2.\u00a0To be able to actually utilize 8 processors, you need to launch Mothur using either of the following commands, depending on whether MPI is enabled or not.</p> <pre><code>MPI: mpirun -np 8 mothur batch.m\nnon-MPI: mothur batch.m\n</code></pre>","title":"Running Mothur"},{"location":"Mothur/#differences-betweem-mpi-and-non-mpi-runs","text":"<p>MPI jobs can run across multiple nodes at the cost of overhead. This can lead to increased memory usage and decreased performance. The additional processor advantages offered by MPI may be cancelled out by I/O waits to disk. If you request many more processes than can be provided by a single node, use MPI mode. If you choose the MPI type, specify number of processes in the SLURM script by <code>--ntask=8</code> for the example above. SLURM will determine how many nodes and tasks per node are needed. Also,\u00a0memory request in this case should be made on a per CPU basis (by defining <code>--mem-per-cpu</code>).</p> <p>Non-MPI jobs run on a single node with multiple threads/processes. For above Mothur command, you should set up something like</p> <p><code>#SBATCH --nodes=1</code></p> <p><code>#SBATCH --ntasks-per-node=1</code></p> <p><code>#SBATCH --cpus-per-task=8</code></p> <p>in your job submission script. If most of the nodes in the cluster are highly occupied, the job scheduler may have a hard time finding the nodes with availability of your desired number of threads.</p>","title":"Differences betweem MPI and non-MPI runs"},{"location":"MySQL_configuration/","text":"","title":"MySQL configuration for OrthoMCL"},{"location":"MySQL_configuration/#overview","text":"<p>OrthoMCL is a program that aids in the identification of orthologs. \u00a0The OrthoMCL tool uses NCBI BLAST and the MCL application in conjunction with a relational database (MySQL). \u00a0OrthoMCL version 2.0.2 is available on the HPCC and can be loaded as a module.</p> <p>However, because of the relational database requirement, the HPCC must be contacted in advance to setup a database for your runs. This tutorial briefly describes how to obtain access to the program, and how to use configuration files provided by the HPCC.</p>","title":"Overview"},{"location":"MySQL_configuration/#database-access","text":"<p>Before beginning your HPCC runs, you will need to complete an\u00a0help request ticket request to request a database and MySQL account for your use of OrthoMCL.\u00a0\u00a0 This information will be provided to you in the form of a configuration file, and you save this to your directory.\u00a0 You can use the filename as a command-line argument to relevant scripts comprising the OrthoMCL application, which tell OrthMCL how to connect to your database.\u00a0</p>","title":"Database Access"},{"location":"MySQL_configuration/#configuration-file","text":"<p>The following is an example of the configuration file you will receive:</p> <pre><code>dbVendor=mysql\ndbConnectString=dbi:mysql:someUserdb:db-01:3306\ndbLogin=someUser\ndbPassword=somePassword\n# DO NOT CHANGE ANYTHING ABOVE THIS LINE UNLESS YOU KNOW WHAT YOU'RE DOING\nsimilarSequencesTable=SimilarSequences\northologTable=Ortholog\ninParalogTable=InParalog\ncoOrthologTable=CoOrtholog\ninterTaxonMatchView=InterTaxonMatch\npercentMatchCutoff=50\nevalueExponentCutoff=-5\n</code></pre>","title":"Configuration File"},{"location":"MySQL_configuration/#command-example","text":"<p>A typical run that would require the database configuration file might look something like the following:</p> <pre><code>orthomclPairs orthomcl.config log_file cleanup=[yes|no|only|all] &lt;startAfter=TAG&gt;\n</code></pre> <p>In the example above, the file \"orthomcl.config\" is the name of the configuration/connection file (provided by the HPCC) that you want to use for your run.</p>","title":"Command Example"},{"location":"MySQL_configuration/#purging-the-database-between-runs","text":"<p>To facilitate multiple concurrent, or faster consecutive runs, many users ask for more than one database at setup time. \u00a0HPCC will typically be able to provide you with up to four (4) such databases. \u00a0Please specify this in your request.</p> <p>Once your run is completed, you will need to purge the database of its contents prior to beginning new runs using the same database. \u00a0To have this performed, please contact ICER\u00a0 via https://contact.icer.msu.edu, select \"other\" in the form and let us know you need your database purge and the name of the database\u00a0 if you have multiple databases (or just provide the connection string like <code>dbConnectString=dbi:mysql:someUserdb:db-01:3306</code> ).</p> <p>When your work with OrthoMCL is complete, please notify the staff via the ICER contact form so we can purge your databases from the system.</p>","title":"Purging the Database Between Runs"},{"location":"MySQL_configuration/#modifying-the-configuration-file","text":"<p>Most users will not need to modify the configuration file provided by the HPCC. \u00a0The most common modification needed will be to change the name of the database to be accessed in those cases where users are provided with access to more than one database.</p> <p>The relevant line to be modified is shown below:</p> <pre><code>dbConnectString=dbi:mysql:someUserdb:db-01:3306 \n</code></pre> <p>In the example above, the database name is \"someUserdb\". \u00a0</p> <p>Let's assume (for example), a user had been issued 4 databases named: someUserdb, someUserdb2, someUserdb3, someUserdb4. \u00a0To perform a run using one of these other databases, we would need to make a copy of the configuration file and change the name in that file, for example:</p> <pre><code>dbConnectString=dbi:mysql:someUserdb2:db-01:3306\n</code></pre> <p>You may then structure the command for each OrthoMCL run to use the configuration file (and related database) desired.</p>","title":"Modifying the Configuration File"},{"location":"MySQL_configuration/#more-information","text":"<p>Refer to the OrthoMCL User Manual.</p>","title":"More Information"},{"location":"NCBI_Entrez_Direct_tools%202/","text":"<p>Entrez Direct (eDirect) provides access to the NCBI's suite of interconnected databases (publication, sequence, structure, gene, variation, expression, etc.) using Linux command lines. People who need to retrieve sequence and other type of data from NCBI regularly should use the eDirect toolset. To use edirect commands on HPCC, run:</p> <pre><code>ssh dev-intel18\nmodule load Perl/5.28.1\nmodule load edirect\n</code></pre> <p>A full documentation of eDirect is available from https://www.ncbi.nlm.nih.gov/books/NBK179288/</p> <p>A few useful examples can be found in this blog http://bioinformatics.cvr.ac.uk/blog/ncbi-entrez-direct-unix-e-utilities/</p> <p>eDirect cookbook:\u00a0https://github.com/NCBI-Hackathons/EDirectCookbook/blob/master/README.md</p>","title":"NCBI Entrez Direct tools"},{"location":"NCBI_Entrez_Direct_tools/","text":"<p>Entrez Direct (eDirect) provides access to the NCBI's suite of interconnected databases (publication, sequence, structure, gene, variation, expression, etc.) using Linux command lines. People who need to retrieve sequence and other type of data from NCBI regularly should use the eDirect toolset. To use edirect commands on HPCC, run:</p> <pre><code>ssh dev-intel18\nmodule load Perl/5.28.1\nmodule load edirect\n</code></pre> <p>A full documentation of eDirect is available from https://www.ncbi.nlm.nih.gov/books/NBK179288/</p> <p>A few useful examples can be found in this blog http://bioinformatics.cvr.ac.uk/blog/ncbi-entrez-direct-unix-e-utilities/</p> <p>eDirect cookbook:\u00a0https://github.com/NCBI-Hackathons/EDirectCookbook/blob/master/README.md</p>","title":"NCBI Entrez Direct tools"},{"location":"Oakland_University_Users/","text":"","title":"Oakland University Users"},{"location":"Oakland_University_Users/#user-specific-information","text":"<p>This information is for users at Oakland University. Users with MSU NetIDs should disregard this information.</p> <p>Oakland University users should refer to the documentation hosted on the Oakland University knowledge base.</p>","title":"User-specific information"},{"location":"Open_OnDemand/","text":"<p>Open OnDemand helps researchers efficiently  utilize the HPCC by providing easy web access with graphical user interfaces from any device. The features include, though are not limited to:</p> <ul> <li>Plugin-Free web experience</li> <li>Easy file management</li> <li>Easy job submission, management, and monitoring </li> <li>Graphical desktop environments and desktop applications;     Jupyter Notebook, RStudio, ...</li> <li>One-click app icons on a desktop to launch your favorite GUI applications </li> <li>Command-line shell access</li> </ul>","title":"Open OnDemand"},{"location":"Open_OnDemand/#connect-to-hpcc-ondemand","text":"<p>To connect to HPCC OnDemand, visit\u00a0https://ondemand.hpcc.msu.edu/.  It will first redirect to the CILogon website for authentication.  </p> <p></p> <p>From here, select \"Michigan State University\" as the identity provider and click \"Log On\". This will redirect you to a page where you can log in with your MSU credentials.  </p>  <p>Note</p> <p>A valid MSU email address with NetID and password are required to access HPCC OnDemand. This may prohibits the access of external collaborators. Please take this into consideration when you choose tools provided on HPCC OnDemand for your group projects involving external collaborators. </p>  <p>After sign in, you will reach the HPCC OnDemand web portal with the following menu options:    </p> <p></p>","title":"Connect to HPCC OnDemand"},{"location":"Open_OnDemand/#files","text":"<p>All user's files in the home, research, scratch, and software file spaces can be accessed.   </p> <p> </p> <p>Select a directory then use the <code>File Explorer</code> to download, upload,  view, edit, and move files.  </p> <p> </p>","title":"Files"},{"location":"Open_OnDemand/#jobs","text":"<p>Active Jobs: List jobs in the queue; monitor or manage those jobs</p> <p></p> <p>Job Composer: Submit a job script with resource requests and  command lines; create new scripts from a job template, copy a previous job submission,  or select an existing job script from a specified directory</p> <p></p>","title":"Jobs"},{"location":"Open_OnDemand/#interactive-apps","text":"<p>Desktops: Request an <code>Interactive Desktop</code>; once it starts, a graphical user interface (GUI) is provided for running GUI based applications</p> <p></p> <p>GUIs: Launch an HPCC provided, GUI based application directly</p> <ul> <li>MATLAB: 2018a-2021a versions are available</li> <li>Stata: 15SE, 15MP, 17SE, 17MP are available</li> <li>Jupyter notebook: By default, Python 3.7.2 will be used; choose      your own Python by selecting 'Launch Jupyter Notebook using     the Anaconda installation in my home directory'</li> <li>RStudio: Various versions are available (3.5.0\\~ 4.1.2)</li> </ul>  <p>Note</p> <p>Please make sure you request enough memory. Otherwise, your interactive  session won't start or your running processes will be terminated prior to completion. </p>","title":"Interactive Apps"},{"location":"Open_OnDemand/#development-nodes","text":"<p>Start a bash session a specified development node and input commands using the terminal's command line interface. </p> <p></p>  <p>Note</p> <p>Currently, there is no remote graphical display capability while in the terminal i.e., no X11 forwarding. For GUI based applications, please use the Interactive Apps feature. </p>","title":"Development Nodes"},{"location":"Open_OnDemand/#my-interactive-sessions","text":"<p>All of a user's interactive jobs are displayed, along with a description of the resources currently allocated to each job. User's may then manage aspects of those jobs using the asscociated action buttons.</p> <p></p> <p>We are currently working to add more functionality to the OnDemand interface. Please feel free to contact us if you have any questions or suggestions.</p>","title":"My Interactive Sessions"},{"location":"Other_Universities/","text":"","title":"Other Universities"},{"location":"Other_Universities/#information-for-central-michigan-university-and-western-michigan-university-users","text":"","title":"Information for Central Michigan University and Western Michigan University Users"},{"location":"Other_Universities/#kettering-users","text":"","title":"Kettering Users"},{"location":"Other_Universities/#oakland-university-users","text":"","title":"Oakland University Users"},{"location":"PC_to_HPC/","text":"<p>Lecture notes from Dr. Xiaoge Wang\u00a0\u2013 \"PC to HPC\":</p>","title":"PC to HPC"},{"location":"Packages/","text":"<ul> <li>R workshop tutorial</li> <li>rstan</li> <li>rjags</li> <li>R2OpenBUGS</li> <li>R interface to TensorFlow/Keras</li> </ul>","title":"Packages"},{"location":"Powertools/","text":"<p>By default, powertools is loaded when users log into a Centos 7 dev node. Users may also run the <code>module list</code> command to check if it is loaded. If it is not, please use the command:</p> <pre><code>'bash$ module load powertools'\n</code></pre> <p>to load the module before running powertools commands.</p>","title":"Powertools"},{"location":"Powertools/#qs","text":"<p>Display job list.</p>","title":"qs"},{"location":"Powertools/#node_status","text":"<p>Display a list of compute nodes and their properties.</p>","title":"node_status"},{"location":"Powertools/#bi","text":"<p>Information of Buy-In Account</p>","title":"bi"},{"location":"Powertools/#js","text":"<p>Display job steps and their resource usages.</p>","title":"js"},{"location":"Powertools/#getexample","text":"<p>Download user examples.</p>","title":"getexample"},{"location":"Powertools_longjob_by_DMTCP/","text":"<p>The following are instructions for trying out longjob powertool on HPCC system. First, you start with a basic submission script. For example, consider the following simple submission script:</p> <pre><code>#!/bin/bash -login\n#SBATCH --nodes=1 --ntasks=1 --cpus-per-task=1\n#SBATCH --time=168:00:00\n#SBATCH --mem=2gb\n#SBATCH --constraint=intel16\n#SBATCH --job-name=MyJob0\n#SBATCH --mail-type=FAIL,END\n\nmodule purge\nmodule load GCC/7.3.0-2.30 OpenMPI/3.1.1\n\nsrcdir=${SLURM_SUBMIT_DIR}/bin/\nWORK=/mnt/scratch/${USER}/KineticSN/${SLURM_JOBID}\nmkdir -p ${WORK}\n\n# Copy files to work directory\ncp -r $srcdir/* $WORK/\n\n#Move to the working directory\ncd $WORK\n\n#Run my program\n./SimulationTest -scattering_flag 0 -weak_reaction_flag 0 -outputVisData 100\nret=$?\n\nscontrol show job ${SLURM_JOBID}\n\nexit $ret\n</code></pre> <p>To get longjob to work, the following modificaitons might need to be made:</p> <ol> <li>Change walltime to be less than 4 hours if you would like to have     more available nodes to your job.</li> <li>Wrap all setup-code that only needs to be run once in an if     statement that checks for the file \"Files_Copied\". This will ensure     that the setup-code only runs the first time the script is run     because in the first time there should be no file with the name     \"Files_Copied\".</li> <li>Add the longjob command before the command in the submission     script that you want to checkpoint.</li> <li> <p>Load the powertools module and turn on aliases. i.e. add the     following lines of code to the script:</p> <pre><code>shopt -s expand_aliases\nmodule load powertools\n</code></pre> </li> <li> <p>Set the following enviornment variables as appropriate for your job:</p> <ul> <li>JobScript\u00a0 \u2013\u00a0 Name of the job script file which will get     resubmitted. The default is the first submitted job script name.</li> <li>DMTCP_Checkpoint_Time\u00a0 \u2013\u00a0\u00a0Time (in seconds) which DMTCP     needs to work on checkpointing. The default is 5 minutes.</li> <li>DMTCP_CHECKPOINT_INTERVAL\u00a0 \u2013\u00a0 Time (in seconds) between     automatic checkpoints. The default is 4-8 hours. For walltime     less than 4 hours, the default will do checkpointing once at     ${DMTCP_Checkpoint_Time} + 1 minute before the end of     walltime.\u00a0</li> <li>DMTCP_CHECKPOINT_DIR\u00a0\u00a0\u2013\u00a0 Name of the directory to save     checkpoint image and log flies. The default is     ckpt_${SLURM_JOB_NAME}. For job array, the default is     ckpt_${SLURM_JOB_NAME}_${SLURM_ARRAY_TASK_ID}. If two     different jobs use the the same directory to run with the     longjob command, please make sure the environment variables     (or SLURM_JOB_NAME) are set different so their image files are     not saved in the same directory.</li> </ul> </li> </ol> <p>The following is a modified example script with the changes:</p> <pre><code>#!/bin/bash -login\n#SBATCH --nodes=1 --ntasks=1 --cpus-per-task=1\n#SBATCH --time=04:00:00\n#SBATCH --mem=2gb\n#SBATCH --constraint=intel16\n#SBATCH --job-name=MyJob0\n#SBATCH --mail-type=FAIL,END\n\nmodule purge\nmodule load GCC/7.3.0-2.30 OpenMPI/3.1.1\nmodule load powertools\n\n# Change checkpointing environment variables if necessary:\n# export DMTCP_Checkpoint_Time=60                     -- change checkpointing time\n# export DMTCP_CHECKPOINT_INTERVAL=7200               -- change time interval between checkpoints\n# export DMTCP_CHECKPOINT_DIR=ckpt_${SLURM_JOB_NAME}  -- change where to save checkpointing files\n\n# Change to a directory other than ${SLURM_SUBMIT_DIR} if necessary:\n# cd /mnt/scratch/${USER}/WorkPlace\n\nif [ ! -f Files_Copied ]\nthen\n    srcdir=${SLURM_SUBMIT_DIR}/bin/\n    WORK=/mnt/scratch/${USER}/KineticSN/${SLURM_JOBID}\n    mkdir -p ${WORK}\n\n    # Copy files to work directory\n    cp -r $srcdir/* $WORK/\n\n    #Run main simulation program\n    cd $WORK\n    touch Files_Copied \n\nfi\nlongjob ./SimulationTest -scattering_flag 0 -weak_reaction_flag 0 -outputVisData 100\nret=$?\n\nexit $ret\n</code></pre> <p>If everything works as expected, you should be able to submit the above job script and it will resubmit itself until the job completes. Note, this is rough code, not completely tested and does work in all cases. For example, one case that could propose a problem is if the main program gets caught in a loop and never exits. In this case, the code will keep submitting itself indefinitely. \u00a0Note that in addition you can't include output redirection as you'd expect, that is a command like \"myprogram.py &gt; myoutput.txt\" \u00a0and \"longjob myprogram.py &gt; myoutput.py\" is not the same (the redirection here applies to longjob, not your program). \u00a0  </p> <p>If you have difficulty, please contact us.</p>","title":"Powertools longjob by DMTCP"},{"location":"Python/","text":"<p>It's difficult for the HPCC to host a vast volume of Python packages, and to resolve conflicts between Python versions. Users are encouraged to install and manage packages on their own, through conda or virtual environments.</p>","title":"Python packages"},{"location":"Python/#using-conda","text":"<p>Users install their own version of Python through Anaconda in the home or research space. This gives them full control on their preferred versions of Python and packages. Isolated virtual environments can be created and controlled by conda environments.</p>","title":"Using conda"},{"location":"Python/#using-virtualenv","text":"<p>By default, Python version 3.6.4 (compiled by GCC/6.4.0) is loaded and ready to use after logging into HPCC dev-nodes. Other versions can be found by running <code>module spider Python</code>. Usually, pre-installed packages are available for each Python version. If users need to install new ones, they can install them in the home or research space through virtual environments.</p>","title":"Using virtualenv"},{"location":"Python_on_HPC/","text":"<p>Python is popular because it makes a great first impression; i) clean, clear syntax, ii) multi-paradigm, iii) interpreted, iv) duck typing, garbage collection, and most of all, v) instant productivity. It keeps up with users' needs. It has i) flexible, full-featured data structures ii) extensive standard libraries iii) reusable open-source package iv) package management tools v) good unit testing frameworks. </p> <p>In exchange for user-friendliness and ease of use, Python becomes one of the slowest computer languages, primarily because it is an interpreted language, and allows a single thread to run in the interpreter's memory space at once. Python is typically 30 to 300 times slower than C or Fortran. However, Python has a powerful and enthusiastic open-source community which continuously improves the capability of Python.\u00a0</p> <p>In this tutorial, we want to</p> <ul> <li>explain what the MSU HPCC is doing to support Python users.</li> <li>provide guidance to help users improve Python performance on the     HPCC.</li> <li>point out tools that support developers of Python on the HPCC.</li> </ul> <p>We assume that</p> <ul> <li>you know and use Python, or</li> <li>you know and use the HPCC and are curious about using Python in your own     HPCC work.</li> </ul> <p>How to use Python on the HPCC?</p> <ul> <li>Python with virtual environments</li> <li>Python with Conda</li> <li>Jupyter Notebook</li> </ul> <p>Can my Python code be faster?</p> <ul> <li>Vectorization</li> <li>Numba</li> <li>Use Threaded Libraries</li> <li>MPI</li> <li>Profiling</li> </ul> <p>Other Python Resources</p>","title":"Python on HPC"},{"location":"Python_on_HPC/#how-to-use-python-on-the-hpcc","text":"<p>Python applications usually use packages and modules that require specific version of libraries. This means one installed application may conflict with another application due to using the same library but with different versions. It is difficult to meet the requirements of every application by one global Python installation. To resolve this issue,</p> <ul> <li>users can create an isolated virtual environment\u00a0with a particular     version of Python on our system in a self-contained directory of     their home or research space.</li> <li>users can\u00a0Install their own version of Python through     Anaconda in their home or research     space. This gives users full control on their preferred versions of     python and packages.</li> </ul>","title":"How to use Python on the HPCC?"},{"location":"Python_on_HPC/#python-with-virtual-environments","text":"<p>More details of how to use virtual environments can be found at this page</p>","title":"Python with virtual environments"},{"location":"Python_on_HPC/#python-with-conda","text":"<p>To use Python on the HPCC, you have to load a Python module. A few helpful module commands would be <code>module avail Python</code>, <code>module spider Python</code>,  and <code>module load Python</code>. For more information for our module system can be found here</p>","title":"Python with Conda"},{"location":"Python_on_HPC/#jupyter-notebook","text":"<p>For Jupyter notebook users, we have the Open OnDemand platform  To connect to HPCC OnDemand, visit\u00a0https://ondemand.hpcc.msu.edu.\u00a0After logging in, choose interactive apps, select Jupyter Notebook, request resources as you need. Your Jupyter Notebook will start when the requested resources are ready.</p> <p></p> <p></p>","title":"Jupyter Notebook"},{"location":"Python_on_HPC/#can-my-python-code-be-faster","text":"<p>Now, you are ready to use Python on the HPCC. Now, let's learn a few tips to make your Python codes faster.</p>","title":"Can my Python code be faster?"},{"location":"Python_on_HPC/#vectorization","text":"<p>Vectorization speeds up Python code without using loops. Instead of loops, NumPy can help by minimizing the running time of code efficiently. NumPy offers various operations to be performed over vectors such as the dot product, cross product, and matrix multiplication. See the Numpy array documentation for more information.</p>","title":"Vectorization"},{"location":"Python_on_HPC/#numba","text":"<p>Numba\u00a0compiles Python codes just in time with a few decorators, without much modification of code.\u00a0In addition, Numba offers automatic parallelization which is very easy to use. You just need to add the one line decorator,\u00a0<code>@njit(parallel=True)</code>. More information can be found here. Numba also supports NVIDIA CUDA. It is easy to use (at least much easier to use than other programming languages). </p>","title":"Numba"},{"location":"Python_on_HPC/#use-threaded-libraries","text":"<p>Packages like NumPy and SciPy are already built with MPI and multithread support via BLAS/LAPACK, and MKL. In general, it is a plausible guess that most solvers have already been implemented in pure Python. In addition, many major threaded libraries and packages already have binds such as PyTrilinos, Petsc4py, Elemental, and SLEPc. So, don't try to reinvent the wheel. If it is not new, it is probably already implemented for high performance.</p>","title":"Use Threaded Libraries"},{"location":"Python_on_HPC/#mpi","text":"<p>Python has a package for MPI, mpi4py.</p> <p>It is</p> <ul> <li>a pythonic wrapping of the system's native MPI.</li> <li>a provider of almost all MPI-1, 2 and common MPI-3 features.</li> <li>very well maintained.</li> <li>distributed with major Python distributions.</li> <li>portable and scalable.</li> <li>dependent only on NumPy, Cython (build only), and MPI libraries.</li> </ul> <p>More information can be found here.</p>","title":"MPI"},{"location":"Python_on_HPC/#other-python-resources","text":"<p>The following are a few Python resource links.</p> <ul> <li>https://www.python.org/about/gettingstarted/</li> <li>https://wiki.python.org/moin/BeginnersGuide/</li> <li>https://www.codecademy.com/learn/python/</li> <li>https://www.coursera.org/specializations/python/</li> <li>https://software-carpentry.org/lessons/</li> <li>https://pymotw.com/</li> <li>HPCC wiki Python page</li> <li>Python video on youtube</li> <li>https://www.py4e.com/ </li> </ul>","title":"Other Python Resources"},{"location":"QIIME_2/","text":"<p>QIIME (Quantitative Insights Into Microbial Ecology) is an open-source bioinformatics pipeline for performing microbiome analysis from raw DNA sequencing data. Since QIIME 1 is no longer supported officially (see announcement at http://qiime.org, it's not installed on the CentOS 7 system of HPCC. The way QIIME 2 is installed and run on the HPCC CentOS 7 is through conda https://docs.qiime2.org/2018.2/install/native/.</p> <p>You may follow https://docs.anaconda.com/anaconda/install/linux to install anaconda in your home directory.</p> <p>Below is how to install QIIME 2 (version 2018.2) in your home directory via conda:</p> <p>Install QIIME 2</p> <pre><code>export PATH=$PATH:$HOME/anaconda3/bin\nwget https://data.qiime2.org/distro/core/qiime2-2018.2-py35-linux-conda.yml\nconda env create -n qiime2-2018.2 --file qiime2-2018.2-py35-linux-conda.yml\nrm qiime2-2018.2-py35-linux-conda.yml\n\nsource activate qiime2-2018.2\nqiime --help # test if installation is successful\n# all your QIIME commands go here\nsource deactivate\n</code></pre> <p>The example below is from a previous version of the current tutorial</p> <p>Example of analysis</p> <pre><code>mkdir qiime2-moving-pictures-tutorial\ncd qiime2-moving-pictures-tutorial\nwget -O \"sample-metadata.tsv\" \"https://data.qiime2.org/2018.2/tutorials/moving-pictures/sample_metadata.tsv\"\n\nmkdir emp-single-end-sequences\nwget -O \"emp-single-end-sequences/barcodes.fastq.gz\" \"https://data.qiime2.org/2018.2/tutorials/moving-pictures/emp-single-end-sequences/barcodes.fastq.gz\"\nwget -O \"emp-single-end-sequences/sequences.fastq.gz\" \"https://data.qiime2.org/2018.2/tutorials/moving-pictures/emp-single-end-sequences/sequences.fastq.gz\"\n\nqiime tools import \\\n  --type EMPSingleEndSequences \\\n  --input-path emp-single-end-sequences \\\n  --output-path emp-single-end-sequences.qza\n\nqiime demux emp-single \\\n  --i-seqs emp-single-end-sequences.qza \\\n  --m-barcodes-file sample-metadata.tsv \\\n  --m-barcodes-column BarcodeSequence \\\n  --o-per-sample-sequences demux.qza\n\nqiime demux summarize \\\n  --i-data demux.qza \\\n  --o-visualization demux.qzv\n\nqiime tools view demux.qzv\n</code></pre>","title":"QIIME 2"},{"location":"R2OpenBUGS/","text":"<p>R package R2OpenBUGS is available on the HPCC.</p> <p>OpenBUGS is a software package that performs Bayesian inference Using Gibbs Sampling. The latest version of OpenBUGS on the HPCC is 3.2.3.\u00a0R2OpenBUGS is an R package that allows users to run OpenBUGS from R.</p> <p>To load R and OpenBUGS, run:</p> <p>module purge module load\u00a0GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 module load OpenBUGS</p> <p>Then, in your R session, use \"library(R2OpenBUGS)\" to load this package.</p> <p>You can execute the following testing R code to see if it works for you.</p>    **Testing R2OpenBUGS**    <pre><code>library(R2OpenBUGS)\n\n# Define the model \nBUGSmodel &lt;- function() {\n\n    for (j in 1:N) {\n        Y[j] ~ dnorm(mu[j], tau)\n        mu[j] &lt;- alpha + beta * (x[j] - xbar)\n    }\n\n    # Priors\n    alpha ~ dnorm(0, 0.001)\n    beta ~ dnorm(0, 0.001)\n    tau ~ dgamma(0.001, 0.001)\n    sigma &lt;- 1/sqrt(tau)\n}\n\n# Data\nData &lt;- list(Y = c(1, 3, 3, 3, 5), x = c(1, 2, 3, 4, 5), N = 5, xbar = 3)\n\n# Initial values for the MCMC\nInits &lt;- function() {\n    list(alpha = 1, beta = 1, tau = 1)\n}\n\n# Run BUGS \nout &lt;- bugs(data = Data, inits = Inits, parameters.to.save = c(\"alpha\", \"beta\", \"sigma\"), model.file = BUGSmodel, n.chains = 1, n.iter = 10000)\n\n# Examine the BUGS output\nout\n\n# Inference for Bugs model at \"/tmp/RtmphywZxh/model77555ea534aa.txt\",\n# Current: 1 chains, each with 10000 iterations (first 5000 discarded)\n# Cumulative: n.sims = 5000 iterations saved\n#          mean  sd 2.5%  25%  50%  75% 97.5%\n# alpha     3.0 0.6  1.9  2.7  3.0  3.3   4.1\n# beta      0.8 0.4  0.1  0.6  0.8  1.0   1.6\n# sigma     1.0 0.7  0.4  0.6  0.8  1.2   2.8\n# deviance 13.0 3.7  8.8 10.2 12.0 14.6  22.9\n#\n# DIC info (using the rule, pD = Dbar-Dhat)\n# pD = 3.9 and DIC = 16.9\n# DIC is an estimate of expected predictive error (lower deviance is better).\n</code></pre>","title":"R2OpenBUGS"},{"location":"RCentOS7generalinfo/","text":"","title":"General information"},{"location":"RCentOS7generalinfo/#a-quick-start","text":"<p>R 4.0.2 has the largest number of libraries installed and is the recommended one to use. To load it, you can run:</p> <p><code>module purge</code> <code>module load GCC/8.3.0 OpenMPI/3.1.4</code> <code>module load R/4.0.2</code></p> <p>Some users may have a local R package directory specified in <code>~/.Renviron</code>; this may create a problem if you load your local packages which were built with an older version of R. Unless you have updated them all, we recommend that you launch R by <code>R --no-environ</code> which suppresses the search of local packages.</p>","title":"A quick start"},{"location":"RCentOS7generalinfo/#be-sure-to-specify-version-number","text":"<p>While it is valid to load R simply by <code>module load R</code>, it may point to a different R version than your desired one. Please be specific about the version, as with any software module.</p>","title":"Be sure to specify version number"},{"location":"RCentOS7generalinfo/#rstudio","text":"<p>The best way to launch an Rstudio session is to log into our OnDemand server, which is dedicated to running GUI applications. Here is a short video showing how to request an Rstudio \"job\" from the HPCC cluster after you've logged into https://ondemand.hpcc.msu.edu/.</p>","title":"Rstudio"},{"location":"RNA_seq_example/","text":"<p>While RNA-seq analysis pipeline is changing, here is an example for demonstration purpose. The pipeline involves <code>tophat</code>, <code>cufflinks</code> and so on. We will provide updates to this page as needed.</p>","title":"RNA-seq example"},{"location":"R_interface_to_TensorFlow_Keras/","text":"-   [Prerequisites](../ITH/R_interface_to_TensorFlow_Keras) -   [Installing R libraries](../ITH/R_interface_to_TensorFlow_Keras) -   [A deep learning example](../ITH/R_interface_to_TensorFlow_Keras)","title":"R interface to TensorFlow/Keras"},{"location":"R_interface_to_TensorFlow_Keras/#prerequisites","text":"<p>In order to run TF/keras from within R, we need to</p> <p>0. Login to a GPU node:\u00a0<code>ssh\u00a0dev-intel16-k80</code></p> <p>1. Install Tensorflow as the backend of running keras \u2013 local install by user</p> <p>2.\u00a0Install R library {tensorflow} and {keras} \u2013 local install by user</p> <p>We show here how to install R TF libraries in one's home directory, after loading the system-wide R (version 3.5.1 at the time of writing).</p> <p>We don't show here how to install TF; it's included in another wiki at\u00a0https://wiki.hpcc.msu.edu/x/04ZaAQ.</p> <p>All the paths presented below should be replaced by your own paths.</p>","title":"Prerequisites"},{"location":"R_interface_to_TensorFlow_Keras/#installing-r-libraries","text":"<p>As mentioned above, we will install the two TF libraries in your home directory. Let's say you have an existing directory named <code>~/Rlibs</code> (create one if you don't), the installation would go as follows:</p>    **Install R libraries in your HOME dir**    <pre><code># Load R\nmodule purge\nmodule load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604\n\n# Install R libraries tensorflow and keras in an existing directory under your home: ~/Rlibs \nR\n&gt; library(devtools)\n&gt; with_libpaths(new = \"~/Rlibs\", install_github('rstudio/tensorflow'))\n&gt; with_libpaths(new = \"~/Rlibs\", install_github('rstudio/keras'))\n</code></pre>","title":"Installing R libraries"},{"location":"R_interface_to_TensorFlow_Keras/#a-deep-learning-example","text":"<p>Now let's try out a deep learning analysis (classifying MNIST handwritten digits using Multi-Layer Perceptrons) with code available from here. For convenience, the code is pasted below; you can run it either by opening an R console or saving the code to an R script so that you can run the script on the command line.</p> <p>Here are some important things to note:</p> <p>(1) Before executing the R code, you need to set up a couple of conda related environment variables.\u00a0That is,</p>   <pre><code>export PATH=/mnt/home/longnany/anaconda3-march2019/bin:$PATH\nexport LD_LIBRARY_PATH=/mnt/home/longnany/anaconda3-march2019/envs/tf_gpu/lib:$LD_LIBRARY_PATH\nexport TF_CPP_MIN_LOG_LEVEL=2 # to filter out warning messages\n</code></pre>   <p>(2) As you may have noted in the R code below, running TF within R requires some configurations (line 3 and line 4) so that R knows where to find your TF conda environment.</p> <p>(3) Tip: while the code is running, you can start a new HPCC login session and, after running <code>ssh dev-intel16-k80</code>, type the command <code>gpustat -cpuP</code>. You should be able to see your GPU processes (R processes in this case).</p>    **R code for deep learning**    <pre><code>.libPaths(\"~/Rlibs\") # add local lib to R search path; o.w. tensorflow/keras can't be loaded\nlibrary(keras)\nuse_python(\"/mnt/home/longnany/anaconda3-march2019/envs/tf_gpu/bin/python\")\nuse_condaenv(\"/mnt/home/longnany/anaconda3-march2019/envs/tf_gpu\")\n\n\n#loading the keras inbuilt mnist dataset\ndata&lt;-dataset_mnist()\n\n#separating train and test file\ntrain_x&lt;-data$train$x\ntrain_y&lt;-data$train$y\ntest_x&lt;-data$test$x\ntest_y&lt;-data$test$y\n\nrm(data)\n\n# converting a 2D array into a 1D array for feeding into the MLP and normalising the matrix\ntrain_x &lt;- array(train_x, dim = c(dim(train_x)[1], prod(dim(train_x)[-1]))) / 255\ntest_x &lt;- array(test_x, dim = c(dim(test_x)[1], prod(dim(test_x)[-1]))) / 255\n\n#converting the target variable to once hot encoded vectors using keras inbuilt function\ntrain_y&lt;-to_categorical(train_y,10)\ntest_y&lt;-to_categorical(test_y,10)\n\n#defining a keras sequential model\nmodel &lt;- keras_model_sequential()\n\n#defining the model with 1 input layer[784 neurons], 1 hidden layer[784 neurons] with dropout rate 0.4 and 1 output layer[10 neurons]\n#i.e number of digits from 0 to 9\n\nmodel %&gt;% \nlayer_dense(units = 784, input_shape = 784) %&gt;% \nlayer_dropout(rate=0.4)%&gt;%\nlayer_activation(activation = 'relu') %&gt;% \nlayer_dense(units = 10) %&gt;% \nlayer_activation(activation = 'softmax')\n\n#compiling the defined model with metric = accuracy and optimiser as adam.\nmodel %&gt;% compile(\nloss = 'categorical_crossentropy',\noptimizer = 'adam',\nmetrics = c('accuracy')\n)\n\n#fitting the model on the training dataset\nmodel %&gt;% fit(train_x, train_y, epochs = 100, batch_size = 128)\n\n#Evaluating model on the cross validation dataset\nloss_and_metrics &lt;- model %&gt;% evaluate(test_x, test_y, batch_size = 128)\n</code></pre>","title":"A deep learning example"},{"location":"R_others/","text":"","title":"Some packages and other information"},{"location":"R_others/#rstan","text":"<p>The example here follows that in\u00a0RStan Getting Started.\u00a0To test rstan on the HPCC, first load R 3.6.2:</p> <pre><code>module purge  \nmodule load GCC/8.3.0 OpenMPI/3.1.4\u00a0R/3.6.2-X11-20180604\n</code></pre> <p>As of February 2020, the rstan version is 2.19.2.</p> <p>The stan model file \"8schools.stan\" contains:</p> <pre><code>data {\n  int&lt;lower=0&gt; J;         // number of schools \n  real y[J];              // estimated treatment effects\n  real&lt;lower=0&gt; sigma[J]; // standard error of effect estimates \n}\nparameters {\n  real mu;                // population treatment effect\n  real&lt;lower=0&gt; tau;      // standard deviation in treatment effects\n  vector[J] eta;          // unscaled deviation from mu by school\n}\ntransformed parameters {\n  vector[J] theta = mu + tau * eta;        // school treatment effects\n}\nmodel {\n  target += normal_lpdf(eta | 0, 1);       // prior log-density\n  target += normal_lpdf(y | theta, sigma); // log-likelihood\n}\n</code></pre> <p>The R code (\"run.R\") to run stan model contains:</p> <pre><code>library(\"rstan\")\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\nschools_dat &lt;- list(J = 8,\ny = c(28, 8, -3, 7, -1, 1, 18, 12),\nsigma = c(15, 10, 16, 11, 9, 11, 10, 18))\nfit &lt;- stan(file = '8schools.stan', data = schools_dat)\nprint(fit)\npairs(fit, pars = c(\"mu\", \"tau\", \"lp__\"))\nla &lt;- extract(fit, permuted = TRUE) # return a list of arrays\nmu &lt;- la$mu\n</code></pre> <p>To run the model from the command line:</p> <p><code>Rscript run.R</code></p> <p>In addition to the results printed to the stdout, there is an R object file named 8schools.rds generated. This is due to that we've set auto_write to TRUE in run.R. More about the auto_write option:</p>  <p>Logical, defaulting to the value of rstan_options(\"auto_write\"), indicating whether to write the object to the hard disk using saveRDS. Although this argument is FALSE by default, we recommend calling rstan_options(\"auto_write\" = TRUE) in order to avoid unnecessary recompilations. If file is supplied and its dirname is writable, then the object will be written to that same directory, substituting a .rds extension for the .stan extension. Otherwise, the object will be written to the tempdir.</p>","title":"rstan"},{"location":"R_others/#rjags","text":"<p>To use {rjags}, first load R/3.5.1 and JAGS\u00a0from a dev-node (dev-intel16 or dev-intel18) as follows:</p> <p>Loading R/3.5.1 and JAGS</p> <pre><code>module purge\nmodule load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2\nmodule load JAGS/4.3.0\n</code></pre> <p>Next, we will run a short example of data analysis using rjags. This example comes from\u00a0this tutorial which presents many Bayesian models using this package.</p> <p>To invoke R from the command line: <code>R --vanilla</code></p> <p>Then, in the R console, you can run the following codes (for detailed explanation refer to the tutorial mentioned above):</p> <p>Sample R code using {rjags} commands</p> <pre><code>library(rjags)\ndata &lt;- read.csv(\"data1.csv\")\nN &lt;- length(data$y)\ndat &lt;- list(\"N\" = N, \"y\" = data$y, \"V\" = data$V)\ninits &lt;- list( d = 0.0 )\n\njags.m &lt;- jags.model(file = \"aspirinFE.txt\", data=dat, inits=inits, n.chains=1, n.adapt=500)\nparams &lt;- c(\"d\", \"OR\")\nsamps &lt;- coda.samples(jags.m, params, n.iter=10000)\nsummary(window(samps, start=5001))\nplot(samps)\n</code></pre> <p>where the two input files, data1.csv and aspirinFE.txt, need to be located in the working directory. The content of the two files is below.</p> <p>data1.csv</p> <pre><code>N,y,V\n1,0.3289011,0.0388957\n2,0.3845458,0.0411673\n3,0.2195622,0.0204915\n4,0.2222206,0.0647646\n5,0.2254672,0.0351996\n6,-0.1246363,0.0096167\n7,0.1109658,0.0015062\n</code></pre> <p>aspirinFE.txt</p> <pre><code>model {\n\n    for ( i in 1:N ) {\n\n        P[i] &lt;- 1/V[i]\n        y[i] ~ dnorm(d, P[i])\n    }\n\n    ### Define the priors\n    d ~ dnorm(0, 0.00001)\n\n    ### Transform the ln(OR) to OR\n    OR &lt;- exp(d)\n}\n</code></pre> <p>A screen shot of the entire run including the output figures is attached here. </p>","title":"rjags"},{"location":"R_others/#r2openbugs","text":"<p>R package R2OpenBUGS is available on the HPCC.</p> <p>OpenBUGS is a software package that performs Bayesian inference Using Gibbs Sampling. The latest version of OpenBUGS on the HPCC is 3.2.3.\u00a0R2OpenBUGS is an R package that allows users to run OpenBUGS from R.</p> <p>To load R and OpenBUGS, run:</p> <pre><code>module purge  \nmodule load\u00a0GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2  \nmodule load OpenBUGS\n</code></pre> <p>Then, in your R session, use \"library(R2OpenBUGS)\" to load this package. You can execute the following testing R code to see if it works for you.</p> <p>Testing R2OpenBUGS</p> <pre><code>library(R2OpenBUGS)\n\n# Define the model \nBUGSmodel &lt;- function() {\n\n    for (j in 1:N) {\n        Y[j] ~ dnorm(mu[j], tau)\n        mu[j] &lt;- alpha + beta * (x[j] - xbar)\n    }\n\n    # Priors\n    alpha ~ dnorm(0, 0.001)\n    beta ~ dnorm(0, 0.001)\n    tau ~ dgamma(0.001, 0.001)\n    sigma &lt;- 1/sqrt(tau)\n}\n\n# Data\nData &lt;- list(Y = c(1, 3, 3, 3, 5), x = c(1, 2, 3, 4, 5), N = 5, xbar = 3)\n\n# Initial values for the MCMC\nInits &lt;- function() {\n    list(alpha = 1, beta = 1, tau = 1)\n}\n\n# Run BUGS \nout &lt;- bugs(data = Data, inits = Inits, parameters.to.save = c(\"alpha\", \"beta\", \"sigma\"), model.file = BUGSmodel, n.chains = 1, n.iter = 10000)\n\n# Examine the BUGS output\nout\n\n# Inference for Bugs model at \"/tmp/RtmphywZxh/model77555ea534aa.txt\",\n# Current: 1 chains, each with 10000 iterations (first 5000 discarded)\n# Cumulative: n.sims = 5000 iterations saved\n#          mean  sd 2.5%  25%  50%  75% 97.5%\n# alpha     3.0 0.6  1.9  2.7  3.0  3.3   4.1\n# beta      0.8 0.4  0.1  0.6  0.8  1.0   1.6\n# sigma     1.0 0.7  0.4  0.6  0.8  1.2   2.8\n# deviance 13.0 3.7  8.8 10.2 12.0 14.6  22.9\n#\n# DIC info (using the rule, pD = Dbar-Dhat)\n# pD = 3.9 and DIC = 16.9\n# DIC is an estimate of expected predictive error (lower deviance is better).\n</code></pre>","title":"R2OpenBUGS"},{"location":"R_others/#r-interface-to-tensorflowkeras","text":"","title":"R interface to TensorFlow/Keras"},{"location":"R_others/#prerequisites","text":"<p>In order to run TF/keras from within R, we need to</p> <p>0. Login to a GPU node:\u00a0<code>ssh\u00a0dev-intel16-k80</code></p> <p>1. Install Tensorflow as the backend of running keras \u2013 local install by user</p> <p>2.\u00a0Install R library {tensorflow} and {keras} \u2013 local install by user</p> <p>We show here how to install R TF libraries in one's home directory, after loading the system-wide R (version 3.5.1 at the time of writing).</p> <p>We don't show here how to install TF; it's included in another wiki</p> <p>All the paths presented below should be replaced by your own paths.</p>","title":"Prerequisites"},{"location":"R_others/#installing-r-libraries","text":"<p>As mentioned above, we will install the two TF libraries in your home directory. Let's say you have an existing directory named <code>~/Rlibs</code> (create one if you don't), the installation would go as follows:</p> <p>Install R libraries in your home dir</p> <pre><code># Load R\nmodule purge\nmodule load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604\n\n# Install R libraries tensorflow and keras in an existing directory under your home: ~/Rlibs \nR\n&gt; library(devtools)\n&gt; with_libpaths(new = \"~/Rlibs\", install_github('rstudio/tensorflow'))\n&gt; with_libpaths(new = \"~/Rlibs\", install_github('rstudio/keras'))\n</code></pre>","title":"Installing R libraries"},{"location":"R_others/#a-deep-learning-example","text":"<p>Now let's try out a deep learning analysis (classifying MNIST handwritten digits using Multi-Layer Perceptrons) with code available from here. For convenience, the code is pasted below; you can run it either by opening an R console or saving the code to an R script so that you can run the script on the command line.</p> <p>Here are some important things to note:</p> <p>(1) Before executing the R code, you need to set up a couple of conda related environment variables.\u00a0That is,</p> <pre><code>export PATH=/mnt/home/longnany/anaconda3-march2019/bin:$PATH\nexport LD_LIBRARY_PATH=/mnt/home/longnany/anaconda3-march2019/envs/tf_gpu/lib:$LD_LIBRARY_PATH\nexport TF_CPP_MIN_LOG_LEVEL=2 # to filter out warning messages\n</code></pre> <p>(2) As you may have noted in the R code below, running TF within R requires some configurations (line 3 and line 4) so that R knows where to find your TF conda environment.</p> <p>(3) Tip: while the code is running, you can start a new HPCC login session and, after running <code>ssh dev-intel16-k80</code>, type the command <code>gpustat -cpuP</code>. You should be able to see your GPU processes (R processes in this case).</p> <p>R code for deep learning</p> <pre><code>.libPaths(\"~/Rlibs\") # add local lib to R search path; o.w. tensorflow/keras can't be loaded\nlibrary(keras)\nuse_python(\"/mnt/home/longnany/anaconda3-march2019/envs/tf_gpu/bin/python\")\nuse_condaenv(\"/mnt/home/longnany/anaconda3-march2019/envs/tf_gpu\")\n\n\n#loading the keras inbuilt mnist dataset\ndata&lt;-dataset_mnist()\n\n#separating train and test file\ntrain_x&lt;-data$train$x\ntrain_y&lt;-data$train$y\ntest_x&lt;-data$test$x\ntest_y&lt;-data$test$y\n\nrm(data)\n\n# converting a 2D array into a 1D array for feeding into the MLP and normalising the matrix\ntrain_x &lt;- array(train_x, dim = c(dim(train_x)[1], prod(dim(train_x)[-1]))) / 255\ntest_x &lt;- array(test_x, dim = c(dim(test_x)[1], prod(dim(test_x)[-1]))) / 255\n\n#converting the target variable to once hot encoded vectors using keras inbuilt function\ntrain_y&lt;-to_categorical(train_y,10)\ntest_y&lt;-to_categorical(test_y,10)\n\n#defining a keras sequential model\nmodel &lt;- keras_model_sequential()\n\n#defining the model with 1 input layer[784 neurons], 1 hidden layer[784 neurons] with dropout rate 0.4 and 1 output layer[10 neurons]\n#i.e number of digits from 0 to 9\n\nmodel %&gt;% \nlayer_dense(units = 784, input_shape = 784) %&gt;% \nlayer_dropout(rate=0.4)%&gt;%\nlayer_activation(activation = 'relu') %&gt;% \nlayer_dense(units = 10) %&gt;% \nlayer_activation(activation = 'softmax')\n\n#compiling the defined model with metric = accuracy and optimiser as adam.\nmodel %&gt;% compile(\nloss = 'categorical_crossentropy',\noptimizer = 'adam',\nmetrics = c('accuracy')\n)\n\n#fitting the model on the training dataset\nmodel %&gt;% fit(train_x, train_y, epochs = 100, batch_size = 128)\n\n#Evaluating model on the cross validation dataset\nloss_and_metrics &lt;- model %&gt;% evaluate(test_x, test_y, batch_size = 128)\n</code></pre>","title":"A deep learning example"},{"location":"R_others/#r-351-with-intel-mkl","text":"<p>Intel MKL can accelerate R's speed in linear algebra calculations (such as cross-product, matrix decomposition, inverse computation, linear regression and etc.) by providing BLAS with higher performance.  On the HPCC, only 3.5.1 has been built by linking to Intel MKL.</p>","title":"R 3.5.1 with Intel MKL"},{"location":"R_others/#loading-r","text":"<p>Loading R 3.5.1 built w/ OpenBLAS</p> <pre><code>module purge\nmodule load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604\n</code></pre> <p>Loading R 3.5.1 built w/ MKL</p> <pre><code>module purge\nmodule load R-Core/3.5.1-intel-mkl\n</code></pre> <p>You could double check the BLAS/LAPACK libraries linked by running sessionInfo() in R.</p>","title":"Loading R"},{"location":"R_others/#benchmarking","text":"<p>We have a simple R code,\u00a0<code>crossprod.R</code>,\u00a0for testing the computation time.\u00a0The code is below, where the function <code>crossprod</code>\u00a0can run in a multi-threaded mode implemented by OpenMP.</p> <p>crossprod.R</p> <pre><code>set.seed(1)\nm &lt;- 5000\nn &lt;- 20000\nA &lt;- matrix(runif (m*n),m,n)\nsystem.time(B &lt;- crossprod(A))\n</code></pre> <p>Now, open an interactive SLURM job session by requesting 4 cores:</p> <p>Benchmarking OpenBLAS vs. MKL (multi-threads)</p> <pre><code>salloc --time=2:00:00 --cpus-per-task=4 --mem=50G --constraint=intel18\n\n# After you are allocated a compute node, do the following.\n\n# 1) Run R/OpenBLAS\nmodule purge\nmodule load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604\nRscript --vanilla crossprod.R &amp;\n# Time output:\n# user     system  elapsed \n# 50.036   1.574   14.156\n\n\n# 2) Run R/MKL\nmodule purge\nmodule load R-Core/3.5.1-intel-mkl\nRscript --vanilla crossprod.R &amp;\n# Time output:\n# user     system  elapsed \n# 28.484   1.664   8.737\n</code></pre> <p>Above, the boost in speed is clear from using MKL as compared with OpenBLAS.</p> <p>Even if we use a single thread (by requesting one core), MKL still shows some advantage.</p> <p>Benchmarking OpenBLAS vs. MKL (single-thread)</p> <pre><code>salloc --time=2:00:00 --cpus-per-task=1 --mem=50G --constraint=intel18\n\n# After you are allocated a compute node, do the following.\n\n# 1) Run R/OpenBLAS\nmodule purge\nmodule load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604\nRscript --vanilla crossprod.R &amp;\n# Time output:\n# user     system  elapsed \n# 47.763   0.598   49.287\n\n\n# 2) Run R/MKL\nmodule purge\nmodule load R-Core/3.5.1-intel-mkl\nRscript --vanilla crossprod.R &amp;\n# Time output:\n# user     system  elapsed \n# 25.846   0.641   27.006\n</code></pre>","title":"Benchmarking"},{"location":"R_others/#notes","text":"<p>When loading R, the OpenMP environment variable\u00a0OMP_NUM_THREADS is left unset.\u00a0This means that when running R code directly on a dev-node, all CPUs on that node will be used by the internal multithreading library compiled into R. This is discouraged since the node will be overloaded and your job may even fail. Therefore, please set OMP_NUM_THREADS to a proper value before running the R code. For example,</p> <p><code>$ OMP_NUM_THREADS=4</code></p> <p><code>$ Rscript --vanilla crossprod.R</code></p> <p>On the other hand, when the code is run on a compute node allocated by SLURM, you don\u2019t need to set OMP_NUM_THREADS as R would automatically detect CPUs available for use (which should have been requested in your salloc command or sbatch script).</p>","title":"Notes"},{"location":"R_workshop_tutorial/","text":"","title":"R workshop tutorial"},{"location":"R_workshop_tutorial/#preparation","text":"<ol> <li>Basic knowledge of R\u00a0language, Linux, and the HPCC environment.</li> <li>Login<ol> <li><code>ssh -XY YourAccount@hpcc.msu.edu</code></li> <li><code>ssh\u00a0dev-amd20</code></li> </ol> </li> <li>We will be using R 4.0.2. To load     it:\u00a0<code>module purge; module load\u00a0GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2</code></li> <li>Copy files that we are using in this workshop to your home     directory:\u00a0<code>cp -r /mnt/research/common-data/workshops/R-workshop .</code></li> </ol>","title":"Preparation"},{"location":"R_workshop_tutorial/#r-startup","text":"<p>When an R session starts, it looks for and loads two hidden configuration files, <code>.Renviron</code> and <code>.Rprofile</code>.</p> <ul> <li><code>.Renviron</code>: contains environment variables to be set in R sessions</li> <li><code>.Rprofile</code>: contains R commands to be run in R sessions</li> </ul> <p>The following search order is applied: your current working directory, your home directory, and the system-wide <code>R_HOME/etc/</code> (you can use R command <code>R.home()</code> to check path of <code>R_HOME</code>).\u00a0Below are examples of the two files which have been placed in our R workshop directory. You need to use\u00a0ls -a\u00a0to list them since they are \"hidden\" files.</p> <p>.Rprofile (an example)</p> <pre><code>cat(\"Sourcing .Rprofile from the R-workshop directory.\\n\")\n\n# To avoid setting the CRAN mirror each time you run install.packages\nlocal({\n  options(repos = \"https://repo.miserver.it.umich.edu/cran/\")\n})\n\noptions(width=100) # max number of columns when printing vectors/matrices/arrays\n\n.First &lt;- function() cat(\"##### R session begins for\", paste(Sys.getenv(\"USER\"),\".\", sep=\"\"), \"You are currently in\", getwd(), \"#####\\n\\n\")\n.Last &lt;- function()  cat(\"\\n##### R session ends for\", paste(Sys.getenv(\"USER\"),\".\", sep=\"\"),  \"You are currently in\", getwd(), \"#####\\n\\n\")\n</code></pre> <p>.Renviron (an example)</p> <pre><code>R_DEFAULT_PACKAGES=\"datasets,utils,grDevices,graphics,stats,methods\"\nR_LIBS_USER=/mnt/home/longnany/Rlibs\n</code></pre> <p>Let's run a short Rscript command to see what we get:</p> <pre><code>$ Rscript -e 'date()'\n</code></pre> <p>Notes:</p> <ul> <li> <p>Personalizing these two files can reduce code portability.</p> </li> <li> <p>Rscript by default doesn't load package <code>{methods}</code> in order to     speed up initialization. To customize your own loading packages, add     them in .Renviron (see above).</p> </li> <li> <p>If you don't want R or Rscript to read any Renviron or Rprofile     files when starting an R session, use option <code>--vanilla</code>. A caveat:\u00a0if you explicitly export an R environment variable, such     as\u00a0 <code>export R_LIBS_USER=~/Rlibs</code>, then adding\u00a0<code>--vanilla</code>\u00a0will not     ignore its value. See below the result.</p> </li> </ul> <pre><code>$ Rscript --vanilla -e '.libPaths()' # .libPaths() is used to see the directories where R searches for libraries\n[1] \"/opt/software/R/4.0.2-foss-2019b/lib64/R/library\"\n$ export R_LIBS_USER=~/Rlibs\n$ Rscript --vanilla -e '.libPaths()'\n[1] \"/mnt/home/longnany/Rlibs\"\n[2] \"/opt/software/R/4.0.2-foss-2019b/lib64/R/library\"\n</code></pre>","title":"R startup"},{"location":"R_workshop_tutorial/#rscript-one-liner","text":"<p>The general format of Rscript:</p> <pre><code>Rscript [options] [-e expression1 -e expression2 ... | source file] [args]\n</code></pre> <ul> <li> <p><code>options</code>: can be multiple; all beginning with <code>--</code>     (e.g.,\u00a0<code>--vanilla</code> as mentioned above). To learn about all the     options, run Rscript --help on the command line.</p> </li> <li> <p><code>expression1, expression2 ...</code>: can be one or multiple. They are R     commands.</p> </li> <li> <p><code>source file</code>: R source code.</p> </li> <li> <p><code>args</code>: arguments to be passed.</p> </li> </ul> <p>You may have both expressions and source file present in your Rscript line. Here are a few one-liner examples:</p> <p>Rscript one-liner examples</p> <pre><code># Ex1: simple loop\n$ Rscript -e 'for (i in 1:5) print(paste(\"g\", i))'\n\n\n# Ex2: print time\n$ Rscript -e 'date()'\n\n\n# Ex3: quick math (calculating quotient and remainder)\n$ Rscript -e '128 %/% 11' -e '128 %% 11'\n\n\n# Ex4: get help for command \"paste\"\n$ Rscript -e 'help(paste)'\n\n\n# Ex5:\u00a0used in conjunction with pipe.\u00a0\n#   Generate three sets of random Normal variables with different means (sd all being one); means are given in file test.dat.\n$ cat &gt; test.dat # ctrl+D to go back when done typing\n1\n10\n20\n$ cat test.dat | Rscript -e 'input_con  &lt;- file(\"stdin\"); open(input_con); while (length(oneLine &lt;- readLines(con = input_con, n = 1, warn = FALSE)) &gt; 0) {print(rnorm(5,mean=as.numeric(oneLine)))};close(input_con)'\n</code></pre>","title":"Rscript one-liner"},{"location":"R_workshop_tutorial/#using-rscript-with-source-code","text":"<p>a. simple usage:</p> <p>Instead of using <code>'-e your_commands</code>', we now put R commands in a source file and run it with Rscript. Below is an R script file and we can run\u00a0<code>Rscript multiplots.R</code> to get a PDF figure <code>multiplots.pdf</code>. To view it:\u00a0<code>evince multiplots.pdf</code></p> <p><code>multiplots.R</code>: a very simple example of making 4 plots on the same page</p> <pre><code>pdf(\"multiplots.pdf\")\npar(mfrow=c(2,2))\nfor (i in 1:4) plot(1:10, 1:10, type=\"b\", xlab=bquote(X[.(i)]), ylab=bquote(Y[.(i)]), main=bquote(\"Multiple plots: #\" ~ .(i)))\ndev.off()\n</code></pre> <p>b. passing command line arguments:</p> <p>We can also pass arguments to our R script, as shown in the example below.</p> <p><code>args-1.R</code></p> <pre><code>args &lt;- commandArgs(trailingOnly = TRUE)\n\nnargs &lt;- length(args)\nfor (i in 1:nargs) {\n    cat(\"Arg\", i, \":\", args[i], \"\\n\")\n}\n\n\ncat(\"Generating\",as.numeric(args[nargs-1]), \"normal variables with mean =\", as.numeric(args[nargs]), \"and sd = 1 \\n\")\nrnorm(as.numeric(args[nargs-1]), mean=as.numeric(args[nargs]))\n</code></pre> <p>Running script <code>args-1.R</code>:</p> <pre><code>$ Rscript args-1.R 5 3\nSourcing .Rprofile from the 11092017-R-Workshop directory.\n##### R session begins for longnany. You are currently in /mnt/home/longnany/Documents/11092017-R-Workshop #####\n\nArg 1 : 5\nArg 2 : 3\nGenerating 5 normal variables with mean = 3 and sd = 1\n[1] 2.707162 3.677923 3.192272 2.531973 3.699060\n\n##### R session ends for longnany. You are currently in /mnt/home/longnany/Documents/11092017-R-Workshop #####\n</code></pre> <p>c. processing command line arguments with {<code>getopt</code>}:</p> <p><code>args-2.R</code></p> <pre><code>require(\"getopt\", quietly=TRUE)\n\nspec = matrix(c(\n    \"Number\", \"n\", 1, \"integer\",\n    \"Mean\", \"m\", 1, \"double\"\n), byrow=TRUE, ncol=4) # cf. https://cran.r-project.org/web/packages/getopt/getopt.pdf\n\nopt = getopt(spec);\n\nif (is.null(opt$Number)) {\n    n &lt;- 5\n} else {\n    n &lt;- opt$Number\n}\n\nif (is.null(opt$Mean)) {\n    m &lt;- 3\n} else {\n    m &lt;- opt$Mean\n} \n\ncat(\"Generating\", n, \"normal variables with mean =\", m, \"and sd = 1 \\n\")\nrnorm(n=n, mean=m)\n</code></pre> <p>Running the script <code>args-2.R</code>:</p> <pre><code># Use long flag names\n$ Rscript --vanilla args-2.R --Number 10 --Mean -2\nGenerating 10 normal variables with mean = -2 and sd = 1\n [1] -0.4776278 -1.7759145 -0.9977682 -2.6452126 -3.4050587 -2.2358362\n [7] -1.2696362 -1.6213633 -2.7013074 -1.9271954\n\n# Use short flag names\n$ Rscript --vanilla args-2.R -n 10 -m -2\nGenerating 10 normal variables with mean = -2 and sd = 1\n [1] -2.2241837 -1.6704711  0.1481244  0.2072124 -1.0385386 -1.5194874\n [7] -2.6744478 -2.4683039 -0.7962113 -1.1901021\n\n# No arguments provided so defaults are used\n$ Rscript --vanilla args-2.R\nGenerating 5 normal variables with mean = 3 and sd = 1\n[1] 3.951492 4.255879 4.485044 2.727223 3.039532\n</code></pre>","title":"Using Rscript with source code"},{"location":"R_workshop_tutorial/#submitting-parallel-jobs-to-the-cluster-using-doparallel-single-node-multiple-cores","text":"<p>To submit a single-node job, we recommend {doParallel}'s multicore functionality.</p> <p><code>R_doParallel_singlenode.R</code>: run 10,000 bootstrap iterations of fitting a logistic regression model</p> <pre><code>library(doParallel)\n\n# Request a single node (this uses the \"multicore\" functionality)\nregisterDoParallel(cores=as.numeric(Sys.getenv(\"SLURM_CPUS_ON_NODE\")[1]))\n\nx &lt;- iris[which(iris[,5] != \"setosa\"), c(1,5)]\ntrials &lt;- 10000\nptime &lt;- system.time({\n    r &lt;- foreach(icount(trials), .combine=cbind) %dopar% {\n        ind &lt;- sample(100, 100, replace=TRUE)\n        result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n        coefficients(result1)\n    }\n})[3]\n\ncat(\"Time elapsed:\", ptime, \"\\n\")\ncat(\"Currently registered backend:\", getDoParName(), \"\\n\")\ncat(\"Number of workers used:\", getDoParWorkers(), \"\\n\")\nprint(str(r)) # column-binded result\n</code></pre> <p>Now, submit the job to the HPCC through the following SLURM script:</p> <p><code>submit-doParallel.sbatch</code></p> <pre><code>#!/bin/bash\n\n# Job name:\n#SBATCH --job-name=doParallel_test\n#\n# Number of nodes needed:\n#SBATCH --nodes=1\n#\n# Tasks per node:\n#SBATCH --ntasks-per-node=1\n#\n# Processors per task:\n#SBATCH --cpus-per-task=4\n#\n# Memory per node:\n#SBATCH --mem=500M\n#\n# Wall time (e.g. \"minutes\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\"):\n#SBATCH --time=3:00:00\n#\n# Standard out and error:\n#SBATCH --output=%x-%j.SLURMout\n\nmodule purge\nmodule load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2\n\nRscript --vanilla R_doParallel_singlenode.R &gt; R_doParallel_singlenode.Rout\n</code></pre> <p>Submission command:\u00a0sbatch submit-doParallel.sbatch</p> <p>The output file <code>R_doParallel_singlenode.Rout</code> \u00a0should look like:</p> <pre><code>Time elapsed: 8.946\nCurrently registered backend: doParallelMC\nNumber of workers used: 4\n num [1:2, 1:10000] -14.6 2.26 -11.86 1.91 -7.75 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:2] \"(Intercept)\" \"x[ind, 1]\"\n  ..$ : chr [1:10000] \"result.1\" \"result.2\" \"result.3\" \"result.4\" ...\nNULL\n</code></pre>","title":"Submitting parallel jobs to the cluster using {doParallel}: single node, multiple cores"},{"location":"R_workshop_tutorial/#submitting-parallel-jobs-to-the-cluster-using-rmpi-multiple-nodes","text":"<p>MPI stands for Message Passing Interface, and the R package Rmpi\u00a0is an implementation of it for R.\u00a0Function usage in {Rmpi} is complicated. For ease of implementation, we recommend the use of another simplified R MPI library, {pbdMPI}, as described next.</p>","title":"Submitting parallel jobs to the cluster using\u00a0{Rmpi}: multiple nodes"},{"location":"R_workshop_tutorial/#submitting-parallel-jobs-to-the-cluster-using-pbdmpi-multiple-nodes","text":"<p>pbdMPI\u00a0is a more recent R MPI package, which simplifies MPI interaction and thus eases programming. It works in Single Program/Multiple Data (SPMD) mode.\u00a0As an illustration, we consider the problem of computing the log likelihood of data following a 2-dimensional Multi-Variate Normal (MVN) distribution. The example below applies Cholesky decomposition on the 2-by-2 covariance matrix, solves a system of linear equations (this is where parallelism plays a vital role), and performs some matrix/vector operations. Here is a graphic illustration of solving a system of linear equations by part.</p> <p><code>MVN.R</code>: MPI in SPMD mode</p> <pre><code># Load pbdMPI and initialize the communicator\nlibrary(pbdMPI, quiet = TRUE)\ninit()\n\n# Check processes\ncomm.cat(\"All processes start...\\n\\n\")\nmsg &lt;- sprintf(\"I am rank %d on host %s of %d processes\\n\", comm.rank(), Sys.info()[\"nodename\"], comm.size())\ncomm.cat(msg, all.rank=TRUE, quiet=TRUE) # quiet=T tells each rank not to \"announce\" itself when it's printing\n\n\nset.seed(1234)\nN &lt;- 100\np &lt;- 2\nX &lt;- matrix(rnorm(N * p), ncol = p)\nmu &lt;- c(0.1, 0.2)\nSigma &lt;- matrix(c(0.9, 0.1, 0.1, 0.9), ncol = p)\n\n# Load data partially by processors\nid.get &lt;- get.jid(N)\nX.spmd &lt;- matrix(X[id.get, ], ncol = p)\ncomm.cat(\"\\nPrint out the matrix on each process/rank:\\n\\n\", quiet=TRUE)\ncomm.print(X.spmd, all.rank=TRUE, quiet=TRUE)\n\n# Cholesky decomposition\nU &lt;- chol(Sigma) # U'U = Sigma\nlogdet &lt;- sum(log(abs(diag(U))))\n\n# Call R's backsolve function for each chunk of the data matrix X (i.e. B.spmd)\nB.spmd &lt;- t(X.spmd) - mu\nA.spmd &lt;- backsolve(U, B.spmd, upper.tri = TRUE, transpose = TRUE) # U'A = B\ndistval.spmd &lt;- colSums(A.spmd * A.spmd)\n\n# Use sum as the reduction operation\nsum.distval &lt;- allreduce(sum(distval.spmd), op = \"sum\")\ntotal.logL &lt;- -0.5 * (N * (p * log(2 * pi) + logdet * 2) + sum.distval)\n\n# Output\ncomm.cat(\"\\nFinal log-likelihood:\\n\\n\", quiet=TRUE)\ncomm.print(total.logL, quiet=TRUE)\n\nfinalize()\n</code></pre> <p>SLURM submission script:</p> <p><code>submit-pbdMPI.sbatch</code></p> <pre><code>#!/bin/bash\n\n# Job name:\n#SBATCH --job-name=pbdMPI_test\n#\n# Number of MPI tasks:\n#SBATCH --ntasks=20\n#\n# Processors per task:\n#SBATCH --cpus-per-task=1\n#\n# Memory:\n#SBATCH --mem-per-cpu=800M\n#\n# Wall clock limit (e.g. \"minutes\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\"):\n#SBATCH --time=30\n#\n# Standard out and error:\n#SBATCH --output=%x-%j.SLURMout\n\necho \"SLURM_NTASKS: $SLURM_NTASKS\"\n\nmodule purge\nmodule load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2\n\n# Suppress warnings about forks and missing CUDA libraries\nexport OMPI_MCA_mpi_warn_on_fork=0\nexport OMPI_MCA_mpi_cuda_support=0\n\nmpirun -n $SLURM_NTASKS Rscript --vanilla MVN.R &gt; MVN.Rout\n</code></pre> <p>Now, submit the job by\u00a0<code>sbatch submit-pbdMPI.sbatch</code>When finished,\u00a0<code>MVN.Rout</code> should contain the following information:</p> <pre><code>COMM.RANK = 0\nAll processes start...\n\nI am rank 0 on host lac-421 of 20 processes\nI am rank 1 on host lac-421 of 20 processes\n... ...\nI am rank 18 on host lac-421 of 20 processes\nI am rank 19 on host lac-421 of 20 processes\n\nPrint out the matrix on each process/rank:\n\n           [,1]       [,2]\n[1,] -0.1850210 -0.2771503\n[2,] -0.8596753 -0.1081122\n[3,] -1.0927853  0.8690750\n[4,] -0.5831948  0.8032846\n[5,]  1.0796870  0.2354514\n            [,1]       [,2]\n[1,]  0.05274001 -0.8410549\n[2,] -0.92568393  0.9461704\n[3,]  1.01632804  0.6875080\n[4,]  0.34271061  0.4905065\n[5,]  0.84192956 -1.6685933\n... ...\n            [,1]       [,2]\n[1,] -0.55673659 -0.1229023\n[2,] -0.03156318 -0.8501178\n[3,] -1.28832627 -1.3115801\n[4,] -0.47546826 -0.4856559\n[5,]  0.81134183  0.7499220\n            [,1]       [,2]\n[1,] -0.95620195  0.6560605\n[2,]  0.04671396  0.6093924\n[3,]  0.18986742  0.2077641\n[4,]  0.73281327 -1.0452661\n[5,]  2.27968859  1.0611428\n\nFinal log-likelihood:\n\n[1] -283.2159\n</code></pre>","title":"Submitting parallel jobs to the cluster using\u00a0{pbdMPI}: multiple nodes"},{"location":"Rclone_-_rsync_for_cloud_storage/","text":"<p>Users could use this software to copy files from/to their Microsoft OneDrive or Google Drive cloud storage to/from HPCC disk space. This tool could also mount their cloud storage to HPCC disk so that the storage on cloud could be used as extended disk space.</p> <p>Rclone is installed on HPCC system wide. To use it, users should first\u00a0 load the software module into their environment using command \"module load Rclone\".\u00a0For more details of using rclone, users can visit Rclone web site at\u00a0https://rclone.org/.</p> <p>To start using it, user should run command \"rclone config\" to configure it.\u00a0\u00a0The instructions of this command could be found at https://rclone.org/commands/rclone_config/. Specifically, to configure for Google Drive, see\u00a0https://rclone.org/drive/, and to configure for Microsoft Onedrive, see\u00a0https://rclone.org/onedrive/ for instructions. The specific details of how to start using this software on HPCC could be found in the document\u00a0Rclone.pdf</p> <p>After successfully configured, users should be able to use \"rclone\" command to copy or mount the cloud storage to HPCC. There many rclone commands could be used to handle the file transfer and manage files on HPCC and cloud storage. To get help, use \"rclone --help\" as show in the following</p> <pre><code>[hpc@dev-intel16-k80 ~]$ module load Rclone\n[hpc@dev-intel16-k80 ~]$ rclone --help\n\nRclone syncs files to and from cloud storage providers as well as\nmounting them, listing them in lots of different ways.\n\nSee the home page (https://rclone.org/) for installation, usage,\ndocumentation, changelog and configuration walkthroughs.\n\nUsage:\n  rclone [flags]\n  rclone [command]\n\nAvailable Commands:\n  about           Get quota information from the remote.\n  authorize       Remote authorization.\n  cachestats      Print cache stats for a remote\n  cat             Concatenates any files and sends them to stdout.\n  check           Checks the files in the source and destination match.\n  cleanup         Clean up the remote if possible\n  config          Enter an interactive configuration session.\n  copy            Copy files from source to dest, skipping already copied\n  copyto          Copy files from source to dest, skipping already copied\n  copyurl         Copy url content to dest.\n  cryptcheck      Cryptcheck checks the integrity of a crypted remote.\n  cryptdecode     Cryptdecode returns unencrypted file names.\n  dbhashsum       Produces a Dropbox hash file for all the objects in the path.\n  dedupe          Interactively find duplicate files and delete/rename them.\n  delete          Remove the contents of path.\n  deletefile      Remove a single file from remote.\n  genautocomplete Output completion script for a given shell.\n  gendocs         Output markdown docs for rclone to the directory supplied.\n  hashsum         Produces an hashsum file for all the objects in the path.\n  help            Show help for rclone commands, flags and backends.\n  link            Generate public link to file/folder.\n  listremotes     List all the remotes in the config file.\n  ls              List the objects in the path with size and path.\n  lsd             List all directories/containers/buckets in the path.\n  lsf             List directories and objects in remote:path formatted for parsing\n  lsjson          List directories and objects in the path in JSON format.\n  lsl             List the objects in path with modification time, size and path.\n  md5sum          Produces an md5sum file for all the objects in the path.\n  mkdir           Make the path if it does not already exist.\n  mount           Mount the remote as file system on a mountpoint.\n  move            Move files from source to dest.\n  moveto          Move file or directory from source to dest.\n  ncdu            Explore a remote with a text based user interface.\n  obscure         Obscure password for use in the rclone.conf\n  purge           Remove the path and all of its contents.\n  rc              Run a command against a running rclone.\n  rcat            Copies standard input to file on remote.\n  rcd             Run rclone listening to remote control commands only.\n  rmdir           Remove the path if empty.\n  rmdirs          Remove empty directories under the path.\n  serve           Serve a remote over a protocol.\n  settier         Changes storage class/tier of objects in remote.\n  sha1sum         Produces an sha1sum file for all the objects in the path.\n  size            Prints the total size and number of objects in remote:path.\n  sync            Make source and dest identical, modifying destination only.\n  touch           Create new file or change file modification time.\n  tree            List the contents of the remote in a tree like fashion.\n  version         Show the version number.\n\nUse \"rclone [command] --help\" for more information about a command.\nUse \"rclone help flags\" for to see the global flags.\nUse \"rclone help backends\" for a list of supported services.\n[hpc@dev-intel16-k80 ~]$ \n</code></pre> <p>A tool \"cloudSync\" is developed for user to synchronize the files between their cloud storages. It is accessible through \"powertools\". To use it, users need to have the module \"powertools\" loaded. Users are welcome to try it and report any problems to us via contact form\u00a0here.</p> <p>Following are a few examples of running rclone commands after successfully configured the cloud storage. Assume that the cloud storage is configured as\u00a0 the name \"MyOneDrive\".\u00a0</p> <p>(1) see current remote storage: As show, we can see that there are currently two remote cloud storage,\u00a0 \"MyOneDrive\" and \"googledoc\" are configured.\u00a0</p> <pre><code>[user@dev-intel18 ~]$ rclone config\nCurrent remotes:\n\nName                 Type\n====                 ====\nMyOneDrive           onedrive\ngoogledoc            drive\n\ne) Edit existing remote\nn) New remote\nd) Delete remote\nr) Rename remote\nc) Copy remote\ns) Set configuration password\nq) Quit config\ne/n/d/r/c/s/q&gt; q\n\n[user@dev-intel18 ~]$\n</code></pre> <p>(2) Check the remote storage information: We could check the remote storage usage and quota using \"rclone about\" command.</p> <pre><code>[user@dev-intel16-k80 ~]$ rclone about MyOneDrive:\nTotal:   5T\nUsed:    450.999M\nFree:    4.998T\nTrashed: 404.576k\n</code></pre> <p>(3) List the contents of the cloud storage</p> <pre><code>[user@dev-intel18 ~]$ rclone lsd MyOneDrive:\n          -1 2018-02-02 08:57:54         0 Attachments\n          -1 2019-08-27 15:43:33         1 IMAGES\n          -1 2019-08-22 15:50:10        42 Matlab\n          -1 2019-02-26 17:12:01        16 Microsoft Teams Chat Files\n          -1 2018-08-24 08:56:32         1 Notebooks\n</code></pre> <p>(4) Copy files on HPCC to remote cloud: \u00a0</p> <pre><code>[user@dev-intel18 ~]$ rclone copy Project MyOneDrive:Project   # copy the content of directory \"Project\" to remote cloud storage\n\n[user@dev-intel18 ~]$ rclone lsd MyOneDrive:               # view the contents of cloud storage to confirm the copy\n          -1 2018-02-02 08:57:54         0 Attachments\n          -1 2019-08-27 15:43:33         1 IMPACT\n          -1 2019-08-22 15:50:10        42 Matlab\n          -1 2019-02-26 17:12:01        16 Microsoft Teams Chat Files\n          -1 2018-08-24 08:56:32         1 Notebooks\n          -1 2020-04-27 15:43:25         2 Project\n[user@dev-intel18 ~]$ rclone lsd MyOneDrive:Project\n          -1 2020-04-27 15:44:39         1 GPAW\n          -1 2020-04-27 15:43:26         3 MATLAB\n</code></pre> <p>(5) Copy files on cloud storage to HPCC:</p> <pre><code>[user@dev-intel18 Project]$ ls                                 # current content of Project directory before copy\nGPAW  MATLAB\n[user@dev-intel18 Project]$ rclone copy MyOneDrive:IMPACT ./   # copy the content of IMPACT in cloud to current directory         \n[user@dev-intel18 Project]$ ls                                 # confirm that the copy is done\nGPAW  impact_run  MATLAB\n</code></pre>  <p>Note</p> <p>Although \"rclone copy\" is similar as unix commands rsync and cp, when using it, users should be aware of the differences and know the details of its behavior.\u00a0</p> <p>(1) \"rclone copy\" does not\u00a0transfer unchanged files, testing by size and modification time or MD5SUM. In this sense, it is similar as linux command rsync;</p> <p>(2) When running \"<code>rclone copy source:sourcepath dest:destpath\", if</code> <code>source:sourcepath</code>\u00a0is a directory, <code>dest:destpath</code>should also be a directory. \u00a0It does not copy the directory <code>source:sourcepath</code>, instead, it will copy the content of the directory <code>source:sourcepath</code>\u00a0to the destination <code>dest:destpath</code>. If\u00a0<code>dest:destpath</code>does not exist, it will be created and the content of <code>source:sourcepath</code>will be stored in it.</p> <p>(3) \"rclone copyto\" is a very similar rclone command to \"rclone copy\". The only difference is that it can\u00a0be used to upload single files to other than their current name. When running \"<code>rclone copyto source:sourcepath dest:destpath\", if</code> <code>source:sourcepath</code>\u00a0is a file, <code>dest:destpath c</code>ould be a new file name. If\u00a0<code>source:sourcepath</code>\u00a0is a directory, it would be the same as using \"<code>rclone copy\".</code></p>  <p>(6)\u00a0Checks the files in the source and destination match.</p> <pre><code>[user@dev-intel18 Project]$ rclone check impact_run MyOneDrive:IMPACT/impact_run   # check if it is matched both sides\n2020/04/27 16:19:01 NOTICE: One drive root 'IMPACT/impact_run': 0 differences found\n2020/04/27 16:19:01 NOTICE: One drive root 'IMPACT/impact_run': 21 matching files\n</code></pre>  <p>Note</p> <p>For archiving your files to your cloud storage, if the connection between HPCC and your cloud storage is not stable, we would NOT recommend using \"rclone move\" because it may loss the data during the transfer. Instead, we recommend using \"rclone copy\" to successfully copy the files over and run \"rclone check\" to check if files are identical. After that, it is safe to delete local copy of the files.</p>   <p>Note</p> <p>When use \"rclone mount\" command to mount your cloud storage to HPCC, there are two things users should be careful:</p> <p>(1) When running rclone mount, the process runs NOT as the user, instead, it runs as a \"root\" of the cloud storage. Therefore, user may see the error message like \"mount helper error: fusermount: failed to open mountpoint for reading: Permission denied\". User could use /tmp space for mount point because that space is accessible for all users. Users should be very careful to open the permission to others for the purpose of using rclone mount.\u00a0</p> <p>(2) The \"rclone mount\" users should unmount it after use using \"fusermount -u \\&lt;endpoint_dir&gt;\". Note that sometimes the endpoint is not unmount from some nodes due to timeout or some reason, you may see the message like \"Transport endpoint is not connected\" when accessing the endpoint directory on the node. Just manually unmount it again should resolve the issue.\u00a0</p>   <p>Note</p> <p>When use \"rclone config\" command to configure your cloud storage on HPCC, the command will\u00a0guide you through an interactive setup process. At the step of auto config, after you chose \"y\", it will start authentication. You will see something like:</p> <pre><code>If your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth\n\nLog in and authorize rclone for access\nWaiting for code...\n</code></pre> <p>At this time, a firefox browser would be open. If you did not get the browser window, check if you used -X option to allow X11 forwarding when you run ssh. \u00a0You may follow the instruction at [Connect to HPCC System\u00a0to get the display right.</p> <p>It will take a few minutes to get the browser open and connected. Please be patient. If the browser window is open but does not open the authentication page, you could manually input the link provided by the \"rclone config\" command to the firefox browser's url address box to connect to the site. DO NOT use the link on your personal computer's browser. The authentication have to use the browser on HPCC development node. \u00a0</p>","title":"Rclone - rsync for cloud storage"},{"location":"Regular_Expressions/","text":"<p>A regular expression is a powerful tool to \u00a0match patterns. With this tool, you can validate text input, search/replace text within a file, batch rename files, test for patterns within strings etc.</p> <p>There are two types of regular expressions: the basic regular expressions (BRE), and the extended regular expressions (ERE). Most utilities (including vi, sed, and grep)\u00a0use the basic regular expression. awk and egrep use the extended expression.</p> <p>There are three parts to a regular expression: anchors, character sets, and modifiers. Anchors\u00a0are used to specify the position of the pattern in relation to a line of text. Character sets match one or more characters in a single position. Modifiers specify how many times the previous character set is repeated.</p>","title":"Regular Expressions"},{"location":"Regular_Expressions/#the-anchor-characters-the-starting-anchor-and-the-end-anchor","text":"<p>\"^\" is the starting anchor, and the character \"$\" is the end anchor. The regular expression \"^A\" will match all lines that start with a capital A. The expression \"A$\" will match all lines that end with the capital A.\u00a0The anchor characters works only if there are located in a proper location. Otherwise, \u00a0they no longer act as anchors. For example, the \"^\" is only an anchor if it is the first character in a regular expression. The \"$\" is only an anchor if it is the last character. The expression \"$1\" and \"1^\"do not have an anchor. If you want to match a \"^\" at the beginning of the line, or a \"$\" at the end of a line, you must escape the special characters with a backslash.</p>    pattern Matches     ^A A at the beginning of a line   A$ A at the end of a line   A^ A^ anywhere on a line   $A $A anywhere on a line   ^^ ^ at the begging of a line   $$ $ at the end of a line","title":"The anchor characters: the starting anchor ^ and the end anchor $"},{"location":"Regular_Expressions/#matching-a-character-with-character-sets","text":"<p>The regular expression \"the\" has three characters: \"t,\" \"h\" and \"e\". It will match any line with the string \"the\" inside it. However, it will also match the word \"there\" or \"them\". To prevent this, put spaces before and after the pattern as \"\u00a0the\u00a0\". You can combine the string with an anchor such as \"^HPCC\".</p>","title":"Matching a character with character sets"},{"location":"Regular_Expressions/#specifying-a-range-of-characters","text":"<p>If you want to match specific characters, you can use the square brackets to identify the exact characters you are searching for. The pattern that will match any line of text that contains exactly one number is \"^[0123456789]$\"</p> <p>You can use the hyphen between two characters to specify a range \"^[0-9]$\"</p> <p>You can intermix explicit characters with character ranges. This pattern will match a single character that is a letter, number, or underscore \"[A-Za-z0-9_]\".</p>","title":"Specifying a range of characters: [ ]"},{"location":"Regular_Expressions/#exceptions-in-character-sets","text":"<p>[^] matches a single character that is not contained within the brackets. For example, [^abc]matches any character other than \"a\", \"b\", or \"c\". [^a-z] matches any single character that is not a lowercase letter from \"a\" to \"z\". Likewise, literal characters and ranges can be mixed.\u00a0To match all characters except vowels use \"[^aeiou]\".</p>    Regular Expression Matches     [] The characters \"[]\"   [0] The character \"0\"   [0-9] Any number   [^0-9] Any character other than a number   [-0-9] Any number or a \"-\"   [0-9-] Any number or a \"-\"   [^-0-9] Any character except a number or a \"-\"   []0-9] Any number or a \"]\"   [0-9]] Any number followed by a \"]\"   [0-9-z] Any number,\u00a0or any character between \"9\" and \"z\".   [0-9\\-a\\]] Any number, or\u00a0a \"-\", a \"a\", or a \"]\"","title":"Exceptions in character sets: [ ]"},{"location":"Regular_Expressions/#wildcard","text":"<p>A dot \".\" is a special meta-characters. It will match any character, except the end-of-line character. For example, the pattern that will match a line with a single characters is \"^.$\", and two characters is \"^..$\".\u00a0You can use \"<code>...\\.</code>\" to match the first three (wildcard) characters, and escape the final wildcard meta-character to match the period instead.\u00a0</p>","title":"Wildcard: ."},{"location":"Regular_Expressions/#repeating-character-sets","text":"<p>\"*\" matches the preceding element zero or more times. For example, ab*c matches \"ac\", \"abc\", \"abbbc\", etc. [xyz]* matches \"\", \"x\", \"y\", \"z\", \"zx\", \"zyx\", \"xyzzy\", and so on. (ab)*matches \"\", \"ab\", \"abab\", \"ababab\", and so on.</p>","title":"Repeating character sets: *"},{"location":"Regular_Expressions/#matching-a-specific-number-of-sets-and","text":"<p>You cannot specify a maximum number of sets with the \"*\" modifier. There is a special pattern you can use to specify the minimum and maximum number of repeats. This is done by putting those two numbers between \"\\{\" and \"\\}\".</p> <p>\"\\{m, n\\}\" matches the preceding element at least m and not more than n times. For example, a\\{3,5\\}matches only \"aaa\", \"aaaa\", and \"aaaaa\" (the extended regular expression is {m, n}). Another example is\u00a0[a-z]\\{4,8\\} which\u00a0matches 4, 5, 6, 7 or 8 lower case letters.</p>","title":"Matching a specific number of sets: \\{ and \\}"},{"location":"Regular_Expressions/#more-examples","text":"Regular expression matches     .og any three-character string ending with \"og\", including \"dog\", \"fog\", and \"hog\".   [df]og \"dog\" and \"fog\".   [^d]og all strings matched by .at except \"dog\".   [^df]og all strings matched by .at other than \"dog\" and \"fog\".   ^[df]og \"dog\" and \"fog\", but only at the beginning of the string or line.   [df]og$\u00a0 \"dog\" and \"fog\", but only at the end of the string or line.   \\[.\\] any single character surrounded by \"[\" and \"]\" since the brackets are escaped, for example: \"[a]\" and \"[b]\".   b.*\u00a0 b followed by zero or more characters, for example: \"b\" and \"boy\" and \"bowl\".","title":"more examples"},{"location":"Regular_Expressions/#extended-regular-expressions","text":"<p>egrep and awk use the extended regular expressions. In extended extensions, special characters preceded by a backslash \u00a0or period no longer have the special meaning as well as '\\digit'.</p> <p>Therefore, \\{\u2026\\} becomes {\u2026} and \\(\u2026\\) becomes (\u2026).</p> <p>Examples:</p> <ul> <li>\"[hc]+at\" matches with \"hat\", \"cat\", \"hhat\", \"chat\", \"hcat\",     \"ccchat\" etc</li> <li>\"[hc]?at\" matches \"hat\", \"cat\" and \"at\"</li> <li>\"([cC]at)|([dD]og)\" matches \"cat\", \"Cat\", \"dog\" and \"Dog\"</li> <li>The characters (,),[,],.,*,?,+,|,^ and $ are special symbols and     have to be escaped with a backslash symbol in order to be treated as     literal characters. For example:</li> <li>\"a\\.(\\(|\\))\" matches with the string \"a.)\" or \"a.(\"</li> </ul> <p>Modern regular expression tools allow a quantifier to be specified as non-greedy, by putting a question mark after the quantifier: (\\[\\[.*?\\]\\]).</p>    BRE ERE matches     \\( \\) ( ) a marked subexpression. The string matched within the parentheses can be recalled later.   \\+ + the preceding element one or more times.   \\? ? the preceding element one or zero times.   \\| | the preceding element or the following element.   \\{m, n\\} {m, n} the preceding element at least m and not more than n times.   \\{m\\} {m} the preceding element exactly m times.   \\{m,\\} {m,} the preceding element at least m times.   \\{,n\\} {,n} the preceding element not more than n times.","title":"Extended regular expressions"},{"location":"Regular_Expressions/#examples","text":"BRE ERE matched results     \\(ab\\)* (ab)* \"\", \"ab\", \"abab\", \"ababab\" etc.   ab\\+c ab+c \"abc\", \"abbbc\", etc, but not \"ac\".   [xyz]|+ xyz+ \"x\", \"y\", \"z\", \"zx\", \"zyx\", \"xyzzy\", etc.   \\(ab\\) (ab)+ \"ab\", \"abab\", \"ababab\" etc.   ab\\?c ab?c \"ac\" or \"abc\".   \\(ab\\)\\? (ab)? \"\" or \"ab\".   abc\\|def abc|def \"abc\" or \"def\".   a\\{3,5\\} a{3,5} \"aaa\", \"aaaa\", and \"aaaaa\".   ba\\{,2\\}b ba{,2}b \"bb\", \"bab\", \"baab\".","title":"examples"},{"location":"Regular_Expressions/#poxis-character-sets","text":"<p>POXIS has added newer and more convenient ways to search for character sets. For example, you can use\u00a0[:upper:] instead of\u00a0[A-Z]. In fact, [A-Z] can be different based on LC_COLLATE value. For further discussion, check here. In HPC at MSU, the default of\u00a0[A-Z] is\u00a0\u00a0a, A, b, B, c, C, ....y, Y, z, Z, which is standard collations (en_US).\u00a0</p> <p>You can use\u00a0[[:upper:]] instead of [:upper:]. You can mix the old style and POSIX styles, such as\u00a0[1-9[:upper:]].</p>    Expression matches     [:alnum:] Alphanumeric   [:alpha:] Alphabetic   [:blank:] Whitespace, tabs, etc   [:cntrl:] Control character   [:digit:] digit   [:graph:] Printable and visible characters   [:lower:] Lower case character   [:print:] Printable character   [:punct:] Punctuation   [:space:] Whitespace   [:upper:] Upper case character   [:xdigit:] Extended digit","title":"POXIS character sets"},{"location":"Research_Space/","text":"<p>A research group's central directory, or research space, is a shared directory established by a MSU\u00a0principal investigator (PI) for use by the members of the PI's  research group.</p> <p>To create a research space the PI must submit a  Research Request form. The initial limit on stroage is 50GB and the initial  limit on the number of files conatined in a research space is 1,000,000 files. A PI may request an increase in storage of up to 1TB of space at no cost by submitting a  Quota Increase Request form or an increase beyond 1TB for an annual fee by submitting a Large Quota Increase Request form. Use the  <code>quota</code> command to check a research group's current space and file quotas.</p> <pre><code>$ quota\nResearch Groups: Space    Space   Space     Space     Files     Files     Files     Files        \n                 Quota    Used    Available % Used    Quota     Used      Available % Used\n-----------------------------------------------------------------------------------------------\n&lt;groupname&gt;      4096G    3733G   363G      91%       2097152   432525    1664627   21%\n</code></pre> <p>The group's research space is associated with an assigned group name and is located at <code>/mnt/research/&lt;groupname&gt;</code>  by default. It is accessible to all users who have been added to the group by the PI. Members of the group must set their user file mode creation mask and file permissions correctly such that the other group members have the appropriate access to the shared files in the research space. See the section Using a research space for more details.  </p> <p>All research space files are periodically, automatically backed up (except those files that a group has opted to store in a specially requested <code>nodr</code>  space).\u00a0To access file backups, please submit a  help ticket containing the file paths and the period i.e., the time frame, from which the files should be restored.  </p>","title":"Research Space"},{"location":"Research_Space/#using-a-research-space","text":"<p>To configure a research space such that all group members have the appropriate access to the files and directories contained within, it is important to read and follow the instructions below. </p> <p>1. Ensure that all directories created in a research space have the group ownership set to the <code>&lt;groupname&gt;</code>  and the set-group-ID (<code>setgid</code>) bit set to  <code>s</code> in the place of the executable bit on the group sector. By default, the group's research space  <code>/mnt/research/&lt;groupname&gt;</code> is set with the correct group ownership and <code>setgid</code> bit. To check the group ownership and <code>setgid</code> bit of a directory within the research space use the  <code>ls -ld &lt;path/to/directory&gt;</code> command. <pre><code>$ ls -ld /mnt/research/&lt;groupname&gt;/&lt;subdirectory&gt;\ndrwxrws--- 9 &lt;username&gt; &lt;groupname&gt; 8192 Jul 22 08:38 /mnt/research/&lt;groupname&gt;/&lt;subdirectory&gt;\n</code></pre> The\u00a0letter  <code>s</code> in the permissions\u00a0<code>drwxrws---</code> of the directory is the <code>setgid</code> bit which makes newly created sub-directories and files within inherit the group ownership of the parent directory rather than the primary group of the individual user. See the page  File Permissions on HPCC for more details. If the settings of a sub-directory are not set correctly, the group may encounter a  <code>Disk quota exceeded</code>  error when creating or copying files. See the section  Quotas on a research space  for more details. Use the following commands to ensure all directories and files in the group's research space have the proper settings:</p> <pre><code>$ find /mnt/research/&lt;groupname&gt;/ -not -group &lt;groupname&gt; -print0 | xargs -0 chgrp &lt;groupname&gt;\n$ find /mnt/research/&lt;groupname&gt;/ -type d -print0 | xargs -0 chmod g+s\n</code></pre> <p>2. Do not use the  <code>mv</code> command or the  <code>-p</code> option when using the  <code>cp</code> command to transfer files into the group's research space directories. Both  <code>mv &lt;filename&gt;</code> and  <code>cp -p &lt;filename&gt;</code> may preserve an undesired group ownership attribute even when transfered into a research space directory with ownership and permissions configured correctly.</p> <p>3. Use the <code>rsync --chmod=Dg+s</code> command to transfer files from a local machine to the HPCC research space. For example, use the command</p> <p><pre><code>$ rsync -avz testdir --chmod=Dg+s &lt;username&gt;@rsync.hpcc.msu.edu:/mnt/research/&lt;groupname&gt;/\n</code></pre> to transfer a directory  <code>testdir</code> from your local machine to the HPCC research space  <code>/mnt/research/&lt;groupname&gt;</code> . This will automatically configure the transferred directory with the <code>setgid</code> bit.</p> <p>4. During a bash session set the user file mode creation mask to  <code>0007</code> or any lower value. Use the  <code>umask</code> command to set the file mode creation mask e.g., </p> <p><pre><code>$ umask 0007\n</code></pre> For the duration of the session, files and directories created by the user are now readable, writable and executable for all members of the research group. Alternatively, a user may run the following Powertools  command one time to add the line\u00a0 <code>umask 0002</code>\u00a0  to the\u00a0user's\u00a0.bashrc\u00a0file:</p> <pre><code>$ module load powertools\n$ umask_in_bash                \n</code></pre> <p>5. Ensure that each group member's currently active group ID is set to the research group of interest.</p> <p>Each time a group member logs in to the HPCC that user's group ID is set to the primary group ID assigned by default when a user's account was created. However, a user may be added to other research groups at the request of that group's PI. After a login, the user may then toggle between group memberships using the  <code>newgrp &lt;groupname&gt;</code>  command when needed. For more information, refer to the  Change Primary Group  page.\u00a0A user may request that the primary group ID be changed from the default setting by submitting a request via a  help ticket .</p>","title":"Using a research space"},{"location":"Research_Space/#quotas-on-a-research-space","text":"<p>The space and file quotas on an research space are calculated by matching the group ownership settings of files stored on the HPCC to that of the research space group name. Hence, a user may not create files larger than 8 MB in a specified research space with a group ownership attribute different from that of the research space. Attempting to do so will likely result in a  <code>Disk quota exceeded</code>  error even though use of the  <code>quota</code>  command indicates that the reseach space quotas have not been exceeded. To resolve the <code>Disk quota exceeded</code> error in the absence of actual quota violations, users should follow the instructions in the section Using a research space  to ensure that:</p> <ul> <li> <p>The directory into which the files will be transfered has the same group ownership as the research space and the set-group-ID bit</p> </li> <li> <p>If the file already exists, its group ownership has been changed to that of the research space</p> </li> <li> <p>If the file is to be created, the primary group of the user creating the file is set to that of the research space</p> </li> </ul>","title":"Quotas on a research space"},{"location":"Restore_Files_from_Back-Up/","text":"<p>If you need files restored, please submit a ticket. Please mention the paths of the folders and file names with the time frame you would like to be restored.</p>","title":"Restore Files from Back-Up"},{"location":"Running-multiple-jobs-sequentially_34963786.html/","text":"","title":"Running multiple jobs sequentially 34963786.html"},{"location":"Running-multiple-jobs-sequentially_34963786.html/#teaching-running-multiple-jobs-sequentially","text":"<p>Create a python script (python_script.py)with the following code.</p> <p>python_script.py </p> <pre><code>print(\"Hello, World!\")\n</code></pre> <p>Create an R script (r_script.R)with the following code.</p> <p>r_script.R </p> <pre><code>z=rnorm(10000,mean=10,sd=2)\nmean(z)\nsd(z)\npdf(file=\"r_histogram.pdf\")\nhist(z,freq=FALSE,nclass=100)\n</code></pre> <p>Make a copy of hello.sb and name it multi_seq.sb</p> <p>Edit multi_seq.sb to run the tasks, python_script.py and r_script.r :</p> <ul> <li>Request resources</li> <li>Load the required modules,\u00a0</li> <li>sequentially run the two scripts</li> </ul> <p>Submit job to compute node</p> <p>Answer </p> <pre><code>#Make a copy of hello.sb and name it multi_seq.sb\ncp hello.sb multi_seq.sb\n\n#Using your favorite editor(nano, vi, emacs, gedit, etc.), \n#make the following edits to the multi_seq.sb script to run the tasks, python_script.py and r_script.r simultaneously:\n\ngedit multi_seq.sb\n\n#Request resources: Typically you'll need to request the largest number of nodes needed for each task. Since each task above only uses one node and one core there is no change to the number of nodes or cores.\n\n#Load the required modules \nmodule load R/3.5.0-X11-20180131\nmodule load python\n\n#sequentially run the two scripts\npython3 python_script.py\nRscript r_script.R\n\n#Submit job to compute node: Execute the following at the command line\nsbatch multi_seq.sb\n</code></pre>","title":"Teaching : Running multiple jobs sequentially"},{"location":"Running_Gaussian_by_Command_Lines/","text":"<p>Here we are going to do an geometry optimization calculation on a simple molecule Formamide (HCONH2). Users can use the Gaussian input file <code>g16.com</code> :</p> <p>g16.com</p> <pre><code>%NProcShared=4\n%Mem=5GB\n%NoSave\n%chk=g16.chk\n# opt freq b3lyp/cc-pvdz\n\nTitle Card: Single Molecule Formamide\n\n0 1\n C                  3.89594917   -4.10509404   -0.06119675\n O                  5.13960552   -4.14687675    0.12626969\n H                  3.41099598   -3.16562892   -0.22589552\n N                  3.10941607   -5.34695260   -0.05391726\n H                  2.79423275   -5.53644967    0.87600227\n H                  2.31994463   -5.24505745   -0.65918762                                                      \n</code></pre> <p>In the file, the <code>%</code> lines (Link 0 section) specify the system resources. <code>%NprocShared</code> gives how many CPUs to use in a node and <code>%Mem</code> indicates how much memory to use. If any file specified before the <code>%NoSave</code> line, it will not be saved once Gaussian finishes the calculation normally.\u00a0 <code>%chk</code> specify a check point file name to save and <code>#</code> line (Route section) specify the methods of Gaussian calculations. You can give this Gaussian input a title name in Title Card section. After that, use\u00a0Molecule Specification section to assign the coordinates of the atoms with the charge and spin multiplicity of the system in the first line. Please make sure there is at least one empty line in the end of file.</p> <p>By logging into HPCC and ssh to a dev node, users can find Gaussian program installed in HPCC with the module commands:</p> <pre><code>$ module spider Gaussian\n\n----------------------------\n  Gaussian:\n----------------------------\n Versions:\n        Gaussian/g16_AVX\n        Gaussian/g16-AVX2\n        Gaussian/g16\n\n----------------------------\n  For detailed information about a specific \"Gaussian\" module (including how to load the modules) use the module's full name.\n  For example:\n\n     $ module spider Gaussian/g16\n----------------------------\n</code></pre> <p>Please load Gaussian module with g16 version:</p> <pre><code>$ module load Gaussian/g16\n$ which g16\n/opt/software/Gaussian/g16-AVX/g16/g16\n</code></pre> <p>and the Gaussian command \"g16\" can be used. Simply run the command by giving the input and output file names:</p> <pre><code>$ g16 &lt; g16.com &gt; g16.log\n</code></pre> <p>and it starts to calculate the system on the node. It will take about 2 minutes to complete the calculation. After it is finished, you can check the output file <code>g16.log</code>. All dev nodes have 2-hour CPU limit. Please restrict your resource usage on them.</p> <p>Users can also use <code>GaussView</code> to create their molecular system and do calculations. To use GaussView, just run the command <code>gview</code> after loading the Gaussian module.</p>","title":"Running Gaussian by Command Lines"},{"location":"Running_Jobs_on_amd20_Test_Nodes/","text":"<p>The amd20 cluster is added to HPCC compute nodes for job runing. The features of these nodes can be seen in the following table:</p>    Cluster Type Node Names Node Num. Processors Cores/Node Memory/Node Disk size/Node GPUs/Node     amd20 amr-[000-101], amr-[137-209], amr-127, test-amr-[000-001] 178 AMD EPYC 7H12 Processor @2.595 GHz 128 493\u00a0GB 412 GB     amr-[104-26], amr-[128-136] 32 AMD EPYC 7H12 Processor @2.595 GHz 128 996 GB 412 GB     amr-[102-103] 2 AMD EPYC 7H12 Processor @2.595 GHz 128 2005 GB 412 GB     nvf-[000-008] 9 Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz 48 178 GB 412 GB 4 Tesla V100S    <p>Users can use the following powertools command to see a list of the amd20 nodes:</p> <pre><code>$ node_status -f amd20\n</code></pre> <p>To launch jobs on <code>amr</code> cluster nodes, please add the following SBATCH line in your job script or use the job submission commands (such as <code>sbatch</code>, <code>salloc</code> or <code>srun</code>) with the specification \"<code>-C amr</code>\":\u00a0</p> <pre><code>#SBATCH -C amr\n</code></pre> <p>For requesting <code>nvf</code> cluster nodes, please add the following line to your job script or specify\u00a0\"<code>-C nvf</code>\" in your job submission on the command line:\u00a0</p> <pre><code>#SBATCH -C nvf\n</code></pre> <p>The amd20 cluster does not support AVX-512. If you\u2019ve found that your executable requires AVX-512 or want to continue to run on the existing clusters, add the following line to your submit scripts:</p> <pre><code>#SBATCH -C '[intel14|intel16|intel18]'\n</code></pre> <p>If you have any question about running jobs on the nodes, please contact us.</p>","title":"Running Jobs on amd20 Test Nodes"},{"location":"SFTP_Mapping_on_HPCC_file_systems/","text":"<p>SFTP Net Drive is the software which can map remote HPCC file systems on your local Windows computers via SFTP. (Mac OS is not supported.) Once connected, you can browse and work with files on HPCC as if they were on a hard drive of your local machine. In order words, they do not need to be downloaded and uploaded when users read or modify them using their local computers.</p> <p>HPCC users can use the  download site to get a free version of SFTP Net Drive. Once it is downloaded and executed, users can do its setup. In the main menu (below), you can choose your own <code>Profile</code> name and <code>Drive Letter</code>. Make sure the <code>Server</code> is set to <code>http://rsync.hpcc.msu.edu</code> <code>Username</code> and <code>Password</code> are the same as your HPCC login.</p> <p></p> <p>For more advanced setting, please click on the <code>Advanced...</code> button. Three setting menus: <code>Connection</code>, <code>Protocol</code> and <code>Drive</code> can be modified. For <code>Connection</code>, the value on <code>Port</code> has to be 22. A longer initiation time can be adjusted on <code>Timeout</code>. You can also set up <code>reconnect times</code> in case the connection is dropped and <code>Send keep-alive</code> to prevent disconnection.</p> <p></p> <p>For <code>Protocol</code>, you may just use the default setting:</p> <p></p> <p>For <code>Drive</code>, if you would like, you may set up a different <code>Root folder</code> to start with other than your home folder. You might want to click on <code>Handle case-sensitive filenames</code> since the HPCC file system is case-sensitive. If you would like to show hidden files, you can click on <code>Show files started with dot</code>.</p> <p></p> <p>Once all of them are set, you can click on <code>Connect</code> button in the main menu. If it is successfully connected, the <code>Connect</code> button will become <code>Disconnect</code>:</p> <p></p> <p>Now, open the file explorer and click on <code>This PC</code>. You should find the HPCC file system shown in the <code>Network locations</code> area with the <code>Profile</code> name and the <code>Drive Letter</code> of your input:</p> <p></p>","title":"SFTP Mapping on HPCC file systems"},{"location":"SFTP_Mapping_on_HPCC_file_systems/#mapping-hpcc-drive-with-more-spaces","text":"<p>If you would like to do SFTP mapping with more than one space in HPCC, the better way is to set static links in the <code>Root folder</code> of your setting. For example, you want to map your home space and research space on your local Windows computer. You can set the <code>Root folder</code> to be your home folder as the setup above. Use\u00a0a ssh client to connect to the HPCC or Web Site Access to HPCC to run a command line on a dev node:</p> <pre><code>[username@dev-intel14-phi ~]$ ln -s /mnt/research/&lt;Research Name&gt; &lt;Research Name&gt;\n</code></pre> <p>where a static link to your research space is set in your home folder. Once it is done, run <code>ls</code> command and the static link <code>&lt;Research Name&gt;</code> should be shown with the light blue color. To access the research space through your local computer, just click on the HPCC drive of your setting in the <code>Network locations</code> area as mentioned in the upper section. Look for the space link <code>&lt;Research Name&gt;</code> and click on it. You can now see the files in the research space. If you would like to map more spaces, such as your scratch spaces or other research spaces, just create more static links in your home folder by the same way.</p>","title":"Mapping HPCC Drive with More Spaces"},{"location":"SLURM_Check_Modify_and_Cancel_a_Job_by_scontrol_scancel_Command/","text":"","title":"SLURM Check, Modify and Cancel a Job using the scontrol &amp; scancel commands"},{"location":"SLURM_Check_Modify_and_Cancel_a_Job_by_scontrol_scancel_Command/#scontrol-command","text":"<p>Besides the brief listing of every job using the <code>squeue</code> command, a user can also see the detailed information of each job. Run the SLURM command <code>scontrol show</code> with a job ID:</p> <pre><code>$ scontrol show job 8929\nJobId=8929 JobName=test\n   UserId=nobody(804293) GroupId=helpdesk(2103) MCS_label=N/A\n   Priority=404 Nice=0 Account=classres QOS=normal\n   JobState=PENDING Reason=Resources Dependency=(null)\n   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   RunTime=00:00:00 TimeLimit=00:01:00 TimeMin=N/A\n   SubmitTime=2018-08-01T14:33:04 EligibleTime=2018-08-01T14:33:04\n   StartTime=Unknown EndTime=Unknown Deadline=N/A\n   PreemptTime=None SuspendTime=None SecsPreSuspend=0\n   LastSchedEval=2018-08-03T12:38:48\n   Partition=general-short-14,general-short-16,general-short-18,general-long-14,general-long-16,general-long-18,classres-14,classres-16 AllocNode:Sid=dev-intel18:4996\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=(null)\n   NumNodes=80-80 NumCPUs=160 NumTasks=80 CPUs/Task=2 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=40,mem=80G,node=40,gres/gpu=40\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   MinCPUsNode=2 MinMemoryNode=2G MinTmpDiskNode=0\n   Features=intel14 DelayBoot=00:00:00\n   Gres=(null) Reservation=(null)\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/mnt/home/changc81/GetExample/helloMPI/test\n   WorkDir=/mnt/home/changc81/GetExample/helloMPI\n   Comment=stdout=/mnt/home/changc81/GetExample/helloMPI/slurm-8929.out\n   StdErr=/mnt/home/changc81/GetExample/helloMPI/slurm-8929.out\n   StdIn=/dev/null\n   StdOut=/mnt/home/changc81/GetExample/helloMPI/slurm-8929.out\n   Power=\n</code></pre> <p>You can check if the information is right for the job. If the job has not started to run and you would like change any specification, you can hold the job first using the <code>scontrol hold</code> command:</p> <pre><code>$ scontrol hold 8929\n$ squeue -l -u $USER\nFri Aug  3 12:26:57 2018\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n              8929 general-s     test   nobody  PENDING       0:00      1:00     80 (JobHeldUser)\n</code></pre> <p>where you can see from the results of the <code>squeue</code> command, the job is pending due to the user's hold. You can choose the information you want to change in <code>scontrol show</code> results. Put them in the <code>scontrol update</code> command and modify the information after the <code>=</code> symbol. For example, the command line</p> <pre><code>$ scontrol update job 8929  NumNodes=2-2 NumTasks=2 Features=intel16\n</code></pre> <p>will change the resource request of the job 8929 from 80 nodes and 80 tasks with intel14 nodes to 2 nodes and 2 tasks with intel16 nodes. After the update, you can use the <code>scontrol show</code> command again to verify the job setting. Once you are done with the update work, you can release the job hold by command <code>scontrol release</code>:</p> <pre><code>$ scontrol release 8929\n$ squeue -l -u $USER\nFri Aug  3 13:18:10 2018\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n              8929 general-s     test   nobody  RUNNING       0:07      1:00      2 lac-[386-387]\n</code></pre> <p>The job is now running due to the change of the resource request by the command <code>scontrol update</code>. Again, we can check the running job using the command <code>scontrol show</code>:</p> <pre><code>$ scontrol show job 8929\nJobId=8929 JobName=test\n   UserId=changc81(804793) GroupId=helpdesk(2103) MCS_label=N/A\n   Priority=379 Nice=0 Account=classres QOS=normal\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   RunTime=00:00:08 TimeLimit=00:01:00 TimeMin=N/A\n   SubmitTime=2018-08-01T14:33:04 EligibleTime=2018-08-01T14:33:04\n   StartTime=2018-08-03T13:18:03 EndTime=2018-08-03T13:18:11 Deadline=N/A\n   PreemptTime=None SuspendTime=None SecsPreSuspend=0\n   LastSchedEval=2018-08-03T13:18:03\n   Partition=general-long-16 AllocNode:Sid=dev-intel18:4996\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=lac-[386-387]\n   BatchHost=lac-386\n   NumNodes=2 NumCPUs=4 NumTasks=2 CPUs/Task=2 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=4,mem=4G,node=2,billing=4\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   MinCPUsNode=2 MinMemoryNode=2G MinTmpDiskNode=0\n   Features=intel16 DelayBoot=00:00:00\n   Gres=(null) Reservation=(null)\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/mnt/home/changc81/GetExample/helloMPI/test\n   WorkDir=/mnt/home/changc81/GetExample/helloMPI\n   Comment=stdout=/mnt/home/changc81/GetExample/helloMPI/slurm-8929.out\n   StdErr=/mnt/home/changc81/GetExample/helloMPI/slurm-8929.out\n   StdIn=/dev/null\n   StdOut=/mnt/home/changc81/GetExample/helloMPI/slurm-8929.out\n   Power=\n</code></pre> <p>For complete usage information about the <code>scontrol</code> command, please refer to https://slurm.schedmd.com/scontrol.html at the SLURM web site.</p>","title":"scontrol command"},{"location":"SLURM_Check_Modify_and_Cancel_a_Job_by_scontrol_scancel_Command/#scancel-command","text":"<p>If at any moment before the job complete, you would like to remove the job, you can use the <code>scancel</code> command to cancel a job. For example, the command</p> <pre><code>$ scancel 8929\n</code></pre> <p>will cancel job 8929. For a complete usage information about the <code>scancel</code> command, please refer to https://slurm.schedmd.com/scancel.html at the SLURM web site.</p>","title":"scancel command"},{"location":"SLURM_overview/","text":"<p>HPCC uses SLURM (Simple Linux Utility for Resource Management) to manage users'  jobs and computing resources. SLURM is an open-source, fault-tolerant, and highly scalable scheduling system. It has been employed by a large number of national and international computing centers. Users can submit a job by SLURM commands and request computing resources with specifications in a job script or on a command line. </p> <p>SLURM uses command-line commands to control jobs and clusters as well as show detailed information about jobs. The table below presents the most frequently used commands on HPCC. A complete list can be found at the SLURM documentation page.</p>    Command Description     sacct displays accounting data for all jobs and job steps in the SLURM job accounting log or SLURM database.   sbatch Used to submit batch jobis to SLURM job queue   sacctmgr Used to view and modify SLURM account information   scancel Used to signal jobs or job steps that are under the control of SLURM.   scontrol Used view and modify SLURM configuration and state.   sinfo view information about SLURM nodes and partitions.   smap graphically view information about SLURM jobs, partitions, and set configurations parameters.   sprio view the factors that comprise a job's scheduling priority.   squeue view information about jobs located in the SLURM scheduling queue.   srun Run parallel jobs.","title":"Overview of job scheduling and management"},{"location":"SSH_Key-Based_Authentication/","text":"<p>While the most common way of login to the HPCC is by using the username/password pair, a more secure authentication method is the use of SSH keys. Although setting up your keys is a little more complex, it is a one-time investment. The HPCC provides key-based authentication as an option, in addition to the usual password-based login. </p>  <p>Note</p> <p>Starting in October 2022, login to our rsync gateway (<code>rsync.hpcc.msu.edu</code>)  will accept SSH keys as the ONLY authentication method. Username/password won't work.</p>","title":"SSH key-based authentication"},{"location":"SSH_Key-Based_Authentication/#generating-ssh-keypairs","text":"<p>An SSH keypair consists of a private key and a public key. Your private key is a secret key just like your password which you should not share with anyone. On the other hand, your public key can be made publicly available in the same way that your name can be made public. The public key is stored on the server you attempt to log into (that is, the HPCC), while the private key is stored on your own computer. When a user attempts to log in, an encryption process starts on the HPCC side, using the public key. With your private key, your computer will be able to decrypt the encrypted message sent from the HPCC. When everything matches up, your login is approved.</p> <p>SSH tool suites usually provide a utility for generating these keypairs. On a Mac, you can run the command <code>ssh-keygen</code> in Terminal. On a Windows computer, you can use PuTTYgen to generate SSH key pairs. When you use these utilities, you will be given an option for protecting your private key with a passphrase. Please do this, as it will prevent your private key from being used by a malicious individual if it is ever stolen. </p> <p>To generate a keypair from command line (e.g., after opening a terminal on Mac), run</p> <p><code>ssh-keygen -t rsa</code></p> <p>After you have set a passphrase and it has generated the keys, you will find the key files in the <code>.ssh</code> directory under your home directory. By default,  <code>id_rsa</code> is the private key file and <code>id_rsa.pub</code> the public key file.</p>","title":"Generating SSH keypairs"},{"location":"SSH_Key-Based_Authentication/#uploading-your-public-key-to-the-hpcc","text":"<p>In order to login to HPCC with key-based authentication from your local computer,  you will need to perform the following:</p> <ol> <li> <p>Log on to HPCC gateway <code>gateway.hpcc.msu.edu</code></p> </li> <li> <p>On the HPCC, make sure you have a directory named <code>.ssh</code> under your home directory. If not, create one by <code>mkdir ~/.ssh</code></p> </li> <li> <p>Upload your public key <code>id_rsa.pub</code> from your computer to <code>gateway.hpcc.msu.edu</code>. There are multiple ways to do so, as given here.</p> </li> <li> <p>Append the public key file to another file <code>~/.ssh/authorized_keys</code>. In order to do so, assuming that the pub key file has been copied to your home directory from Step  3, you can run the following command</p> <p><code>cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</code></p> </li> <li> <p>Set correct permissions by running </p> <p><code>chmod 700 ~/.ssh</code> </p> <p><code>chmod 600 ~/.ssh/authorized_keys</code></p> </li> </ol>","title":"Uploading your public key to the HPCC"},{"location":"Scavenger_Queue%202/","text":"<p>The scavenger queue allows users to run preemptable jobs on idle cores.</p> <p>With few exceptions, each researcher using the HPCC is limited in the number of jobs or cores they can run at one time. Annually, non-buyin users are limited in the total number of CPU and GPU hours they can use. These limits do not apply to jobs submitted to the scavenger queue. Jobs in this queue can start on resources that would otherwise be left idle, improving research throughput. Similar to jobs submitted to the general-long queue, these jobs can request up to a 7-day wall time; however, jobs in the scavenger queue may be interrupted if resources are required for other non-scavenger jobs. The default behavior for interrupted jobs is to be re-queued, but users can opt for cancellation if it is more conducive to their workflow. We recommend that only users who can checkpoint and restart or have a workflow implemented that can manage jobs being canceled or requeued use this queue.</p> <p>To use the scavenger queue, add the following line to your job script: <pre><code>#SBATCH --qos=scavenger\n</code></pre></p> <p>To prevent your job from requeuing automatically if interrupted, add the following line to your job script: <pre><code>#SBATCH --no-requeue\n</code></pre></p>","title":"Scavenger Queue"},{"location":"Scavenger_Queue/","text":"<p>The scavenger queue allows users to run preemptible jobs on idle cores. Jobs in the scavenger queue may be interrupted if resources are required for other non-scavenger jobs.</p> <p>With few exceptions, each researcher using the HPCC is limited in the number of jobs or cores they can run at one time. Annually, non-buyin users are limited in the total number of CPU and GPU hours they can use. These limits do not apply to jobs submitted to the scavenger queue. Jobs in this queue can start on resources that would otherwise be left idle, improving research throughput. Similar to jobs submitted to the general-long queue, these jobs can request up to a 7-day wall time. The default behavior for interrupted jobs is to be re-queued, but users can opt for cancellation if it is more conducive to their workflow. </p>  <p>Note</p> <p>We recommend that only users who can checkpoint and restart or have a workflow implemented that can manage jobs being canceled or requeued use this queue.</p>","title":"Scavenger Queue"},{"location":"Scavenger_Queue/#usage","text":"<p>To use the scavenger queue, add the following line to your job script: <pre><code>#SBATCH --qos=scavenger\n</code></pre></p> <p>To prevent your job from requeuing automatically if interrupted, add the following line to your job script: <pre><code>#SBATCH --no-requeue\n</code></pre></p> <p>The scavenger queue is not affected by the amount of wall time requested in your job script, e.g. 24 hours wall time is treated with the same scavenger queue priority as 4 hours wall time.</p> <p>Scavenger queue jobs will be automatically assigned to the <code>scavenger</code> account, regardless of the <code>-A</code> setting in your job script.</p>","title":"Usage"},{"location":"Scavenger_Queue/#scheduling","text":"<p>The scavenger queue runs using the backfill scheduler (see How Jobs are Scheduled). Job scheduling may take on the order of minutes to occur, depending on the current load on the HPCC. Scavenger queue jobs run for a minimum of 1 minute before they can be preempted, but typically scavenger queue jobs run for approximately 1 hour before preemption.</p>","title":"Scheduling"},{"location":"Science_DMZ/","text":"<p>What\u00a0is\u00a0a\u00a0DMZ? </p> <p>In the\u00a0cyberinfrastructure\u00a0context,\u00a0a\u00a0Demilitarized\u00a0Zone (DMZ) is a portion of the network designed to optimize high-performance for research applications. The Science DMZ enables researchers to disseminate terabytes or even petabytes of specialized data more easily\u00a0and\u00a0at speeds of 10 to 100 gigabits per second to other institutions and cloud providers. This ability to share data immeasurably increases its value, as the insights extrapolated from it by additional researchers have the potential to change society in significant and meaningful ways.\u00a0</p> <p>What\u00a0is the Science\u00a0DMZ? </p> <p>A Science DMZ enables\u00a0cyber-enabled\u00a0big-data scientific\u00a0research to be shared globally\u00a0with ease. The\u00a0Science DMZ\u00a0at MSU\u00a0will offer increased network speeds and reliability, broadly enhancing MSU\u2019s research and education cyberinfrastructure. All campus network users will benefit from the high-speed network connections that will be used for sharing data already stored at MSU\u2019s High Performance Computing System and on the NSF-funded OSIRIS storage infrastructure.\u00a0</p> <p>The creation of a Science DMZ at MSU helps eliminate obstacles for better access to valuable data. By sharing resources and working together, researchers are better positioned to collaboratively find solutions to our biggest problems. This project also lays the foundation for a new relationship between MSU IT and the Office of Research and Innovation, strengthening collaboration and strategic planning as MSU develops cyberinfrastructure capabilities to enhance scientific research support.\u00a0\u00a0</p> <p>More general information on the Science DMZ at Michigan State University can be found at\u00a0tech.msu.edu.\u00a0</p> <p>How do I use the Science\u00a0DMZ\u00a0at MSU? </p> <p>Globus is the recommended method to transfer big data files using MSU\u2019s Science DMZ. For a general overview of Globus and information on setting up a Globus account, see Transferring data with Globus For walk-through training on using Globus, please self-enroll in ICER's DMZ Globus Training D2L course.</p>","title":"Science DMZ"},{"location":"Scratch_File_Systems/","text":"<p>Each user is provided with a working directory know as scratch space. This space is intended for intensive input/output (I/O) operations i.e., heavy reading and writing of data, involving very large files and/or a very large number of small files. Research groups may also request a scratch space. Unlike the home space and research\u00a0space, the scratch space is not intended for long-term storage and cannot be accessed from a gateway node, with the exception of the rsync gateway used for file transfer. </p> <p>Data stored in a user's scratch space is not backed-up, and files in a scratch space with no I/O operations for 45 days will be automatically deleted to ensure the space is available to all users.  The limit on stroage is 50TB and the initial limit on the number of files conatined in a research space is 1,000,000 files. </p> <p>A user's scratch space is available at <code>/mnt/scratch/$USER</code>, or use the bash environemntal variable <code>$SCRATCH</code>. Use the <code>quota</code> command to check a user's current space and file quotas.</p> <pre><code>$ quota\n\nTemporary Filesystems:\n---------------------------------------------------------------------------------------------------------------------------------------\n\n/mnt/scratch (/mnt/gs21)        Space Quota  Space Used   Space Free   Space % Used Filess Quota Files Used   Files Free   Files % Used\n                                51200G       0G           51200G       0%           1048576      1            1048575      0%       \n</code></pre>","title":"Scratch Space"},{"location":"Scratch_File_Systems/#using-a-scratch-space","text":"<p>Scratch Space can sustain high data transfer rates and is a good  choice for data files used in running parallel on multiple nodes with intensive I/O requirements. Jobs of this type will run much faster with  data accessed from a scratch space and users should follow the proceedure  below for best practice:    </p> <ol> <li>Configure the job script and/or the main program for scratch space I/O using the path <code>/mnt/scratch/$USER</code> or the variable <code>$SCRATCH</code></li> <li>Copy input data from the home space or research space to scratch space; to maintain data integrity keep the original data files in the home space or research space</li> <li>Schedule the job and confirm sucessful completion of the I/O operations</li> <li>Move the resulting output data back to either the home space or research space</li> <li>Delete that data from the scratch space </li> </ol>","title":"Using a Scratch Space"},{"location":"Scratch_File_Systems/#time-limits-on-scratch-space","text":"<p>Files in a scratch space with no I/O operations for 45 days will be automatically deleted to ensure the space is available to all users. To find files in a scratch space approaching the 45 day limit run the following command:</p> <p><code>find $SCRATCH -type f -mtime +40</code></p> <p>Here the <code>+40</code> arguement specifies files with no I/O for more than 40 days. Users may set this arguement to any number of days desired, upto the 45 day limit.</p>","title":"Time Limits on Scratch Space"},{"location":"Sensitive_Data_on_the_HPCC/","text":"<p>Sensitive Data Hosting: Data that are subject to external security requirements are considered sensitive data. These requirements may be imposed by regulations or contractual obligations. In either case, users that intend to store sensitive data on HPCC systems must work with the HPCC to ensure data security. Examples of sensitive data include:</p> <ul> <li>Personally Identifiable Information (PII)</li> <li>Protected Health Information (PHI)</li> <li>Controlled Unclassified Information (CUI)</li> <li>Data covered under the Heath Insurance Portability and     Accountability Act (HIPAA)</li> <li>Data covered under the Federal Information Security Management Act     (FISMA)</li> <li>Data covered under the Federal Education Rights and Privacy Act     (FERPA)</li> <li>Data with security requirements set by the MSU Institutional Review     Board (IRB)</li> <li>Controlled access data from the NIH Database of Genotypes and     Phenotypes (dbGaP)</li> </ul> <p>Information about the HPCC's Sensitive Data Policy is available to users with MSU credentials.</p> <p>MSU users may also visit https://data-storage-finder.tech.msu.edu/ for more information about data storage choices on campusn (NOTE: The page only accessible from campus).</p>","title":"Sensitive Data on the HPCC"},{"location":"Show_Job_Steps_by_sacct_and_srun_Commands/","text":"<p>SLURM provides commands to show the execution information of each command line in a job script. This can be helpful for debugging and testing. In order to get  such information, the wrapper command <code>srun</code> needs to be used. Let's take a look at the following job script:</p> <pre><code>#!/bin/bash\n\n#SBATCH -N 4 -n 4 -c 2\n#SBATCH --time=00:05:00\n#SBATCH --mem=1G\n\nmodule purge; module load GCC/6.4.0-2.28 OpenMPI/2.1.2\nmodule list\n\nmpicc mpi-hello.c -o hello.exe\n\necho; echo \"====== mpirun hello.exe ======\"\nmpirun hello.exe                                            #0 Step\n\necho; echo \"====== srun hello.exe ======\"\nsrun hello.exe                                              #1 Step\n\necho; echo \"====== srun -n 8 -c 1 hello.exe ======\"\nsrun -n 8 -c 1 hello.exe                                    #2 Step\n\necho; echo \"====== srun  ======\"\nsrun NoSuchCommand                                          #3 Step\n\necho; echo \"====== mpirun  ======\"\nmpirun NoSuchCommand                                        #4 Step\n\necho; echo \"====== scontrol show job $SLURM_JOB_ID ======\"\nsrun -N 1 -n 1 -c 1 scontrol show job $SLURM_JOB_ID         #5 Step\n</code></pre> <p>Although there are many command lines, only 6 of them are executed with either mpirun or srun wrapper and marked with the comments (from step 0 to 5) in the end. SLURM can record each of the 6 executions as a job step. Once the job is submitted by sbatch command and starts running, you can use sacct command to check the steps:</p> <pre><code>$ sacct -j 10732\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n10732              test general-l+   classres          8  COMPLETED      0:0\n10732.batch       batch              classres          2  COMPLETED      0:0\n10732.extern     extern              classres          8  COMPLETED      0:0\n10732.0           orted              classres          6  COMPLETED      0:0\n10732.1       hello.exe              classres          8  COMPLETED      0:0\n10732.2       hello.exe              classres          8  COMPLETED      0:0\n10732.3      NoSuchCom+              classres          8     FAILED      2:0\n10732.4           orted              classres          6  COMPLETED      0:0\n10732.5        scontrol              classres          1  COMPLETED      0:0\n</code></pre> <p>where the Job has ID 10732 and the 6 steps are shown from JobID 10732.0 to 10732.5.</p> <p>We can also use a powertools command js to see more detailed information (such as memory usage and a list of used nodes) about the steps:</p> <pre><code>$ js -j 10732 -C5\n\nSLURM Job ID: 10732\n===============================================================================================================================\n          JobID |               10732 |         10732.batch |        10732.extern |             10732.0 |             10732.1 |\n        JobName |                test |               batch |              extern |               orted |           hello.exe |\n           User |            changc81 |                     |                     |                     |                     |\n       NodeList |       lac-[380-383] |             lac-380 |       lac-[380-383] |       lac-[381-383] |       lac-[380-383] |\n         NNodes |                   4 |                   1 |                   4 |                   3 |                   4 |\n         NTasks |                     |                   1 |                   4 |                   3 |                   4 |\n          NCPUS |                   8 |                   2 |                   8 |                   6 |                   8 |\n         ReqMem |                 1Gn |                 1Gn |                 1Gn |                 1Gn |                 1Gn |\n      Timelimit |            00:05:00 |                     |                     |                     |                     |\n        Elapsed |            00:00:16 |            00:00:16 |            00:00:16 |            00:00:02 |            00:00:01 |\n      SystemCPU |           00:03.283 |           00:00.562 |           00:00.001 |           00:00.646 |           00:00.572 |\n        UserCPU |           00:02.119 |           00:00.753 |           00:00.003 |           00:00.396 |           00:00.281 |\n       TotalCPU |           00:05.403 |           00:01.316 |           00:00.005 |           00:01.042 |           00:00.853 |\n     AveCPULoad |            0.337687 |             0.08225 |           0.0003125 |               0.521 |               0.853 |\n         MaxRSS |                     |              10409K |                120K |                861K |                863K |\n      MaxVMSize |                     |             652100K |             173968K |             324440K |             324436K |\n          Start | 2018-08-06T13:22:44 | 2018-08-06T13:22:44 | 2018-08-06T13:22:44 | 2018-08-06T13:22:54 | 2018-08-06T13:22:57 |\n            End | 2018-08-06T13:23:00 | 2018-08-06T13:23:00 | 2018-08-06T13:23:00 | 2018-08-06T13:22:56 | 2018-08-06T13:22:58 |\n       ExitCode |                 0:0 |                 0:0 |                 0:0 |                 0:0 |                 0:0 |\n          State |           COMPLETED |           COMPLETED |           COMPLETED |           COMPLETED |           COMPLETED |\n===============================================================================================================================\n          JobID |             10732.2 |             10732.3 |             10732.4 |             10732.5 |\n        JobName |           hello.exe |       NoSuchCommand |               orted |            scontrol |\n           User |                     |                     |                     |                     |\n       NodeList |       lac-[380-383] |       lac-[380-383] |       lac-[381-383] |             lac-380 |\n         NNodes |                   4 |                   4 |                   3 |                   1 |\n         NTasks |                   8 |                   4 |                   3 |                   1 |\n          NCPUS |                   8 |                   8 |                   6 |                   1 |\n         ReqMem |                 1Gn |                 1Gn |                 1Gn |                 1Gn |\n      Timelimit |                     |                     |                     |                     |\n        Elapsed |            00:00:01 |            00:00:00 |            00:00:01 |            00:00:00 |\n      SystemCPU |           00:01.141 |           00:00.051 |           00:00.289 |           00:00.017 |\n        UserCPU |           00:00.521 |           00:00.031 |           00:00.096 |           00:00.035 |\n       TotalCPU |           00:01.663 |           00:00.083 |           00:00.385 |           00:00.053 |\n     AveCPULoad |               1.663 |                     |               0.385 |                     |\n         MaxRSS |              34812K |                865K |                865K |                840K |\n      MaxVMSize |             324436K |             324436K |             324440K |             324436K |\n          Start | 2018-08-06T13:22:58 | 2018-08-06T13:22:59 | 2018-08-06T13:22:59 | 2018-08-06T13:23:00 |\n            End | 2018-08-06T13:22:59 | 2018-08-06T13:22:59 | 2018-08-06T13:23:00 | 2018-08-06T13:23:00 |\n       ExitCode |                 0:0 |                 2:0 |                 0:0 |                 0:0 |\n          State |           COMPLETED |              FAILED |           COMPLETED |           COMPLETED |\n=========================================================================================================\n</code></pre> <p>From the results above, we can see the executions by mpirun are different from srun. First of all, for mpirun, the JobName only show \"orted\" no matter what commands are used in the steps 10732.0 and 10732.4. However, srun shows the correct commands in all of the steps (10732.1, 10732.2, 10732.3 and 10732.5). Secondly, mpirun results show only 3 tasks with 6 CPUs are used but srun results correctly show 4 tasks with 8 CPUs in step 10732.1, 8 tasks with 8 CPUs in step 10732.2 and 1 task with 1 CPU in 10732.5 step. Finally, both steps 10732.3 and 10732.4 ran the same command NoSuchCommand where there is no such file or directory and should cause an error execution. However, mpirun wrapper still consider it is complete without error. Only srun wrapper get the FAIL state with an exit code 2.</p> <p>From the job output in the following results, we see no difference between the outputs of the step 10732.0 (mpirun hello.exe) and the step 10732.1\u00a0(srun hello.exe). SLURM seems to get a good sacct information with srun but not with mpirun. If you wish to use the step information, do not forget to put srun in the command lines.</p> <pre><code>Currently Loaded Modules:\n  1) GCCcore/6.4.0   2) binutils/2.28   3) GCC/6.4.0-2.28   4) OpenMPI/2.1.1\n\n\n====== mpirun hello.exe ======\nHello From: lac-380 I am the recieving processor 1 of 4\nHello From: lac-381              I am processor 2 of 4\nHello From: lac-382              I am processor 3 of 4\nHello From: lac-383              I am processor 4 of 4\n\n====== srun hello.exe ======\nHello From: lac-380 I am the recieving processor 1 of 4\nHello From: lac-381              I am processor 2 of 4\nHello From: lac-382              I am processor 3 of 4\nHello From: lac-383              I am processor 4 of 4\n\n====== srun -n 8 -c 1 hello.exe ======\nHello From: lac-380 I am the recieving processor 1 of 8\nHello From: lac-380              I am processor 2 of 8\nHello From: lac-381              I am processor 3 of 8\nHello From: lac-381              I am processor 4 of 8\nHello From: lac-382              I am processor 5 of 8\nHello From: lac-382              I am processor 6 of 8\nHello From: lac-383              I am processor 7 of 8\nHello From: lac-383              I am processor 8 of 8\n\n====== srun  ======\nslurmstepd: error: execve(): NoSuchCommand: No such file or directory\nslurmstepd: error: execve(): NoSuchCommand: No such file or directory\nslurmstepd: error: execve(): NoSuchCommand: No such file or directory\nsrun: error: lac-381: task 1: Exited with exit code 2\nsrun: error: lac-383: task 3: Exited with exit code 2\nsrun: error: lac-382: task 2: Exited with exit code 2\nslurmstepd: error: execve(): NoSuchCommand: No such file or directory\nsrun: error: lac-380: task 0: Exited with exit code 2\n\n====== mpirun  ======\n--------------------------------------------------------------------------\nmpirun was unable to find the specified executable file, and therefore\ndid not launch the job.  This error was first reported for process\nrank 0; it may have occurred for other processes as well.\n\nNOTE: A common cause for this error is misspelling a mpirun command\n      line parameter option (remember that mpirun interprets the first\n      unrecognized command line token as the executable).\n\nNode:       lac-380\nExecutable: NoSuchCommand\n--------------------------------------------------------------------------\n\n====== scontrol show job 10732 ======\nJobId=10732 JobName=test\n   UserId=changc81(804793) GroupId=helpdesk(2103) MCS_label=N/A\n   Priority=103 Nice=0 Account=classres QOS=normal\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   RunTime=00:00:18 TimeLimit=00:05:00 TimeMin=N/A\n   SubmitTime=2018-08-06T13:22:43 EligibleTime=2018-08-06T13:22:43\n   StartTime=2018-08-06T13:22:44 EndTime=2018-08-06T13:27:44 Deadline=N/A\n   PreemptTime=None SuspendTime=None SecsPreSuspend=0\n   LastSchedEval=2018-08-06T13:22:44\n   Partition=general-long-16 AllocNode:Sid=lac-249:5133\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=lac-[380-383]\n   BatchHost=lac-380\n   NumNodes=4 NumCPUs=8 NumTasks=4 CPUs/Task=2 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=8,mem=4G,node=4,billing=8\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   MinCPUsNode=2 MinMemoryNode=1G MinTmpDiskNode=0\n   Features=(null) DelayBoot=00:00:00\n   Gres=(null) Reservation=(null)\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/mnt/home/changc81/GetExample/helloMPI/test\n   WorkDir=/mnt/home/changc81/GetExample/helloMPI\n   Comment=stdout=/mnt/home/changc81/GetExample/helloMPI/slurm-10732.out\n   StdErr=/mnt/home/changc81/GetExample/helloMPI/slurm-10732.out\n   StdIn=/dev/null\n   StdOut=/mnt/home/changc81/GetExample/helloMPI/slurm-10732.out\n   Power=\n</code></pre> <p>For a complete instruction of sacct command, please refer to the SLURM web site.</p>","title":"Showing job steps"},{"location":"Singularity_I._Introduction/","tags":["how-to guide"],"text":"<p>Singularity is installed on our HPCC. However, you may want\u00a0to develop your own containers first on a local machine.</p> <p>Many HPC centers including MSU HPCC do not allow Docker containers through Docker. However,\u00a0Singularity is compatible with Docker, and you can use Docker containers through Singularity. There are a few distinct difference between Docker and Singularity.</p> <p>Docker:</p> <ul> <li>Inside a Docker image, the user's privilege is escalated to root on     the host system. This privilege is not supported by most HPCCs     including MSU HPCC. It means that Docker will not be installed on     our system.\u00a0</li> </ul> <p>Singularity:</p> <ul> <li>User has root privileges if elevated with <code>sudo</code> when a container     runs.</li> <li>Can run and modify Docker images and containers</li> </ul> <p>These key difference mean that Singularity is installed on most HPCCs. In addition, virtually all Docker containers can be run through Singularity, so users can effectively run Docker on MSU HPCC.</p>","title":"Singularity: I. Introduction"},{"location":"Singularity_I._Introduction/#installation","tags":["how-to guide"],"text":"<p>Singularity exits as two major versions, 2 and 3. The current version on the MSU HPCC is 3.8.0. Therefore, in this tutorial, I\u00a0will use version 3.</p> <p>To Install Singularity on your local machine, click here:\u00a0</p> <p>https://www.sylabs.io/guides/3.0/user-guide/installation.html#installation</p> <ul> <li>The version of singularity on MSU HPCC is currently 3.8.0. The     official documentation for this version is at     https://www.sylabs.io/guides/3.8/user-guide/index.html.\u00a0All     singularity commands are built into the system such as <code>singularity     shell</code> and <code>singularity exec</code>, which means you can invoke these     commands directly from the command line.</li> </ul>","title":"Installation"},{"location":"Singularity_I._Introduction/#check-installation","tags":["how-to guide"],"text":"<p>When you install Singularity on your local machine, you can check the installation with</p> <pre><code>$ singularity pull shub://vsoch/hello-world\nINFO:    Downloading shub image\n 59.75 MiB / 59.75 MiB [========================================================================================] 100.00% 10.46 MiB/s 5s\n</code></pre> <p>In the above example, I\u00a0used the Singularity Hub \u201cunique resource identifier,\u201d or <code>uri</code>, <code>shub://</code> which tells the software to run an image from Singularity Hub.</p> <p>To get help, you can use the <code>help</code> command which\u00a0gives a general overview of Singularity options and subcommands as follows:</p> <pre><code>$ singularity --help\n\nLinux container platform optimized for High Performance Computing (HPC) and\nEnterprise Performance Computing (EPC)\n\nUsage:\n  singularity [global options...]\n\nDescription:\n  Singularity containers provide an application virtualization layer enabling\n  mobility of compute via both application and environment portability. With\n  Singularity one is capable of building a root file system that runs on any\n  other Linux system where Singularity is installed.\n\nOptions:\n  -d, --debug     print debugging information (highest verbosity)\n  -h, --help      help for singularity\n      --nocolor   print without color output (default False)\n  -q, --quiet     suppress normal output\n  -s, --silent    only print errors\n  -v, --verbose   print additional information\n      --version   version for singularity\n\nAvailable Commands:\n  build       Build a Singularity image\n  cache       Manage the local cache\n  capability  Manage Linux capabilities for users and groups\n  config      Manage various singularity configuration (root user only)\n  delete      Deletes requested image from the library\n  exec        Run a command within a container\n  help        Help about any command\n  inspect     Show metadata for an image\n  instance    Manage containers running as services\n  key         Manage OpenPGP keys\n  oci         Manage OCI containers\n  plugin      Manage Singularity plugins\n  pull        Pull an image from a URI\n  push        Upload image to the provided URI\n  remote      Manage singularity remote endpoints\n  run         Run the user-defined default command within a container\n  run-help    Show the user-defined help for an image\n  search      Search a Container Library for images\n  shell       Run a shell within a container\n  sif         siftool is a program for Singularity Image Format (SIF) file manipulation\n  sign        Attach a cryptographic signature to an image\n  test        Run the user-defined tests within a container\n  verify      Verify cryptographic signatures attached to an image\n  version     Show the version for Singularity\n\nExamples:\n  $ singularity help &lt;command&gt; [&lt;subcommand&gt;]\n  $ singularity help build\n  $ singularity help instance start\n\n\nFor additional help or support, please visit https://www.sylabs.io/docs/\n</code></pre> <p>You can use the <code>help</code> command if you want to see the information about subcommands. For example, to see the <code>pull</code> command help,</p> <pre><code>$ singularity help pullPull an image from a URI\n\nUsage:\nsingularity pull [pull options...] [output file] &lt;URI&gt;\n\nDescription:\nThe 'pull' command allows you to download or build a container from a given\nURI. Supported URIs include:\n\nlibrary: Pull an image from the currently configured library\nlibrary://user/collection/container[:tag]\n\ndocker: Pull an image from Docker Hub\ndocker://user/image:tag\n\nshub: Pull an image from Singularity Hub\nshub://user/image:tag\n\noras: Pull a SIF image from a supporting OCI registry\noras://registry/namespace/image:tag\n\nhttp, https: Pull an image using the http(s?) protocol\nhttps://library.sylabs.io/v1/imagefile/library/default/alpine:latest\n\nOptions:\n--arch string architecture to pull from library (default \"amd64\")\n--dir string download images to the specific directory\n--disable-cache dont use cached images/blobs and dont create them\n--docker-login login to a Docker Repository interactively\n-F, --force overwrite an image file if it exists\n-h, --help help for pull\n--library string download images from the provided library\n(default \"https://library.sylabs.io\")\n--no-cleanup do NOT clean up bundle after failed build, can be\nhelpul for debugging\n--nohttps do NOT use HTTPS with the docker:// transport\n(useful for local docker registries without a\ncertificate)\n\n\nExamples:\nFrom Sylabs cloud library\n$ singularity pull alpine.sif library://alpine:latest\n\nFrom Docker\n$ singularity pull tensorflow.sif docker://tensorflow/tensorflow:latest\n\nFrom Shub\n$ singularity pull singularity-images.sif shub://vsoch/singularity-images\n\nFrom supporting OCI registry (e.g. Azure Container Registry)\n$ singularity pull image.sif oras://&lt;username&gt;.azurecr.io/namespace/image:tag\n\n\nFor additional help or support, please visit https://www.sylabs.io/docs/\n</code></pre>","title":"Check Installation"},{"location":"Singularity_I._Introduction/#downloading-pre-built-images","tags":["how-to guide"],"text":"<p>I already downloaded a pre-built image \"hello-world\" from shub, one of the registries, using the <code>pull</code> command. This is the easiest way to use Singularity.</p> <p>You can use the <code>pull</code> command to download pre-built images from a number of Container Registries, here we\u2019ll be focusing on the Singularity-Hub or DockerHub. The following are some of the container registries.</p> <ul> <li><code>library</code>- images hosted on Sylabs Cloud</li> <li><code>shub</code>- images hosted on Singularity Hub</li> <li><code>docker</code>- images hosted on Docker Hub</li> <li><code>localimage</code>- images saved on your machine</li> <li><code>yum</code>- yum based systems such as CentOS and Scientific Linux</li> <li><code>debootstrap</code>- apt based systems such as Debian and Ubuntu</li> <li><code>arch</code>- Arch Linux</li> <li><code>busybox</code>- BusyBox</li> <li><code>zypper</code>- zypper based systems such as Suse and OpenSuse</li> </ul>","title":"Downloading pre-built images"},{"location":"Singularity_I._Introduction/#pulling-an-images-from-sylabs-cloud-library","tags":["how-to guide"],"text":"<p>In this example, I will pull a base Alpine container from Sylabs cloud:</p> <pre><code>$ singularity pull library://sylabsed/linux/alpine\nINFO:    Downloading library image\n 2.08 MiB / 2.08 MiB [===========================================================================================] 100.00% 4.74 MiB/s 0s\n</code></pre> <p>You can rename the container using the \u2013name flag:</p> <pre><code>$ singularity pull --name my_alpine.sif library://sylabsed/linux/alpine\nINFO:    Downloading library image\n 2.08 MiB / 2.08 MiB [===========================================================================================] 100.00% 9.65 MiB/s 0s\n</code></pre> <p>The above example will save the image as <code>my_alpine.sif</code></p>","title":"Pulling an images from Sylabs cloud library"},{"location":"Singularity_I._Introduction/#pulling-an-images-from-docker-hub","tags":["how-to guide"],"text":"<p>This example pulls an Alpine image from Docker hub</p> <pre><code>$ singularity pull docker://alpine\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\nCopying blob df20fa9351a1 done\nCopying config 0f5f445df8 done\nWriting manifest to image destination\nStoring signatures\n2020/08/20 15:53:52  info unpack layer: sha256:df20fa9351a15782c64e6dddb2d4a6f50bf6d3688060a34c4014b0d9a752eb4c\nINFO:    Creating SIF file...\nINFO:    Build complete: alpine_latest.sif\n</code></pre>","title":"Pulling an images from Docker hub"},{"location":"Singularity_I._Introduction/#interact-with-images","tags":["how-to guide"],"text":"<p>You can interact with images with <code>shell</code>, <code>exec</code>, and <code>run</code> commands.</p> <p>To learn how to interact with images, let's first pull an image <code>lolcow_latest.sif</code> from the libray.</p> <pre><code>$ singularity pull library://sylabsed/examples/lolcow\n</code></pre>","title":"Interact with images"},{"location":"Singularity_I._Introduction/#shell","tags":["how-to guide"],"text":"<p>The <code>shell</code>\u00a0command allows you to spawn a new shell within your container and interact with it as if it is a virtual machine.</p> <pre><code>$singularity shell lolcow_latest.sif\n  Singularity&gt;\n</code></pre> <p>The change in prompt indicates that you have entered the container. Once inside of a container, you are the same user as you are on the host system.</p> <pre><code>Singularity&gt;whoami\nchoiyj\n</code></pre> <p>To exit from a container, type <code>exit</code>.</p> <pre><code>Singularity&gt;exit\n$\n</code></pre>","title":"shell"},{"location":"Singularity_I._Introduction/#exec","tags":["how-to guide"],"text":"<p>The <code>exec</code> command allows you to execute a custom command within a container by specifying the image file. For instance, to execute the cowsay\u00a0program within the <code>lolcow_latest.sif</code> container:</p> <pre><code>$ singularity exec lolcow_latest.sif cowsay container camp rocks\n______________________\n&lt; container camp rocks &gt;\n ----------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> <p>You can also use the <code>shell</code> command to run the program in the container.</p> <pre><code>Singularity&gt;  cowsay container camp rocks\n ______________________\n&lt; container camp rocks &gt;\n ----------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre>","title":"exec"},{"location":"Singularity_I._Introduction/#run","tags":["how-to guide"],"text":"<p>Singularity containers contain runscripts.\u00a0These are user defined scripts which define the actions of a container when user runs it. The runscript can be performed with the run\u00a0command, or simply by calling the container as though it were an executable.</p> <pre><code>$ singularity run lolcow_latest.sif\n _________________________________________\n/ You're ugly and your mother dresses you \\\n\\ funny.                                  /\n -----------------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre>","title":"run"},{"location":"Singularity_I._Introduction/#cache-setting","tags":["how-to guide"],"text":"<p>By default, Singularity uses a temporary directory to save Docker files as tarballs:</p> <pre><code>$ ls ~/.singularity\ncache/  docker/  metadata/\n$ ls .singularity/docker/\nsha256:0efe07335a049e6afcd757db2d17ba37a12b717eb807acb03ddf3cd756b9fc2a.tar.gz\nsha256:43a98c1873995475a895f3d79f405232ef5230076b3f610c949c2e8341743af7.tar.gz\nsha256:6b1bb01b3a3b72463ae8ac5666d57b28f1a21d5256271910ac8df841aa04ecd1.tar.gz\nsha256:c2ca09a1934b951505ecc4d6b2e4ab7f9bf27bcdfb8999d0181deca74daf7683.tar.gz\nsha256:d6c3619d2153ffdefa4a9c19f15c5d566ce271b397a84537baa9ee45b24178f2.tar.gz\n</code></pre> <p>You can change these by theses cache directories by specifying the location on your localhost as following:</p> <pre><code>$ mkdir -p $SCRATCH/singularity_tmp\n$ mkdir -p $SCRATCH/singularity_scratch\n$ SINGULARITY_TMPDIR=$SCRATCH/singularity_scratch SINGULARITY_CACHEDIR=$SCRATCH/singularity_tmp singularity --debug pull --name ubuntu-tmpdir.sif docker://ubuntu\n</code></pre>","title":"Cache setting"},{"location":"Singularity_I._Introduction/#creating-writable-containers-with-sandbox-options","tags":["how-to guide"],"text":"<p>If you want to build a container within a writable directory (called a sandbox), you can do that with <code>--sandbox</code> option.\u00a0We\u2019ve already covered how you can pull an existing container from Docker Hub, but we can also build a Singularity container from docker using the build command:</p> <pre><code>$ singularity build --sandbox ubuntu-latest/  docker://ubuntu\n</code></pre> <p>With the <code>--sandbox</code> option, you can changes and write files within the directory, but those changes will not remain when you finished using the container. To make those changes persistent, use <code>--writable</code> option when you start a container.</p> <pre><code>$ singularity shell --writable ubuntu-latest/\n</code></pre>","title":"Creating writable containers with --sandbox options"},{"location":"Singularity_II._Running_a_container_on_HPC/","text":"<ul> <li>Why use a     container?</li> <li>Changes from the old CentOS 6     system</li> <li>General information of singularity on the     HPCC</li> <li>Running a docker     container</li> <li>Building     containers</li> <li>Submitting a singularity job to our     cluster</li> </ul>","title":"Singularity: II. Running a container on HPC"},{"location":"Singularity_II._Running_a_container_on_HPC/#why-use-a-container","text":"<p>Singularity allows users to run software inside of containers. A popular container system is 'Docker', which is interoperable with singularity.</p> <p>A Linux container provides an environment that's different from the Linux host server you may be running on. For example, you could run a different version of Linux (e.g., running Ubuntu on our CentOS system). One advantage of containers is if your software requires a newer version of system libraries (e.g. glibc) than is available in our operating system, then you can run your software in a container. The main reason for using containers is that all of the dependencies are pre-installed.</p>","title":"Why use a container?"},{"location":"Singularity_II._Running_a_container_on_HPC/#changes-from-the-old-centos-6-system","text":"<p>First, 'module load singularity' is no longer required now on the CentOS 7 system. All singularity commands are built into the system such as 'singularity shell' and 'singularity exec', which means you can invoke these commands directly from the command line.</p> <p>Second, many software that used to require singularity images can now be installed without singularity after we have upgraded to CentOS 7. For example, you may use Rstudio without a singularity container. Use \"module spider\" to see if the software you need is installed (see more on\u00a0module commands ).</p>","title":"Changes from the old CentOS 6 system"},{"location":"Singularity_II._Running_a_container_on_HPC/#general-information-of-singularity-on-the-hpcc","text":"<p>The version of singularity is currently 3.4.1. The official documentation for this version is at https://www.sylabs.io/guides/3.4/user-guide/index.html.</p> <p>In general, a singularity image file contains all the software you need to run a program, and you use the command \"<code>singularity exec &lt;imgfile&gt; &lt;command&gt;</code>\" to run your command inside that image. For example, if I have a special version of R, I can type the following command:</p> <pre><code>singularity exec r-special.simg Rscript myprogram.R\n</code></pre>","title":"General information of singularity on the HPCC"},{"location":"Singularity_II._Running_a_container_on_HPC/#running-a-docker-container","text":"<p>Many programs are available as docker containers pre-built, and many of those are available on the docker hub https://hub.docker.com.</p> <p>For details about running a docker container with singularity, see  user guide here. Here is a quick example of running a command in Ubuntu linux even though we use CentOS Linux:</p> <pre><code>singularity shell docker://ubuntu:latest          # log-in to Ubuntu, use exit to log-out  \nsingularity run docker://ubuntu:latest uname -a   # show details of the Linux version\n</code></pre>","title":"Running a docker container"},{"location":"Singularity_II._Running_a_container_on_HPC/#building-containers","text":"<p>Building your own containers requires administrative access (e.g., root privileges) so you can't do this. However you may build them on your laptop following the singularity documentation and transfer the image file over to the HPCC and use it here.</p>","title":"Building containers"},{"location":"Singularity_II._Running_a_container_on_HPC/#submitting-a-singularity-job-to-our-cluster","text":"<p>In general, running singularity commands is the same as running any kinds of programs when you prepare your SLURM script. Two typical situations are:</p> <p>(1) The program you are going to run inside of the container is as simple as one that runs on a single node. In this case, you put your singularity commands (singularity exec <code>&lt;imgfile&gt;</code> <code>&lt;command&gt;</code>) right after all the sbatch directive lines. If the program needs to use multiple threads/cores on a node, say 8, you would request 8 cores by <code>#SBATCH --cpus-per-task=8</code> as you would do with any regular program running.</p> <p>(2) You are running an MPI program within your container. In this case, you must use <code>srun -n $SLURM_NTASKS</code> before the <code>singularity</code> command to launch the processes on the cluster nodes. See a template script below.</p> <p>SLURM script</p> <pre><code>#!/bin/bash\n\n# Job name:\n#SBATCH --job-name=singularity-test\n#\n# Number of MPI tasks needed for use case:\n#SBATCH --ntasks=18\n#\n# Processors per task:\n#SBATCH --cpus-per-task=1\n#\n# Memory per CPU\n#SBATCH --mem-per-cpu=1G\n#\n# Wall clock limit:\n#SBATCH --time=30\n#\n# Standard out and error:\n#SBATCH --output=%x-%j.SLURMout\n\ncd &lt;directory containing the singularity image file (.sif)&gt;\nsrun -n $SLURM_NTASKS singularity exec xxx.sif &lt;commands&gt;\n</code></pre>","title":"Submitting a singularity job to our cluster"},{"location":"Singularity_III._Advanced_skills/","text":"<p>In this tutorial, we will learn how to create containers from a recipe file, called Singularity (which is equivalent to Dockerfile)</p> <ul> <li>Cache setting</li> <li>Building containers</li> <li>Building containers from Singularity definition files</li> </ul>","title":"Singularity: III. Advanced skills"},{"location":"Singularity_III._Advanced_skills/#cache-setting","text":"<p>By default, Singularity uses a temporary directory to save Docker files as tarballs:</p> <pre><code>$ ls ~/.singularity\ncache/  docker/  metadata/\n$ ls .singularity/docker/\nsha256:0efe07335a049e6afcd757db2d17ba37a12b717eb807acb03ddf3cd756b9fc2a.tar.gz\nsha256:43a98c1873995475a895f3d79f405232ef5230076b3f610c949c2e8341743af7.tar.gz\nsha256:6b1bb01b3a3b72463ae8ac5666d57b28f1a21d5256271910ac8df841aa04ecd1.tar.gz\nsha256:c2ca09a1934b951505ecc4d6b2e4ab7f9bf27bcdfb8999d0181deca74daf7683.tar.gz\nsha256:d6c3619d2153ffdefa4a9c19f15c5d566ce271b397a84537baa9ee45b24178f2.tar.gz\n</code></pre> <p>You can change these by theses cache directories by specifying the location on your localhost as following:</p> <pre><code>$ mkdir -p $SCRATCH/singularity_tmp\n$ mkdir -p $SCRATCH/singularity_scratch\n$ SINGULARITY_TMPDIR=$SCRATCH/singularity_scratch SINGULARITY_CACHEDIR=$SCRATCH/singularity_tmp singularity --debug pull --name ubuntu-tmpdir.sif docker://ubuntu\n</code></pre>","title":"Cache setting"},{"location":"Singularity_III._Advanced_skills/#building-containers","text":"","title":"Building containers"},{"location":"Singularity_III._Advanced_skills/#creating-writable-containers-with-sandbox-options","text":"<p>If you want to build a container within a writable directory (called a sandbox), you can do that with --sandbox option.\u00a0We\u2019ve already covered how you can pull an existing container from Docker Hub, but we can also build a Singularity container from docker using the build command:</p> <pre><code>$ singularity build --sandbox ubuntu-latest/  docker://ubuntu\n</code></pre> <p>With --sandbox option, you can changes and write files within the directory, but those changes will not remain when you finished using the container. To make those changes persistent, use --writable option when you start a container.</p> <pre><code>$ singularity shell --writable ubuntu-latest/\n</code></pre>","title":"Creating writable containers with --sandbox options"},{"location":"Singularity_III._Advanced_skills/#building-containers-from-singularity-definition-files","text":"<p>To build containers, Singularity uses a file Singularity\u00a0which is equivalent to Dockerfile in Docker. You can use different name but it is better practice to put it in a director and name it Singularity because it will be helpful later on when developing on repositories.</p> <p>For detailed information on writing Singularity recipe files, please refer to the container definition docs.</p> <p>To create a container using a custom Singularity file, you use build command:</p> <pre><code>$ singularity build ubuntu-latest.sif Singularity\n</code></pre> <p>You would get an error message because you do not have a Singularity file yet.</p> <p>Let's assume that you already have the following Singularity definition file, called \"lowcow.def\" and you want ot use it to build a SIF container.</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:16.04\n\n%post\n    apt-get -y update\n    apt-get -y install fortune cowsay lolcat\n\n%environment\n    export LC_ALL=C\n    export PATH=/usr/games:$PATH\n\n%runscript\n    fortune | cowsay | lolcat\n</code></pre>","title":"Building containers from Singularity definition files"},{"location":"Slurm_Environment_Variables/","text":"<p>The Slurm controller will set variables in the environment of the batch script. Below is a list of SLURM variables and the corresponding Torque/MOAB environment variables for the comparison.</p>    SLURM Variables Torque/MOAB Description     SLURM_ARRAY_TASK_COUNT  Total number of tasks in a job array   SLURM_ARRAY_TASK_ID PBS_ARRAYID Job array ID (index) number   SLURM_ARRAY_TASK_MAX  Job array's maximum ID (index) number   SLURM_ARRAY_TASK_MIN  Job array's minimum ID (index) number   SLURM_ARRAY_TASK_STEP  Job array's index step size   SLURM_ARRAY_JOB_ID PBS_JOBID Job array's master job ID number   SLURM_CLUSTER_NAME  Name of the cluster on which the job is executing   SLURM_CPUS_ON_NODE  Number of CPUS on the allocated node   SLURM_CPUS_PER_TASK PBS_VNODENUM Number of cpus requested per task. Only set if the --cpus-per-task option is specified.   SLURM_JOB_ACCOUNT  Account name associated of the job allocation   SLURM_JOBID, SLURM_JOB_ID PBS_JOBID The ID of the job allocation   SLURM_JOB_CPUS_PER_NODE PBS_NUM_PPN Count of processors available to the job on this node.   SLURM_JOB_DEPENDENCY  Set to value of the --dependency option   SLURM_JOB_NAME PBS_JOBNAME Name of the job   SLURM_NODELIST, SLURM_JOB_NODELIST PBS_NODEFILE List of nodes allocated to the job   SLURM_NNODES, SLURM_JOB_NUM_NODES  Total number of different nodes in the job's resource allocation   SLURM_MEM_PER_NODE  Same as --mem   SLURM_MEM_PER_CPU  Same as --mem-per-cpu   SLURM_NTASKS, SLURM_NPROCS PBS_NUM_NODES Same as -n, --ntasks   SLURM_NTASKS_PER_NODE  Number of tasks requested per node. Only set if the --ntasks-per-node option is specified.   SLURM_NTASKS_PER_SOCKET  Number of tasks requested per socket. Only set if the --ntasks-per-socket option is specified.   SLURM_SUBMIT_DIR PBS_O_WORKDIR The directory from which sbatch was invoked   SLURM_SUBMIT_HOST PBS_O_HOST The hostname of the computer from which sbatch was invoked   SLURM_TASK_PID  The process ID of the task being started   SLURMD_NODENAME  Name of the node running the job script   SLURM_JOB_GPUS  GPU IDs allocated to the job (if any).","title":"SLURM Environment Variables"},{"location":"Slurm_Environment_Variables/#using-variables-in-slurm-jobs","text":"<p>You may also set your own variables for use in your SLURM jobs.    One way is to set them inside the script itself, but that requires modifying the script.  It is possible to pass variables into a SLURM job when you submit the job using the --export flag.  For example to pass the value of the variables REPS and X into the job script named jobs.sb you can use:</p> <pre><code>sbatch --export=REPS=500,X='test' jobs.sb\n</code></pre> <p>These are then available in your jobs as $REPS and $X</p>","title":"Using Variables in SLURM Jobs"},{"location":"Slurm_Environment_Variables/#using-variables-to-set-slurm-job-name-and-output-files","text":"<p>SLURM does not support using variables in the #SBATCH lines within a job script (for example, <code>#SBATCH -N=$REPS</code> will NOT work).   A very limited number of variables are available in the #SBATCH just as %j for JOB ID.   However, values passed from the command line have precedence over values defined in the job script and you can set certain SLURM variables in the command line.  For example, you could set the job name and output/error files can be passed on the sbatch command line:</p> <pre><code>RUNTYPE='test'\nRUNNUMBER=5\nsbatch --job-name=$RUNTYPE.$RUNNUMBER.run --output=$RUNTYPE.$RUNUMBER.txt --export=A=$A,b=$b jobscript.sbatch\n</code></pre> <p>However note in this example, the output file doesn't have the job ID, which is not available from the command line, only inside the sbatch shell script.  </p>","title":"Using variables to set SLURM job name and output files"},{"location":"Software_Installation_by_EasyBuild/","text":"<p>FOR CENTOS 7 ONLY</p> <p>EasyBuild is a python program installed on HPCC and can be used to do software installation. Since it is easy to use, HPCC users can load a EasyBuild module to build their software and module system, and they can work together with HPCC software system. To use EasyBuild after you log into Centos 7 nodes, please run</p>   <pre><code>$ module purge; module load EasyBuild; module list\n\nCurrently Loaded Modules:\n  1) EasyBuild/3.6.2\n</code></pre>   <p>The commands remove any loaded module (by module purge) and load the EasyBuild module of the current version. The purge of loaded modules is strongly suggested since any loaded module might affect software installation by Easybuild. (Warning will also show during installation if any module created by EasyBuild is loaded.) After the module is loaded, EasyBuild commands (such as eb, easy_install, ...) can function normally.</p> <p>By default, software is set to be built under each user's home directory with the path $HOME/software and module system built in $HOME/modules. (For users with helpdesk group, the default directories are /opt/software and /opt/modules respectively.) If you would like to choose different directories, you can reset the environment variables EASYBUILD_INSTALLPATH_SOFTWARE and EASYBUILD_INSTALLPATH_MODULES to your preferred places:</p>   <pre><code>$ export EASYBUILD_INSTALLPATH_SOFTWARE=&lt;path to the directory where software is installed&gt;\n$ export EASYBUILD_INSTALLPATH_MODULES=&lt;path to the directory where module system is installed&gt;\n</code></pre>   <p>Please make sure the directories are built (by mkdir -p command) and run module use command so your module system ($EASYBUILD_INSTALLPATH_MODULES) is used with HPCC system (/opt/modules):</p>   <pre><code>$ mkdir -p $EASYBUILD_INSTALLPATH_SOFTWARE $EASYBUILD_INSTALLPATH_MODULES\n$ module use $EASYBUILD_INSTALLPATH_MODULES /opt/modules\n</code></pre>   <p>In order to make the installation conveniently, three commands are created in our system and they are introduced here:</p>","title":"Software Installation by EasyBuild"},{"location":"Software_Installation_by_EasyBuild/#ebf","text":"<p>EasyBuild can install software according to the information from a eb file. EasyBuild has many eb files created for many different software installations with any possible compilers and libraries. If a eb file is provided without any path, EasyBuild will try to find it from\u00a0EasyBuild default paths. Since the path names are long and many directories and files are inside, you can use ebF command to find eb files associated with a input software name:</p>   <pre><code>$ ebF Intel\n\nebF_PATH=/opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs\n\n====== $ebF_PATH/i/IntelClusterChecker/\nIntelClusterChecker-2017.1.016.eb\n\n====== $ebF_PATH/c/CrayIntel/\nCrayIntel-2015.06.eb  CrayIntel-2015.11.eb  CrayIntel-2016.06.eb\n</code></pre>   <p>The command tries to find the software directories with the name containing the input keyword Intel and all eb files inside (including any possible compilers). If you are not sure the name of a software with lower or upper case, you can use -i option to see them all:</p>   <pre><code>$ ebF -i Intel\n\nebF_PATH=/opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs\n\n====== $ebF_PATH/i/IntelClusterChecker/\nIntelClusterChecker-2017.1.016.eb\n\n====== $ebF_PATH/i/intelcuda/\nintelcuda-2016.10.eb\n\n====== $ebF_PATH/i/intel/\nintel-2014.06.eb  intel-2015.08.eb  intel-2016.02-GCC-4.9.eb  intel-2016a.eb    intel-2017.09.eb  intel-2018a.eb\nintel-2014.10.eb  intel-2015a.eb    intel-2016.02-GCC-5.3.eb  intel-2016b.eb    intel-2017a.eb\nintel-2014.11.eb  intel-2015b.eb    intel-2016.03-GCC-4.9.eb  intel-2017.00.eb  intel-2017b.eb\nintel-2014b.eb    intel-2016.00.eb  intel-2016.03-GCC-5.3.eb  intel-2017.01.eb  intel-2018.00.eb\nintel-2015.02.eb  intel-2016.01.eb  intel-2016.03-GCC-5.4.eb  intel-2017.02.eb  intel-2018.01.eb\n\n====== $ebF_PATH/c/CrayIntel/\nCrayIntel-2015.06.eb  CrayIntel-2015.11.eb  CrayIntel-2016.06.eb\n\n====== $ebF_PATH/__archive__/i/intel-para/\nintel-para-2014.12.eb\n</code></pre>   <p>where any software with name containing lower and upper case of the keyword Intel is shown. If you know the exact name of the software and would like to list less eb files, you can use -w option:</p>   <pre><code>$ ebF -w intel\n\nebF_PATH=/opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs\n\n====== $ebF_PATH/i/intel/\nintel-2014.06.eb  intel-2015.08.eb  intel-2016.02-GCC-4.9.eb  intel-2016a.eb    intel-2017.09.eb  intel-2018a.eb\nintel-2014.10.eb  intel-2015a.eb    intel-2016.02-GCC-5.3.eb  intel-2016b.eb    intel-2017a.eb\nintel-2014.11.eb  intel-2015b.eb    intel-2016.03-GCC-4.9.eb  intel-2017.00.eb  intel-2017b.eb\nintel-2014b.eb    intel-2016.00.eb  intel-2016.03-GCC-5.3.eb  intel-2017.01.eb  intel-2018.00.eb\nintel-2015.02.eb  intel-2016.01.eb  intel-2016.03-GCC-5.4.eb  intel-2017.02.eb  intel-2018.01.eb\n\n====== $ebF_PATH/__archive__/i/intel-para/\nintel-para-2014.12.eb\n</code></pre>   <p>The command now shows all eb files of the software related to the exact keyword intel. If you would like to see eb files just for the exact name intel only (no related software), you can use the symbol $ at the end of the software name:</p>   <pre><code>$ ebF -w intel$\n\nebF_PATH=/opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs\n\n====== $ebF_PATH/i/intel/\nintel-2014.06.eb  intel-2015.08.eb  intel-2016.02-GCC-4.9.eb  intel-2016a.eb    intel-2017.09.eb  intel-2018a.eb\nintel-2014.10.eb  intel-2015a.eb    intel-2016.02-GCC-5.3.eb  intel-2016b.eb    intel-2017a.eb\nintel-2014.11.eb  intel-2015b.eb    intel-2016.03-GCC-4.9.eb  intel-2017.00.eb  intel-2017b.eb\nintel-2014b.eb    intel-2016.00.eb  intel-2016.03-GCC-5.3.eb  intel-2017.01.eb  intel-2018.00.eb\nintel-2015.02.eb  intel-2016.01.eb  intel-2016.03-GCC-5.4.eb  intel-2017.02.eb  intel-2018.01.eb\n</code></pre>   <p>where only eb files of the software name intel are show. If no eb file can be found for a particular software installation, you can also try to modify a similar one. Copy the file using the path name (leading with $ebF_PATH) and file name. Modify the installation information inside and give it a try.</p>","title":"ebF"},{"location":"Software_Installation_by_EasyBuild/#ebs","text":"<p>After a eb file is found or created, you can use ebS command to install the software and create the module file. Since ebS will try to build or rebuild software (and its module file) no matter whether it exists in /opt/software, please check if you just need a module file before you run the command. If EasyBuild can not find a module file of a dependent software listed in the eb file, it will also try to built or rebuilt the dependence (and its module file) automatically. To use ebS command with input of a eb file found in the default paths, you can just use the name without the path:</p>   <pre><code>$ ebS intel-2018a.eb\n== temporary log file in case of crash /tmp/eb-lxUlvt/easybuild-maTXAl.log\n== resolving dependencies ...\n== processing EasyBuild easyconfig /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs/i/intel/intel-2018a.eb\n== building and installing Core/intel/2018a...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== permissions...\n== packaging...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file(s) /opt/software/intel/2018a/easybuild/easybuild-intel-2018a-20180330.141146.log\n== Build succeeded for 1 out of 1\n== Temporary log file(s) /tmp/eb-lxUlvt/easybuild-maTXAl.log* have been removed.\n== Temporary directory /tmp/eb-lxUlvt has been removed.\n</code></pre>   <p>If the eb file is not in the default path, please provide the path and file name to install the software. After it is installed successfully, you may check if the software is in\u00a0/opt/software directory and if the module file is in the proper place by module spider command.</p>","title":"ebS"},{"location":"Software_Installation_by_EasyBuild/#ebm","text":"<p>If a software is installed and functions properly but its module file is missed and can not be found by module spider command, you can use ebM command to recreate the module file. The command also needs input of a eb file name as ebS command:</p>   <pre><code>$ ebM intel-2018a.eb\n== temporary log file in case of crash /tmp/eb-ZTyyR3/easybuild-XiMfNO.log\n== resolving dependencies ...\n== processing EasyBuild easyconfig /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs/i/intel/intel-2018a.eb\n== building and installing Core/intel/2018a...\n== fetching files [skipped]\n== creating build dir, resetting environment...\n== backup of existing module file stored at /opt/modules/Core/intel/2018a.lua.bak_20180330141928\n== unpacking [skipped]\n== patching [skipped]\n== preparing...\n== configuring [skipped]\n== building [skipped]\n== testing [skipped]\n== installing [skipped]\n== taking care of extensions [skipped]\n== postprocessing [skipped]\n== sanity checking [skipped]\n== cleaning up [skipped]\n== creating module...\n== comparing module file with backup /opt/modules/Core/intel/2018a.lua.bak_20180330141928; no differences found\n== permissions [skipped]\n== packaging [skipped]\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file(s) /opt/software/intel/2018a/easybuild/easybuild-intel-2018a-20180330.141933.log\n== Build succeeded for 1 out of 1\n== Temporary log file(s) /tmp/eb-ZTyyR3/easybuild-XiMfNO.log* have been removed.\n== Temporary directory /tmp/eb-ZTyyR3 has been removed.\n</code></pre>   <p>It will also try to build or rebuilt the module file no matter whether it exists in the proper directory. While using ebM command, if you see an error message of failed to copy the new eb file to the software directory:</p>   <pre><code>$ ebM GCCcore-7.2.0.eb\n== temporary log file in case of crash /tmp/eb-W5U4vp/easybuild-F9E32g.log\n== resolving dependencies ...\n== processing EasyBuild easyconfig /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs/g/GCCcore/GCCcore-7.2.0.eb\n== building and installing Core/GCCcore/7.2.0...\n== fetching files [skipped]\n== creating build dir, resetting environment...\n== backup of existing module file stored at /opt/modules/Core/GCCcore/7.2.0.lua.bak_20180330143152\n== unpacking [skipped]\n== patching [skipped]\n== preparing...\n== configuring [skipped]\n== building [skipped]\n== testing [skipped]\n== installing [skipped]\n== taking care of extensions [skipped]\n== postprocessing [skipped]\n== sanity checking [skipped]\n== cleaning up [skipped]\n== creating module...\n== comparing module file with backup /opt/modules/Core/GCCcore/7.2.0.lua.bak_20180330143152; diff is:\n--- /opt/modules/Core/GCCcore/7.2.0.lua.bak_20180330143152\n+++ /opt/modules/Core/GCCcore/7.2.0.lua\n@@ -23,7 +23,6 @@\n prepend_path(\"CPATH\", pathJoin(root, \"include\"))\n prepend_path(\"LD_LIBRARY_PATH\", pathJoin(root, \"lib\"))\n prepend_path(\"LD_LIBRARY_PATH\", pathJoin(root, \"lib64\"))\n-prepend_path(\"LD_LIBRARY_PATH\", pathJoin(root, \"lib/gcc/x86_64-pc-linux-gnu/7.2.0\"))\n prepend_path(\"LIBRARY_PATH\", pathJoin(root, \"lib\"))\n prepend_path(\"LIBRARY_PATH\", pathJoin(root, \"lib64\"))\n prepend_path(\"MANPATH\", pathJoin(root, \"share/man\"))\n\n== permissions [skipped]\n== packaging [skipped]\nERROR: Traceback (most recent call last):\n  File \"/opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_framework-3.5.2-py2.7.egg/easybuild/main.py\", line 128, in build_and_install_software\n    (ec_res['success'], app_log, err) = build_and_install_one(ec, init_env, hooks=hooks)\n  File \"/opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_framework-3.5.2-py2.7.egg/easybuild/framework/easyblock.py\", line 2751, in build_and_install_one\n    copy_file(spec, newspec)\n  File \"/opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_framework-3.5.2-py2.7.egg/easybuild/tools/filetools.py\", line 1575, in copy_file\n    raise EasyBuildError(\"Failed to copy file %s to %s: %s\", path, target_path, err)\nEasyBuildError: \"Failed to copy file /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs/g/GCCcore/GCCcore-7.2.0.eb to /opt/software/GCCcore/7.2.0/easybuild/GCCcore-7.2.0.eb: [Errno 1] Operation not permitted: '/opt/software/GCCcore/7.2.0/easybuild/GCCcore-7.2.0.eb'\"\n</code></pre>   <p>please remove the old eb file under the software directory:</p>   <pre><code>$ rm /opt/software/GCCcore/7.2.0/easybuild/GCCcore-7.2.0.eb\n</code></pre>   <p>and try again. It should resolve the error. For more details about EasyBuild program, please refer to the EasyBuild documentation web site.</p>","title":"ebM"},{"location":"Specifications_of_Job_submission/","text":"<p>The following is a comparison of submission options between PBS and SBATCH lines. It is only for help on the transition between the two systems.</p> <p>Important differences between SLURM and PBS</p> <p>Please be careful on the specifications --ntask= (-n) and --cpus-per-task= (-c) in SLURM since they are not in PBS specifications (and there is no CPUs per node or ppn in SLURM). The two requests are for software specifications in Parallel Computing. The number of tasks (-n) is the number of parallel processes in distributed memory (such as MPI model). The number of CPUs per task (-c) is for the number of threads in shared memory (such as OpenMP model). If you would like to specify how many different nodes (in hardware), please use --nodes= (-N) in SLURM job script or on sbatch command.</p>             Torque SLURM Description for  Torque Torque Example SLURM Example   #PBS #SBATCH Headof each line     -A -A, --account= This option tells TORQUE to use the Account (not Username) credential specified. #PBS -A mybuyin #SBATCH -A mybuyin #SBATCH --account=mybuyin   -a -begin= This option tells PBS to run a job at the given time. #PBS -a 0615 #SBATCH --begin=06:15   -e -o -e, --error= pattern&gt;  -o, --output= pattern&gt; Need a file name pattern. Can not be a directory name. For the details of valid filename pattern, check the manual of sbatch by \"man sbatch\" and look into filename pattern section. The location for Output_Path and Error_Path attributes as demonstrated below. Note that you only need to use one if you use the -j option. #PBS -e ~/ErrorFile #SBATCH -e ~/ErrorFile_%j_%u By default both standard output and standard error are directed to the same file.   qsub -I salloc srun --pty /bin/bash Declares that the job is to be run \"interactively\". qsub -I qsub -I -X srun --pty /bin/bash salloc --x11   -j  Using an eo option will combine STDOUT and STDERR in the file specified in Error_Path; oe will combine them in Output_Path. #PBS -j oe By default both standard output and standard error are directed to the same file.   -l -N, --nodes=\\-n, --ntasks=\\--ntasks-per-node=-c, --cpus-per-task=\\--gres=\\  -t, --time=\\  --mem=\\  -C, --constraint=\\ --tmp=\\ Separate them with \",\" nodes=#; Number and/or type of nodes to be reserved. ppn=# specify the number of processors per node requested. Defaults to 1.  gpus=# specify the number of gpus to use. walltime= the total run time in the form: HH:MM:SS or DD:HH:MM:SS mem= Maximum amount of memory needed by a job. feature= the name of the type of compute noded related to our cluster configuration file= Maximum amount of local disk space needed by a job. #PBS -l walltime=01:00:00 #PBS -l mem=8gb #PBS -l feature=intel14|intel16 #PBS -l file=40GB (See more explanation in the start of this page) #SBATCH -n 4 -c 1 --gres=gpu:2#SBATCH    --time=01:00:00 #SBATCH --mem=2G #SBATCH -C NOAUTO:intel14|intel16 Constraints using \"|\" must be prepended with 'NOAUTO:'. Click here for more information.#SBATCH --tmp=40G   -M --mail-user=\\ Emails account(s) to notify once a job changes states as specified by -m #PBS -M usr@msu.edu #SBATCH --mail-user=usr@msu.edu   -m --mail-type=\\ a- sends mail when job is aborted by batch system b- sends mail when begins execution, e- sends mail when job ends, n- does not send mail #PBS -m abe #SBATCH --mail-type=FAIL,BEGIN,END NONE - does not send mail   -N -J, --job-name=\\ Names the job #PBS -N MySuperComputing #SBATCH -J MySuperComputing   -t -a, --array= Submits a Array Job with n identical tasks.  Each task has the same \\$PBS_JOBID but different \\$PBS_ARRAYID variables. #PBS -t 5 #PBS -t 3-10 #SBATCH -a 5 #SBATCH --array=3-10   -V --export=&lt;environment variables [ALL] NONE&gt; Passes all current environment variables to the job. #PBS -V   -v --export=&lt;environment variables [ALL] NONE&gt; Defines additional environment variables for the job. #PBS -v ev1=ph3,ev2=50   -W -L, --licenses=\\ Special Generic Resources such as software licenses can be requested using the -W option. This is most commonly used with matlab (see Matlab Licenses for more information.) #PBS -W gres:MATLAB #SBATCH -L matlab@27000@lm-01.i","title":"Specifications of Job submission"},{"location":"Stata/","text":"<p>Many versions of Stata are installed on the ICER HPC. When you log-in, Stata is not available by default, but it may be load easily using this command\u00a0 (note you must type Stata with a capital 'S'</p> <p>run stata batch</p> <pre><code>[hpc@dev-intel18 ~]$ module load Stata\n</code></pre> <p>This loads Stata SE version 15.\u00a0 This is equivalent to using the command</p> <pre><code>[hpc@dev-intel18 ~]$ module load Stata/15.0.SE\n</code></pre> <p>Stata has a command line version and a GUI (windowed) version.\u00a0\u00a0\u00a0 To use the command line, type stata at the prompt.\u00a0\u00a0\u00a0 You will see this :</p> <p>Using Stata command line</p> <pre><code>[hpc@dev-intel18 ~]$ module load Stata\n[hpc@dev-intel18 ~]$ stata\n\n  ___  ____  ____  ____  ____ (R)\n /__    /   ____/   /   ____/\n___/   /   /___/   /   /___/   15.0   Copyright 1985-2017 StataCorp LLC\n  Statistics/Data Analysis            StataCorp\n                                      4905 Lakeway Drive\n                                      College Station, Texas 77845 USA\n                                      800-STATA-PC        http://www.stata.com\n                                      979-696-4600        stata@stata.com\n                                      979-696-4601 (fax)\n\n15-user Stata network perpetual license:\n       Serial number:  401506213245\n         Licensed to:  iCER / HPCC at Michigan State University\n                       East Lansing, MI\n\nNotes:\n      1.  Unicode is supported; see help unicode_advice.\n\n.\n</code></pre> <p>In which you may type Stata commands.\u00a0 Type 'exit' to quit this version.\u00a0</p> <p>To run a Stata do file from the command line in 'batch', you use the syntax</p> <p>run stata batch</p> <pre><code>[hpc@dev-intel18 ~]$stata -b do dofilename.do\n</code></pre>","title":"Stata"},{"location":"Stata/#versions","text":"<p>Stata comes in several versions: IC, SE, and MP; see https://www.stata.com/products/which-stata-is-right-for-me/ for details for the differences.\u00a0 Stata/IC has limitations on the numbers of variables that affect most users but has no licensing restrictions (see below). \u00a0 \u00a0 While the default version of Stata available when you load the module is Stata/SE, you currently have to use the command 'stata-se' to start the 'SE' version</p> <p>run stata-se interactive session</p> <pre><code>[hpc@dev-intel18 ~]$module load Stata/15.0.SE\n[hpc@dev-intel18 ~]$stata-se  # to run the interactive version\n[hpc@dev-intel18 ~]$stata-se -b do dofilename.do\u00a0 # to run a do file \n</code></pre> <p>However, to use the \"MP\" Version, you must load it explicitly.\u00a0</p> <p>run stata-mp interactive session</p> <pre><code>[hpc@dev-intel18 ~]$ module load Stata/15.0.MP\n[hpc@dev-intel18 ~]$ stata-mp\n</code></pre> <p>Note that even if you load Stata/MP or SE module, as above, if you just use the command 'stata' it will load the IC version.\u00a0\u00a0 To see which version of Stata you are current in, use the \"about\" command at the dot prompt.\u00a0\u00a0 to use these special versions to run a bach do file, use stata-se and stata-mp instead of plain stata</p> <p>For Stata/SE There are 15 licenses available, so 15 users may use it, for MP there are 5 user licenses of 8-cores each. \u00a0 Please exit the program when you are finished with it.\u00a0\u00a0</p>","title":"Versions"},{"location":"Stata/#gui-version","text":"<p>To use the GUI version, you must first be connected to HPCC with X11 forwarding ( MobaXterm for Windows, XQuartz for Mac - see\u00a0 instruction on installing SSH client )\u00a0 or using a web-base connection to HPCC (see\u00a0 instruction on Connecting via web site ). Once an X11 or remote desktop client is connected, you can run xstata on a dev node:</p> <p>run stata-mp interactive session</p> <pre><code>[hpc@dev-intel18 ~]$ module load Stata\n[hpc@dev-intel18 ~]$ xstata\n</code></pre> <p></p>","title":"GUI version"},{"location":"Stata/#variables-limits","text":"<p>Even if you load the MP or SE versions, Stata limits the number of variables to 5000 unless you tell it otherwise.\u00a0\u00a0\u00a0 For information use the help set_maxvar command at the dot prompt.\u00a0\u00a0 You can set the maxvar for your session or in your do file with (for example to 6000)</p> <p>run stata batch</p> <pre><code>. set maxvar 6000\n</code></pre> <p>There are other settings related to memory usage which are important as Stata attempts to be very conservative.\u00a0 For more information use the Stata \"memory\" command</p>","title":"Variables Limits"},{"location":"Stata/#running-jobs","text":"<p>Note you must use the command line version inside a sb script when running jobs.\u00a0 To copy a working example of Stata job file into your\u00a0 home directory, you can use our getexample tool</p> <p>Getting Stata Example</p> <pre><code>module load powertools\ncd ~\ngetexample STATA_example\ncd STATA_example\n\n# look into the README file in this folder for details\ncat README\n</code></pre>","title":"Running Jobs"},{"location":"Stata/#more-helps","text":"<p>For questions requiring deeper knowledge of statistics, \u00a0users could contact\u00a0CSTAT services at\u00a0 https://cstat.msu.edu/cstat-services\u00a0and use\u00a0the \"schedule a meeting\" link to submit an intake form.</p>","title":"More helps"},{"location":"Submitting-multiple-jobs-simultaneously_40337501.html/","text":"","title":"Submitting multiple jobs simultaneously 40337501.html"},{"location":"Submitting-multiple-jobs-simultaneously_40337501.html/#teaching-submitting-multiple-jobs-simultaneously","text":"<ul> <li>Make a copy of the job script multi_seq.sb and name it     multi_sim.sb</li> <li>Edit multi_sim.sb to simultaneously run python_script.py and     r_script.R Be sure to update the resources required to run     multi_sim.sb if necessary.</li> <li> <p>Submit job to compute node</p> <p>Answer </p> </li> </ul> <pre><code>    #Make a copy of the job script multi_seq.sb and name it multi-sim.sb\n    cp multi_seq.sb multi_sim.sb\n\n    #Edit multi_sim.sb to simultaneously run python_script.py and r_script.R\n    gedit multi_sim.sb\n    python3 python_script.py&amp;\n    Rscript r_script.R&amp;\n    wait\n\n    #Note: Be sure to use \u201c&amp;\u201d (otherwise run in sequential) and \u201cwait\u201d (otherwise job exit immediately)\n    #Be sure to update the resources required to run multi-sim.sb if necessary. Note, the number of nodes/cores requested should be the sum of the nodes/cores needed to run each job. In this case since each job uses one node and one core. Requesting one node is sufficient.\n\n    #Submit job to compute node. Type the following at the command line:\n    sbatch multi_sim.sb\n</code></pre>","title":"Teaching : Submitting multiple jobs simultaneously"},{"location":"Submitting_a_TensorFlow_job/","text":"<p>We assume that you've installed your TensorFlow virtual environment on a GPU dev-node (dev-intel16-k80). The Python code we are going to run in the SLURM job script is named <code>matmul.py</code>, with content being</p> <p>matmul.py</p> <pre><code>import sys\nimport numpy as np\nimport tensorflow as tf\nfrom datetime import datetime\n\ndevice_name = sys.argv[1]  # Choose device from cmd line. Options: gpu or cpu\nshape = (int(sys.argv[2]), int(sys.argv[2]))\nif device_name == \"gpu\":\n    device_name = \"/gpu:6\"\nelse:\n    device_name = \"/cpu:0\"\n\nwith tf.device(device_name):\n    random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1)\n    dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix))\n    sum_operation = tf.reduce_sum(dot_operation)\n\nstartTime = datetime.now()\nwith tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as session:\n        result = session.run(sum_operation)\n        print(result)\n\nprint(\"\\n\" * 5)\nprint(\"Shape:\", shape, \"Device:\", device_name)\nprint(\"Time taken:\", datetime.now() - startTime)\nprint(\"\\n\" * 5)\n</code></pre> <p>Now we write a SLURM script to submit the job to the cluster:</p> <p>Submitting GPU TensorFlow job: test_matmul.sbatch</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=test_matmul\n#SBATCH --gres=gpu:1\n#SBATCH --mem-per-cpu=20G\n#SBATCH --time=20\n#SBATCH --output=%x-%j.SLURMout\n\necho $CUDA_VISIBLE_DEVICES\n\nmodule purge\nmodule load GCC/6.4.0-2.28  OpenMPI/2.1.2\nmodule load CUDA/10.0.130 cuDNN/7.5.0.56-CUDA-10.0.130\nmodule load Python/3.6.4\nsource ~/tf-1.13.1-env/bin/activate\nexport TF_CPP_MIN_LOG_LEVEL=2 # disables the warning, doesn't enable AVX/FMA.\n\nsrun python matmul.py gpu 1500\n</code></pre> <p>To submit it, simply run</p> <pre><code>sbatch test_matmul.sbatch\n</code></pre> <p>The final result will be written to file \"<code>test_matmul-&lt;jobid&gt;.SLURMout</code>\".</p>","title":"Submitting a TensorFlow job"},{"location":"System_Commands/","text":"<p>Please check the table below to find the SLURM correspondence of a Torque command:</p>           Command Description Torque SLURM   Batch job submission qsub \\&lt;Job File Name&gt; sbatch \\&lt;Job File Name&gt;   Interactive job submission qsub -I salloc | srun --pty /bin/bash   Job list qstat squeue -l   Job list by users qstat -u \\&lt;User Name&gt; squeue -l -u \\&lt;User Name&gt;   Job deletion qdel \\&lt;Job ID&gt; scancel \\&lt;Job ID&gt;   Job hold qhold \\&lt;Job ID&gt; scontrol hold \\&lt;Job ID&gt;   Job release qrls \\&lt;Job ID&gt; scontrol release \\&lt;Job ID&gt;   Job update qalter \\&lt;Job ID&gt; scontrol update job \\&lt;Job ID&gt;   Job details qstat -f \\&lt;Job ID&gt; scontrol show job \\&lt;Job ID&gt;   Node list pbsnodes -l sinfo -N   Node details pbsnodes scontrol show nodes    <p>Please check the table below to find the SLURM correspondence of a Moab command:</p>           Command Description Moab SLURM   Job start time showstart \\&lt;Job ID&gt; squeue --start -j \\&lt;Job ID&gt;   Status of nodes mdiag -n sinfo -N -l   User's account mdiag -u \\&lt;User Name&gt; sacctmgr show association user=\\&lt;User Name&gt;   Account members mdiag -a \\&lt;Account Name&gt; sacctmgr show assoc account=\\&lt;Account Name&gt;   Nodes of accounts mdiag -s sinfo -a    <p>Finally, it is suggested to use srun as the parallel command wrapper although mpirun can still be used in SLRUM system.</p>           Command Description OpenMPI SLURM   Parallel wrapper mpirun srun","title":"System Commands"},{"location":"Targeting_Cluster_Architectures/","text":"<p>While all HPCC nodes are the x86_64 architecture, some newer processors have features that are not supported by older processors. A program compiled on a newer processor may not run on an older processor and may result in an 'Illegal Instruction' error. This can be corrected by specifying compilers parameters that control which processor instruction are used.</p> <p>Use the following options for your compiler to ensure your programs will run on all HPCC nodes.</p>            Compiler Type Min Version Max Version Arguments   GCC 6.4 N/A -march=core-avx-i -mtune=skylake-avx512   GCC 4.9 \\&lt; 6.4 -march=core-avx-i -mtune=silvermont   GCC 4.8 \\&lt; 4.9 -march=core-avx-i -mtune=core-avx2   GCC 4.6 \\&lt; 4.8 -march=core-avx-i   GCC 4.3 \\&lt; 4.6 -march=core2   GCC 3.3 \\&lt; 4.3 -march=nocona   Intel 2015.1 N/A -mAVX -axCORE-AVX-I,CORE-AVX2,CORE-AVX512    <p>The new amd20 cluster does not support AVX-512.</p>","title":"Targeting Cluster Architectures"},{"location":"TensorFlow_2.5_installation_as_of_Jul_2021_/","text":"<p>Warning</p> <p>CUDA version <code>&gt;=\u00a011.1</code>is currently disabled on the HPCC GPU nodes. It'll be enabled again in Jan 2022 after system upgrade. Meanwhile, you can either install TF with a version older than 2.5 or use HPCC-installed TF (run \"getexample TensorFlow_example\").</p>  <p>To run TF-GPU , we need to log into a GPU dev-node such as dev-amd20-v100.</p> <p>To install</p> <pre><code>module purge\nmodule load GCC/8.2.0-2.31.1  \nmodule load cuDNN/8.0.5.39-CUDA-11.1.1\nmodule load Python\n\nvirtualenv -p python3 tf2env-Jul2021\nsource tf2env-Jul2021/bin/activate\npip install --upgrade tensorflow\ndeactivate\n</code></pre> <p>To test</p> <pre><code>module purge\nmodule load GCC/8.2.0-2.31.1  \nmodule load cuDNN/8.0.5.39-CUDA-11.1.1\nmodule load Python\nsource tf2env-Jul2021/bin/activate\npython\n&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; print (tf.__version__)\n&gt;&gt;&gt; print(\"Num GPUs Available:\", len(tf.config.experimental.list_physical_devices('GPU')))\n</code></pre>","title":"TensorFlow 2.5 installation"},{"location":"Torque_vs._SLURM/","text":"<p>For users familiar with Torque/MOAB system, please check the following sections for comparisons between using Torque and using SLURM system. This could help you to use SLURM commands and transfer your job script from PBS to SBATCH.</p>","title":"Torque vs. SLURM"},{"location":"Torque_vs._SLURM/#system-commands","text":"","title":"System Commands"},{"location":"Torque_vs._SLURM/#specifications-of-job-submission","text":"","title":"Specifications of Job submission"},{"location":"Torque_vs._SLURM/#environment-variables","text":"<p>Please also notice that the installation of a parallel compiler (such as OpenMPI, MVAPICH, ...) needs to be configured with job scheduler. If users would like to use their own installations of parallel compilers, please make sure they are compiled with appropriate settings.</p>","title":"Environment Variables"},{"location":"Transferring_data_with_Globus/","text":"<p>What is\u00a0Globus? </p> <p>Globus is a\u00a0free service to the MSU community\u00a0for secure, reliable research data management. Globus\u00a0gives\u00a0users\u00a0the ability to\u00a0move\u00a0and\u00a0share data\u00a0regardless of\u00a0user or\u00a0file location\u00a0through a single web browser-based interface.\u00a0Users\u00a0can manage data from any device (e.g.,\u202fsupercomputer, tape archive, lab cluster or equipment, public cloud, or personal computer/laptop) from anywhere in the world\u00a0using\u00a0their\u00a0existing\u00a0institutional\u00a0identities.\u00a0Use of Globus\u00a0removes data management roadblocks by providing unified access to all storage locations, making\u00a0it easy\u00a0to work with data while ensuring reliability\u00a0and security.\u00a0For more information on\u00a0Globus, check out  the Globus website here.</p> <p>Why\u00a0should\u00a0I use\u00a0Globus? </p> <p>Globus is ideal for moving large files and data transfers between ICER\u2019s HPCC or external research collaborators\u00a0because of its truly fire-and-forget method of transferring data.\u00a0After you\u00a0initiate\u00a0a file transfer,\u00a0Globus will work on your behalf to optimize transfer performance, monitor for transfer completion and correctness, and recover from network errors, credential expiration, and collection downtime without restarting the transfer. This allows you to\u00a0navigate away from File Manager, close the browser window, and even logout.\u00a0</p> <p>Transferring data with Globus</p> <p>The HPCC has a Globus data transfer endpoint, <code>msu#hpcc</code>. This can be used to do large data transfers to/from your personal computer, to/from collaborators or to/from external HPC sites. \u00a0You can also use it to share data.</p> <p>With Globus, you can create a transfer (to or from) between your computer and a folder on the HPCC which you have access to. This could be a folder in your home, research or scratch spaces.</p> <p>You can also create Globus \"shares\" to your external colleagues. They can use your created link to access the HPCC directory. You can not create a globus share on any folder in your home directory.\u00a0 You can only share folders in scratch space or any research space you have access to.\u00a0\u00a0 \u00a0</p> <p>To use Globus Online, please perform the following steps:</p> <ol> <li>If you do not have a globus account, create one     at\u00a0https://www.globus.org</li> <li>Log into the MSU Globus Online portal,     https://globus.msu.edu\u00a0and set up\u00a0a free Globus     Online account.</li> <li>If you wish to use Globus to transfer data to/from your local     computer, install the Globus Connect Personal tool.</li> <li>On the Globus\u00a0Start Transfer page,      enter <code>msu#hpcc</code>\u00a0as the end point on one side. This will pull up an authentication window. Use your MSU NetID for the     username, and your MSU NetID password for the passphrase. This will     set up an authenticated session that will last as long as specified     in the Credential Lifetime field, up to a limit of 2 weeks.</li> <li>Select and authenticate with the other endpoint for your transfer,     and initiate your transfer. An example can be seen from     How To Transfer Files with Globus.</li> <li>To share data in a HPCC folder accessible to you with other     persons,\u00a0 please check     How To Share Data Using Globus.</li> </ol> <p>More information about using Globus can be found on\u00a0Data transfer and sharing using Globus or Globus support page.</p> <p>For further training on the Science DMZ and how to use Globus, please self-enroll in ICER's DMZ Globus Training\u00a0D2L course.</p> <p>ICER's DMZ Globus Training - Globus Walkthrough Documents (optional)</p> <ol> <li> <p>Find and Connect to HPCC using Globus</p> </li> <li> <p>Transfer from PC to HPCC using Globus</p> </li> <li> <p>Transferring Data between Endpoints Using Globus</p> </li> <li> <p>Transferring Data with Google using Globus</p> </li> <li> <p>Sharing Data using Globus</p> </li> </ol>","title":"Transferring data with Globus"},{"location":"Trimmomatic/","text":"<p>Trimmomatic is a tool for trimming Illumina FASTQ data and removing adapters. \u00a0When data is sequenced on Illumina, adapters are added for the fragments to attach to the beads. \u00a0If these adapters are not removed they can result in false assembly or other issues. \u00a0Additionally, the quality of the sequences varies across the length of the read, and poorer quality regions can be trimmed using Trimmomatic. \u00a0Running Trimmomatic is a good first step in quality filtering your Illumina data.\u00a0</p> <p>To run it on the HPCC (for example trimming paired-end reads):</p>   <pre><code>java -jar /opt/software/Trimmomatic/0.36-Java-1.8.0_92/trimmomatic-0.36.jar PE [-threads &lt;threads] [-phred33 | -phred64] [-trimlog &lt;logFile&gt;] &lt;input 1&gt; &lt;input 2&gt; &lt;paired output 1&gt; &lt;unpaired output 1&gt; &lt;paired output 2&gt; &lt;unpaired output 2&gt; &lt;step 1&gt; ...\n</code></pre>   <p>Read\u00a0the manual\u00a0for how to use it. Note that\u00a0the adapter sequence files are in\u00a0<code>/opt/software/Trimmomatic/0.36-Java-1.8.0_92/adapters/</code>. If you couldn't find the adapters you need in that directory, you will need to obtain them from elsewhere (for example asking the person who ran the library prep for you).</p>","title":"Trimmomatic"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/","text":"","title":"Trinity for RNA-seq de novo assembly"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#loading-module","text":"<p>Take loading Trinity 2.6.6 as an example, we run:</p> <pre><code>module purge\nmodule load icc/2017.4.196-GCC-6.4.0-2.28 impi/2017.3.196 Trinity/2.6.6\n</code></pre>","title":"Loading module"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#most-basic-run-transcript-assembly","text":"<p>A typical Trinity command for assembling strand-specific paired-end RNA-seq data would look like:</p> <p>A typical run of Trinity</p> <pre><code>Trinity \\\n  --seqType fq \\\n  --max_memory 2G \\\n  --left reads.left.fq \\\n  --right reads.right.fq \\\n  --SS_lib_type RF \\\n  --CPU 10\n</code></pre> <p>This will generate output files in a new directory \"<code>trinity_out_dir</code>\" in the working directory. Among them, the assembled transcripts file is \"<code>Trinity.fasta</code>\". For more detail, check out\u00a0https://github.com/trinityrnaseq/trinityrnaseq/wiki </p> <p>When you submit the above command as a job to the cluster, you need to request 10 CPUs in the sbatch script with the following lines (in addition to your other sbatch directives):</p> <p>sbatch code snippet</p> <pre><code>#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=10\n</code></pre>","title":"Most basic run (transcript assembly)"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#transcript-quantification","text":"<p>Trinity provides abundant utility scripts for post-assembly analysis, such as quality assessment, transcript quantification and differential expression tests. For some of them, external software tools need to be installed separately (that is, they are not bundled with Trinity). For example, for the transcript quantification step, we will need one of RSEM, eXpress, kalllisto and salmon (cf.\u00a0https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-Transcript-Quantification). We have made all these four available on the HPCC. As instructed by Trinity, \"the tools should be available via your PATH setting\". So, in the next example where we choose to use RSEM to align reads to the assembled transcript and then quantify transcript abundance, we first set the\u00a0<code>PATH</code>\u00a0variable so that RSEM can be automatically searched for by trinity.</p> <p>Using RSEM for transcript quantification</p> <pre><code># Assuming\n#    1) you've loaded Trinity module already and\n#    2) your current working directory is trinity_out_dir generated from the previous assembly step. \n\nexport PATH=/opt/software/RSEM/1.3.1-GCCcore-6.4.0/usr/local/bin:$PATH\n\n/opt/software/Trinity/2.6.6/util/align_and_estimate_abundance.pl --seqType fq --transcripts Trinity.fasta \\\n    --est_method RSEM \\\n    --left ../reads.left.fq \\\n    --right ../reads.right.fq \\\n    --SS_lib_type RF \\\n    --aln_method bowtie \\\n    --trinity_mode \\\n    --prep_reference \\\n    --thread_count 10 \\\n    --output_dir RSEM_out\n</code></pre> <p>The RSEM computation generates two primary output files containing estimated abundances in the subdirectory\u00a0<code>RSEM_out</code>\u00a0as specified in the command above:\u00a0<code>RSEM.isoforms.results</code>\u00a0(transcript level) and\u00a0<code>RSEM.genes.results</code>\u00a0(gene level).</p>","title":"Transcript quantification"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#more-utilities","text":"<p>Please consult\u00a0https://github.com/trinityrnaseq/trinityrnaseq/wiki\u00a0for detail.</p> <p>Note that a few R packages are needed for differential expression analysis (https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-Differential-Expression). These have been installed in\u00a0<code>R/4.0.2</code>\u00a0which can be loaded by</p> <pre><code>module purge; module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2\n</code></pre>","title":"More utilities"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#version-note","text":"<p>The latest version is 2.91. After loading it, you may load R 4.0.2 for DE analysis.</p> <p><code>module purge</code> <code>module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 Trinity/2.9.1</code> <code>module load R/4.0.2</code></p>","title":"Version note"},{"location":"Tutorials-and-Exercises_34963691.html/","text":"","title":"Tutorials and Exercises 34963691.html"},{"location":"Tutorials-and-Exercises_34963691.html/#teaching-tutorials-and-exercises","text":"","title":"Teaching : Tutorials and Exercises"},{"location":"Tutorials-and-Exercises_34963691.html/#file-permission-in-research-space","text":"<p>How to make users able to access files or directories in their research space?</p>","title":"File Permission in Research Space"},{"location":"Tutorials-and-Exercises_34963691.html/#hpcc-job-submission-workflow","text":"<p>Understand basic workflow for job submission to the clusters.</p>","title":"HPCC Job Submission Workflow"},{"location":"Tutorials-and-Exercises_34963691.html/#running-multiple-jobs-sequentially","text":"","title":"Running multiple jobs sequentially"},{"location":"Tutorials-and-Exercises_34963691.html/#submitting-multiple-jobs-simultaneously","text":"","title":"Submitting multiple jobs simultaneously"},{"location":"Tutorials-and-Exercises_34963691.html/#job-arrays-run-multiple-similar-jobs-simultaneously","text":"","title":"Job Arrays - Run multiple similar jobs simultaneously"},{"location":"User_Created_Modules/","text":"<p>This is for advanced usage, since creating your own module files for software access is usually not necessary.</p> <p>If you develop or install your own software, you might consider writing a modulefile to help manage your environment variables. HPCC presently uses the LMOD module package, developed at TACC. The following is a typical module file with comments. Name your files with a <code>.lua</code> extension.</p> <pre><code>-- -*- lua -*- \n help( \n [[ \n Describe your software here. \n ]]) \n\n -- comments are prefaced with two dashes \n\n whatis(\"Description: Name of software\") \n whatis(\"URL:  www.ucc.org \") \n\n local install_path = \"/mnt/home/ongbw/opt/mysoftware\" \n\n -- set an environment variable \n setenv(\"MYSOFTWARE_HOME\",install_path) \n\n -- add to PATH variable \n prepend_path('PATH', pathJoin(install_path,\"bin\")) \n\n -- add Library Paths \n prepend_path('LD_LIBRARY_PATH',pathJoin(install_path,\"lib\")) \n prepend_path('LIBRARY_PATH',pathJoin(install_path,\"lib\")) \n\n -- add include paths \n prepend_path('INCLUDE',pathJoin(install_path,\"include\"))\n</code></pre>","title":"User Created Modules"},{"location":"Using_Git_from_a_Unix_Shell/","text":"","title":"Using Git from a Unix Shell"},{"location":"Using_Git_from_a_Unix_Shell/#overview","text":"<p>This document applies to people attempting to use the standard Git client to access <code>vcs.icer.msu.edu</code> from a Unix shell, such as provided on <code>gateway.hpcc.msu.edu</code>. This document also applies to people using Git in Cygwin or MSysGit in MSys on Windows, or using Git from the Fink project or the Darwin Ports project and running in a Mac OS X terminal window.</p>","title":"Overview"},{"location":"Using_Git_from_a_Unix_Shell/#anonymous-access","text":"<p>A project which has enabled anonymous access to its repository can be cloned via the following command:</p> <p><code>git clone</code> <code>http://vcs.icer.msu.edu/git-repos/</code>project<code>.git</code></p> <p>where project is the name of the project.</p> <p>A repository cloned in this manner cannot be used to push to the repository from which it was cloned, but can be used to repeatedly pull in updates from it.</p>","title":"Anonymous Access"},{"location":"Using_Git_from_a_Unix_Shell/#authenticated-access-via-https","text":"<p>A designated developer for a project can clone the project's repository with the following command:</p> <p><code>git clone https://</code>username<code>@vcs.icer.msu.edu/git-repos/</code>project<code>.git</code></p> <p>where username is either the MSU Net ID or other specially assigned ID of the developer and project is the name of the project.</p> <p>The person attempting to access a repository via this method will be prompted for a password. Note that, when needed, the person's password will be transmitted over a secure channel.</p> <p>A repository cloned in this manner can be used to push updates to and pull updates from the repository from which it was cloned.</p>","title":"Authenticated Access via HTTPS"},{"location":"Using_Git_from_a_Unix_Shell/#authenticated-access-via-ssh","text":"<p>A designated developer for a project can clone the project's repository with the following command:</p> <p><code>git clone ssh://</code>username<code>@vcs.icer.msu.edu/git-repos/</code>project<code>.git</code></p> <p>where username is the MSU Net ID of a designated developer and project is the name of the project. Note that designated develoeprs not having valid MSU Net IDs cannot presently access a repository with this command; such developers should use the HTTPS access method instead.</p> <p>The person attempting to access a repository via this method will be prompted for a password, unless that person has previously arranged for key-based authentication. Note that, when needed, the person's password will be transmitted over a secure channel.</p> <p>A repository cloned in this manner can be used to push updates to and pull updates from the repository from which it was cloned.</p>    **HELPFUL TIP:** If you are accessing your git repository from the HPCC, change **vcs.icer.msu.edu** to **vcs-00-dmz.dmz** in your git clone command. This change will allow you to access your repository without retyping your password.  Note: this alternative URL will not work from anywhere but the HPCC.  If you have already have a clone of your repository you can manually change the URL by editing the .git/config file from within your repository.","title":"Authenticated Access via SSH"},{"location":"Using_Python_in_HPCC_with_virtualenv/","text":"<p>Python applications usually use packages and modules that require specific version of libraries. This means one installed application may conflict with another application due to using the same library but with different versions. It is difficult to meet the requirements of every application by one global Python installation. To resolve this issue, users can create an isolated virtual environment with a particular version of Python in a self-contained directory of their home or research space. Any package and the dependent libraries installed inside the directory can be available only through the virtual environment. Different applications can then use different virtual environments to avoid any conflict.</p>","title":"Using Python in HPCC with virtualenv"},{"location":"Using_Python_in_HPCC_with_virtualenv/#create-and-use-virtual-environments","text":"<p>To create python virtual environments, please make sure your preferred version of Python is loaded. It is also a good idea to create a directory of the python version to store different environments and their applications:</p> <pre><code>[UserName@dev-intel18 ~]$ module list Python\n\nCurrently Loaded Modules Matching: Python\n  1) Python/3.6.4\n\n[UserName@dev-intel18 ~]$ which python\n/opt/software/Python/3.6.4-foss-2018a/bin/python\n\n[UserName@dev-intel18 ~]$ mkdir Python3.6.4\n[UserName@dev-intel18 ~]$ cd Python3.6.4\n[UserName@dev-intel18 Python3.6.4]$\n</code></pre> <p>Currently, two common tools can be used to create Python virtual environments. Please use only one of them:</p> <ol> <li> <p>venv is available for Python 3.3 and later by default. The application <code>pip</code> and <code>setuptools</code> should be ready to use in HPCC system. A virtual environment can be created by running \"<code>python3 -m venv &lt;DIR&gt;</code>\", where <code>&lt;DIR&gt;</code> is the directory of the created environment. Following is an example of the command, and the directory <code>tutorial</code> is created for the virtual environment.</p> <pre><code>[UserName@dev-intel18 Python3.6.4]$ python3 -m venv tutorial  \n[UserName@dev-intel18 Python3.6.4]$ ls tutorial  \nbin  include  lib  lib64  pyvenv.cfg\n</code></pre> </li> <li> <p>virtualenv supports all Python versions. By default, HPCC system has     <code>pip</code>, <code>setuptools</code> and <code>wheel</code> installed and available. Similarly     to venv, a virtual environment can be created by executing     \"virtualenv \\&lt;DIR&gt;\", where applications of\u00a0the virtual environment are installed in <code>tutorial</code> directory.</p> <p>[UserName@dev-intel18 Python3.6.4]$ virtualenv tutorial Using base prefix '/opt/software/Python/3.6.4-foss-2018a' New python executable in /mnt/home/UserName/Python3.6.4/tutorial/bin/python Installing setuptools, pip, wheel...done. [UserName@dev-intel18 Python3.6.4]$ ls tutorial bin  include  lib  lib64  pip-selfcheck.json</p> </li> </ol> <p>Please use one of them to create the virtual environment.</p> <p>The created <code>tutorial</code> environment can now be used by sourcing the script file <code>activate</code> under the <code>bin</code> directory:</p> <pre><code>[UserName@dev-intel18 Python3.6.4]$ source tutorial/bin/activate\n(tutorial) [UserName@dev-intel18 Python3.6.4]$\n</code></pre> <p>where the name inside the\u00a0parentheses <code>(tutorial)</code> in front of the prompt line shows the current Python environment. To leave the environment, just run \"deactivate\":</p> <pre><code>(tutorial) [UserName@dev-intel18 Python3.6.4]$ deactivate\n[UserName@dev-intel18 Python3.6.4]$\n</code></pre> <p>and the parentheses disappear. Any time, users want to use <code>tutorial</code> environment. Simply source the file again: \"<code>source ~/Python3.6.4/tutorial/bin/activate</code>\" and the environment is back. More information can be found about venv or virtualenv.</p>","title":"Create and use virtual environments"},{"location":"Using_Python_in_HPCC_with_virtualenv/#install-packages-from-pypi-using-pip","text":"<p>The most common usage of pip is to install python packages from the Python Package Index with a requirement specifier. Users can also check other usages with pip. Below, some of the common usage scenarios are introduced.</p> <p>To install the latest version of a python package, users can run \"<code>pip install &lt;Package Name&gt;</code>\", for example, using <code>sympy</code> as <code>&lt;Package Name&gt;</code>:</p> <pre><code>(tutorial) [UserName@dev-intel18 Python3.6.4]$ pip install \"sympy\"\nCollecting sympy\nUsing cached https://files.pythonhosted.org/packages/21/21/f4105795ca7f35c541d82c5b06be684dd2f5cb4f508fb487cd7aea4de776/sympy-1.4-py2.py3-none-any.whl\nCollecting mpmath&gt;=0.19 (from sympy)\nInstalling collected packages: mpmath, sympy\nSuccessfully installed mpmath-1.1.0 sympy-1.4\n</code></pre> <p>To install a specific version of a python package, please run \"<code>pip install &lt;Package Name&gt;==&lt;Version Number&gt;</code>\". For example, install <code>numpy</code> with <code>&lt;version number&gt;</code> as <code>1.16.2</code>:</p> <pre><code>(tutorial) [UserName@dev-intel18 Python3.6.4]$ pip install \"numpy==1.16.2\"\nCollecting numpy==1.16.2\nDownloading https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n    |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17.3MB 10.5MB/s\nInstalling collected packages: numpy\nSuccessfully installed numpy-1.16.2\n</code></pre> <p>To upgrade an already installed package to the latest from PyPI, users can run \"<code>pip install --upgrade</code>\". For example, upgrade the installed package <code>numpy</code>:</p> <pre><code>(tutorial) [UserName@dev-intel18 Python3.6.4]$ pip install --upgrade numpy\nCollecting numpy\nUsing cached https://files.pythonhosted.org/packages/e5/e6/c3fdc53aed9fa19d6ff3abf97dfad768ae3afce1b7431f7500000816bda5/numpy-1.17.2-cp36-cp36m-manylinux1_x86_64.whl\nInstalling collected packages: numpy\nFound existing installation: numpy 1.16.2\n    Uninstalling numpy-1.16.2:\n    Successfully uninstalled numpy-1.16.2\nSuccessfully installed numpy-1.17.2\n</code></pre> <p>With pip, you can also list all installed packages and their versions with the command \"<code>pip freeze</code>\":</p> <pre><code>(tutorial) [UserName@dev-intel18 Python3.6.4]$ pip freeze\nmpmath==1.1.0\nnumpy==1.17.2\nsympy==1.4\n</code></pre> <p>For more detail, see the pip docs, which includes a complete Reference Guide. More examples of using <code>virtualenv</code> and <code>pip install</code> can be found in  TensorFlow page.</p>","title":"Install packages from PyPI using pip"},{"location":"Using_Python_in_HPCC_with_virtualenv/#pythonpath-environment-variable","text":"<p>You can use the environment variable <code>PYTHONPATH</code> to include the packages already installed by the same python version in other directories. By adding the <code>site-packages</code> paths to <code>PYTHONPATH</code> environment variable and separating them by \"<code>:</code>\" sign:</p> <pre><code>(tutorial) [UserName@dev-intel18 Python3.6.4]$ export PYTHONPATH=~/Python3.6.4/tutorial/lib/python3.6/site-packages:/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages\n(tutorial) [UserName@dev-intel18 Python3.6.4]$ pip freeze\nabsl-py==0.5.0\nalabaster==0.7.12\nappdirs==1.4.3\nartemis==0.1.4\n...\nnose==1.3.7\nnumpy==1.17.2\nnumpydoc==0.8.0\n...\nsuspenders==0.2.6\nsympy==1.4\ntensorboard==1.10.0\ntensorflow==1.10.1\n...\nvirtualenv==15.1.0\nwcwidth==0.1.7\nWerkzeug==0.14.1\nxopen==0.3.5\n</code></pre> <p>all packages inside the paths are now ready to use. Please make sure the <code>site-packages</code> path of the current environment is set the first in\u00a0<code>PYTHONPATH</code> variable. If a package is installed in more than one path (possibly with different versions), the package of the first path showing in the variable (<code>PYTHONPATH</code>) will be used.</p>","title":"PYTHONPATH environment variable"},{"location":"Using_Version_Control_Systems/","text":"","title":"Using Version Control Systems"},{"location":"Using_Version_Control_Systems/#overview","text":"<p>ICER provides git and svn to assist researchers in collaboratively developing code. \u00a0We recommend hosting the repositories at http://gitlab.msu.edu.</p> <ul> <li>The following site provides a nice 15 minute, hands-on tutorial for     using git:\u00a0http://try.github.com/</li> <li>The following git manual is much more     comprehensive:\u00a0http://git-scm.com/docs</li> </ul>","title":"Overview"},{"location":"Using_Version_Control_Systems/#accessing-repositories-with-ssh-key-based-authentication","text":"","title":"Accessing Repositories with SSH Key-Based Authentication"},{"location":"Using_Version_Control_Systems/#using-git-from-a-unix-shell","text":"","title":"Using Git from a Unix Shell"},{"location":"Using_conda/","text":"<p>This wiki is based on conda version prior to 4.6. It may not apply to 4.6 and later versions (where, for example, conda environment is activated by conda activate ). Check\u00a0the manual before you start using conda.</p>","title":"Using conda"},{"location":"Using_conda/#introduction","text":"<p>Anaconda\u00a0is a distribution of python that contains a free, easy-to-install package manager, environment manager and Python distribution with a collection of 1,000+ open source packages with free community support. Conda is the package and environment manager that installs and updates packages and their dependencies, included in all versions of Anaconda.\u00a0</p> <p>Anaconda needs to be installed in your home or project directory and so does any software which will be installed using it.\u00a0 This allows users to have full control of their programs.</p> <p>This wiki serves only as a mini-version of conda tutorial. You are highly recommended to read the\u00a0User Guide.</p> <p>conda to install and run a program called \"macs2\".\u00a0First we go to Anaconda Cloud to search for it. Once it's found, we pick a source and click on the link to get the installation command (i.e.,\u00a0<code>conda install -c bioconda macs2</code>\u00a0below)</p> <p>Installation of macs2</p> <pre><code>export PATH=$PATH:$HOME/anaconda3/bin\nconda create --name macs2\nsource activate macs2\nconda install -c bioconda macs2 \nsource deactivate\n</code></pre> <p>To launch it later on (for example, in your SLURM job submission script):</p> <p>Using it</p> <pre><code>export PATH=$PATH:$HOME/anaconda3/bin\nsource activate macs2\n#[your commands]\nsource deactivate\n</code></pre>","title":"Introduction"},{"location":"Using_conda/#installation-of-anaconda","text":"<p>Visit https://docs.anaconda.com/anaconda/install/linux\u00a0for installation instruction. And\u00a0Linux installers are available from\u00a0https://www.anaconda.com/download/#linux</p> <p>Note:</p> <p>1) Choose 64-Bit (x86) Installer.</p> <p>2) Version 2 or 3? You should choose python2 or python3 depending on what your software requirements are.\u00a0 Some libraries/packages only work on Python 2 so you should check with the python package you ultimately want to use.\u00a0 If you don't know, select the latest version 3.\u00a0</p> <p>3) Anaconda now partners with Microsoft and asks if you want to install \"VSCode\"\u00a0 THIS WILL NOT WORK. \u00a0The installation requires administrative privileges that you don't have.\u00a0\u00a0</p> <p>You can download the install file directly into your home directory by first getting the URL of the download (right-click on the download link and select \"copy link address\" or \"copy link location\").\u00a0 Then open a terminal to log onto a dev-node, and use this command:</p> <p><code>curl -O &lt;download link&gt;</code> </p> <p>An example link can be: https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh</p> <p>Once the file\u00a0<code>Anaconda3-2022.05-Linux-x86_64.sh</code>\u00a0is downloaded, you can run the command:\u00a0</p> <p><code>bash Anaconda3-2022.05-Linux-x86_64.sh</code></p> <p>During Installation the installer will ask you</p> <ol> <li>Accept licence agreement : you must type yes to agree.\u00a0 We can not     install for the who system, you must install yourself and agree to     this license</li> <li>Where to install : you can install in any location in your home     directory, the default is Anaconda2 or Anaconda3.\u00a0 Note you can have     multiple installations of Anaconda</li> <li>Initialize Anaconda (run conda init?) : please answer \"no\". A more     careful setup is suggested. Please follow the instruction in the     next section after this installation.</li> <li>Install VS Code?\u00a0 This is a new bundle that comes with Anaconda.\u00a0\u00a0     This won't work and you must answer \"no\"</li> </ol> <p>Please remember where Anaconda is installed in the step 2 above. The installation path will be used for module setup in the next section.</p> <p>In case your installed Anaconda is auto-activated on startup, please run the command:</p> <p><code>conda config --set auto_activate_base false</code></p> <p>after close and re-open your current shell. Please read the next section for the setup and the start-up of your Anaconda.</p>","title":"Installation of Anaconda"},{"location":"Using_conda/#module-setup-for-anaconda","text":"<p>In order to have no conflict between user-installed Anaconda and system-installed Python versions, a setup in the <code>$HOME/.bashrc</code> file for module system is necessary. The <code>.bashrc</code>file under your home directory can be used to do environment setting for every time you log in to a HPCC node. You can modify the file by an editor program (such as nano, vim) for Anaconda setup:</p> <p>(1) Please check if any conda initialization or setup script is in the     <code>~/.bashrc</code> file. The script starts with <code># &gt;&gt;&gt; conda init &gt;&gt;&gt;</code> and     ends with <code># &lt;&lt;&lt; conda init &lt;&lt;&lt;</code>:</p> <pre><code># &gt;&gt;&gt; conda init &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup= ... ...\n... ...\n... ...\nunset __conda_setup\n# &lt;&lt;&lt; conda init &lt;&lt;&lt;\n</code></pre> <p>If there is, please remove or comment all of the command lines out (by adding the sign \"<code>#</code>\" in its front). If there is any <code>export</code> command line to add the installed Anaconda bin path to <code>PATH</code> environment variable, please also comment it out.</p> <p>(2) If your installed Anaconda path is <code>~/anaconda3</code> or <code>~/anaconda2</code>, you can skip this step and go to the next one. If not, please set the variable <code>CONDA3PATH</code> to be the place of installed Anaconda3 by adding the command line:</p> <pre><code>export CONDA3PATH=&lt;your installed Anaconda3 path&gt;\n</code></pre> <p>in the <code>~/.bashrc</code> file. If you have Anaconda2 installed, you may set the variable <code>CONDA2PATH</code> to be the installed path of Anaconda2. After the environment setup, please save the <code>~/.bashrc</code> file. Log out and log into a dev node.</p> <p>(3) Whenever you want to use Anaconda3 commands, just run:</p> <pre><code>module load Conda/3\n</code></pre> <p>to initiate conda and <code>CONDA3PATH</code> is in your <code>PATH</code> variable. Loading the module will also replace any loaded Python module so there is no conflict. If <code>CONDA2PATH</code> variable is set up, please load <code>Conda/2</code> module for using Anaconda2.</p> <p>(4) If you would like to have your installed Anaconda3 to be the default Python program when your session starts up, you can add the command:</p> <pre><code>module load Conda/3 2&gt; /dev/null\n</code></pre> <p>to the\u00a0 <code>~/.bashrc</code> file after <code>export CONDA3PATH=...</code> command line. Using Conda module can also avoid a conflict with Web-based Remote Desktop gateway. If anaconda2 is installed, you can use <code>Conda/2</code> instead of <code>Conda/3</code>\u00a0module.</p>","title":"Module Setup for Anaconda"},{"location":"Using_conda/#managing-conda","text":"<p>To manage conda, you should use a Terminal window to check the following.</p> <p>You can begin by checking if conda is installed by using the command:\u00a0</p> <p>Checking the version</p> <pre><code>conda --version\n</code></pre> <p>If it is installed, the number of the version you have installed should show up after the previous command in Terminal.</p> <p>Version example</p> <pre><code>conda 4.2.9\n</code></pre> <p>If there\u2019s an error message, check to see if you\u2019re logged into the same account that you used to install Anaconda or Miniconda. You can update conda to the current version by typing in to your command line the following in line 1:</p> <p>Updating conda</p> <pre><code>conda update conda\nProceed ([y]/n)?\n</code></pre> <p>Line 2 asks you if you wish to proceed.\u00a0</p> <p>Type \u2018y\u2019 and hit Enter to update.</p>","title":"Managing conda"},{"location":"Using_conda/#managing-environments","text":"<p>You will use a Terminal window for the following steps.</p> <p>To create an environment, use the following command:</p> <p>Creating an environment</p> <pre><code>conda create --name testing biopython\n</code></pre> <p>Two things to note from this: you can replace \u2018testing\u2019 with whatever name you want, and biopython is just the program that was chosen for this. If your program is not yet installed, you will be prompted to install a new set of packages with a similar message to how you are prompted if you want to update conda. Type \u2018y\u2019 and hit \"Enter\" to confirm that you want to install the package. To activate this new environment, choose the appropriate command (keep in mind that our environment name is \u2018testing\u2019):</p> <p>Activate an environment</p> <pre><code>source activate testing\n</code></pre> <p>To deactivate an environment and return to the root, use the following command:</p> <p>Deactivating an environment</p> <pre><code>source deactivate testing\n</code></pre> <p>Now let\u2019s say you want to create a new environment and install a different version of Python along with a couple of packages. We will use the Astroid and Babel packages.</p> <p>Run this command:</p> <p>Environment with multiple packages</p> <pre><code>conda create --name test2 python=3.5 astroid babel\n</code></pre> <p>Your new environment is named \u2018test2\u2019 (which can be interchanged for whatever name you like when you\u2019re running an environment) and it has Python 3, Astroid, and Babel installed.</p> <p>If you wish to find out more about the create command, run:\u00a0</p> <p>create help</p> <pre><code>conda create --help\n</code></pre> <p>To display your environments, use the command:\u00a0</p> <p>Displaying environments</p> <pre><code>conda info --envs\n</code></pre> <p>Your environments should be outputted, and there will be an asterisk next to the active environment.</p> <p>Your current environment should be in (parentheses) or [brackets] in front of your prompt. To switch to another environment, just use the activate command with the environment name. If you wish to make a copy of an environment, use the following command:\u00a0</p> <p>Copying an environment</p> <pre><code>conda create --name hello --clone testing\n</code></pre> <p>You can verify that the clone was made by displaying your environments.</p> <p>You can delete an environment by using:\u00a0</p> <p>Deleting an environment</p> <pre><code> conda remove --name hello --all\n</code></pre> <p>Since we don\u2019t want a clone of testing, we decided to delete it. You can check that it is deleted by displaying your environments.</p>","title":"Managing Environments"},{"location":"Using_conda/#managing-python","text":"<p>Use a Terminal window again for these steps.</p> <p>You can search for which versions of Python are available to be installed by using the command:\u00a0</p> <p>Searching for available Python versions</p> <pre><code>conda search --full-name python\n</code></pre> <p>\u201c--full-name\u201d lists only the packages whose full name is exactly \u2018python\u2019.</p> <p>If you want to search packages which contain the text \u2018python\u2019, but that is not its full name, use the command:\u00a0</p> <p>Versions with the word \"python\"</p> <pre><code>conda search python\n</code></pre> <p>We will go over how to install a new verison of Python without overwriting our original Python version. Let\u2019s start off by creating a new environment called py3. Use the create command:</p> <p>Environment for installing a new version of Python</p> <pre><code>conda create --name py3 python=3\n</code></pre> <p>If you get prompted to install a new package, type \u2018y\u2019 and hit \"Enter\". Display your list of environments with</p> <p>Displaying new environment</p> <pre><code>conda info --envs\n</code></pre> <p>to check if py3 was successfully created, and activate py3 by using:\u00a0</p> <p>Activating py3</p> <pre><code>source activate py3\n</code></pre> <p>Now check the version of Python being used with:\u00a0</p> <p>Checking py3's Python version</p> <pre><code>python --version\n</code></pre> <p>Now we can switch to one of our previous environments that we first made when installing conda. Let\u2019s switch back to testing. Using the command:\u00a0</p> <p>Switching to an environment with a different Python version</p> <pre><code>source activate testing\n</code></pre> <p>Now you check which version we are using with the same used earlier.</p> <p>This should show the same version that was used when you first installed conda.</p> <p>Now let\u2019s deactivate testing with the command:\u00a0</p> <p>Deactivating testing</p> <pre><code>source deactivate testing\n</code></pre>","title":"Managing Python"},{"location":"Using_conda/#managing-packages","text":"<p>Use the Terminal window for this section.</p> <p>The following command will allow you to see the packages installed in your environment:\u00a0</p> <p>Listing packages command</p> <pre><code>conda list\n</code></pre> <p>This website provides a list of packages available to install:\u00a0http://docs.continuum.io/anaconda/pkg-docs.html</p> <p>The package that we will install to practice is called \u201cbeautifulsoup4\u201d \u2013 this is a package which provides a Python library designed for screen-scraping/MIT.</p> <p>To search if this package is available to be installed, try to use the following command:\u00a0</p> <p>Searching for a package</p> <pre><code>conda search beautifulsoup4\n</code></pre> <p>If it displays the package, then it is available to install. Use the following command to install a package:\u00a0</p> <p>Install a Package</p> <pre><code>conda install --name test2 beautifulsoup4\n</code></pre> <p>You will be prompted if you want to proceed. Type \u2018y\u2019 and hit \"Enter\"</p> <p>Now activate the environment that this package was installed in, in this case test2:\u00a0</p> <p>Activating environment with the new package</p> <pre><code>source activate test2\n</code></pre> <p>Now let\u2019s check if the package is installed by using:</p> <p>Listing the packages</p> <pre><code> conda list\n</code></pre> <p>It should be listed if it is installed.</p>","title":"Managing Packages"},{"location":"Using_conda/#installing-packages","text":"<p>Use the Terminal window and an internet browser for this section.</p> <p>Not all packages can be installed with conda install, so we can pull packages from\u00a0anaconda.org. The following website has other packages that we can use and install:\u00a0http://anaconda.org. For the example, we will install a package called \u2018bottleneck\u2019.</p> <p>Go to this page and use the search bar in the upper left corner to search for \u2018bottleneck\u2019.</p> <p>Choose the most frequently downloaded version of bottleneck. The format of the text should be something similar to \u2018conda-forge/bottleneck1.2.1\u2019. This is split into two places that you can click on. The first is \u2018conda-forge\u2019, which is the owner of the package. We can ignore this one and click on \u2018bottleneck1.2.1\u2019. \u00a0At the middle to bottom portion of the page, there should be a command listed to install this package. In this case, the command is:</p> <pre><code>conda install -c conda-forge bottleneck\n</code></pre> <p>Activate your test2 environment and enter this command to install bottleneck.</p> <p>Installing a package from Anaconda.org</p> <pre><code>source activate test2\nconda install -c conda-forge bottleneck\n</code></pre> <p>You will be prompted if you want to proceed, again type \u2018y\u2019 and hit \"Enter\". We can now check if the new package is installed.</p> <pre><code>conda list\n</code></pre> <p>If it is installed, it show in the Terminal window after using the previous command.</p>","title":"Installing Packages"},{"location":"Using_conda/#installing-a-package-with-pip","text":"<p>Pip is used to install packages that are not available from conda or\u00a0anaconda.org. It is only used as a package manager and does not manage environments. If you don\u2019t have the environment \u2018test2\u2019 (or whatever environment you want to install a package in) active, activate it now. We will install a program named \u2018see\u2019 into this environment. The command for this is:\u00a0</p> <p>Installing with pip</p> <pre><code>pip install see\n</code></pre> <p>Now use the listing packages command to check if \u2018see\u2019 is installed.</p> <p>Note: You can replace \u2018see\u2019 with whatever pip package that you are trying to install. This is just an example.</p>","title":"Installing a Package with pip"},{"location":"Using_conda/#installing-commercial-packages","text":"<p>If you wish to install a commercial package, follow the format for installing packages from conda, which was described earlier in this document in the \"Managing Packages\" section.</p>","title":"Installing Commercial Packages"},{"location":"Using_conda/#removing-packages-environments-or-conda","text":"<p>To remove a package from an environment, use the following command:</p> <p>Removing a package from an environment</p> <pre><code>conda remove --name ENVIRONMENT_NAME PACKAGE_NAME\n</code></pre> <p>You would replace ENIVRONMENT_NAME with the name of your environment, and also replace PACKAGE_NAME with the name of the package. For an example, we will remove the \u2018astroid\u2019 package from test2:\u00a0</p> <p>Ex. Removing a package</p> <pre><code>conda remove --name test2 astroid\n</code></pre> <p>You will be prompted if you want to proceed, type \u2018y\u2019 and hit \"Enter\". Now use the list command to check if \u2018astroid\u2019 has been removed.</p> <p>To remove an environment, use the command:</p> <p>Removing an environment</p> <pre><code>conda remove --name ENVIRONMENT_NAME --all\n</code></pre> <p>Replace ENVIRONMENT_NAME with the environment you want to remove. For an example, we will create a new environment called \u2018trash\u2019. Remembering how to create an environment from before, use the command:\u00a0</p> <p>Ex. Removing an Environment</p> <pre><code>conda create --name trash python=3.5\n</code></pre> <p>Check that this environment exists by using:\u00a0</p> <p>Ex. Removing an Environment</p> <pre><code>conda info --envs\n</code></pre> <p>Now we run:\u00a0</p> <p>Ex. Removing an Environment</p> <pre><code>conda remove --name trash --all\n</code></pre> <p>You will be prompted to proceed, type \u2018y\u2019 and hit \"Enter\". Now check if the environment exists by looking at all of your environments again with the same command used a moment ago.</p> <p>The environment \u2018trash\u2019 should no longer be there.</p> <p>If you want to remove anaconda or miniconda, you can use the following commands:</p> <p>Removing anaconda or miniconda</p> <pre><code>rm -rf ~/miniconda\nrm -rf ~/anaconda\n</code></pre>","title":"Removing Packages, Environments, or conda"},{"location":"Virtual_Terminals/","text":"","title":"Virtual Terminals"},{"location":"Virtual_Terminals/#gnu-screen","text":"<p>GNU Screen is a program that allows you to create a virtual terminal session inside a single terminal window. It is useful for dealing with multiple programs from a command line interface and for separating programs from the Unix shell that started the program.</p> <p>To start screen, simply type</p> <pre><code>screen\n</code></pre> <p>at the command prompt. Even if it looks like nothing has happened, you are now in a new window within screen.\u00a0\u00a0 Describing the details of how screen works is beyond the scope of this entry, but it allows you to leave a session running even after you've logged out (or disconnected because of network issues).\u00a0\u00a0</p> <p>One challenge with the 'screen' command is that by default you can't load any modules or other system commands.\u00a0 That's because 'screen' does not run the shell profile commands (in /etc/profile and /etc/profile.d).\u00a0 HPCC configures the modules system and several other system variables/settings in these profile scripts. Without running them, the module system won't work.</p> <p>However, you can ask screen to run this profile by including the line shell=-$SHELL in the screen config file \".screenrc\" in your home directory.\u00a0 To make a screen config file, try this (provided you don't already have a .screenrc file)</p> <p>create default .screenrc</p> <pre><code>echo 'shell -$SHELL' &gt;&gt; ~/.screenrc\n</code></pre> <p>You'll have to exit and restart screen to see the changes.</p>","title":"GNU Screen"},{"location":"Virtual_Terminals/#screen-commands","text":"<p>To send commands to screen (instead of the window you're working in), you preface them with Ctrl-a, i.e., you type the Ctrl key and \"a\" together, release both keys, then type the next letter to invoke the command.</p> <ul> <li> <p>Ctrl-A then:</p>          ? Display available screen commands   \" List running screen windows   N Display number of current window   c Open a new window   [number] Switch to window [number]   [ Copy (so you can paste to another window)   ] Paste   k Kill the current window   Ctrl-\\ Quit and close all screen windows   A Label the screen window   M Monitor for activity   d Detach the current screen session    </li> <li> <p>Startup commands:</p>          <code>screen -ls</code> List detached screen sessions on the server   screen -S (name for new screen) Create a screen with specified name   <code>screen --r [id]</code> Reattach the specified screen    </li> </ul>","title":"Screen Commands"},{"location":"Virtual_Terminals/#detachingreattaching-screen-sessions","text":"<p>To detach a running screen session, type <code>Ctrl-A d</code> This will detach the screen session with all of its windows but leave all of the related processes running. You can start and detach as many screen sessions as you wish, each with its own windows and processes. You can even logout of the server, and your screen sessions will continue running while you're gone.</p> <p>To reattach a screen session, find the id of the one you want with <code>screen -ls</code> then reattach it with <code>screen -r id</code></p> <p>If you don't remember which screen sessions you have opened on our cluster, you can type:</p> <pre><code>module load powertools\nuserinfo &lt;uid&gt;\n</code></pre> <p>If you are running a licensed software (such as MATLAB) within a screen session, please remember to kill it when you're done developing for the day. This will help us better manage licenses for the MSU research community.</p>","title":"Detaching/Reattaching Screen Sessions"},{"location":"Virtual_Terminals/#screen-customizations","text":"<p>The attached file (click here) includes customizations to screen that places an information bar at the bottom of the terminal. Many people find it very usefu.\u00a0\u00a0 View the file and copy selected or all lines to\u00a0 your \".screenrc\" file.\u00a0</p> <p>See also http://tmux.sourceforge.net/.</p>","title":"Screen customizations"},{"location":"Virtual_Terminals/#tmux","text":"<p>tmux is intended to be the modern, BSD-licensed alternative to screen. Both programs are available on HPCC. It allows splitting a window horizontally and vertically, and copying and pasting between multiple buffers. More information is available at http://tmux.sourceforge.net.</p>","title":"Tmux"},{"location":"Web-based_Remote_Desktop_Protocol/","text":"<p>ICER offers a way for users to connect to the main systems using the Web-based Remote Desktop Protocol for users who are more comfortable using a desktop environment, or are otherwise having issues connecting via SSH.\u00a0 Users only need to have a browser install on their machines in order for this feature to function. \u00a0</p>  <p>Warning</p> <p>Web RDP is planned to retire by the end of 2022. We recommend using OnDemand access for your interactive desktop needs.</p>","title":"Web-based Remote Desktop Protocol"},{"location":"Web-based_Remote_Desktop_Protocol/#connecting-to-hpcc-via-web-browser","text":"<p>First, users need to open a web browser on their local machine, and connecting to the web site https://webrdp.hpcc.msu.edu. It will ask for user name and password.</p> <p>Input your MSU NetID and password and then click on \"Log In\" button to get connection.</p> <p></p> <p>After you are connected, the page showing the available session types look like the following at the first time.\u00a0</p> <p></p> <p>Click on \"Launch Session\" button, a pop-up window show the several choices of sessions:\u00a0</p> <p>Mate Desktop</p> <p>Xterm terminal (Terminal only)</p> <p>Firefox (Browser only)</p> <p></p> <p>You can select your choice by click on the icon, then click the button \"Launch\" to launch the session.</p>  <p>Warning</p> <p>Opening both Mate desktop and XFCE desktop at the same time will cause the sessions to crash as well as possibly causing your web browser to crash.</p>","title":"Connecting to HPCC via web browser"},{"location":"Web-based_Remote_Desktop_Protocol/#desktops-available","text":"","title":"Desktops Available"},{"location":"Web-based_Remote_Desktop_Protocol/#mate-desktop","text":"<p>Note</p> <p>Mate desktop is intended to be the default desktop environment. The terminal quick launch icon is configured to start Xterm by default, so that the users complete profile will load. If you would like to use one of the other available terminals you will need to enable the login shell option. Please see below for instructions.</p>  <p></p>","title":"Mate Desktop:"},{"location":"Web-based_Remote_Desktop_Protocol/#other-bookmarks","text":"","title":"Other Bookmarks:"},{"location":"Web-based_Remote_Desktop_Protocol/#xterm-terminal-only","text":"<p>This option will open a single terminal only.\u00a0 You may then launch any futher applications you may need from the terminal.</p> <p></p>","title":"Xterm (Terminal Only):"},{"location":"Web-based_Remote_Desktop_Protocol/#firefox-web-browser-only","text":"<p>This option will open a web browser only.</p>","title":"Firefox (Web Browser only):"},{"location":"Web-based_Remote_Desktop_Protocol/#session-actions","text":"","title":"Session Actions"},{"location":"Web-based_Remote_Desktop_Protocol/#detaching-from-a-session","text":"<ul> <li>The web based remote desktop will allow for detaching from a     session, while still leaving the session open.\u00a0 This allows for you     to resume your work, or continue your work if you are\u00a0     disconnected.\u00a0</li> <li>To detach from a session simply close your web browser window     containing that session.  </li> </ul>  <p>Note</p> <p>Sessions will automatically close after 12 hours, if a user is not attached to the session.</p>","title":"Detaching from a session"},{"location":"Web-based_Remote_Desktop_Protocol/#reattaching-to-a-session","text":"<ul> <li>To reattach to a session, log into the web remote desktop if needed,     in the sessions browser tab click\u00a0 the session you would like to     access.  </li> </ul>","title":"Reattaching to a session"},{"location":"Web-based_Remote_Desktop_Protocol/#ending-a-session","text":"<ul> <li>To end a desktop session simply logout of the desktop environment</li> <li>To end a terminal session type \"exit\" or \"logoff\"</li> <li>To end a web browser only session simply close the web browser  </li> </ul>","title":"Ending a session"},{"location":"Web-based_Remote_Desktop_Protocol/#terminating-a-session","text":"<p>If any session will not close or you no longer need a session it can be terminated from the sessions web broser tab</p> <ol> <li> <p>Select the checkbox next to the session or sessions you would like     to terminate</p> </li> <li> <p>Click\u00a0 the Actions button</p> </li> <li> <p>Select terminate  </p> </li> </ol>  <p>Note</p> <p>Please note that it may take thirty seconds to a minute for some sessions to terminate</p>","title":"Terminating a session"},{"location":"Web-based_Remote_Desktop_Protocol/#enabling-a-login-shell-on-a-terminal-program","text":"","title":"Enabling a login shell on a terminal program:"},{"location":"Web-based_Remote_Desktop_Protocol/#mate-terminal","text":"<ol> <li>Open MATE Terminal from the applications menu.</li> <li>Select Edit from the terminals menu bar</li> <li>Select Profile Preferences from the drop down menu</li> <li>Select the Title and Command tab</li> <li>Make sure the checkbox next to \"Run command as a login shell\" is     checked.</li> <li>Close terminal and restart  </li> </ol>","title":"Mate Terminal:"},{"location":"Web-based_Remote_Desktop_Protocol/#features-of-the-web-based-remote-desktop","text":"<p>The Web-based Remote Desktop functions as the same as the Remote Desktop Client:</p> <ul> <li>You can run Graphical-based programs without needing to setup a X11     instance on your local machine.</li> <li>You can submit jobs from the RDP Gateway just like our regular     Gateway.</li> <li>If your connection is interrupted, your session will remain active     in the background. \u00a0When you reconnect, you will be placed right     back into your remote session.</li> </ul>","title":"Features of the Web-based Remote Desktop"},{"location":"Web-based_Remote_Desktop_Protocol/#conflict-issue-with-conda","text":"<p>If you have Anaconda (or Miniconda) installed in your home space, the <code>dbus</code> commands in Anaconda (or Miniconda) bin path will conflicts with those in <code>/usr/bin</code>. If your Anaconda is set auto-activated, launch to Remote Desktop will fail with error messages. In order to avoid this issue, please run the command:</p> <pre><code>conda config --set auto_activate_base false\n</code></pre> <p>on a dev node to stop activating Anaconda automatically. Please also refer to Module Setup for Anaconda for how to set up and start your installed Anaconda.</p>","title":"Conflict issue with Conda"},{"location":"Web-based_Remote_Desktop_Protocol/#troubleshooting","text":"<p>If you are getting errors when starting a session please open a ticket and provide the errors you received.</p>","title":"Troubleshooting:"},{"location":"Web_Site_Access_to_HPCC/","tags":["reference"],"text":"<p>For new HPCC users without much Linux/terminal experience, we highly recommend that you start by using a web browser to access the system. We offer two ways for web access, both requiring a valid MSU NetID.</p> <ol> <li> <p>Open OnDemand</p> <p>To connect, log into\u00a0https://ondemand.hpcc.msu.edu/</p> </li> <li> <p>Web-based Remote Desktop Protocol (webrdp)</p> <p>To connect, log into https://webrdp.hpcc.msu.edu/</p> </li> </ol> <p>Note that webrdp is planned to retire by the end of 2022 and tech support is limited. We highly recommend choosing OnDemand for your web access to the HPCC.</p>","title":"Web browser access to the HPCC"},{"location":"Why_Use_HPCC/","text":"<p>Let us make a comparison between your pc and our HPCC:</p>","title":"Why Use HPCC"},{"location":"Why_Use_HPCC/#hardwares","text":"Laptop/Desktop HPCC Clusters     Number of Nodes 1 979   Sockets per node 1 2, 8   Cores per node 4,8,or 16 20,28,40,128 or 144   Cores total 4,8,or 16 50,084   Core Speed 2.7 - 3.5 ghz 2.5-3 ghz   RAM memory 8, 16 or 32 GB 64, 128, 92, 500 GB or 6TB   File Storage 250, 500 GB or 1TB 1TB(Home), 50TB(Scratch)   Connection to other computers Campus ethernet1 Gbit/sec \"Infiniband\" 100 Gbit/sec   Users 1 ~1,500   Schedule On Demand 24/7 via queue","title":"Hardwares"},{"location":"Why_Use_HPCC/#software","text":"<p>You can use more than 500 titles and 3,000 different versions of software installed in HPCC:</p> <ul> <li>Compilers \u2014 \u00a0 GNU, intel, PGI, CUDA, ...</li> <li>Parallel\u00a0 \u2014\u00a0\u00a0 OpenMPI, IMPI, MVAPICH2, ...</li> <li>Bioinformatics\u00a0 \u2014\u00a0 BLAST, Trinity, Mothur, Samtools, Trimmomatic,     ...</li> <li>Libraries\u00a0 \u2014\u00a0 MKL, OpenBLAS, LAPACK, FFTW, ...</li> <li>Commerical\u00a0 \u2014\u00a0 MATLAB, ANSYS, COMSOL, STATA, GAUSSIAN, ...  </li> </ul> <p>for FREE !!!</p>","title":"Software"},{"location":"accessHPCC_overview/","text":"<p>Accessing the HPCC resources requires a user account and a proper login method. In this section, you will find out information about</p> <p>1)  How to apply for an HPCC account</p> <p>2)  Different methods of SSH connection</p> <p>3)  Non-SSH connection, just using your web browser</p> <p>4)  Information for users from other universities in Michigan</p> <p>For details, please click on the relevant links in the navigation menu under the current section.</p>","title":"Overview of HPCC access"},{"location":"bi/","tags":["reference"],"text":"<p>Information of Buy-In Account</p> <pre><code>$ bi -h\n\n  Usage:\n    bi\n    bi -h\n    bi [ -a &lt;account&gt; | -u &lt;user&gt; ]\n    bi -d [ -u &lt;user&gt; ]\n    bi -l [ -u &lt;user&gt; ]\n\n    -h | --help               Display this help message\n    -a | --account &lt;account&gt;  Display users and nodes of a buyin account\n    -u | --user &lt;user&gt;        Display buyin accounts for this user\n    -d | --default            Display only the user's default buyin account\n    -l | --list               List jobs and usage status of buyin nodes\n</code></pre>","title":"bi"},{"location":"development_nodes/","text":"<p>HPCC has several development nodes that are available for users to compile their code, and do short (CPU time less than 2 hours) runs to estimate run-time and memory usage.\u00a0</p> <p>These development nodes run the latest operating system and have similar configuration and environment setup to the compute nodes of the same clusters.\u00a0 Please use these development nodes to compile your program and test the work flow of your job script. For running long-time or large-resource computations, please submit jobs to use compute nodes.</p> <p>To access a certain one, for example, dev-intel14, please log into a gateway node and run:\u00a0<code>ssh dev-intel14</code></p>    Node Hostname Cores Memory Notes     dev-intel14 20 256GB Large memory intel14 node   dev-intel14-k20 20 128GB Two Nvidia K20 GPUs   dev-intel16 28 128GB Two 2.4Ghz 14-core Intel Xeon E5-2680v4 (28 cores total)   dev-intel16-k80 28 256GB Intel16 node with 4 Nvidia Tesla K80 GPUs   dev-intel18 40 377GB Two 2.4Ghz 20-core Intel Xeon Gold 6148 CPU (40 cores total)   dev-amd20-v100 48 187GB Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz and 4 Tesla V100S   dev-amd20 128 960GB AMD EPYC 7H12 64-Core Processor @ 2.6GHz    <p>Nodes with -k20, -k80, or -v100 suffixes have GPU cards required by GPU-enabled software, but may be used for any software.\u00a0Note there is not a development node containing the AMD20 A100 GPUs.\u00a0 </p> <p>Once your program is compiled and job script is tested, users can submit it to the SLURM queue by specifying various constraints such as\u00a0job duration, memory usage, number of CPUs, software license reservations and so on. \u00a0</p>  <p>Warning</p> <p>Any long running (total CPU run time <code>&gt; 2hrs</code>) jobs on dev nodes will be killed automatically without advance notice.</p>","title":"Development nodes"},{"location":"filesystem_overview/","text":"<p>Home, Research and Scratch are network file systems, meaning that each node in the cluster must go through the network switch to access these spaces. Please refer to individual wiki pages (Home, Research, and Scratch) in this section for how to use each of them.</p> <p><code>/tmp</code> and <code>/mnt/local</code> are locally accessible in the hard drive of each node. The space is not affected by the network and has larger size compared with the RAMDISK <code>/dev/shm</code> which is located inside the node's RAM.  However, <code>/dev/shm</code> is the closest storage location for files. Files stored here take up some of the node's memory space. For more information, please have a look at Local file system and Guidelines for Choosing File Storage and I/O.</p> <p>The ufs18 file system is a 2018 IBM General Parallel File System (GPFS) installed for the HPCC to store the home and research spaces. While it is faster and more stable, users need to learn its differences from other file systems and understand how to use it. Please see UFS18 file system.</p> <p>Data that are subject to external security requirements are considered sensitive data. These requirements may be imposed by regulations or contractual obligations. In either case, users that intend to store sensitive data on HPCC systems must work with us to ensure data security. For more information, check out our Sensitive data hosting wiki page.</p>","title":"Overview of HPCC file systems"},{"location":"getexample/","text":"<p>Download user examples.</p> <pre><code>$ getexample\nDownload an HPC example:\nusage:\n   getexample &lt;examplename&gt;\n\nWhere &lt;examplename&gt; is the name of the example you want to\ndownload.  This will create a directory named examplename which\nyou can cd into and read the README file for details (if one is\navaliable) or read .qsub or .sb file for how the example is run.\nYou may submit the example with 'qsub *.qsub' or 'sbatch *.sb'.\n\nFor Example:\n  getexample helloworld\n\nPossible example names:\nabaqus_example        cuda_hybrid     Intro2Linux_Oct_2019    MATLAB_threadPool  PETSc_example\nABySS             DDT_examples    Job_dependency      MKL_benchmark      Python_MPI\nADMB_example          dmtcp_longjob   LAPACK_example      MKL_c_eigenvalues  Python_numpy\naffinity          espresso_benchmark  magma_example       MKL_Example        QuantumESPRESSO\nAmber_example         FFTW        makefile_example    MKL_FFTW       R_parallel_examples\nbasic_array_job       FI491       maker_MPI       MKL_parallel       SAS_example\nblast             fluentMPI       MATLAB_basic        mothur_example     ScaLAPACK\nblender_farm          fortran_openmp      MATLAB_compiler     MPI_OpenMP_GPU     simpleMatlab\nBOOST_example         FreeSurfer      MATLAB_GPU          MPI_pi         STATA_array\nbowtie            gmp_mpfr        MATLAB_many_jobs    multi_variable     STATA_example\nbrother_test          helloHPCC       MATLAB_parallel     NAMD_example       tbb_example\nClang_example         helloMPI        MATLAB_parameter_sweep  OpenACC_example    TotalView_MPI_example\nCMakePackageExamples  helloworld      MATLAB_parfor       openmp_exercise    VASP_example\ncuda              intro2hpcc      MATLAB_patternsearch    PC2HPC         XSEDE_MPI_WORKSHOP\n</code></pre>","title":"getexample"},{"location":"install_ssh_client/","text":"<p>In order to use HPCC system robustly by command lines, please read the instruction to have a SSH client and X11 server installed in your computer.</p>","title":"Install SSH Client"},{"location":"install_ssh_client/#for-windows-machines","text":"<p>We recommend installing\u00a0MobaXterm Home Edition (Installer edition). This program provides both an SSH client (command line) and an X-Windows (X11 server) system for running graphical user interface (GUI) programs on HPCC. (See MoabXTerm SSH Tutorial &amp; SFTP Tutorial.)</p>","title":"For Windows machines"},{"location":"install_ssh_client/#for-mac-os-machines","text":"<p>You can use Terminal program (installed with the operation system) on the task bar (or search for terminal) as your SSH Client. However, for running\u00a0graphical user interface (GUI) programs on HPCC, you will need to install the X11-server program XQuartz. See https://www.xquartz.org/\u00a0 for download instructions.</p>","title":"For Mac OS machines"},{"location":"job_policies/","text":"<p>The following limits apply generally to all MSU users of the HPCC. Those at affiliate institutions may be working under slightly different policies. The limits are in place to help our large user community share the HPC. However, if these policies are an impediment to completing your research, please contact us.</p>","title":"Job Policies"},{"location":"job_policies/#cpu-and-gpu-usage-limits","text":"<ul> <li> <p>HPCC users who do not have a buy-in account are given a 'general'     SLURM account. The general account is limited to\u00a0500,000 CPU hours     (30,000,000 minutes) and 10,000\u00a0GPU hours (600,000 minutes)\u00a0every     year (from January 1st to December 31st) starting from 2021.</p> </li> <li> <p>There is no yearly usage limit on CPU or GPU time with a buy-in     account. If you have a buy-in account, your jobs will be run under     that account by default, unless the manager of the buy-in account     has chosen to opt-in (jobs submitted with the -A flag) instead of     opt-out.</p> </li> <li> <p>Users with general account can use the powertools command <code>SLURMUsage</code>     to check their used CPU and GPU time (in minutes) and left CPU and     GPU time (in hours):</p> <p><code>$ ml powertools # run this command if powertools not loaded</code></p> <p><code>$ SLURMUsage</code></p> </li> <li> <p>If users without a buy-in account needs more CPU or GPU time due to     running out of the limits, they can request additional CPU/GPU hours     by filling out the CPU/GPU Increase Request online form.</p> </li> </ul>","title":"CPU and GPU usage limits"},{"location":"job_policies/#limits-on-job-resource-requests","text":"<ul> <li>Time: Users can schedule jobs and run for at most 7 days (168     hours)\u00a0 ( <code>--time=168:00:00</code>)</li> <li>CPU: Users can utilize up to a total of 1040 cores or 520 jobs     running at any one time.\u00a0 (Buyin groups who have purchased more than     1040 cores can exceed this limit)</li> <li>Queue: The maximum number of jobs that can be queued or running per user is     1000 jobs.</li> </ul>","title":"Limits on job resource requests"},{"location":"job_policies/#buy-in-program","text":"<p>Faculty can purchase nodes via our buy-in program. The program guarantees jobs submitted with a buy-in group will start running on their buy-in nodes in 4 hours. However, due to contention between buy-in group jobs, the guarantee might not be fulfilled if requested resources are occupied or reserved by other jobs of the buy-in group.</p>","title":"Buy-in program"},{"location":"job_policies/#policy-summary","text":"<ul> <li>Jobs that run under 4 hours are able to run on the largest set of     nodes ( the combination of community + specialized hardware + buy-in     nodes.\u00a0 see below for details)\u00a0</li> <li>Jobs that request more resources (processors or RAM) have priorities     over smaller jobs because these jobs are more difficult to schedule.</li> <li>Jobs accrue priority based on how long they have been queued.</li> <li>The scheduler will attempt to balance usage among all users. (See     Fairshare Policy below.)</li> <li>It is against our fair use policy to artificially increase the     priority of a job in the queue (e.g. by requesting more resources     which will not be used). Jobs found to be manipulating the scheduler     will be canceled, and users continuing to attempt this will be     suspended.</li> </ul>","title":"Policy summary"},{"location":"job_policies/#more-about-queue-time","text":"","title":"More about queue time"},{"location":"job_policies/#fairshare","text":"<p>As jobs wait in the queue, they accrue priority to run. Another factor that contributes to a job's priority value is Fairshare. The scheduler will attempt to ensure fair resource utilization of all HPCC users by adjusting the initial priorities of the users who have recently used HPCC resources. Due to the policy, if users had jobs running with many resources recently, their current pending jobs might wait longer than before. Users can find the Fairshare contribution to a job priority by running command \"<code>sprio -u $USER</code>\":</p> <pre><code>[UserID@dev-intel18 UserID]$ sprio -u $USER\n          JOBID PARTITION     USER   PRIORITY       SITE        AGE  FAIRSHARE        QOS                 TRES\n       53381467 general-l   UserID      49432          0          0      49318          0       cpu=100,mem=15\n       53381467 general-s   UserID      49432          0          0      49318          0       cpu=100,mem=15\n</code></pre> <p>where it is found under <code>FAIRSHARE</code> column and the values are between 60,000 (highest priority contribution) and 0 (lowest priority contribution).  The more resources your jobs used recently, the less your Faireshre value will become, resulting in lower overall priority for your jobs. For other contributions of <code>sprio</code> results, please check Job Priority Factors.</p>","title":"Fairshare"},{"location":"job_policies/#shorter-jobs-can-run-on-more-nodes","text":"<p>Jobs that request a total running (wall-clock) time of four hours or less can run on any available buy-in and specialized nodes. Because they can access any nodes, they are likely to start running more quickly than the jobs which have to wait for the general-long partition nodes.</p>","title":"Shorter jobs can run on more nodes"},{"location":"job_policies/#bigger-jobs-are-prioritized-small-jobs-are-backfilled","text":"<p>The scheduler attempts to gather resources for large jobs and then backfill smaller jobs around them. The size of the job is determined by the number of CPUs and amount of memory requested.</p> <p>The scheduler packs small jobs together to allow more resources to be gathered for multi-core jobs.\u00a0 Resource requests are monitored. Abusive resource requests may violate MSU policy.</p>","title":"Bigger jobs are prioritized &amp; small jobs are backfilled"},{"location":"job_priority_factors/","text":"<p>Multiple factors contribute to the priority of SLURM jobs. The jobs age, the resources it requests, the submitting user\u2019s cluster utilization, and whether it was submitted to a buy-in account all contribute to the job\u2019s priority.</p>    Priority Factor Description Maximum Contribution to Priority     Age Starts at zero at job submission, then increases linearly to a maximum of 60000 after 30 days 60000 after 30 days   Fairshare Starts at 60000 and decreases and users' recent usage goes up. Usage for this calculation is decayed 50% each day 60000 for no recent cluster usage   Size Scales linearly with the amount of CPU and memory requested by a job. 100 per CPU, 20 per GB. 52000+ depending on memory requested   QOS Adds 1500 to buy-in jobs to ensure they are always above backfill schedulers minimum priority for reserving resources 1500","title":"Job Priority Factors"},{"location":"job_priority_factors/#backfill-scheduler-requirements-to-reserve-resources","text":"","title":"Backfill Scheduler Requirements to Reserve Resources"},{"location":"job_priority_factors/#minimum-priority-of-1500","text":"<p>The backfill scheduler will not reserve future resources for jobs with an overall priority less than 1500. Jobs with a priority less than 1500 may be started immediately by the backfill scheduler if resources are available. Jobs submitted to buy-in accounts are always above this threshold.</p>","title":"Minimum Priority of 1500"},{"location":"job_priority_factors/#minimum-age-of-30-minutes","text":"<p>The backfill scheduler will not reserve future resources for jobs that have been queued for less than 30 minutes. Jobs queued for less than 30 minutes may be started immediately by the backfill scheduler if resources are available.</p>","title":"Minimum Age of 30 Minutes"},{"location":"js/","text":"<p>The SLURM command sacct can be used to show the job steps of a job:</p> <pre><code>sacct -j 40410\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n40410               job classres-+   classres         28  COMPLETED      0:0\n40410.batch       batch              classres         28  COMPLETED      0:0\n40410.extern     extern              classres         28  COMPLETED      0:0\n40410.0            pw.x              classres         28  COMPLETED      0:0\n40410.1            ph.x              classres         28  COMPLETED      0:0\n</code></pre> <p>and the resource usages after it finished running. However, to display your desired results, it might take you some time to look into the web site and learn how to use the command. Here we introduce the powertools command \"<code>js</code>\" to display the resource usages of your jobs.</p> <p>[ Display Usage Info of a Job ] [ Display a List of Jobs ] [ More Selections of js Command ]</p>","title":"js"},{"location":"js/#display-usage-info-of-a-job","text":"<p>Users can simply run the powertools command \"<code>js</code>\" and it gives you most of the useful resource usages. To see the resource usages of a job, just use the command \"<code>js -j &lt;JobID&gt;</code>\", e.g.,</p> <pre><code>$ js -j 45251                      # powertools command\n\nSLURM Job ID: 45251\nWrkDir=/mnt/home/changc81/GetExample/GaAs\nstdout=/mnt/home/changc81/GetExample/GaAs/slurm-45251.out\n=========================================================================================================\n          JobID |               45251 |         45251.batch |        45251.extern |             45251.0 |\n        JobName |                 job |               batch |              extern |                pw.x |\n           User |            UserName |                     |                     |                     |\n       NodeList |             lac-421 |             lac-421 |             lac-421 |             lac-421 |\n         NNodes |                   1 |                   1 |                   1 |                   1 |\n         NTasks |                     |                   1 |                   1 |                  28 |\n          NCPUS |                  28 |                  28 |                  28 |                  28 |\n         ReqMem |               112Gn |               112Gn |               112Gn |               112Gn |\n      Timelimit |            04:00:00 |                     |                     |                     |\n        Elapsed |            00:00:16 |            00:00:16 |            00:00:16 |            00:00:14 |\n       TotalCPU |           05:39.544 |           00:00.999 |           00:00.001 |           05:38.543 |\n     AveCPULoad |             21.2215 |           0.0624375 |            6.25e-05 |             24.1816 |\n         MaxRSS |                     |                     |                 20K |              60296K |\n      MaxVMSize |                     |             189200K |               4184K |             605104K |\n          Start | 2018-08-28T20:27:40 | 2018-08-28T20:27:40 | 2018-08-28T20:27:40 | 2018-08-28T20:27:41 |\n            End | 2018-08-28T20:27:56 | 2018-08-28T20:27:56 | 2018-08-28T20:27:56 | 2018-08-28T20:27:55 |\n       ExitCode |                 0:0 |                 0:0 |                 0:0 |                 0:0 |\n          State |           COMPLETED |           COMPLETED |           COMPLETED |           COMPLETED |\n=========================================================================================================\n</code></pre> <p>If you would like to show more data of a job, you can also use the specification -F:</p> <pre><code>$ js -j &lt;Job ID&gt; -F                   # powertools command\n</code></pre> <p>to list all stored data of the job steps.</p>","title":"Display Usage Info of a Job"},{"location":"js/#display-a-list-of-jobs","text":"<p>If users would like to know a list of jobs submitted before, they can use\u00a0 \"<code>js -z</code>\" command. Simply provide a period of time when job was running with -S (start time of the period) and -E (end time of the period) options:</p> <pre><code>$ js -z -S &lt;Start Time&gt; -E &lt;End Time&gt;\n</code></pre> <p>and a list of the jobs with their properties and resource usages is displayed. For example, user can run the command:</p> <pre><code>$ js -z -S 2021-04-12 -E 2021-04-19\n       JobID    JobName NNo NTas NCPU   Timelimit     Elapsed  AveCPU     MaxRSS Stat Exit               Start          NodeList\n------------ ---------- --- ---- ---- ----------- ----------- ------- ---------- ---- ---- ------------------- -----------------\n    21043834 ondemand/+   1    1    1    01:00:00    01:00:16 0.05487    467.41M TIM+  0:0 2021-04-12T08:59:10           css-033\n    21127831    fi_info   1    1    2    00:05:00    00:03:19 1.18321  58191.45M COM+  0:0 2021-04-16T10:14:33           skl-033\n    21158898  hello.exe   1    8    1    00:20:00    00:01:09   0.236    644.61M COM+  0:0 2021-04-17T20:17:57           amr-133\n    21158916 interacti+   1    1    1    03:00:00    02:00:04 0.77325   1244.61M COM+  0:0 2021-04-18T20:18:29           css-033\n    21158973     SPAdes   1    4    8    09:30:00    09:00:04   7.254     13.20G FAI+  1:0 2021-04-19T20:20:44           lac-421\n</code></pre> <p>to see a list of jobs running between April 12th 2021 and April 19th 2021. If any one of the options -S or -E is not specified, the time will be considered as the current time of \"<code>js</code>\" execution.</p>","title":"Display a List of Jobs"},{"location":"js/#more-selections-of-js-command","text":"<p>To see all possible usages of the command, please use the specification -h:</p> <pre><code>$ js -h\n\njs [&lt;OPTION&gt;]\n     Valid &lt;OPTION&gt; values are:\n     -a, --allusers:\n                   Display jobs for all users. By default, only the\n                   current user's jobs are displayed.  If ran by user root\n                   this is the default.\n     -A, --accounts:\n                   Use this comma separated list of accounts to select jobs\n                   to display.  By default, all accounts are selected.\n     -b, --brief:\n                   Equivalent to '--format=jobstep,state,error'.\n     -c, --completion: Use job completion instead of accounting data.\n         --delimiter:\n                   ASCII characters used to separate the fields when\n                   specifying the  -p  or  -P options. The default delimiter\n                   is a '|'. This options is ignored if -p or -P options\n                   are not specified.\n     -C:\n                   Display results in columns rather than rows. Each\n                   column shows all data of a job step. A number can\n                   be specified after -C for how many columns in a row.\n     -D, --duplicates:\n                   If Slurm job ids are reset, some job numbers may\n                   appear more than once referring to different jobs.\n                   Without this option only the most recent jobs will be\n                   displayed.\n     -e, --helpformat:\n                   Print a list of fields that can be specified with the\n                   '--format' option\n     -E, --endtime:\n                   Select jobs eligible before this time.  If states are\n                   given with the -s option return jobs in this state before\n                   this period.\n         --federation: Report jobs from federation if a member of a one.\n     -f, --file=file:\n                   Read data from the specified file, rather than Slurm's\n                   current accounting log file. (Only appliciable when\n                   running the filetxt plugin.)\n     -F:\n                   Display data of all fields (--format=ALL) in columns.\n                   By default, three columns are shown in a row. See -C\n                   to change the default column number.\n     -g, --gid, --group:\n                   Use this comma separated list of gids or group names\n                   to select jobs to display.  By default, all groups are\n                   selected.\n     -h, --help:   Print this description of use.\n     -i, --nnodes=N:\n                   Return jobs which ran on this many nodes (N = min[-max])\n     -I, --ncpus=N:\n                   Return jobs which ran on this many cpus (N = min[-max])\n     -j, --jobs:\n                   Format is &lt;job(.step)&gt;. Display information about this\n                   job or comma-separated list of jobs. The default is all\n                   jobs. Adding .step will display the specific job step of\n                   that job. (A step id of 'batch' will display the\n                   information about the batch step.)\n     -k, --timelimit-min:\n                   Only send data about jobs with this timelimit.\n                   If used with timelimit_max this will be the minimum\n                   timelimit of the range.  Default is no restriction.\n     -K, --timelimit-max:\n                   Ignored by itself, but if timelimit_min is set this will\n                   be the maximum timelimit of the range.  Default is no\n                   restriction.\n         --local   Report information only about jobs on the local cluster.\n                   Overrides --federation.\n     -l, --long:\n                   Equivalent to specifying\n                   '--format=jobid,jobname,partition,maxvmsize,maxvmsizenode,\n                             maxvmsizetask,avevmsize,maxrss,maxrssnode,\n                             maxrsstask,averss,maxpages,maxpagesnode,\n                             maxpagestask,avepages,mincpu,mincpunode,\n                             mincputask,avecpu,ntasks,alloccpus,elapsed,\n                             state,exitcode,avecpufreq,reqcpufreqmin,\n                             reqcpufreqmax,reqcpufreqgov,consumedenergy,\n                             maxdiskread,maxdiskreadnode,maxdiskreadtask,\n                             avediskread,maxdiskwrite,maxdiskwritenode,\n                             maxdiskwritetask,avediskread,allocgres,reqgres\n     -L, --allclusters:\n                   Display jobs ran on all clusters. By default, only jobs\n                   ran on the cluster from where sacct is called are\n                   displayed.\n     -M, --clusters:\n                   Only send data about these clusters. Use \"all\" for all\n                   clusters.\n     -n, --noheader:\n                   No header will be added to the beginning of output.\n                   The default is to print a header.\n     --noconvert:\n                   Don't convert units from their original type\n                   (e.g. 2048M won't be converted to 2G).\n     -N, --nodelist:\n                   Display jobs that ran on any of these nodes,\n                   can be one or more using a ranged string.\n     --name:\n                   Display jobs that have any of these name(s).\n     -o, --format:\n                   Comma separated list of fields. (use \"--helpformat\"\n                   for a list of available fields).\n     -p, --parsable: output will be '|' delimited with a '|' at the end\n     -P, --parsable2: output will be '|' delimited without a '|' at the end\n     -q, --qos:\n                   Only send data about jobs using these qos.  Default is all.\n     -r, --partition:\n                   Comma separated list of partitions to select jobs and\n                   job steps from. The default is all partitions.\n     -s, --state:\n                   Select jobs based on their current state or the state\n                   they were in during the time period given: running (r),\n                   completed (cd), failed (f), timeout (to), resizing (rs),\n                   deadline (dl) and node_fail (nf).\n     -S, --starttime:\n                   Select jobs eligible after this time.  Default is\n                   00:00:00 of the current day, unless '-s' is set then\n                   the default is 'now'.\n     -T, --truncate:\n                   Truncate time.  So if a job started before --starttime\n                   the start time would be truncated to --starttime.\n                   The same for end time and --endtime.\n     -u, --uid, --user:\n                   Use this comma separated list of uids or user names\n                   to select jobs to display.  By default, the running\n                   user's uid is used.\n     --units=[KMGTP]:\n                   Display values in specified unit type. Takes precedence\n                   over --noconvert option.\n     --usage:      Display brief usage message.\n     -v, --verbose:\n                   Primarily for debugging purposes, report the state of\n                   various variables during processing.\n     -V, --version: Print version.\n     -W, --wckeys:\n                   Only send data about these wckeys.  Default is all.\n     --whole-hetjob=[yes|no]:\n                   If set to 'yes' (or not set), then information about all\n                   the heterogeneous components will be retrieved. If set\n                   to 'no' only the specific filtered components will be\n                   retrieved.\n     -x, --associations:\n                   Only send data about these association id.  Default is all.\n     -X, --allocations:\n                   Only show statistics relevant to the job allocation\n                   itself, not taking steps into consideration.\n     -z:           Show simple summary data only.\n\n     Note, valid start/end time formats are...\n                   HH:MM[:SS] [AM|PM]\n                   MMDD[YY] or MM/DD[/YY] or MM.DD[.YY]\n                   MM/DD[/YY]-HH:MM[:SS]\n                   YYYY-MM-DD[THH:MM[:SS]]\n</code></pre>","title":"More Selections of js Command"},{"location":"make/","text":"","title":"make"},{"location":"make/#makefile","text":"<p>Makeifles are a simple way to organize code compilation. This tutorial offers a very basic idea of what is possible using make. For this tutorial, please download three files, a main program (hello.c), a functional code (hellofunc.c), and an include file (hello.h) [hello.c, hellofunc.c, hello.h] under the 'hello' directory. To compile these codes, you would use the following command:</p> <pre><code>gcc -o hello hello.c hellofunc.c -I.\n</code></pre> <p>This command compiles the two c files, and names the executable hello. With the '-I.' flag, gcc will look in the current directory for the include file 'hello.h'. With only two .c files, it is easy to compile with the above approach, but with more files, it is more likely having typos. In addition, if you are only making changes to one .c file, the above approach recompiles all of .c files every time which is time-consuming and inefficient.</p> <p>So it is time to learn how makefile will be helpful for such cases. First, create a file which has the following two lines. (The filename should be makefile or Makefile), and put it under the 'hello' directory.</p> <pre><code>hello: hello.c hellofunc.c\n      gcc -o hello hello.c hellofunc.c -I.\n</code></pre> <p>Now, type make on the terminal and check if the executable is created. The make command will execute the compile command as you have written it in the makefile. Note that make with no arguments executes the first rule in the file. Furthermore, by putting the list of files on which the command depends on the first line after the ':', make knows that the rule hello needs to be executed if any of those files change. One very important thing to note is that there should be a tab before the gcc command in the makefile (multiple spaces do not work!). There must be a tab at the beginning of any command, otherwise, you will get a lot of errors.</p> <p>Can we make it a little bit more efficient? Let's modify our makefile as following:</p> <pre><code>CC=gcc\nCFLAGS=-I.\nhello: hello.o hellofunc.o\n    $(CC) -o hello hello.o hellofunc.o\n</code></pre> <p>In this makefile, we define CC and CFLAGS, which are special macros communicating to make how we want to compile the files hello.c and hellofunc.c. In particular, CC is for the C compiler, and CFLAGS is the list of flags to pass to C compiler. By putting the object files (hello.o and hellofunc.o) in the dependency list and in the rule, make knows it must first compile the .c files individually, and then build the executable hello. If your project is small, like consisting of a few separate codes, this form of makefile is enough to handle the set of codes. However, this makefile misses include files. For example, if you made a change to 'hello.h' make would not recompile the .c files, even though they needed to be. In order to fix this problem, we need to tell make that all .c files depend on certain .h files. It can be done by writing a simple rule and adding it to the makefile.</p> <pre><code>CC=gcc\nCFLAGS=-I.\nDEPS = hello.h\n\n%.o: %.c $(DEPS)\n    $(CC) -c -o $@ $&lt; $(CFLAGS)\n\nhello: hello.o hellofunc.o\n    $(CC) -o hello hello.o hellofunc.o\n</code></pre> <p>This addition first creates the macro DEPS (the macro name does not have to be DEPS. You can use any name.), which is the set of .h files on which the .c files depend. Then we define a rule for all .o files. The rule says that the .o file depends on the .c files, and the .h files which are included in the DEPS. Next, the rule says that to generate the .o file, make needs to compile the .c file using the compiler defined in the CC. The -c flag says to generate the object file, the -o \\$@ says to put the output of the compilation in the file named on the left side of the :, the \\$&lt; is the first item in the dependencies list, and the CFLAGS macro is defined on the 2nd line.</p> <p>For the simplification, you can use special macros \\$@ and \\$^, which are the left and right sides of the :, respectively, to make the overall compilation rule more general. In the example below, all of the include files should be listed as part of the macro DEPS, and all of the object files should be listed as part of the macro OBJ.</p> <pre><code>CC=gcc\nCFLAGS=-I.\nDEPS = hello.h\nOBJ = hello.o hellofunc.o\n\n%.o: %.c $(DEPS)\n    $(CC) -c -o $@ $&lt; $(CFLAGS)\n\nhello: $(OBJ)\n    $(CC) -o $@ $^ $(CFLAGS)\n</code></pre> <p>Now you have a good sense of makefile. For more information on makefiles and the make function, check out the GNU Make Manual, which will tell you everything on makefile.</p> <p>You can download some makefile examples using getexample in our HPC.</p>","title":"Makefile"},{"location":"node_status/","text":"<p>Display a list of compute nodes and their properties.</p> <pre><code>$ node_status -h\n\n    node_status:      Display status of all compute nodes or nodes with following specifications.\n\n    -h | --help       Display this help message\n    -f | --feature    Selected features (csm,csn,csp,css,qml,lac,vim,nvl,skl,intel14,intel16,intel18,gpu,k80)\n    -w | --nodelist   List of nodes (conflict with -f)\n    -A | --account    Account names (such as general,classres, ...)\n    -c | --cpus       Available number of CPU\n    -m | --mem        Available memory (in Mb)\n    -g | --gpus       Available number of GPU\n</code></pre> <pre><code>$ node_status\n\nWed Apr 22 11:14:40 EDT 2020\n\nNodeName       Account         State     CPU(Load:Aloc Idl:Tot)    Mem(Aval:Tot)Mb   GPU(I:T)   Reason\n----------------------------------------------------------------------------------------------------------\ncsm-001        general       ALLOCATED      13.61: 20    0: 20       45186: 246640      N/A\ncsm-002       albrecht         MIXED        10.14: 15    5: 20        1072: 246640      N/A\ncsm-003         colej        ALLOCATED       7.45: 20    0: 20       50032: 246640      N/A\ncsm-004         colej        ALLOCATED       6.24: 20    0: 20       50032: 246640      N/A\ncsm-005         colej          MIXED         9.02: 16    4: 20       29552: 246640      N/A\ncsm-007         colej          MIXED         3.85: 16    4: 20       29552: 246640      N/A\ncsm-008     horticulture       DOWN*          N/A:  0   20: 20      246640: 246640      N/A     DEAD - to be removed [climer@2020-04-08T12:48:08]\ncsm-009     horticulture       DOWN*          N/A:  0   20: 20      246640: 246640      N/A     Climer - Need power reset 4-21-20 [climer@2020-04-21T07:59:30]\ncsm-010          ged         ALLOCATED       6.34: 20    0: 20       50032: 246640      N/A\ncsm-017       eisenlohr     DOWN*+DRAIN       N/A:  0   20: 20      246640: 246640      N/A     Parks testing ssh puppet module [parksjo@2020-04-02T10:10:52]\ncsm-018       eisenlohr      IDLE+DRAIN      0.06:  0   20: 20      246640: 246640      N/A     Parks testing ssh puppet module [parksjo@2020-04-02T10:10:40]\ncsm-019       eisenlohr      DOWN+DRAIN      0.01:  0   20: 20      246640: 246640      N/A     DEAD - to be removed [climer@2020-04-08T12:48:45]\ncsm-020       eisenlohr      ALLOCATED       6.75: 20    0: 20       50032: 246640      N/A\ncsm-021        dworkin       ALLOCATED       6.68: 20    0: 20       65392: 246640      N/A\ncsm-022       mitchmcg       ALLOCATED       6.08: 20    0: 20       65392: 246640      N/A\ncsn-001        general         MIXED         9.76: 18    2: 20       18208: 118012    k20(0:2)\ncsn-002        general         MIXED         1.64:  8   12: 20       61628: 118012    k20(0:2)\ncsn-003        general         MIXED         2.29:  6   14: 20       80878: 118012    k20(0:2)\ncsn-004        general         MIXED         9.10:  9   11: 20        9742: 118012    k20(0:2)\ncsn-005        general         MIXED         9.92: 12    8: 20       16160: 118012    k20(0:2)\ncsn-006        general         MIXED         5.51:  6   14: 20         252: 118012    k20(0:2)\ncsn-007        general         MIXED         1.16: 14    6: 20       95964: 118012    k20(0:2)\ncsn-008        general         MIXED         1.67:  8   12: 20       61628: 118012    k20(0:2)\ncsn-009        general         MIXED         1.29:  7   13: 20       78012: 118012    k20(0:2)\ncsn-010        general         MIXED         1.59:  8   12: 20       61628: 118012    k20(0:2)\ncsn-011        general         MIXED         1.57:  8   12: 20       61628: 118012    k20(0:2)\ncsn-012        general         MIXED         4.54: 11    9: 20       12476: 118012    k20(0:2)\ncsn-013        general         MIXED         4.53: 11    9: 20       12476: 118012    k20(0:2)\ncsn-014        general         MIXED         4.75: 11    9: 20       12476: 118012    k20(0:2)\ncsn-015        general         MIXED         4.59: 11    9: 20       12476: 118012    k20(0:2)\ncsn-016        general         MIXED         0.62:  7   13: 20       78012: 118012    k20(0:2)\ncsn-017        general         MIXED         1.77:  8   12: 20       61628: 118012    k20(0:2)\ncsn-018        general         MIXED         1.16:  5   15: 20       89820: 118012    k20(0:2)\ncsn-019        general       ALLOCATED       3.98: 20    0: 20        8718: 118012    k20(0:2)\ncsn-020        general          IDLE        10.51:  0   20: 20      118012: 118012    k20(2:2)\ncsn-021        general         MIXED         6.05:  6   14: 20       16160: 118012    k20(0:2)\ncsn-022        general         MIXED         1.73:  8   12: 20       26812: 118012    k20(0:2)\ncsn-023        general         MIXED         0.35:  8   12: 20       78012: 118012    k20(0:2)\ncsn-024        general         MIXED         1.46:  8   12: 20       61628: 118012    k20(0:2)\ncsn-025        general         MIXED         1.73:  5   15: 20       57052: 118012    k20(0:2)\ncsn-026        general         MIXED         1.67: 15    5: 20       12476: 118012    k20(0:2)\ncsn-027        general       ALLOCATED      12.84: 20    0: 20        1276: 118012    k20(0:2)\ncsn-028        general         MIXED         1.97:  8   12: 20       78012: 118012    k20(0:2)\ncsn-029        general         MIXED         0.68:  7   13: 20       78012: 118012    k20(0:2)\ncsn-030        general         MIXED         1.77:  8   12: 20       61628: 118012    k20(0:2)\ncsn-031        general         MIXED         2.08:  8   12: 20       78012: 118012    k20(0:2)\ncsn-032        general         MIXED         2.14:  6   14: 20       31726: 118012    k20(0:2)\ncsn-033        general         MIXED         2.15:  6   14: 20       73436: 118012    k20(0:2)\ncsn-034        general         MIXED         1.85:  8   12: 20       61628: 118012    k20(0:2)\ncsn-035        general         MIXED         4.82: 11    9: 20       28860: 118012    k20(0:2)\ncsn-036        general         MIXED         0.63:  8   12: 20       78012: 118012    k20(0:2)\ncsn-037       eisenlohr        MIXED         1.16:  9   11: 20       37052: 118012    k20(0:2)\ncsn-038     christlibuyin      MIXED         1.43: 15    5: 20       12476: 118012    k20(0:2)\ncsn-039     christlibuyin      MIXED         2.11: 15    5: 20       12476: 118012    k20(0:2)\ncsp-006        general       ALLOCATED      20.14: 20    0: 20        2304: 118012      N/A\ncsp-007        general         DOWN*          N/A:  0   20: 20      118012: 118012      N/A     DEAD - to be removed [climer@2020-04-08T12:48:21]\ncsp-016        general       ALLOCATED      20.02: 20    0: 20       38140: 118012      N/A\ncsp-017        general       ALLOCATED      20.08: 20    0: 20        1276: 118012      N/A\ncsp-018        general       ALLOCATED      20.09: 20    0: 20       30972: 118012      N/A\ncsp-019     christlibuyin      MIXED         8.12:  9   11: 20       68860: 118012      N/A\ncsp-020        general         MIXED         7.01:  7   13: 20         252: 118012      N/A\ncsp-025        general       ALLOCATED      12.64: 20    0: 20        1276: 118012      N/A\ncsp-026     christlibuyin      MIXED         3.98:  4   16: 20        3324: 118012      N/A\ncss-001         colej          MIXED         3.72: 19    1: 20       38704: 246640      N/A\ncss-002         colej          MIXED         1.08:  1   19: 20       77052: 118012      N/A\ncss-003       plzbuyin         MIXED        12.19: 14    6: 20       11516: 118012      N/A\ncss-007       albrecht         MIXED        13.28: 13    7: 20        2928: 246640      N/A\ncss-008        general       ALLOCATED      22.18: 20    0: 20       19658: 246640      N/A\ncss-009        general       ALLOCATED      15.18: 20    0: 20        7024: 246640      N/A\ncss-010        general       ALLOCATED      19.25: 20    0: 20       45936: 246640      N/A\ncss-011        general       ALLOCATED      20.17: 20    0: 20       16518: 246640      N/A\ncss-012        general       ALLOCATED      21.14: 20    0: 20        4846: 246640      N/A\ncss-013        general       ALLOCATED       7.22: 20    0: 20        2178: 246640      N/A\ncss-014        general         DOWN*          N/A:  0   20: 20      246640: 246640      N/A     Climer - Need power reset 4-21-20 [climer@2020-04-21T07:56:26]\ncss-016        general       ALLOCATED      21.24: 20    0: 20       41032: 246640      N/A\ncss-017        general       ALLOCATED      20.14: 20    0: 20       17682: 246640      N/A\ncss-018        general         MIXED        18.01: 18    2: 20         134: 246640      N/A\ncss-019        general       ALLOCATED      20.03: 20    0: 20        8322: 246640      N/A\ncss-020      yueqibuyin        MIXED        15.87: 16    4: 20        7420: 118012      N/A\ncss-023        general       ALLOCATED      13.98: 20    0: 20         718: 118012      N/A\ncss-032        general       ALLOCATED      21.08: 20    0: 20       36262: 118012      N/A\ncss-033       classres         MIXED         8.98: 11    9: 20        5564: 118012      N/A\ncss-034        general         MIXED        12.98: 13    7: 20        1276: 118012      N/A\ncss-035        general         MIXED         6.25:  5   15: 20        3324: 118012      N/A\ncss-036        general       ALLOCATED       7.85: 20    0: 20         174:  53248      N/A\ncss-038        general       ALLOCATED      20.26: 20    0: 20       23552:  53248      N/A\ncss-039        general         MIXED        17.10: 18    2: 20           4:  53248      N/A\ncss-040        general         MIXED        14.06: 19    1: 20         548:  53248      N/A\ncss-041        general       ALLOCATED      19.42: 20    0: 20        1850:  53248      N/A\ncss-042        general         MIXED         5.92: 18    2: 20         274:  53248      N/A\ncss-043        general         MIXED         7.10:  9   11: 20         346:  53248      N/A\ncss-044        general       ALLOCATED      20.13: 20    0: 20       12028:  53248      N/A\ncss-045        general       ALLOCATED      19.02: 20    0: 20        4240:  53248      N/A\ncss-047        general         MIXED         8.10: 18    2: 20        2048:  53248      N/A\ncss-048        general       ALLOCATED       6.98: 20    0: 20        5544:  53248      N/A\ncss-049        general         MIXED        12.82: 11    9: 20       24374:  53248      N/A\ncss-050        general         MIXED         5.01: 14    6: 20           0:  53248      N/A\ncss-052        general         MIXED        16.22: 19    1: 20       12188:  53248      N/A\ncss-053        general       DOWN+DRAIN      0.01:  0   20: 20       53248:  53248      N/A     DEAD - to be removed [climer@2020-04-08T12:48:35]\ncss-054        general         MIXED        17.07: 17    3: 20         288:  53248      N/A\ncss-055        general         MIXED        21.75:  8   12: 20       11336:  53248      N/A\ncss-056        general         MIXED        20.96: 16    4: 20       10932:  53248      N/A\ncss-057        general       ALLOCATED      20.17: 20    0: 20       19532:  53248      N/A\ncss-058        general       ALLOCATED      19.07: 20    0: 20       14344:  53248      N/A\ncss-059        general         MIXED         7.10:  7   13: 20         346:  53248      N/A\ncss-060        general       ALLOCATED      16.04: 20    0: 20        8740:  53248      N/A\ncss-061        general         MIXED         3.06:  3   17: 20        1298:  53248      N/A\ncss-062        general       ALLOCATED      20.14: 20    0: 20       12912:  53248      N/A\ncss-063        general       ALLOCATED      15.98: 20    0: 20         144:  53248      N/A\ncss-064        general       ALLOCATED      20.10: 20    0: 20       37152:  53248      N/A\ncss-065        general       ALLOCATED      10.01: 20    0: 20        2668:  53248      N/A\ncss-066        general         MIXED        18.05: 19    1: 20         552:  53248      N/A\ncss-067      yueqibuyin        MIXED         1.01:  1   19: 20       12288:  53248      N/A\ncss-071        general      DOWN*+DRAIN       N/A:  0   20: 20      215232: 215232      N/A     DEAD - to be removed [climer@2020-04-08T12:47:23]\ncss-072         colej          MIXED         6.85:  7   13: 20           0:  53248      N/A\ncss-074      yueqibuyin        MIXED         2.82:  3   17: 20        4096:  53248      N/A\ncss-075        general       ALLOCATED      19.32: 20    0: 20       32798:  53248      N/A\ncss-076        general         DOWN*          N/A:  0   20: 20       53248:  53248      N/A     DEAD - to be removed [climer@2020-04-08T12:47:14]\ncss-079        general      DOWN*+DRAIN       N/A:  0   20: 20       53248:  53248      N/A     Climer - Need power reset - 4-8-20 [climer@2020-04-08T12:46:08]\ncss-080        general      DOWN*+DRAIN       N/A:  0   20: 20       53248:  53248      N/A     Climer - Need power reset - 4-8-20 [climer@2020-04-08T12:46:20]\ncss-081        toulson         MIXED         6.32:  9   11: 20        7360:  53248      N/A\ncss-082      yueqibuyin        DOWN*          N/A:  0   20: 20       53248:  53248      N/A     Climer - waiting Power reset 4-8-20 [climer@2020-04-08T12:51:51]\ncss-083        general         MIXED        10.40: 18    2: 20         424:  53248      N/A\ncss-084        toulson         MIXED         3.06:  3   17: 20        4096:  53248      N/A\ncss-085        toulson         MIXED         2.98:  3   17: 20        4096:  53248      N/A\ncss-087        general       ALLOCATED      16.09: 20    0: 20        1644:  53248      N/A\ncss-088        general       ALLOCATED      20.27: 20    0: 20        1374:  53248      N/A\ncss-089        general         MIXED        10.10: 16    4: 20        1442:  53248      N/A\ncss-090        manning         MIXED         6.33:  9   11: 20        7360:  53248      N/A\ncss-091        manning         MIXED         2.98:  3   17: 20        4096:  53248      N/A\ncss-092        general         MIXED        14.08: 18    2: 20          20:  53248      N/A\ncss-093        general       ALLOCATED      19.10: 20    0: 20       42798:  53248      N/A\ncss-094        general         MIXED        18.16: 17    3: 20         698:  53248      N/A\ncss-095        general         MIXED         5.02: 15    5: 20           0:  53248      N/A\ncss-097        general       ALLOCATED      20.11: 20    0: 20       24576:  53248      N/A\ncss-098        general         MIXED        15.08: 15    5: 20         692:  53248      N/A\ncss-099        general       ALLOCATED      17.07: 20    0: 20        9764:  53248      N/A\ncss-100        cukier          MIXED         3.00:  3   17: 20        4096:  53248      N/A\ncss-101        cukier          MIXED         3.04:  3   17: 20        4096:  53248      N/A\ncss-102         black          MIXED         2.82:  3   17: 20        4096:  53248      N/A\ncss-103         black          MIXED         6.82:  7   13: 20           0:  53248      N/A\ncss-106      yueqibuyin        MIXED         2.13:  2   18: 20       20480:  53248      N/A\ncss-107        general       ALLOCATED      14.03: 20    0: 20        8196:  53248      N/A\ncss-108      yueqibuyin        MIXED        14.01: 14    6: 20           0:  53248      N/A\ncss-109      yueqibuyin        MIXED         1.01:  3   17: 20        4288:  53248      N/A\ncss-111      yueqibuyin        MIXED        14.00: 14    6: 20           0:  53248      N/A\ncss-112      yueqibuyin        MIXED         1.03:  1   19: 20       12288:  53248      N/A\ncss-113      yueqibuyin        MIXED         2.93:  3   17: 20        4096:  53248      N/A\ncss-114      yueqibuyin        MIXED        14.22: 14    6: 20           0:  53248      N/A\ncss-115      yueqibuyin        MIXED        14.03: 14    6: 20           0:  53248      N/A\ncss-116      yueqibuyin        MIXED         1.02:  1   19: 20       12288:  53248      N/A\ncss-117      yueqibuyin        MIXED         1.08:  1   19: 20       12288:  53248      N/A\ncss-118        general         MIXED        17.15: 19    1: 20          92:  53248      N/A\ncss-119      yueqibuyin        MIXED        14.01: 14    6: 20           0:  53248      N/A\ncss-120      yueqibuyin        MIXED         5.78:  8   12: 20       20480:  53248      N/A\ncss-121        general         MIXED        14.01: 16    4: 20        1442:  53248      N/A\ncss-122         baek           MIXED         2.98:  3   17: 20        4096:  53248      N/A\ncss-123         baek           MIXED         1.10:  1   19: 20       12288:  53248      N/A\ncss-124        general       ALLOCATED      20.26: 20    0: 20        4312:  53248      N/A\ncss-125        aeimit          MIXED         5.06:  5   15: 20        8192:  53248      N/A\ncss-126        general       ALLOCATED      16.13: 20    0: 20        6418:  53248      N/A\ncss-127        toulson         MIXED         1.02:  1   19: 20       12288:  53248      N/A\n\ncs*      =&gt;   33.3%(buyin)   91.4%(162)     43.6%: 59.5%( 3240)      69.9%(17.0Tb)    97%( 78)   Usage%(Total)\n\nlac-000     deyoungbuyin     ALLOCATED       0.06: 28    0: 28        6012: 118012      N/A\nlac-001     deyoungbuyin     ALLOCATED       0.07: 28    0: 28        6012: 118012      N/A\nlac-002     deyoungbuyin     ALLOCATED       0.11: 28    0: 28        6012: 118012      N/A\nlac-003     deyoungbuyin       MIXED        25.86: 26    2: 28        5372: 118012      N/A\nlac-004     deyoungbuyin     ALLOCATED       0.15: 28    0: 28        6012: 118012      N/A\nlac-005     deyoungbuyin     ALLOCATED       0.11: 28    0: 28        6012: 118012      N/A\nlac-006     deyoungbuyin     DOWN+DRAIN      1.00:  0   28: 28      118012: 118012      N/A     NHC: check_hw_ib:  No IB port is ACTIVE (LinkUp 56 Gb/sec). [root@2020-04-20T08:18:40]\nlac-007     deyoungbuyin     ALLOCATED       0.15: 28    0: 28        6012: 118012      N/A\nlac-008     deyoungbuyin     ALLOCATED       0.17: 28    0: 28        6012: 118012      N/A\nlac-009     deyoungbuyin     ALLOCATED       0.12: 28    0: 28        6012: 118012      N/A\nlac-010     deyoungbuyin     ALLOCATED       0.08: 28    0: 28        6012: 118012      N/A\nlac-011     deyoungbuyin     ALLOCATED       0.08: 28    0: 28        6012: 118012      N/A\nlac-012     deyoungbuyin     ALLOCATED       0.22: 28    0: 28        6012: 118012      N/A\nlac-013     deyoungbuyin     ALLOCATED       0.07: 28    0: 28        6012: 118012      N/A\nlac-014     deyoungbuyin     ALLOCATED       0.07: 28    0: 28        6012: 118012      N/A\nlac-015     deyoungbuyin     ALLOCATED       0.08: 28    0: 28        6012: 118012      N/A\nlac-016     deyoungbuyin     ALLOCATED       0.17: 28    0: 28        6012: 118012      N/A\nlac-017     deyoungbuyin     ALLOCATED       0.05: 28    0: 28        6012: 118012      N/A\nlac-018     deyoungbuyin     ALLOCATED       0.09: 28    0: 28        6012: 118012      N/A\nlac-019     deyoungbuyin     ALLOCATED       0.10: 28    0: 28        6012: 118012      N/A\nlac-020     deyoungbuyin     ALLOCATED       0.09: 28    0: 28        6012: 118012      N/A\nlac-021     deyoungbuyin     ALLOCATED       0.10: 28    0: 28        6012: 118012      N/A\nlac-022     deyoungbuyin     ALLOCATED       0.11: 28    0: 28        6012: 118012      N/A\nlac-023     deyoungbuyin     ALLOCATED       0.15: 28    0: 28        6012: 118012      N/A\nlac-024     deyoungbuyin       MIXED         1.39: 10   18: 28       86640: 246640    k80(4:8)\nlac-025      DicksonLab        MIXED         6.53: 16   12: 28        6768: 246640    k80(1:8)\nlac-026      DicksonLab        MIXED         6.98: 16   12: 28        6768: 246640    k80(1:8)\nlac-027      DicksonLab        MIXED         6.73: 16   12: 28        6768: 246640    k80(1:8)\nlac-028      DicksonLab        MIXED         6.79: 16   12: 28        6768: 246640    k80(1:8)\nlac-029      DicksonLab        MIXED         3.62: 13   15: 28       55920: 246640    k80(1:8)\nlac-030      feig-covid        MIXED         1.24: 14   14: 28       66640: 246640    k80(3:8)\nlac-031       merzjrke         MIXED         4.10: 10   18: 28      197488: 246640    k80(0:8)\nlac-032        allenmc         MIXED         5.15:  5   23: 28       15612: 118012      N/A\nlac-033        allenmc         MIXED         5.10:  5   23: 28       15612: 118012      N/A\nlac-034        allenmc         MIXED         0.29: 26    2: 28        5372: 118012      N/A\nlac-035        allenmc         MIXED         7.98: 26    2: 28        5372: 118012      N/A\nlac-036        allenmc       ALLOCATED      28.32: 28    0: 28       60668: 118012      N/A\nlac-037        allenmc       ALLOCATED      28.19: 28    0: 28       60668: 118012      N/A\nlac-038        general       ALLOCATED      23.36: 28    0: 28       33028: 118012      N/A\nlac-039        general       ALLOCATED      24.67: 28    0: 28         260: 118012      N/A\nlac-040        general         MIXED        21.14: 26    2: 28         252: 118012      N/A\nlac-041        general         MIXED         9.10:  9   19: 28         252: 118012      N/A\nlac-042        general         MIXED        22.86: 25    3: 28         252: 118012      N/A\nlac-043        general         MIXED        20.15: 21    7: 28         252: 118012      N/A\nlac-044        general         MIXED        11.51: 21    7: 28        4540: 118012      N/A\nlac-045          ptg           MIXED         5.22:  7   21: 28        7612: 118012      N/A\nlac-046          ptg           MIXED        25.51: 26    2: 28        5372: 118012      N/A\nlac-047          ptg           MIXED         5.28:  7   21: 28        7612: 118012      N/A\nlac-048          ptg           MIXED        12.23: 12   16: 28        8444: 118012      N/A\nlac-049          ptg           MIXED        12.37: 14   14: 28         444: 118012      N/A\nlac-050          ptg           MIXED        11.05: 13   15: 28        1468: 118012      N/A\nlac-051          ptg           MIXED         5.09:  5   23: 28       15612: 118012      N/A\nlac-052          ptg           MIXED         5.06:  7   21: 28        7612: 118012      N/A\nlac-053          ptg           MIXED         5.10:  5   23: 28       15612: 118012      N/A\nlac-054          ptg           MIXED        12.20: 12   16: 28        8444: 118012      N/A\nlac-055          ptg           MIXED        12.10: 14   14: 28         444: 118012      N/A\nlac-056          ptg           MIXED        12.41: 14   14: 28         444: 118012      N/A\nlac-057          ptg           MIXED        12.21: 14   14: 28         444: 118012      N/A\nlac-058          ptg           MIXED         7.08:  9   19: 28        5564: 118012      N/A\nlac-059          ptg         ALLOCATED      23.79: 28    0: 28       15612: 118012      N/A\nlac-060          ptg         ALLOCATED      12.93: 28    0: 28       15612: 118012      N/A\nlac-061          ptg         ALLOCATED      13.39: 28    0: 28       15612: 118012      N/A\nlac-062          ptg         ALLOCATED      10.32: 28    0: 28       15612: 118012      N/A\nlac-063          ptg         ALLOCATED      12.18: 28    0: 28       15612: 118012      N/A\nlac-064          ptg         ALLOCATED      10.46: 28    0: 28       15612: 118012      N/A\nlac-065          ptg         ALLOCATED      16.00: 28    0: 28       15612: 118012      N/A\nlac-066          ptg         ALLOCATED      13.01: 28    0: 28       15612: 118012      N/A\nlac-067          ptg           MIXED         5.36:  5   23: 28       15612: 118012      N/A\nlac-068          ptg         ALLOCATED      21.58: 28    0: 28       15612: 118012      N/A\nlac-069          ptg           MIXED        12.17: 14   14: 28         444: 118012      N/A\nlac-070          ptg           MIXED        12.19: 12   16: 28        8444: 118012      N/A\nlac-071          ptg           MIXED        12.08: 12   16: 28        8444: 118012      N/A\nlac-072          ptg           MIXED        12.17: 14   14: 28         444: 118012      N/A\nlac-073          ptg           MIXED         7.14:  7   21: 28       13564: 118012      N/A\nlac-074          ptg           MIXED         5.08:  8   20: 28        2492: 118012      N/A\nlac-075          ptg           MIXED         5.04:  7   21: 28        7612: 118012      N/A\nlac-076          ptg         ALLOCATED      25.55: 28    0: 28       15612: 118012      N/A\nlac-077          ptg           MIXED         5.18:  7   21: 28        7612: 118012      N/A\nlac-078        general         MIXED        11.38:  8   20: 28       69884: 118012      N/A\nlac-079          ptg         ALLOCATED      22.37: 28    0: 28       15612: 118012      N/A\nlac-080       merzjrke         MIXED         2.48: 16   12: 28       50032: 246640    k80(0:8)\nlac-081       merzjrke         MIXED        13.85: 23    5: 28        9072: 246640    k80(0:8)\nlac-082       merzjrke         MIXED         5.85: 18   10: 28      131952: 246640    k80(0:8)\nlac-083       merzjrke         MIXED         4.74: 19    9: 28         880: 246640    k80(0:8)\nlac-084       merzjrke         MIXED         3.43:  4   24: 28      238448: 246640    k80(4:8)\nlac-085       merzjrke         MIXED         4.68: 19    9: 28         880: 246640    k80(0:8)\nlac-086       merzjrke         MIXED         1.20: 16   12: 28       50032: 246640    k80(0:8)\nlac-087        general         MIXED        18.91: 19    9: 28        9072: 246640    k80(0:8)\nlac-088          ptg           MIXED         4.03:  6   22: 28        7612: 118012      N/A\nlac-089          ptg           MIXED         4.04:  6   22: 28        7612: 118012      N/A\nlac-090          ptg           MIXED         4.06:  6   22: 28        7612: 118012      N/A\nlac-091          ptg           MIXED         4.01:  6   22: 28        7612: 118012      N/A\nlac-092          ptg           MIXED         4.06:  6   22: 28        7612: 118012      N/A\nlac-093          ptg         ALLOCATED       9.42: 28    0: 28       15612: 118012      N/A\nlac-094          ptg         ALLOCATED      14.45: 28    0: 28       15612: 118012      N/A\nlac-095          ptg         ALLOCATED      12.50: 28    0: 28       15612: 118012      N/A\nlac-096          ptg         ALLOCATED       6.97: 28    0: 28       15612: 118012      N/A\nlac-097          ptg         ALLOCATED       8.46: 28    0: 28       15612: 118012      N/A\nlac-098          ptg         ALLOCATED       6.59: 28    0: 28       15612: 118012      N/A\nlac-099          ptg         ALLOCATED       7.12: 28    0: 28       15612: 118012      N/A\nlac-100          ptg         ALLOCATED      19.14: 28    0: 28       15612: 118012      N/A\nlac-101          ptg         ALLOCATED      17.22: 28    0: 28       15612: 118012      N/A\nlac-102          ptg           MIXED        11.19: 13   15: 28         444: 118012      N/A\nlac-103          ptg           MIXED         7.07:  9   19: 28        4540: 118012      N/A\nlac-104          ptg           MIXED         5.12:  7   21: 28         444: 118012      N/A\nlac-105          ptg           MIXED         8.21:  8   20: 28        5372: 118012      N/A\nlac-106          ptg         ALLOCATED      20.78: 28    0: 28       15612: 118012      N/A\nlac-107          ptg           MIXED        11.02: 13   15: 28         444: 118012      N/A\nlac-108          ptg           MIXED        11.22: 13   15: 28         444: 118012      N/A\nlac-109          ptg           MIXED        11.15: 11   17: 28        8444: 118012      N/A\nlac-110          ptg           MIXED        11.18: 13   15: 28         444: 118012      N/A\nlac-111          ptg           MIXED        11.17: 11   17: 28        8444: 118012      N/A\nlac-112          ptg           MIXED        11.09: 13   15: 28         444: 118012      N/A\nlac-113          ptg           MIXED        11.23: 13   15: 28         444: 118012      N/A\nlac-114          ptg           MIXED        11.01: 11   17: 28        8444: 118012      N/A\nlac-115          ptg           MIXED         8.07: 10   18: 28        3516: 118012      N/A\nlac-116          ptg           MIXED         4.89:  7   21: 28         444: 118012      N/A\nlac-117          ptg           MIXED         7.20:  7   21: 28       37116: 118012      N/A\nlac-118          ptg           MIXED        11.05: 13   15: 28         444: 118012      N/A\nlac-119          ptg           MIXED        11.16: 13   15: 28         444: 118012      N/A\nlac-120          ptg           MIXED         4.19:  4   24: 28       40188: 118012      N/A\nlac-121          ptg           MIXED         5.08:  7   21: 28        6588: 118012      N/A\nlac-122          ptg           MIXED         4.99:  5   23: 28        8444: 118012      N/A\nlac-123        general       ALLOCATED      29.19: 28    0: 28       38904: 118012      N/A\nlac-124          ptg           MIXED         5.18:  5   23: 28       15612: 118012      N/A\nlac-125          ptg           MIXED         5.07:  7   21: 28        7612: 118012      N/A\nlac-126          ptg           MIXED         5.29:  7   21: 28        7612: 118012      N/A\nlac-127          ptg         ALLOCATED      22.09: 28    0: 28       15612: 118012      N/A\nlac-128          ptg         ALLOCATED      24.32: 28    0: 28       15612: 118012      N/A\nlac-129          ptg           MIXED         5.15:  7   21: 28        7612: 118012      N/A\nlac-130          ptg         ALLOCATED      24.86: 28    0: 28       15612: 118012      N/A\nlac-131          ptg         ALLOCATED      10.80: 28    0: 28       15612: 118012      N/A\nlac-132          ptg           MIXED        12.01: 14   14: 28         444: 118012      N/A\nlac-133          ptg           MIXED        12.22: 14   14: 28         444: 118012      N/A\nlac-134          ptg           MIXED        11.25: 13   15: 28        1468: 118012      N/A\nlac-135          ptg           MIXED         5.20:  7   21: 28        7612: 118012      N/A\nlac-136       merzjrke      MIXED+DRAIN     56.36:  4   24: 28      238448: 246640    k80(4:8)  NHC: Script timed out while executing \"icer_check_gpu_count 8 syslog die\". [root@2020-04-22T04:03:33]\nlac-137      feig-covid        MIXED         3.69: 13   15: 28       74480: 246640    k80(3:8)\nlac-138       merzjrke         MIXED         9.92:  1   27: 28      181104: 246640    k80(4:8)\nlac-139       merzjrke         MIXED        17.77: 11   17: 28       66416: 246640    k80(0:8)\nlac-140       merzjrke         MIXED        15.97: 24    4: 28      206256: 246640    k80(0:8)\nlac-141       merzjrke         MIXED         5.56:  8   20: 28      181104: 246640    k80(0:8)\nlac-142       merzjrke         MIXED        17.78:  8   20: 28      230256: 246640    k80(0:8)\nlac-143        general         MIXED        11.22: 11   17: 28        7024: 246640    k80(7:8)\nlac-144          ptg           MIXED         4.84:  7   21: 28         444: 118012      N/A\nlac-145          ptg           MIXED         5.07:  7   21: 28         444: 118012      N/A\nlac-146          ptg           MIXED         5.11:  5   23: 28        8444: 118012      N/A\nlac-147          ptg           MIXED         4.62:  7   21: 28         444: 118012      N/A\nlac-148          ptg           MIXED         5.05:  7   21: 28         444: 118012      N/A\nlac-149          ptg           MIXED         5.10:  7   21: 28         444: 118012      N/A\nlac-150          ptg           MIXED         5.03:  7   21: 28         444: 118012      N/A\nlac-151          ptg           MIXED         5.15:  5   23: 28        8444: 118012      N/A\nlac-152          ptg           MIXED         5.09:  7   21: 28         444: 118012      N/A\nlac-153          ptg           MIXED         5.02:  7   21: 28         444: 118012      N/A\nlac-154          ptg           MIXED         5.04:  7   21: 28         444: 118012      N/A\nlac-155          ptg           MIXED        10.08: 13   15: 28         444: 118012      N/A\nlac-156          ptg           MIXED         5.51:  7   21: 28         444: 118012      N/A\nlac-157          ptg           MIXED         5.06:  7   21: 28         444: 118012      N/A\nlac-158          ptg           MIXED         5.27:  5   23: 28        8444: 118012      N/A\nlac-159          ptg           MIXED         5.01:  7   21: 28         444: 118012      N/A\nlac-160          ptg           MIXED         4.26:  6   22: 28        7612: 118012      N/A\nlac-161          ptg           MIXED         6.34:  6   22: 28        7420: 118012      N/A\nlac-162          ptg           MIXED        11.08: 13   15: 28         444: 118012      N/A\nlac-163          ptg           MIXED        11.07: 13   15: 28         444: 118012      N/A\nlac-164          ptg           MIXED        11.26: 11   17: 28        8444: 118012      N/A\nlac-165          ptg           MIXED        11.11: 13   15: 28         444: 118012      N/A\nlac-166          ptg           MIXED        11.10: 13   15: 28         444: 118012      N/A\nlac-167          ptg           MIXED        11.07: 11   17: 28        8444: 118012      N/A\nlac-168          ptg           MIXED        11.37: 13   15: 28         444: 118012      N/A\nlac-169          ptg           MIXED        11.21: 13   15: 28         444: 118012      N/A\nlac-170          ptg           MIXED        11.15: 13   15: 28         444: 118012      N/A\nlac-171          ptg           MIXED        11.28: 13   15: 28         444: 118012      N/A\nlac-172          ptg           MIXED        11.06: 13   15: 28         444: 118012      N/A\nlac-173          ptg           MIXED        11.28: 13   15: 28         444: 118012      N/A\nlac-174          ptg           MIXED        11.21: 13   15: 28         444: 118012      N/A\nlac-175          ptg           MIXED        11.04: 13   15: 28         444: 118012      N/A\nlac-176          ptg           MIXED        11.07: 13   15: 28         444: 118012      N/A\nlac-177          ptg           MIXED        12.03: 14   14: 28         444: 118012      N/A\nlac-178          ptg           MIXED        11.10: 11   17: 28        8444: 118012      N/A\nlac-179          ptg           MIXED        11.04: 13   15: 28         444: 118012      N/A\nlac-180          ptg           MIXED        11.07: 13   15: 28         444: 118012      N/A\nlac-181          ptg           MIXED        10.06: 10   18: 28        9468: 118012      N/A\nlac-182          ptg           MIXED         4.24:  6   22: 28        7612: 118012      N/A\nlac-183          ptg           MIXED         4.01:  6   22: 28        7612: 118012      N/A\nlac-184          ptg           MIXED         4.05:  6   22: 28        7612: 118012      N/A\nlac-185          ptg           MIXED         4.05:  6   22: 28        7612: 118012      N/A\nlac-186          ptg           MIXED         4.04:  6   22: 28        7612: 118012      N/A\nlac-187          ptg           MIXED         4.23:  6   22: 28        7612: 118012      N/A\nlac-188          ptg           MIXED         4.24:  6   22: 28        7612: 118012      N/A\nlac-189          ptg         ALLOCATED       8.34: 28    0: 28       15612: 118012      N/A\nlac-190          ptg         ALLOCATED       7.25: 28    0: 28       15612: 118012      N/A\nlac-191          ptg         ALLOCATED       8.49: 28    0: 28       15612: 118012      N/A\nlac-192        general         MIXED         5.92: 15   13: 28       23280: 246640    k80(2:8)\nlac-193     guowei-search      MIXED         2.09: 22    6: 28       33872: 246640    k80(3:8)\nlac-194     guowei-search      MIXED         6.74: 20    8: 28        2320: 246640    k80(5:8)\nlac-195     guowei-search      MIXED         7.76: 17   11: 28        1840: 246640    k80(6:8)\nlac-196     guowei-search      MIXED         3.95: 17   11: 28       13392: 246640    k80(3:8)\nlac-197     guowei-search      MIXED         3.65: 17   11: 28       13392: 246640    k80(3:8)\nlac-198        general         MIXED         0.78: 10   18: 28       86640: 246640    k80(4:8)\nlac-199        general         MIXED         1.98: 11   17: 28       66160: 246640    k80(3:8)\nlac-200          ptg           MIXED        11.07: 13   15: 28         444: 118012      N/A\nlac-201          ptg           MIXED        11.21: 13   15: 28         444: 118012      N/A\nlac-202          ptg           MIXED        11.10: 13   15: 28         444: 118012      N/A\nlac-203          ptg           MIXED        11.08: 13   15: 28         444: 118012      N/A\nlac-204          ptg           MIXED        11.05: 13   15: 28         444: 118012      N/A\nlac-205          ptg           MIXED        11.21: 13   15: 28         444: 118012      N/A\nlac-206          ptg           MIXED        11.01: 13   15: 28         444: 118012      N/A\nlac-207          ptg           MIXED        11.01: 13   15: 28         444: 118012      N/A\nlac-208          ptg           MIXED         8.06: 10   18: 28        3516: 118012      N/A\nlac-209        general         MIXED         3.43: 10   18: 28       27900: 118012      N/A\nlac-210          ptg         ALLOCATED       8.52: 28    0: 28       15612: 118012      N/A\nlac-211          ptg         ALLOCATED       8.60: 28    0: 28       15612: 118012      N/A\nlac-212          ptg         ALLOCATED       7.35: 28    0: 28       15612: 118012      N/A\nlac-213          ptg         ALLOCATED       7.67: 28    0: 28       15612: 118012      N/A\nlac-214          ptg           MIXED        26.31: 26    2: 28        5372: 118012      N/A\nlac-215        tonggao         MIXED        22.49: 26    2: 28        7612: 118012      N/A\nlac-216        weilai        ALLOCATED      24.65: 28    0: 28        5372: 118012      N/A\nlac-217        general       ALLOCATED      23.03: 28    0: 28        4420: 118012      N/A\nlac-218        weilai        ALLOCATED      25.05: 28    0: 28        5372: 118012      N/A\nlac-219        weilai        ALLOCATED      25.12: 28    0: 28        5372: 118012      N/A\nlac-220        weilai          MIXED        25.44: 25    3: 28        5372: 118012      N/A\nlac-221        weilai        ALLOCATED      25.26: 28    0: 28        5372: 118012      N/A\nlac-222        weilai          MIXED        16.52: 26    2: 28        5372: 118012      N/A\nlac-223        weilai          MIXED         8.72:  9   19: 28        1276: 118012      N/A\nlac-224        weilai        ALLOCATED      25.00: 28    0: 28      134000: 246640      N/A\nlac-225        general         MIXED        12.28: 23    5: 28        2928: 246640      N/A\nlac-228        general         MIXED        16.26: 16   12: 28       62320: 246640      N/A\nlac-229         bazil          MIXED        23.93: 24    4: 28         880: 246640      N/A\nlac-230        general         MIXED        41.60: 18   10: 28      131952: 246640      N/A\nlac-231        general       ALLOCATED      14.49: 28    0: 28       13168: 246640      N/A\nlac-232        general         MIXED        23.93: 24    4: 28         884: 246640      N/A\nlac-233        general         MIXED        26.96: 24    4: 28        8052: 246640      N/A\nlac-234        general         MIXED         2.12:  2   26: 28      164720: 246640      N/A\nlac-235        general         MIXED         5.08:  5   23: 28       74608: 246640      N/A\nlac-236         cmich        ALLOCATED       0.53: 28    0: 28      134640: 246640      N/A\nlac-237         cmich        ALLOCATED       4.72: 28    0: 28       34032: 246640      N/A\nlac-238         cmich        ALLOCATED      14.00: 28    0: 28       34800: 246640      N/A\nlac-239         cmich          MIXED        14.06: 18   10: 28        1264: 246640      N/A\nlac-240         cmich        ALLOCATED       8.02: 28    0: 28       35568: 246640      N/A\nlac-241         cmich          MIXED        15.06: 15   13: 28         880: 246640      N/A\nlac-242         cmich          MIXED        15.28: 15   13: 28         880: 246640      N/A\nlac-243         cmich          MIXED        15.10: 15   13: 28         880: 246640      N/A\nlac-244         cmich          MIXED         6.06:  6   22: 28      148336: 246640      N/A\nlac-245         cmich          MIXED         8.08:  8   20: 28      115568: 246640      N/A\nlac-246        general       ALLOCATED      16.25: 28    0: 28       17264: 246640      N/A\nlac-247        general         MIXED         6.10:  6   22: 28       79728: 246640      N/A\nlac-248       iceradmin      DOWN+DRAIN      0.06:  0   28: 28      246640: 246640      N/A     NHC: check_fs_mount:  /mnt/gs18 not mounted [root@2020-04-14T11:10:52]\nlac-250         lirac        ALLOCATED      22.19: 28    0: 28       37152: 503520      N/A\nlac-251       wenhuang       ALLOCATED      26.65: 28    0: 28      382880: 503520      N/A\nlac-252        general         MIXED         0.20: 18   10: 28      431520: 503520      N/A\nlac-253        general         MIXED        14.81: 15   13: 28      257760: 503520      N/A\nlac-254        beacon          MIXED         6.78:  7   21: 28        3324: 118012      N/A\nlac-255        chomiuk         MIXED        10.33: 13   15: 28        3324: 118012      N/A\nlac-256       quantgen       ALLOCATED       4.10: 28    0: 28      290912: 503520      N/A\nlac-257         hirn         ALLOCATED      17.81: 28    0: 28      279200: 503520      N/A\nlac-258         hirn         ALLOCATED      28.15: 28    0: 28      216800: 503520      N/A\nlac-259       quantgen       ALLOCATED      28.28: 28    0: 28      216800: 503520      N/A\nlac-260       quantgen       ALLOCATED       5.17: 28    0: 28      391520: 503520      N/A\nlac-261          ccg           MIXED        25.99: 25    3: 28      260000: 503520      N/A\nlac-276        general         MIXED        15.60: 16   12: 28         598: 118012      N/A\nlac-277        general       ALLOCATED      20.14: 28    0: 28          56: 118012      N/A\nlac-278        general         MIXED         6.11:  6   22: 28         880: 246640      N/A\nlac-279        general         MIXED        15.65: 25    3: 28        2928: 246640      N/A\nlac-280        general         MIXED        18.05: 15   13: 28         880: 246640      N/A\nlac-281        general         MIXED        26.14: 27    1: 28          48: 246640      N/A\nlac-282        general         MIXED         6.05:  6   22: 28         880: 246640      N/A\nlac-283        general         MIXED         5.02:  5   23: 28       74608: 246640      N/A\nlac-284        general       ALLOCATED      25.07: 28    0: 28        4980: 246640      N/A\nlac-285         qian           MIXED         6.10: 27    1: 28         880: 246640      N/A\nlac-286       scbbuyin       ALLOCATED       7.98: 28    0: 28       17264: 246640    k80(8:8)\nlac-287        general         MIXED         1.97: 11   17: 28       70256: 246640    k80(3:8)\nlac-288        general       ALLOCATED       5.97: 28    0: 28       52080: 246640    k80(6:8)\nlac-289        general         MIXED         1.02: 10   18: 28       86640: 246640    k80(4:8)\nlac-290        general         MIXED         1.10: 10   18: 28       86640: 246640    k80(4:8)\nlac-291       merzjrke         MIXED         5.79:  8   20: 28      181104: 246640    k80(0:8)\nlac-292        general         MIXED         0.86: 10   18: 28       86640: 246640    k80(4:8)\nlac-293        general         MIXED         0.65: 10   18: 28       86640: 246640    k80(4:8)\nlac-294     wang-krishnan      MIXED        16.83: 27    1: 28        4976: 246640      N/A\nlac-295     wang-krishnan      MIXED        15.76: 26    2: 28       21360: 246640      N/A\nlac-296     wang-krishnan      MIXED        16.59: 26    2: 28       21360: 246640      N/A\nlac-297     wang-krishnan      MIXED        17.39: 27    1: 28        4976: 246640      N/A\nlac-298     wang-krishnan      MIXED        17.44: 26    2: 28       21360: 246640      N/A\nlac-299     wang-krishnan      MIXED        15.17: 26    2: 28       21360: 246640      N/A\nlac-300        general         MIXED         3.06:  3   25: 28      123760: 246640      N/A\nlac-301        general         MIXED        16.10: 16   12: 28      181104: 246640      N/A\nlac-302          ccg           MIXED        24.21: 24    4: 28       12000: 503520      N/A\nlac-303          ccg           MIXED        24.06: 24    4: 28       12000: 503520      N/A\nlac-304          ccg           MIXED        24.13: 24    4: 28       12000: 503520      N/A\nlac-305          ccg           MIXED        24.05: 24    4: 28       12000: 503520      N/A\nlac-306        general       ALLOCATED      28.24: 28    0: 28      216800: 503520      N/A\nlac-307          ccg           MIXED        24.09: 26    2: 28        4000: 503520      N/A\nlac-308          ccg           MIXED        24.08: 26    2: 28        4000: 503520      N/A\nlac-309          ccg           MIXED        24.13: 26    2: 28        4000: 503520      N/A\nlac-310          ccg         ALLOCATED       7.82: 28    0: 28      224992: 503520      N/A\nlac-311   oakland-universi     MIXED        24.11: 24    4: 28       12000: 503520      N/A\nlac-312   oakland-universi     MIXED        24.10: 24    4: 28       12000: 503520      N/A\nlac-313   oakland-universi     MIXED        24.29: 24    4: 28       12000: 503520      N/A\nlac-314   oakland-universi   ALLOCATED       2.17: 28    0: 28      349920: 503520      N/A\nlac-315          ccg           MIXED        24.10: 24    4: 28       12000: 503520      N/A\nlac-316         cmich        ALLOCATED       7.50: 28    0: 28      358112: 503520      N/A\nlac-317         cmich        ALLOCATED       0.55: 28    0: 28      391520: 503520      N/A\nlac-318         cmich          MIXED        21.51: 26    2: 28        5372: 118012      N/A\nlac-319         cmich          MIXED        15.65: 16   12: 28         252: 118012      N/A\nlac-320         cmich          MIXED        10.17: 10   18: 28         252: 118012      N/A\nlac-321         cmich          MIXED        10.09: 10   18: 28         252: 118012      N/A\nlac-322         cmich          MIXED         8.04:  8   20: 28        2300: 118012      N/A\nlac-323         cmich          MIXED         4.15: 16   12: 28        4476: 118012      N/A\nlac-324         cmich          MIXED        12.69: 26    2: 28        5372: 118012      N/A\nlac-325         cmich        ALLOCATED       0.41: 28    0: 28        6012: 118012      N/A\nlac-326         cmich          MIXED         7.04:  7   21: 28        3324: 118012      N/A\nlac-327         cmich          MIXED         0.25: 26    2: 28        5372: 118012      N/A\nlac-328         cmich          MIXED        12.52: 26    2: 28        5372: 118012      N/A\nlac-329         cmich          MIXED         9.17: 13   15: 28         636: 118012      N/A\nlac-330        beacon          MIXED        12.16: 14   14: 28         444: 118012      N/A\nlac-331        beacon          MIXED        12.24: 14   14: 28         444: 118012      N/A\nlac-332        beacon          MIXED        12.12: 14   14: 28         444: 118012      N/A\nlac-333        beacon          MIXED        11.12: 13   15: 28        1468: 118012      N/A\nlac-334          ptg         ALLOCATED       1.82: 28    0: 28       15612: 118012      N/A\nlac-335          ptg         ALLOCATED       1.92: 28    0: 28       15612: 118012      N/A\nlac-336        general         MIXED        20.78: 21    7: 28        3324: 118012      N/A\nlac-337        general         MIXED        12.59: 26    2: 28         252: 118012      N/A\nlac-338        general         MIXED        15.39: 27    1: 28        3328: 118012      N/A\nlac-339        general       ALLOCATED      13.06: 28    0: 28       29948: 118012      N/A\nlac-340          ptg         ALLOCATED       1.87: 28    0: 28       15612: 118012      N/A\nlac-341          ptg         ALLOCATED       1.64: 28    0: 28       15612: 118012      N/A\nlac-342        general         MIXED         1.49: 14   14: 28       66640: 246640    k80(3:8)\nlac-343       merzjrke         MIXED        33.27: 25    3: 28      124336: 246640    k80(0:8)\nlac-344       merzjrke         MIXED        12.17: 22    6: 28       25456: 246640    k80(0:8)\nlac-345       merzjrke      MIXED+COMPL     16.13: 22    6: 28      173488: 246640    k80(2:8)\nlac-346       merzjrke         MIXED        14.60:  8   20: 28      181104: 246640    k80(0:8)\nlac-347       merzjrke         DOWN*          N/A:  0   28: 28      246640: 246640    k80(8:8)  Climer - Need power reset 4-21-20 [climer@2020-04-21T07:58:48]\nlac-348      feig-covid        MIXED         1.12: 14   14: 28       66640: 246640    k80(3:8)\nlac-349       merzjrke      MIXED+DRAIN     59.62:  4   24: 28      238448: 246640    k80(4:8)  NHC: Script timed out while executing \"icer_check_gpu_count 8 syslog die\". [root@2020-04-22T03:38:40]\nlac-350         phani          MIXED        16.54: 26    2: 28        5372: 118012      N/A\nlac-351         phani          MIXED        15.30: 26    2: 28        5372: 118012      N/A\nlac-352         phani          MIXED         0.36: 26    2: 28        5372: 118012      N/A\nlac-353        general       ALLOCATED      27.20: 28    0: 28        1564: 118012      N/A\nlac-354        general         MIXED        12.16: 15   13: 28        7616: 118012      N/A\nlac-355        general         MIXED         8.23: 23    5: 28         252: 118012      N/A\nlac-356        general         MIXED        20.96: 20    8: 28        8444: 118012      N/A\nlac-357        general         MIXED        23.99: 23    5: 28        3328: 118012      N/A\nlac-358        general         MIXED        21.31: 21    7: 28        7420: 118012      N/A\nlac-359        general         MIXED        20.98: 21    7: 28         252: 118012      N/A\nlac-360        general         MIXED        12.09: 17   11: 28        2300: 118012      N/A\nlac-361        junlin          MIXED        10.38: 13   15: 28        3324: 118012      N/A\nlac-362        junlin          MIXED         6.62:  7   21: 28        3324: 118012      N/A\nlac-363        general       ALLOCATED      18.79: 28    0: 28       16640: 118012      N/A\nlac-364        general         MIXED        13.36: 26    2: 28       11790: 118012      N/A\nlac-365          SPG         ALLOCATED      28.32: 28    0: 28       15612: 118012      N/A\nlac-366          SPG         ALLOCATED      28.36: 28    0: 28       15612: 118012      N/A\nlac-367          SPG         ALLOCATED      28.12: 28    0: 28       15612: 118012      N/A\nlac-368         hirn           MIXED         4.85: 26    2: 28        5372: 118012      N/A\nlac-369         hirn           MIXED        19.90: 26    2: 28        5372: 118012      N/A\nlac-372        general         MIXED        22.19: 26    2: 28        1352: 118012      N/A\nlac-374        general         MIXED        20.98: 22    6: 28         256: 118012      N/A\nlac-375        general         MIXED        24.19: 27    1: 28         328: 118012      N/A\nlac-376        general         MIXED        23.12: 23    5: 28        5372: 118012      N/A\nlac-377        general         MIXED        26.12: 26    2: 28        4766: 118012      N/A\nlac-378        general         MIXED        11.36: 13   15: 28        1280: 118012      N/A\nlac-379        general         MIXED        13.28: 14   14: 28       51452: 118012      N/A\nlac-380        general         MIXED        18.07: 26    2: 28         328: 118012      N/A\nlac-381        general         MIXED        22.88: 20    8: 28        9208: 118012      N/A\nlac-382        general       ALLOCATED      15.15: 28    0: 28        1276: 118012      N/A\nlac-383        general         MIXED         7.11: 16   12: 28       85244: 118012      N/A\nlac-384        general         MIXED         2.30:  2   26: 28       75004: 118012      N/A\nlac-385        general       ALLOCATED      13.14: 28    0: 28        3612: 118012      N/A\nlac-386        general       ALLOCATED      24.06: 28    0: 28       11054: 118012      N/A\nlac-387        general         MIXED        16.07: 27    1: 28       13564: 118012      N/A\nlac-388        general       ALLOCATED      24.46: 28    0: 28        5396: 118012      N/A\nlac-389        general         MIXED        25.71: 24    4: 28       44284: 118012      N/A\nlac-390        general         MIXED        21.11: 25    3: 28         252: 118012      N/A\nlac-391        general         MIXED        26.68: 27    1: 28        6420: 118012      N/A\nlac-392        general         MIXED        16.20: 16   12: 28         256: 118012      N/A\nlac-393        general         MIXED        14.26: 20    8: 28       12540: 118012      N/A\nlac-394        general         MIXED         5.98: 24    4: 28         252: 118012      N/A\nlac-395        general       ALLOCATED      15.00: 28    0: 28        3324: 118012      N/A\nlac-396        general         MIXED        20.06: 25    3: 28        1280: 118012      N/A\nlac-397        general         MIXED         6.43: 25    3: 28         252: 118012      N/A\nlac-398        general         MIXED        17.20: 17   11: 28       11516: 118012      N/A\nlac-399        general         MIXED        20.98: 25    3: 28        6396: 118012      N/A\nlac-400         tsang        DOWN+DRAIN      0.02:  0   28: 28      118012: 118012      N/A     Testing climer-2-12-2- [climer@2020-02-12T07:43:21]\nlac-401        general         MIXED        29.11: 24    4: 28        4290: 118012      N/A\nlac-402        general       ALLOCATED      26.10: 28    0: 28        1300: 118012      N/A\nlac-403        general         MIXED        22.28: 25    3: 28        8444: 118012      N/A\nlac-404        general       ALLOCATED      25.60: 28    0: 28        1276: 118012      N/A\nlac-405        general         MIXED         9.17: 15   13: 28        3324: 118012      N/A\nlac-406        general         MIXED        17.90: 25    3: 28        3324: 118012      N/A\nlac-407        general         MIXED        21.03: 25    3: 28         252: 118012      N/A\nlac-408        general         MIXED        17.83: 18   10: 28         252: 118012      N/A\nlac-409        general         MIXED        26.63: 24    4: 28        3324: 118012      N/A\nlac-410        general         MIXED        15.10: 18   10: 28         252: 118012      N/A\nlac-411        general         MIXED        17.07: 22    6: 28         256: 118012      N/A\nlac-412        general         MIXED        17.11: 27    1: 28         252: 118012      N/A\nlac-413        general         MIXED        16.18: 16   12: 28         256: 118012      N/A\nlac-414        general         MIXED        17.79: 18   10: 28         252: 118012      N/A\nlac-415        general         MIXED        16.32: 18   10: 28         252: 118012      N/A\nlac-416        general         MIXED         9.03:  9   19: 28         252: 118012      N/A\nlac-417        general         MIXED        10.28: 18   10: 28        1276: 118012      N/A\nlac-418        general         MIXED         6.25:  6   22: 28         252: 118012      N/A\nlac-419        general       ALLOCATED      19.17: 28    0: 28        2300: 118012      N/A\nlac-420        general       ALLOCATED      14.05: 28    0: 28        8444: 118012      N/A\nlac-421       classres         MIXED         9.21: 17   11: 28        3324: 118012      N/A\nlac-422        general         MIXED        18.12: 26    2: 28        1016: 118012      N/A\nlac-423        general         MIXED        20.79: 24    4: 28        4348: 118012      N/A\nlac-424        general         MIXED        18.08: 27    1: 28        4348: 118012      N/A\nlac-425        general         MIXED        12.11: 25    3: 28         252: 118012      N/A\nlac-426        general         MIXED        12.41: 19    9: 28        1276: 118012      N/A\nlac-427        general         MIXED         9.20:  9   19: 28         252: 118012      N/A\nlac-428        general         DOWN*          N/A:  0   28: 28      118012: 118012      N/A     Climer - waiting repair 4-8-20 [climer@2020-04-08T12:49:48]\nlac-429        general         MIXED        15.89: 27    1: 28        1276: 118012      N/A\nlac-430        general         MIXED        13.12: 18   10: 28         256: 118012      N/A\nlac-431        general         MIXED        24.13: 26    2: 28        1276: 118012      N/A\nlac-432        general         MIXED         5.13: 11   17: 28         252: 118012      N/A\nlac-433        general         MIXED        25.08: 26    2: 28         742: 118012      N/A\nlac-434        general         MIXED        22.59: 26    2: 28        2300: 118012      N/A\nlac-435        general         MIXED        17.03: 24    4: 28         252: 118012      N/A\nlac-436        general         MIXED        20.95: 21    7: 28        3328: 118012      N/A\nlac-437        general         MIXED        20.11: 21    7: 28        1280: 118012      N/A\nlac-438        general         MIXED        13.43: 20    8: 28       37116: 118012      N/A\nlac-439        general         MIXED        27.93: 25    3: 28        3324: 118012      N/A\nlac-440        general       ALLOCATED      24.46: 28    0: 28        2300: 118012      N/A\nlac-441        general         MIXED        13.90: 14   14: 28         252: 118012      N/A\nlac-442        general       ALLOCATED      28.16: 28    0: 28        3266: 118012      N/A\nlac-443        general         MIXED         9.47: 17   11: 28         252: 118012      N/A\nlac-444        general         MIXED         8.46: 26    2: 28         252: 118012      N/A\nlac-445        general       ALLOCATED      28.01: 28    0: 28        7444: 118012      N/A\n\nlac      =&gt;   69.0%(buyin)   98.8%(426)     43.5%: 65.7%(11928)      81.6%(68.2Tb)    70%(384)   Usage%(Total)\n\nnvl-000       piermaro         MIXED        11.84: 18   22: 40      183394: 376162   v100(3:8)\nnvl-001         cmse           MIXED        39.45: 36    4: 40      228706: 376162   v100(8:8)\nnvl-002      DicksonLab      ALLOCATED      36.61: 40    0: 40      208706: 376162   v100(7:8)\nnvl-003      DicksonLab      ALLOCATED      36.77: 40    0: 40      208706: 376162   v100(7:8)\nnvl-004      DicksonLab        MIXED        10.23: 18   22: 40      183394: 376162   v100(3:8)\nnvl-005     alexrd-covid       MIXED         7.60:  8   32: 40        1378: 376162   v100(1:8)\nnvl-006     alexrd-covid       MIXED        10.77: 24   16: 40      239090: 376162   v100(0:8)\nnvl-007        general         MIXED         8.04:  8   32: 40      370162: 376162   v100(0:8)\n\nnvl      =&gt;   87.5%(buyin)  100.0%(  8)     50.4%: 60.0%(  320)      46.0%(2.87Tb)    55%( 64)   Usage%(Total)\n\nqml-000        general       ALLOCATED      40.04: 48    0: 48      446368:3067808      N/A\nqml-001       gmiaslab       ALLOCATED      48.19: 48    0: 48     1037264:1528784      N/A\nqml-002          ged         ALLOCATED      48.05: 48    0: 48     1037264:1528784      N/A\nqml-003     horticulture     ALLOCATED      43.91: 48    0: 48      909300:1015776      N/A\nqml-004       mitchmcg       ALLOCATED      48.27: 48    0: 48     1037264:1528784      N/A\nqml-005        general         MIXED        69.44: 74   22: 96     3198784:6145856      N/A\n\nqml      =&gt;   66.7%(buyin)  100.0%(  6)     88.7%: 93.5%(  336)      48.3%(14.1Tb)    N/A(  0)   Usage%(Total)\n\nskl-000        devolab         MIXED        17.59:  5   35: 40       35682:  85858      N/A\nskl-001        devolab         MIXED        11.33:  7   33: 40       33634:  85858      N/A\nskl-002        devolab         MIXED         5.62:  4   36: 40       36706:  85858      N/A\nskl-003        devolab         MIXED        12.21: 12   28: 40        3938:  85858      N/A\nskl-004        tsangm        ALLOCATED       3.16: 40    0: 40        3938:  85858      N/A\nskl-005       plzbuyin         MIXED         5.22: 12   28: 40        3938:  85858      N/A\nskl-006        allenmc         MIXED         9.52:  6   34: 40       61282:  85858      N/A\nskl-007        allenmc         MIXED         8.55:  6   34: 40       61282:  85858      N/A\nskl-008        allenmc         MIXED         9.07:  9   31: 40       12130:  85858      N/A\nskl-009        allenmc         MIXED         6.03:  6   34: 40       61282:  85858      N/A\nskl-010        allenmc         MIXED        12.01: 12   28: 40        3938:  85858      N/A\nskl-011        allenmc         MIXED        12.03: 12   28: 40        3938:  85858      N/A\nskl-012        seiswei         MIXED        12.12: 12   28: 40        3938:  85858      N/A\nskl-013        junlin          MIXED        24.62:  6   34: 40       61282:  85858      N/A\nskl-014        junlin          MIXED         6.03:  6   34: 40       61282:  85858      N/A\nskl-015        junlin          MIXED        34.88: 34    6: 40        3938:  85858      N/A\nskl-016        junlin          MIXED         5.66: 12   28: 40        3938:  85858      N/A\nskl-017       klausner       ALLOCATED      40.05: 40    0: 40        3938:  85858      N/A\nskl-018       klausner         MIXED        12.16: 12   28: 40        3938:  85858      N/A\nskl-019       klausner         MIXED        12.34: 12   28: 40        3938:  85858      N/A\nskl-020       klausner         MIXED        12.18: 12   28: 40        3938:  85858      N/A\nskl-021        cbcclab       ALLOCATED      32.85: 40    0: 40       21858:  85858      N/A\nskl-022        cbcclab         MIXED        12.28: 12   28: 40        3938:  85858      N/A\nskl-023     guowei-search      MIXED         5.18: 12   28: 40        3938:  85858      N/A\nskl-024       edgerpat         MIXED         7.65: 38    2: 40        3938:  85858      N/A\nskl-025       edgerpat         MIXED        12.18: 12   28: 40        3938:  85858      N/A\nskl-026     guowei-search      MIXED         5.48: 12   28: 40        3938:  85858      N/A\nskl-027     guowei-search    ALLOCATED      38.56: 40    0: 40        3938:  85858      N/A\nskl-028     guowei-search      MIXED         5.07: 12   28: 40        3938:  85858      N/A\nskl-029     guowei-search      MIXED         5.14: 12   28: 40        3938:  85858      N/A\nskl-030     guowei-search      MIXED         5.11: 12   28: 40        3938:  85858      N/A\nskl-031     guowei-search      MIXED         5.07: 12   28: 40        3938:  85858      N/A\nskl-032     guowei-search      MIXED         5.12: 12   28: 40        3938:  85858      N/A\nskl-033     guowei-search      MIXED         5.06: 12   28: 40        3938:  85858      N/A\nskl-034     guowei-search      MIXED         5.23: 12   28: 40        3938:  85858      N/A\nskl-035     guowei-search      MIXED         5.17: 12   28: 40        3938:  85858      N/A\nskl-036     guowei-search      MIXED         5.12: 12   28: 40        3938:  85858      N/A\nskl-037     guowei-search      MIXED        28.03: 28   12: 40       28514:  85858      N/A\nskl-038     guowei-search      MIXED         5.27: 12   28: 40        3938:  85858      N/A\nskl-039     guowei-search      MIXED         5.23: 12   28: 40        3938:  85858      N/A\nskl-040     guowei-search      MIXED         5.13: 12   28: 40        3938:  85858      N/A\nskl-041     guowei-search      MIXED         5.07: 12   28: 40        3938:  85858      N/A\nskl-042     guowei-search      MIXED         5.08: 12   28: 40        3938:  85858      N/A\nskl-043     guowei-search      MIXED         5.07: 12   28: 40        3938:  85858      N/A\nskl-044     guowei-search      MIXED         5.10: 12   28: 40        3938:  85858      N/A\nskl-045     guowei-search      MIXED         5.10: 12   28: 40        3938:  85858      N/A\nskl-046     guowei-search      MIXED         5.14: 13   27: 40        1890:  85858      N/A\nskl-047     guowei-search      MIXED         5.06: 13   27: 40        1890:  85858      N/A\nskl-048     guowei-search      MIXED         5.16: 13   27: 40        1890:  85858      N/A\nskl-049     guowei-search      MIXED         5.07: 13   27: 40        1890:  85858      N/A\nskl-050     guowei-search      MIXED        28.02: 28   12: 40       28514:  85858      N/A\nskl-051     guowei-search      MIXED        28.83: 29   11: 40        8034:  85858      N/A\nskl-052     guowei-search      MIXED         5.21: 25   15: 40        1890:  85858      N/A\nskl-053     guowei-search      MIXED         4.79: 25   15: 40        1890:  85858      N/A\nskl-054     guowei-search      MIXED         3.27: 19   21: 40       14178:  85858      N/A\nskl-055     guowei-search      MIXED         2.68: 16   24: 40       20322:  85858      N/A\nskl-056     guowei-search      MIXED         2.66: 16   24: 40       20322:  85858      N/A\nskl-057        general       ALLOCATED      77.50: 40    0: 40        2466:  85858      N/A\nskl-058        general         MIXED        21.20: 27   13: 40         874:  85858      N/A\nskl-059        general         MIXED        20.06: 31    9: 40         870:  85858      N/A\nskl-060        general         MIXED         8.15:  7   33: 40         866:  85858      N/A\nskl-061        general         MIXED        19.24: 18   22: 40       12130:  85858      N/A\nskl-062        general         MIXED        36.90: 38    2: 40        1914:  85858      N/A\nskl-063        general         MIXED        33.71: 36    4: 40         890:  85858      N/A\nskl-064        general         MIXED        25.42: 26   14: 40         890:  85858      N/A\nskl-065        general         MIXED        38.71: 39    1: 40         890:  85858      N/A\nskl-066        general       ALLOCATED      40.02: 40    0: 40        3938:  85858      N/A\nskl-067        general       ALLOCATED      40.09: 40    0: 40        3938:  85858      N/A\nskl-068        general         MIXED        15.94: 25   15: 40        1890:  85858      N/A\nskl-069        general         MIXED        21.71: 23   17: 40        3942:  85858      N/A\nskl-070        general       ALLOCATED      22.31: 40    0: 40        1894:  85858      N/A\nskl-071        general         MIXED        16.65: 30   10: 40        1890:  85858      N/A\nskl-072        general         MIXED        22.15: 22   18: 40         866:  85858      N/A\nskl-073        general       ALLOCATED      40.09: 40    0: 40        3938:  85858      N/A\nskl-074        general         MIXED        19.91: 22   18: 40         870:  85858      N/A\nskl-075        general         MIXED        37.57: 36    4: 40        4514:  85858      N/A\nskl-076        general       ALLOCATED      38.08: 40    0: 40        1442:  85858      N/A\nskl-077        general         MIXED        23.94: 34    6: 40         866:  85858      N/A\nskl-078        general         MIXED        25.81: 28   12: 40         870:  85858      N/A\nskl-079        general         MIXED        22.93: 36    4: 40         866:  85858      N/A\nskl-080        general         MIXED        16.86: 20   20: 40         866:  85858      N/A\nskl-081        general         MIXED        35.24: 35    5: 40         418:  85858      N/A\nskl-082        general         MIXED        24.07: 30   10: 40         866:  85858      N/A\nskl-083        general         MIXED         2.61:  6   34: 40         866:  85858      N/A\nskl-084        general         MIXED        21.03: 23   17: 40         866:  85858      N/A\nskl-085        general         MIXED        26.66: 25   15: 40         866:  85858      N/A\nskl-086        general         MIXED        39.28: 39    1: 40        1442:  85858      N/A\nskl-087        general         MIXED        17.81: 22   18: 40        1890:  85858      N/A\nskl-088        general         MIXED         5.93:  2   38: 40        3938:  85858      N/A\nskl-089        general         MIXED        16.11: 15   25: 40         866:  85858      N/A\nskl-090        general         MIXED         5.74:  6   34: 40         866:  85858      N/A\nskl-091        general         MIXED         8.68: 14   26: 40         866:  85858      N/A\nskl-092        general         MIXED        16.46: 19   21: 40        1890:  85858      N/A\nskl-093        general         MIXED        22.51: 22   18: 40         870:  85858      N/A\nskl-094        general         MIXED        31.44: 39    1: 40        1906:  85858      N/A\nskl-095        general         MIXED        27.04: 29   11: 40         870:  85858      N/A\nskl-096        general         MIXED         9.76: 10   30: 40         866:  85858      N/A\nskl-097        general         MIXED         9.76: 10   30: 40         866:  85858      N/A\nskl-098        general         MIXED        21.58: 22   18: 40         866:  85858      N/A\nskl-099        general         MIXED        24.98: 36    4: 40         870:  85858      N/A\nskl-100        general         MIXED        20.48: 21   19: 40        1890:  85858      N/A\nskl-101        general         MIXED        18.06: 21   19: 40         866:  85858      N/A\nskl-102        general         MIXED         7.36: 15   25: 40         866:  85858      N/A\nskl-103        general         MIXED        39.21: 39    1: 40        1442:  85858      N/A\nskl-104        general         MIXED        18.07: 27   13: 40        1890:  85858      N/A\nskl-105        general       ALLOCATED       9.77: 40    0: 40        3938:  85858      N/A\nskl-106        general         MIXED        10.78:  6   34: 40        1890:  85858      N/A\nskl-107        general         MIXED        16.02: 18   22: 40         870:  85858      N/A\nskl-108        general         MIXED        22.08: 33    7: 40         866:  85858      N/A\nskl-109        general         MIXED         6.94: 11   29: 40         866:  85858      N/A\nskl-110        general         MIXED         9.05:  6   34: 40        3938:  85858      N/A\nskl-111        general         MIXED        37.53: 37    3: 40         418:  85858      N/A\nskl-112        general         MIXED         5.12:  4   36: 40         866:  85858      N/A\nskl-113   zayernouri_fmath     MIXED         9.68: 16   24: 40        2402: 182626      N/A\nskl-114   zayernouri_fmath     MIXED         9.21: 16   24: 40        2402: 182626      N/A\nskl-115   zayernouri_fmath     MIXED         9.39: 16   24: 40        2402: 182626      N/A\nskl-116       niederhu         MIXED         9.19: 16   24: 40        2402: 182626      N/A\nskl-117        daylab          MIXED        12.15: 12   28: 40        2402: 182626      N/A\nskl-118        junlin          MIXED        12.01: 12   28: 40        2402: 182626      N/A\nskl-119       pollyhsu         MIXED        24.21: 24   16: 40        2402: 182626      N/A\nskl-120      yueqibuyin      ALLOCATED      40.23: 40    0: 40       18786: 182626      N/A\nskl-121      yueqibuyin      ALLOCATED      40.05: 40    0: 40       18786: 182626      N/A\nskl-122      yueqibuyin      ALLOCATED      40.09: 40    0: 40       18786: 182626      N/A\nskl-123      yueqibuyin      ALLOCATED      40.09: 40    0: 40       18786: 182626      N/A\nskl-124       plzbuyin         MIXED        12.01: 12   28: 40        2402: 182626      N/A\nskl-125          hcy           MIXED        12.06: 12   28: 40        2402: 182626      N/A\nskl-126          hcy           MIXED        12.02: 12   28: 40        2402: 182626      N/A\nskl-127          hcy           MIXED        10.16: 10   30: 40       10594: 182626      N/A\nskl-128        junlin          MIXED        37.04: 37    3: 40       14690: 182626      N/A\nskl-129   zayernouri_fmath     MIXED        11.95: 30   10: 40       20834: 182626      N/A\nskl-130   zayernouri_fmath     MIXED         4.10:  4   36: 40      133474: 182626      N/A\nskl-131   zayernouri_fmath     MIXED         6.21: 12   28: 40       67938: 182626      N/A\nskl-132         qian           MIXED        13.45: 34    6: 40        7522: 376162      N/A\nskl-133        vmante        IDLE+DRAIN      0.01:  0   40: 40      376162: 376162      N/A     Low RealMemory [slurm@2020-04-22T09:05:12]\nskl-134        liulab          MIXED         9.15:  9   31: 40        7522: 376162      N/A\nskl-135        chenlab         MIXED        11.39: 24   16: 40       81250: 376162      N/A\nskl-136        junlin          MIXED        18.01: 18   22: 40        7522: 376162      N/A\nskl-137       mitchmcg         MIXED        13.91: 20   20: 40       97634: 376162      N/A\nskl-138       eisenlohr        MIXED         8.38:  8   32: 40      245090: 376162      N/A\nskl-139       davidroy         MIXED        17.32: 24   16: 40       15714: 376162      N/A\nskl-140    shadeash-colej      MIXED        35.14: 35    5: 40       79202: 763234      N/A\nskl-141    shadeash-colej    ALLOCATED      40.17: 40    0: 40      566626: 763234      N/A\nskl-142        cbcclab       ALLOCATED      34.51: 40    0: 40      699234: 763234      N/A\nskl-143        general         MIXED        32.10: 32    8: 40      370018: 763234      N/A\nskl-144     guowei-search      MIXED         7.21: 35    5: 40      152930: 763234      N/A\nskl-145        general       ALLOCATED      32.83: 40    0: 40      304482: 763234      N/A\nskl-146     guowei-search      MIXED        36.05: 36    4: 40      615778: 763234      N/A\nskl-147        general         MIXED         3.03: 13   27: 40       42338: 763234      N/A\nskl-148       davidroy         MIXED        19.03: 19   21: 40       19810: 376162      N/A\nskl-149       davidroy         MIXED        19.02: 19   21: 40       19810: 376162      N/A\nskl-150       davidroy         MIXED        35.41: 35    5: 40        3426: 376162      N/A\nskl-151       davidroy         MIXED        18.07: 37    3: 40       42338: 376162      N/A\nskl-152       davidroy         MIXED        19.02: 19   21: 40       19810: 376162      N/A\nskl-153       davidroy       ALLOCATED      40.38: 40    0: 40      179554: 376162      N/A\nskl-154       davidroy         MIXED        35.09: 35    5: 40        3426: 376162      N/A\nskl-155       davidroy         MIXED        35.24: 35    5: 40        3426: 376162      N/A\nskl-156       davidroy         MIXED        19.02: 19   21: 40       19810: 376162      N/A\nskl-157       davidroy         MIXED        23.04: 23   17: 40        3426: 376162      N/A\nskl-158       davidroy         MIXED        23.06: 23   17: 40        3426: 376162      N/A\nskl-159       davidroy       ALLOCATED      40.11: 40    0: 40      179554: 376162      N/A\nskl-160       davidroy       ALLOCATED      40.29: 40    0: 40      146786: 376162      N/A\nskl-161   zayernouri_fmath     MIXED        22.03: 22   18: 40        3426: 376162      N/A\nskl-162        general         MIXED        37.09: 37    3: 40        3426: 376162      N/A\nskl-163        general       ALLOCATED      40.48: 40    0: 40      107874: 376162      N/A\nskl-164        general       ALLOCATED      40.10: 40    0: 40       83298: 376162      N/A\nskl-165        general       ALLOCATED       2.42: 40    0: 40      179554: 376162      N/A\nskl-166     guowei-search      MIXED        11.29: 31    9: 40        3426: 376162      N/A\nskl-167     guowei-search      MIXED        11.12: 30   10: 40       23906: 376162      N/A\n\nskl      =&gt;   62.5%(buyin)   99.4%(168)     45.6%: 55.6%( 6720)      80.3%(28.4Tb)    N/A(  0)   Usage%(Total)\n\nvim-000          hsu           MIXED      1474.82: 23   41: 64       53152:3067808      N/A\nvim-001        general         MIXED        10.42: 31   33: 64      970656:3067808      N/A\nvim-002          ccg           MIXED        66.14: 63   81:144     5427008:6145856      N/A\n\nintel14  =&gt;   34.5%(buyin)   91.7%(168)     47.8%: 62.7%( 3576)      60.1%(31.1Tb)    97%( 78)   Usage%(Total)\nintel16  =&gt;   69.0%(buyin)   98.8%(429)     55.2%: 65.1%(12200)      76.6%(79.9Tb)    70%(384)   Usage%(Total)\nintel18  =&gt;   63.6%(buyin)   99.4%(176)     45.8%: 55.8%( 7040)      77.1%(31.3Tb)    55%( 64)   Usage%(Total)\n\nSummary  =&gt;   60.3%(buyin)   97.4%(773)     51.2%: 61.9%(22816)      73.1%( 142Tb)    72%(526)   Usage%(Total)\n</code></pre>","title":"node_status"},{"location":"obtain_an_hpcc_account/","text":"<p>Every user needs to have an account to use the HPCC. To obtain an HPCC account, follow the directions below.</p>","title":"Obtain an HPCC account"},{"location":"obtain_an_hpcc_account/#current-msu-affiliated-student-or-personnel","text":"<p>HPCC accounts are free for all MSU researchers. To obtain (or re-activate) an account, a tenure-track faculty MSU Principal Investigator (PI) must complete a New Account Request for themselves and their collaborators (students, staff, post-docs, and collaborators with NetIDs.) Information required to complete the form includes:</p> <ul> <li>a list of the names and MSU NetIDs of collaborators (students,     post-docs, visiting scholars and/or off-campus colleagues) requiring     accounts.\u00a0For collaborators without a MSU NetID, the PI can\u00a0sponsor     MSU Department Sponsored NetIDs by paying     MSU     ID Office with a department account     number. See below section for more details on departmental NetIDs.</li> </ul>  <p>Note</p> <p>All MSU Net IDs used to access the HPCC must be associated with a valid email address.</p>  <ul> <li> <p>a statement on whether or not     export     controlled software or data will be used</p> </li> <li> <p>a project abstract (description about the user's research.)</p> </li> </ul> <p>By applying for a HPCC account, the Principal Investigator is agreeing that all group members will abide by MSU's Acceptable Use Policy.</p>","title":"Current MSU-affiliated student or personnel"},{"location":"obtain_an_hpcc_account/#icer-workshops","text":"<p>If you are going to attend an ICER workshop, you may get a temporary HPCC account. Please contact ICER</p>","title":"ICER workshops"},{"location":"obtain_an_hpcc_account/#external-collaborators","text":"<p>To request an HPCC account for an external collaborator, please obtain a login-only, non-email FPID NetID for the collaborator first. Please contact the ID Office at idoffice@msu.edu or 517-355-4500 for more information, or visit their website. Once the NetID is acquired, a MSU PI can follow the above instructions to apply for their HPCC account. Since the purchased NetID is not associated with a MSU e-mail account, the PI must provide a current contact e-mail address of the external collaborator when filling out the New Account Request form.</p>","title":"External collaborators"},{"location":"obtain_an_hpcc_account/#previous-users-affiliated-with-msu","text":"<p>If you were previously affiliated with MSU and had a HPCC account, then you may find that your HPCC account was disabled after a time. If you are still collaborating with a regular MSU faculty or staff member, you will need a Principal Investigator (PI) with a tenure-track appointment at MSU to sponsor the renewal of your account on an annual basis. The form for sponsoring the renewal of an existing HPCC account is at Sponsored Renewal. Only PIs may fill out this form.</p>","title":"Previous users affiliated with MSU"},{"location":"obtain_an_hpcc_account/#other-universities-in-michigan","text":"<p>If you are affiliated with the following universities, please contact the respective personnel to apply for a HPCC account:</p>    University Contact More information     Oakland University (oakland.edu) Mario Nowak: nowak@oakland.eduThomas Hajek: hajek@oakland.edu Oakland University info page   Kettering University (kettering.edu)  (kettering buy-in) Salomon Turgman Cohen: sturgman@gmail.com Kettering info   (ryankettering buy-in) Gillian Ryan:gryan@kettering.edu    Western Michigan University (wmich.edu) Joel Fletcher:joel.fletcher@wmich.edu Western Michigan info   Central Michigan University (cmich.edu) Mel Taylor:taylo1ml@msu.edu or mel.taylor@cmich.edu Central Michigan info","title":"Other universities in Michigan"},{"location":"orthomcl-pipeline%202/","text":"<p>OrthoMCL Pipeline (https://github.com/apetkau/orthomcl-pipeline) is a wrapper that automates running of OrthoMCL. If you prefer to run OrthoMCL from scratch, please skip this tutorial.</p>","title":"orthomcl-pipeline"},{"location":"orthomcl-pipeline%202/#installation-guide","text":"<p>You could install orthomcl pipeline to your home directory (or research space), following the instruction of installing OrthoMCL pipeline. All the Perl dependencies have been installed by iCER staff, and you only need to run a couple of commands to complete the installation. Importantly, it's assumed that you have already prepared your MySQL configuration file (see https://wiki.hpcc.msu.edu/x/aYe1).</p>","title":"Installation guide"},{"location":"orthomcl-pipeline%202/#sample-installation","text":"<p>I am going to install the pipeline in a subdirectory under my home <code>~/Software/</code>.</p> <p>Installing OrthoMCL Pipeline</p> <pre><code>ssh dev-intel18\n\n# Load necessary modules\nmodule purge\nmodule load icc/2016.3.210-GCC-5.4.0-2.26 impi/5.1.3.181\nmodule load OrthoMCL/2.0.9-custom-Perl-5.24.0\nmodule load BLAST/2.2.26-Linux_x86_64\nmodule load GCCcore/5.4.0 libxml2/2.9.4\n\n\n# Download source and configure\ncd Software\ngit clone https://github.com/apetkau/orthomcl-pipeline.git\ncd orthomcl-pipeline\nperl scripts/orthomcl-pipeline-setup.pl # set paths to dependencies\ncat etc/orthomcl-pipeline.conf # parameters in this file can be adjusted; consult the instruction linked above\n    # ---\n    # blast:\n    #   F: 'm S'\n    #   b: '100000'\n    #   e: '1e-5'\n    #   v: '100000'\n    # filter:\n    #   max_percent_stop: '20'\n    #   min_length: '10'\n    # mcl:\n    #   inflation: '1.5'\n    # path:\n    #   blastall: /opt/software/BLAST/2.2.26-Linux_x86_64/bin/blastall\n    #   formatdb: /opt/software/BLAST/2.2.26-Linux_x86_64/bin/formatdb\n    #   mcl: /opt/software/MCL/14.137-intel-2016b/bin/mcl\n    #   orthomcl: /opt/software/OrthoMCL/orthomclsoftware-custom/bin\n    # scheduler: fork\n    # split: '4'\n\n\n# Testing\nexport PATH=~/Software/orthomcl-pipeline/bin:~/Software/orthomcl-pipeline/scripts:$PATH\nperl t/test_pipeline.pl -m ~/Practice/general_test/orthomcl/my_orthomcl_dir/orthomcl.config -s fork -t ~/tmp # replace the path to orthomcl.config with your own\n</code></pre>","title":"Sample installation"},{"location":"orthomcl-pipeline%202/#example-ortholog-identification","text":"<p>The tutorial is adapted from a tutorial hosted at https://github.com/apetkau/microbial-informatics-2014/tree/master/labs/orthomcl. We strongly recommend that you read it fully before starting the hands-on practice below, which is a much simplified version of the original one and serves as a demo only. The datasets containing a set of V. Cholerae genomes are located in <code>mnt/research/common-data/Bio/orthomcl-data/</code>.</p> <pre><code>ssh dev-intel18\n\n# Then go to your orthomcl working directory\n\n# Load necessary modules\nmodule purge\nmodule load icc/2016.3.210-GCC-5.4.0-2.26  impi/5.1.3.181\nmodule load OrthoMCL/2.0.9-custom-Perl-5.24.0\nmodule load BLAST/2.2.26-Linux_x86_64\nmodule load GCCcore/5.4.0 libxml2/2.9.4\nexport PATH=~/Software/orthomcl-pipeline/bin:~/Software/orthomcl-pipeline/scripts:$PATH\n\n# Run orthomcl pipeline (replace the path to orthomcl.config with your own)\northomcl-pipeline -i /mnt/research/common-data/Bio/orthomcl-data -o orthomcl_out_tmp -m ~/Practice/general_test/orthomcl/my_orthomcl_dir/orthomcl.config --nocompliant\n\n# Visualize the results by drawing a Venn Diagram using a pipeline utility script\nnml_parse_orthomcl.pl -i orthomcl_out_tmp/groups/groups.txt -g /mnt/research/common-data/Bio/orthomcl-data/genome-groups.txt -s --draw -o orthomcl-stats.txt --genes\n\n# View the Venn Diagram plot (just for demo; you should transfer the svg to your local computer for better display effect)\njava -jar /opt/software/batik/batik-1.9/batik-squiggle-1.9.jar genome-groups.txt.svg\n</code></pre> <p>Side note: as mentioned in the full tutorial, you need to answer \"yes\" to the database removal question in the course of the run.</p>","title":"Example: ortholog identification"},{"location":"orthomcl-pipeline/","text":"<p>OrthoMCL Pipeline (https://github.com/apetkau/orthomcl-pipeline) is a wrapper that automates running of OrthoMCL. If you prefer to run OrthoMCL from scratch, please skip this tutorial.</p>","title":"orthomcl-pipeline"},{"location":"orthomcl-pipeline/#installation-guide","text":"<p>You could install orthomcl pipeline to your home directory (or research space), following the instruction of installing OrthoMCL pipeline. All the Perl dependencies have been installed by iCER staff, and you only need to run a couple of commands to complete the installation. Importantly, it's assumed that you have already prepared your MySQL configuration file (see https://wiki.hpcc.msu.edu/x/aYe1).</p>","title":"Installation guide"},{"location":"orthomcl-pipeline/#sample-installation","text":"<p>I am going to install the pipeline in a subdirectory under my home <code>~/Software/</code>.</p> <p>Installing OrthoMCL Pipeline</p> <pre><code>ssh dev-intel18\n\n# Load necessary modules\nmodule purge\nmodule load icc/2016.3.210-GCC-5.4.0-2.26 impi/5.1.3.181\nmodule load OrthoMCL/2.0.9-custom-Perl-5.24.0\nmodule load BLAST/2.2.26-Linux_x86_64\nmodule load GCCcore/5.4.0 libxml2/2.9.4\n\n\n# Download source and configure\ncd Software\ngit clone https://github.com/apetkau/orthomcl-pipeline.git\ncd orthomcl-pipeline\nperl scripts/orthomcl-pipeline-setup.pl # set paths to dependencies\ncat etc/orthomcl-pipeline.conf # parameters in this file can be adjusted; consult the instruction linked above\n    # ---\n    # blast:\n    #   F: 'm S'\n    #   b: '100000'\n    #   e: '1e-5'\n    #   v: '100000'\n    # filter:\n    #   max_percent_stop: '20'\n    #   min_length: '10'\n    # mcl:\n    #   inflation: '1.5'\n    # path:\n    #   blastall: /opt/software/BLAST/2.2.26-Linux_x86_64/bin/blastall\n    #   formatdb: /opt/software/BLAST/2.2.26-Linux_x86_64/bin/formatdb\n    #   mcl: /opt/software/MCL/14.137-intel-2016b/bin/mcl\n    #   orthomcl: /opt/software/OrthoMCL/orthomclsoftware-custom/bin\n    # scheduler: fork\n    # split: '4'\n\n\n# Testing\nexport PATH=~/Software/orthomcl-pipeline/bin:~/Software/orthomcl-pipeline/scripts:$PATH\nperl t/test_pipeline.pl -m ~/Practice/general_test/orthomcl/my_orthomcl_dir/orthomcl.config -s fork -t ~/tmp # replace the path to orthomcl.config with your own\n</code></pre>","title":"Sample installation"},{"location":"orthomcl-pipeline/#example-ortholog-identification","text":"<p>The tutorial is adapted from a tutorial hosted at https://github.com/apetkau/microbial-informatics-2014/tree/master/labs/orthomcl. We strongly recommend that you read it fully before starting the hands-on practice below, which is a much simplified version of the original one and serves as a demo only. The datasets containing a set of V. Cholerae genomes are located in <code>mnt/research/common-data/Bio/orthomcl-data/</code>.</p> <pre><code>ssh dev-intel18\n\n# Then go to your orthomcl working directory\n\n# Load necessary modules\nmodule purge\nmodule load icc/2016.3.210-GCC-5.4.0-2.26  impi/5.1.3.181\nmodule load OrthoMCL/2.0.9-custom-Perl-5.24.0\nmodule load BLAST/2.2.26-Linux_x86_64\nmodule load GCCcore/5.4.0 libxml2/2.9.4\nexport PATH=~/Software/orthomcl-pipeline/bin:~/Software/orthomcl-pipeline/scripts:$PATH\n\n# Run orthomcl pipeline (replace the path to orthomcl.config with your own)\northomcl-pipeline -i /mnt/research/common-data/Bio/orthomcl-data -o orthomcl_out_tmp -m ~/Practice/general_test/orthomcl/my_orthomcl_dir/orthomcl.config --nocompliant\n\n# Visualize the results by drawing a Venn Diagram using a pipeline utility script\nnml_parse_orthomcl.pl -i orthomcl_out_tmp/groups/groups.txt -g /mnt/research/common-data/Bio/orthomcl-data/genome-groups.txt -s --draw -o orthomcl-stats.txt --genes\n\n# View the Venn Diagram plot (just for demo; you should transfer the svg to your local computer for better display effect)\njava -jar /opt/software/batik/batik-1.9/batik-squiggle-1.9.jar genome-groups.txt.svg\n</code></pre>  <p>Note</p> <p>As mentioned in the full tutorial, you need to answer \"yes\" to the database removal question in the course of the run.</p>","title":"Example: ortholog identification"},{"location":"parallel_computing/","text":"<p>Here we introduce three basic parallel models: Shared Memory, Distributed Memory and Hybrid Model.</p>","title":"Parallel Computing"},{"location":"parallel_computing/#shared-memory-with-threads","text":"<ul> <li>A main program loads and acquires all of the necessary resources to run the \"heavy weight\" process.</li> <li>It performs some serial work, and then creates a number of threads (\"light weight\") running by CPU cores concurrently.</li> <li>Each thread can have local data, but also, shares the entire resources, including RAM memory of the main program.</li> <li>Threads communicate with each other through global memory (RAM). This requires synchronization operations to ensure that no than one thread is updating the same RAM address at any time.</li> <li>Threads can come and go, but the main program remains present to provide the necessary shared resources until the application has completed.</li> </ul> <p>Examples: POSIX Threads, OpenMP, CUDA threads for GPUs</p>","title":"Shared Memory with Threads"},{"location":"parallel_computing/#distributed-memory-with-tasks","text":"<ul> <li> <p>A main program creates a set of tasks that use their own local memory during computation. Multiple tasks can reside on the same physical machine and/or across an arbitrary number of machines.</p> </li> <li> <p>Tasks exchange data through communications by sending and receiving messages through fast network (e.g. infinite band).</p> </li> <li> <p>Data transfer usually requires cooperative operations to be performed by each process. For example, a send operation must have a matching receive operation.</p> </li> <li> <p>Synchronization operations are also required to prevent race condition. Example: Message Passing Interface (MPI)</p> </li> </ul>","title":"Distributed Memory with Tasks"},{"location":"parallel_computing/#hybrid-parallel","text":"<ul> <li>A hybrid model combines more than one of the previously described programming models.</li> <li>A simple example is the combination of the message passing model (MPI) with the threads model (OpenMP).</li> </ul>  <ul> <li> <p>Threads perform computationally intensive kernels using local, on-node data</p> </li> <li> <p>Communications between processes on different nodes occurs over the network using MPI</p> </li> </ul>  <ul> <li> <p>Works well to the most popular hardware environment of clustered multi/many-core machines.</p> </li> <li> <p>Other example: MPI with CPU-GPU (Graphics Processing Unit)</p> </li> </ul> <p>Hybrid OpenMP-MPI Parallel Model:</p> <p></p> <p>Hybrid CUDA-MPI Parallel Model:</p> <p> (Click Source)</p>","title":"Hybrid Parallel"},{"location":"qs/","text":"<p>Display job list.</p> <pre><code>$ qs -h\n\nUsage:   -a  --&gt;  all jobs\n         -F  --&gt;  all fields\n         -j  --&gt;  specific job\n         -u  --&gt;  specific user\n         -r  --&gt;  all job array elements\n\nDefault: -u $USER\n</code></pre>","title":"qs"},{"location":"rjags/","text":"<p>To use {rjags}, first load R/3.5.1 and JAGS\u00a0from a dev-node (dev-intel16 or dev-intel18) as follows:</p>    **Loading R/3.5.1 and JAGS**    <pre><code>module purge\nmodule load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2\nmodule load JAGS/4.3.0\n</code></pre>   <p>Next, we will run a short example of data analysis using rjags. This example comes from\u00a0this tutorial which presents many Bayesian models using this package.</p> <p>To invoke R from the command line:</p>   <pre><code>R --vanilla\n</code></pre>   <p>Then, in the R console, you can run the following codes (for detailed explanation refer to the tutorial mentioned above):</p>    **Sample R code using {rjags} commands**    <pre><code>library(rjags)\ndata &lt;- read.csv(\"data1.csv\")\nN &lt;- length(data$y)\ndat &lt;- list(\"N\" = N, \"y\" = data$y, \"V\" = data$V)\ninits &lt;- list( d = 0.0 )\n\njags.m &lt;- jags.model(file = \"aspirinFE.txt\", data=dat, inits=inits, n.chains=1, n.adapt=500)\nparams &lt;- c(\"d\", \"OR\")\nsamps &lt;- coda.samples(jags.m, params, n.iter=10000)\nsummary(window(samps, start=5001))\nplot(samps)\n</code></pre>   <p>where the two input files, data1.csv and aspirinFE.txt, need to be located in the working directory. The content of the two files is below.</p>    **data1.csv**    <pre><code>N,y,V\n1,0.3289011,0.0388957\n2,0.3845458,0.0411673\n3,0.2195622,0.0204915\n4,0.2222206,0.0647646\n5,0.2254672,0.0351996\n6,-0.1246363,0.0096167\n7,0.1109658,0.0015062\n</code></pre>      **aspirinFE.txt**    <pre><code>model {\n\n    for ( i in 1:N ) {\n\n        P[i] &lt;- 1/V[i]\n        y[i] ~ dnorm(d, P[i])\n    }\n\n    ### Define the priors\n    d ~ dnorm(0, 0.00001)\n\n    ### Transform the ln(OR) to OR\n    OR &lt;- exp(d)\n}\n</code></pre>   <p>A screen shot of the entire run including the output figures is attached here.</p> <p></p>   ## Attachments:     [rjags.png](attachments/22709657/22709656.png) (image/png)","title":"rjags"},{"location":"rstan/","text":"<p>The example here follows that in\u00a0RStan Getting Started.\u00a0To test rstan on the HPCC, first load R 3.6.2:</p> <p>module purge module load GCC/8.3.0 OpenMPI/3.1.4\u00a0R/3.6.2-X11-20180604</p> <p>As of February 2020, the rstan version is 2.19.2.</p> <p>The stan model file \"8schools.stan\" contains:</p>   <pre><code>data {\n  int&lt;lower=0&gt; J;         // number of schools \n  real y[J];              // estimated treatment effects\n  real&lt;lower=0&gt; sigma[J]; // standard error of effect estimates \n}\nparameters {\n  real mu;                // population treatment effect\n  real&lt;lower=0&gt; tau;      // standard deviation in treatment effects\n  vector[J] eta;          // unscaled deviation from mu by school\n}\ntransformed parameters {\n  vector[J] theta = mu + tau * eta;        // school treatment effects\n}\nmodel {\n  target += normal_lpdf(eta | 0, 1);       // prior log-density\n  target += normal_lpdf(y | theta, sigma); // log-likelihood\n}\n</code></pre>   <p>The R code (\"run.R\") to run stan model contains:</p>   <pre><code>library(\"rstan\")\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\nschools_dat &lt;- list(J = 8,\ny = c(28, 8, -3, 7, -1, 1, 18, 12),\nsigma = c(15, 10, 16, 11, 9, 11, 10, 18))\nfit &lt;- stan(file = '8schools.stan', data = schools_dat)\nprint(fit)\npairs(fit, pars = c(\"mu\", \"tau\", \"lp__\"))\nla &lt;- extract(fit, permuted = TRUE) # return a list of arrays\nmu &lt;- la$mu\n</code></pre>   <p>To run the model from the command line:</p> <p><code>Rscript run.R</code></p> <p>In addition to the results printed to the stdout, there is an R object file named 8schools.rds generated. This is due to that we've set auto_write to TRUE in run.R. More about the auto_write option:</p>  <p>Logical, defaulting to the value of rstan_options(\"auto_write\"), indicating whether to write the object to the hard disk using saveRDS. Although this argument is FALSE by default, we recommend calling rstan_options(\"auto_write\" = TRUE) in order to avoid unnecessary recompilations. If file is supplied and its dirname is writable, then the object will be written to that same directory, substituting a .rds extension for the .stan extension. Otherwise, the object will be written to the tempdir.</p>","title":"rstan"},{"location":"singularity_01/","text":"<p>Singularity is installed on our HPCC. However, you may want to develop your own containers first on a local machine.</p> <p>Many HPC centers including MSU HPCC do not allow Docker containers through Docker. However, Singularity is compatible with Docker, and you can use Docker containers through Singularity. There are a few distinct difference between Docker and Singularity.</p> <p>Docker:</p> <ul> <li>Inside a Docker image, the user's privilege is escalated to root on the host system. This privilege is not supported by most HPCC including MSU HPCC. It means that Docker will not be installed on our system. </li> </ul> <p>Singularity:</p> <ul> <li>User has root privileges if elevated with \"sudo\" when a container runs.</li> <li>Can run and modify Docker images and containers  These key difference make Singularity be installed on most HPCC. In addition, virtually all Docker containers can be run through Singularity, users can effectively run Docker on MSU HPCC.</li> </ul> <p>### Installation  Singularity exits as two major version, 2 and 3. Current version on MSU HPCC is 3.5.3. Therefore, in this tutorial, I will use version 3.</p> <p>To Install Singularity on your local machine, click here:  https://www.sylabs.io/guides/3.0/user-guideinstallation.html#installation</p> <ul> <li>The version of singularity on MSU HPCC is currently 3.4.1. The official documentation for this version is at https://www.sylabs.io/guides/3.4/user-guide/index.html. All singularity commands are built into the system such as 'singularity shell' and 'singularity exec', which means you can invoke these commands directly from the command line.</li> </ul>","title":"Singularity: I. Introduction"},{"location":"singularity_01/#check-installation","text":"<p>When you install Singularity on your local machine, then you can check the installation with</p> <pre><code>$ singularity pull shub://vsoch/hello-world\nINFO:    Downloading shub image\n59.75 MiB / 59.75 MiB      [========================================================================================] 100.00% 10.46 MiB/s 5s\n</code></pre> <p>In the above example, I used the Singularity Hub \u201cunique resource identifier,\u201d or uri, \"shub://\" which tells the software to run an image from Singularity Hub. To get help, you can use the help command which gives a general overview of Singularity options and subcommands as follows:</p> <pre><code>$ singularity --help\n\nLinux container platform optimized for High Performance Computing (HPC) and\nEnterprise Performance Computing (EPC)\n\nUsage:\n  singularity [global options...]\n\nDescription:\n  Singularity containers provide an application virtualization layer enabling\n  mobility of compute via both application and environment portability. With\n  Singularity one is capable of building a root file system that runs on any\n  other Linux system where Singularity is installed.\n\nOptions:\n  -d, --debug     print debugging information (highest verbosity)\n  -h, --help      help for singularity\n  --nocolor   print without color output (default False)\n  -q, --quiet     suppress normal output\n  -s, --silent    only print errors\n  -v, --verbose   print additional information\n      --version   version for singularity\n ...\n</code></pre> <p>You can use the help command if you want to see the information about subcommands. For example, to see the pull command help,</p> <pre><code>$ singularity help pullPull an image from a URI\n\nUsage:\nsingularity pull [pull options...] [output file] &lt;URI&gt;\n\nDescription:\nThe 'pull' command allows you to download or build a container from a given\nURI. Supported URIs include:\n\nlibrary: Pull an image from the currently configured library\nlibrary://user/collection/container[:tag]\n\ndocker: Pull an image from Docker Hub\ndocker://user/image:tag\n\nshub: Pull an image from Singularity Hub\nshub://user/image:tag\n\noras: Pull a SIF image from a supporting OCI registry\noras://registry/namespace/image:tag\n\nhttp, https: Pull an image using the http(s?) protocol\nhttps://library.sylabs.io/v1/imagefile/library/default/alpine:latest\n...\n</code></pre>","title":"Check installation"},{"location":"singularity_01/#downloading-pre-built-images","text":"<p>I already downloaded a pre-built image \"hello-world\" from shub, one of the registries, using pull command. This is the easiest way to use Singularity.</p> <p>You can use the pull command to download pre-built images from a number of Container Registries, here we\u2019ll be focusing on the Singularity-Hub or DockerHub. The following are some of container registries.</p> <ul> <li>library - images hosted on Sylabs Cloud</li> <li>shub - images hosted on Singularity Hub</li> <li>docker - images hosted on Docker Hub</li> <li>localimage - images saved on your machine</li> <li>yum - yum based systems such as CentOS and Scientific Linux</li> <li>debootstrap - apt based systems such as Debian and Ubuntu</li> <li>arch - Arch Linux</li> <li>busybox - BusyBox</li> <li>zypper - zypper based systems such as Suse and OpenSuse</li> </ul>","title":"Downloading pre-built images"},{"location":"singularity_01/#pulling-an-images-from-sylabs-cloud-library","text":"<p>In this example, I will pull a base Alpine container from Sylabs cloud:</p> <pre><code>$ singularity pull library://sylabsed/linux/alpine\nINFO:    Downloading library image\n 2.08 MiB / 2.08 MiB [===========================================================================================] 100.00% 4.74 MiB/s 0s\n</code></pre> <p>You can rename the container using the \u2013name flag:</p> <pre><code>$ singularity pull --name my_alpine.sif library://sylabsed/linux/alpine\nINFO:    Downloading library image\n 2.08 MiB / 2.08 MiB [===========================================================================================] 100.00% 9.65 MiB/s 0s\n</code></pre> <p>The above example will save the image as \"my_alpine.sif\"</p>","title":"Pulling an images from Sylabs cloud library"},{"location":"singularity_01/#pulling-an-images-from-docker-hub","text":"<p>This example pulls an Alpine image from Docker hub</p> <pre><code>$ singularity pull docker://alpine\nINFO:    Converting OCI blobs to SIF format\n INFO:    Starting build...\nGetting image source signatures\nCopying blob df20fa9351a1 done\nCopying config 0f5f445df8 done\nWriting manifest to image destination\nStoring signatures\n2020/08/20 15:53:52  info unpack layer: sha256:df20fa9351a15782c64e6dddb2d4a6f50bf6d3688060a34c4014b0d9a752eb4c\nINFO:    Creating SIF file...\nINFO:    Build complete: alpine_latest.sif\n</code></pre>","title":"Pulling an images from Docker hub"},{"location":"singularity_01/#interact-with-images","text":"<p>You can interact with images with shell, exec, and run commands. To learn how to interact with images, let's first pull an image \"lolcow_latest.sif\" from the libray.</p> <pre><code>$ singularity pull library://sylabsed/examples/lolcow\n</code></pre> <p>shell</p> <p>The shell command allows you to spawn a new shell within your container and interact with it as if it is a virtual machine.</p> <pre><code>$singularity shell lolcow_latest.sif\n  Singularity&gt;\n</code></pre> <p>The change in prompt indicates that you have entered the container. Once inside of a container, you are the same user as you are on the host system.</p> <pre><code>Singularity&gt;whoami\nchoiyj\n</code></pre> <p>To exit from a container, type exit.</p> <pre><code>Singularity&gt;exit\n$\n</code></pre> <p>exec The exec command allows you to execute a custom command within a container by specifying the image file. For instance, to execute the cowsay program within the lolcow_latest.sif container:</p> <pre><code>$ singularity exec lolcow_latest.sif cowsay container camp rocks\n______________________\n&lt; container camp rocks &gt;\n ----------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> <p>You can also use shell command to run the program in the container.</p> <pre><code>Singularity&gt;  cowsay container camp rocks\n ______________________\n&lt; container camp rocks &gt;\n ----------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> <p>run Singularity containers contain runscripts. These are user defined scripts which define the actions of a container when user runs it. The runscript can be performed with the run command, or simply by calling the container as though it were an executable.                </p> <pre><code>$ singularity run lolcow_latest.sif\n _________________________________________\n/ You're ugly and your mother dresses you \\\n\\ funny.                                  /\n -----------------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre>","title":"interact with images"},{"location":"singularity_01/#cache-setting","text":"<p>By default, Singularity uses a temporary directory to save Docker files as tarballs:</p> <pre><code>$ ls ~/.singularity\ncache/  docker/  metadata/\n$ ls .singularity/docker/\nsha256:0efe07335a049e6afcd757db2d17ba37a12b717eb807acb03ddf3cd756b9fc2a.tar.gz\nsha256:43a98c1873995475a895f3d79f405232ef5230076b3f610c949c2e8341743af7.tar.gz\nsha256:6b1bb01b3a3b72463ae8ac5666d57b28f1a21d5256271910ac8df841aa04ecd1.tar.gz\nsha256:c2ca09a1934b951505ecc4d6b2e4ab7f9bf27bcdfb8999d0181deca74daf7683.tar.gz\nsha256:d6c3619d2153ffdefa4a9c19f15c5d566ce271b397a84537baa9ee45b24178f2.tar.gz\n</code></pre> <p>You can change these by theses cache directories by specifying the location on your localhost as following:</p> <pre><code>$ mkdir -p $SCRATCH/singularity_tmp\n$ mkdir -p $SCRATCH/singularity_scratch\n$ SINGULARITY_TMPDIR=$SCRATCH/singularity_scratch SINGULARITY_CACHEDIR=$SCRATCH/singularity_tmp singularity --debug pull --name ubuntu-tmpdir.sif docker://ubuntu\n</code></pre>","title":"Cache setting"},{"location":"singularity_01/#creating-writable-containers-with-sandbox-options","text":"<p>If you want to build a container within a writable directory (called a sandbox), you can do that with --sandbox option. We\u2019ve already covered how you can pull an existing container from Docker Hub, but we can also build a Singularity container from docker using the build command:</p> <pre><code>$ singularity build --sandbox ubuntu-latest/  docker://ubuntu\n</code></pre> <p>With --sandbox option, you can changes and write files within the directory, but those changes will not remain when you finished using the container. To make those changes persistent, use --writable option when you start a container.</p> <pre><code>$ singularity shell --writable ubuntu-latest/\n</code></pre>","title":"Creating writable containers with --sandbox options"},{"location":"ssh-agent/","text":"<p>Although there are ways to use some private keys directly with your SSH client, you will likely want to run a SSH agent to manage any private keys that you have protected with passphrases. On HPCC and most other Unix systems, there is a program called <code>ssh-agent</code> for doing this.  you install the full PuTTY SSH suite on Windows, then you will have a similar utility called <code>Pageant</code>. If you are using Mac OSX, then see Adding a Private Key to Your Mac OSX Keychain.</p> <p>What an SSH agent does is cache your private keys and allow your SSH clients to refer to this cache when attempting to establish new sessions. When you attempt a key-based connection to a remote SSH server, that server will look up the public keys in your authorized keys file on that remote machine and then challenge the connecting client to prove that it has a matching private key by decrypting a message encrypted with a public key. Your client will refer to the cache of private keys maintained by your SSH agent, for the purpose of decrypting this challenge message. If it finds the matching private key and is thus able to decrypt the challenge message from the remote SSH server, then you will be allowed to login once the client has proven this to the server. This all happens without you noticing anything different... except that you no longer need to type in your password during login.</p> <p>To start <code>ssh-agent</code> on a Unix system using a Bourne shell-compatible shell (the default on HPCC), you can use the following command:</p> <pre><code>eval `ssh-agent`\n</code></pre> <p>This starts the agent and sets some Unix environment variables that tell the SSH client, <code>ssh</code>, where to talk to the running agent process. The equivalent command for those using a C shell-compatible shell is:</p> <pre><code>eval `ssh-agent -c`\n</code></pre> <p>Once it is running, you will want to load your private key(s) into the agent's cache. For example:</p> <pre><code>ssh-add\n</code></pre> <p>will prompt your for the passphrase on <code>~/.ssh/id_rsa</code> and load it into the agent's cache. If you are on some other system and have multiple keys pairs, then you may wish to load additional keys; for example:</p> <pre><code>ssh-add ~/.ssh/id_rsa_hpcc ~/.ssh/id_rsa_hpcc_vcs ~/.ssh/id_rsa_nersc\n</code></pre> <p>Once you have <code>ssh-agent</code> running and have some private keys loaded, then you should be able to make key-based connections to other systems where you have placed your corresponding public keys into the authorized keys files... no password necessary. Try it.</p> <p>You don't want to leave stray <code>ssh-agent</code> processes running when you logout. To clean up after yourself, you can use:</p> <pre><code>eval `ssh-agent -k`\n</code></pre> <p>with Bourne shells and</p> <pre><code>eval `ssh-agent -c -k`\n</code></pre> <p>with C shells.</p> <p>If you find yourself using the agent frequently, then you may wish to consider adding the <code>ssh-agent</code> startup and shutdown commands to your shell control files. For Bourne shells, you would want to add</p> <pre><code>eval `ssh-agent`\n</code></pre> <p>to <code>~/.bash_profile</code> and</p> <pre><code>eval `ssh-agent -k`\n</code></pre> <p>to <code>~/.bash_logout</code>. For C shells, you would want to add</p> <pre><code>eval `ssh-agent -c`\n</code></pre> <p>to your <code>~/.login</code> file and</p> <pre><code>eval `ssh-agent -c -k`\n</code></pre> <p>to your <code>~/.logout</code> file.</p> <p>You should NOT add the <code>ssh-add</code> command to your shell initialization file. You should still plan on running this command by hand. As it issues prompts to your terminal, it is not a good idea to run it during shell initialization as the shell may not have fully configured your terminal yet.</p> <p>Another way of managing <code>ssh-agent</code> is via a program called <code>keychain</code>. You can learn more at the Keychain web site.</p>","title":"Running the SSH agent"},{"location":"tags/","text":"<p>Docs pages organized by tag.</p>","title":"Tags"},{"location":"tags/#ros","text":"<ul> <li>LabNotebook ROS</li> </ul>","title":"ROS"},{"location":"tags/#visit","text":"<ul> <li>LabNotebook VisIt</li> </ul>","title":"VisIt"},{"location":"tags/#antismash","text":"<ul> <li>Lab Notebook: AntiSMASH</li> </ul>","title":"antismash"},{"location":"tags/#bactopia","text":"<ul> <li>LabNotebook Bactopia</li> </ul>","title":"bactopia"},{"location":"tags/#centos8","text":"<ul> <li>LabNotebook Using an alternate OS Singularity</li> </ul>","title":"centos8"},{"location":"tags/#conda","text":"<ul> <li>Lab Notebook: Singularity Overlays</li> <li>Lab Notebook: AntiSMASH</li> <li>LabNotebook Pymesh</li> </ul>","title":"conda"},{"location":"tags/#explanation","text":"<ul> <li>File Systems</li> </ul>","title":"explanation"},{"location":"tags/#how-to-guide","text":"<ul> <li>CUDA Example</li> <li>Docker</li> <li>Singularity: I. Introduction</li> </ul>","title":"how-to guide"},{"location":"tags/#lab-notebook","text":"<ul> <li>Lab Notebook: Singularity Overlays</li> <li>Lab Notebook: AntiSMASH</li> <li>LabNotebook Bactopia</li> <li>LabNotebook Pymesh</li> <li>LabNotebook ROS</li> <li>LabNotebook Using an alternate OS Singularity</li> <li>LabNotebook VisIt</li> <li>LabNotebook template</li> <li>Lab Notebooks</li> </ul>","title":"lab notebook"},{"location":"tags/#pymesh","text":"<ul> <li>LabNotebook Pymesh</li> </ul>","title":"pymesh"},{"location":"tags/#reference","text":"<ul> <li>Linux shells</li> <li>Quick start - web access</li> <li>SLURM - buyin information</li> </ul>","title":"reference"},{"location":"tags/#singularity","text":"<ul> <li>Lab Notebook: Singularity Overlays</li> <li>LabNotebook Using an alternate OS Singularity</li> </ul>","title":"singularity"},{"location":"tags/#tutorial","text":"<ul> <li>Connect to the HPCC</li> </ul>","title":"tutorial"},{"location":"ufs18_File_Systems/","text":"<p>The ufs18 file system is a 2018 IBM General Parallel File System (GPFS) installed in HPCC for storing our home or research spaces.\u00a0While it is faster and more stable, users need to learn its differences from other file systems and understand how to use it.</p>","title":"ufs18 File Systems"},{"location":"ufs18_File_Systems/#space-quota","text":"<p>The only way to get quota information of ufs18 file space is to run the command <code>quota</code>\u00a0:</p> <pre><code>$ quota\nHome Directory:             Space        Space        Space        Space        Files        Files        Files        Files\n                            Quota        Used         Available    % Used       Quota        Used         Available    % Used\n-----------------------------------------------------------------------------------------------------------------------------------\n/mnt/home/UserName          1024G        1213G        -198G        118%         1048576      540389       508187       52%\n\n\nResearch Groups:            Space        Space        Space        Space        Files        Files        Files        Files\n                            Quota        Used         Available    % Used       Quota        Used         Available    % Used\n-----------------------------------------------------------------------------------------------------------------------------------\nResearchSpace1              2048G        1778G        270G         87%          2097152      432525       1664627      21%\nResearchSpace2              1024G        126G         898G         12%          1048576      11755        1036821      1%\n\n\nTemporary Filesystems:\n-----------------------------------------------------------------------------------------------------------------------------------\n\n/mnt/scratch (/mnt/gs18)    Space Quota  Space Used   Space Free   Space % Used Filess Quota Files Used   Files Free   Files % Used\n                            51200G         64G          51136G          0%          1048576      5724         1042852      1%\n\n/mnt/ffs17                  Used          Quota\n                            35.66GiB      100.00GiB\n\n/mnt/ls15 (legacy scratch)  Inodes Used   Quota         Free\n                            125721        1000000       874279\n</code></pre> <p>where all file spaces accessible to the user are listed, including home, research, scratch and ffs17. In each space, the information of quota, usage and availablility on space size and number of files can be found. If \"Free\" or \"Available\" is a negative value (such as \"Space Available\" in home directory of the above example), the usage is over the quota. Please remove, transfer or compress some files so the used space or the file count can be lower than the \"Quota\" value. Since GPFS uses a different compression algorithm, you may notice higher space size after files are copied to the ufs18 file system.</p>","title":"Space quota"},{"location":"ufs18_File_Systems/#actual-disk-usage-different-from-quota-results","text":"<p>The new file system has a smallest file block size of 64k. This means that files between 2K and 64K will occupy 64K of space. This causes space usage inflated greatly for users with large amounts of small files. One suggested solution would be to compress many small files into one large file (at least larger than 64K). If you still have any difficulty, a temporarily larger quota can be requested by a user if their quota is at 1T with many small files.</p>","title":"Actual disk usage different from quota results"},{"location":"ufs18_File_Systems/#limit-on-number-of-files","text":"<p>Besides the quota on the size of space, users are also limited to 1 million files in their home or research directory. We need to set this limit because with a great number of files the file system will spend too much time on back-up to be able to function normally. If possible, users can compress many files into one to reduce the file number.</p> <p>If users do not wish to have the limit, they can request an extra space under <code>/mnt/ufs18/nodr/</code> or <code>/mnt/ufs18/nodr/research/</code> where there is no limit on the file count yet no back-up on the files either. Users will be responsible for their own back-up on the <code>nodr</code> space.</p> <p>By default, one half of the allowed quota on space size will be assigned to the requested <code>nodr</code> space. The quota of the original space under <code>/mnt/home/</code> or <code>/mnt/research/</code> is then downsized to its half so the total disk space quota remains the same. A different proportion on their sizes can also be requested as user's desire. Once this <code>nodr</code> space is created, the path and the quota information can be found from the results of the <code>quota</code> command mentioned above.</p>","title":"Limit on number of files"},{"location":"ufs18_File_Systems/#quota-setting-on-research-space","text":"<p>The quota setting on research space is based on the group ownership of the files. Any files with the group ownership the same as the research space are followed by the <code>quota</code> command. However, any files (larger than 8 MB) with a group ownership different from the research space are not allowed to exist.</p> <p>Due to this reason, even though there is no over-quota issue from the results of <code>quota</code> command, users might still get an error message such as \u201cfailed to ... ... Disk quota exceeded\u201d while create, copy or write a file to his research space. To resolve this \"Disk quota exceeded\" problem, users may do the following:</p> <ol> <li>","title":"Quota setting on research space"},{"location":"ufs18_File_Systems/#make-sure-the-directory-to-which-files-are-copied-has-the-same-group-ownership-as-the-research-space-and-has-the-set-group-id-bit","text":"<p>For example, you get the error message when trying to transfer files from your local computer to a directory <code>Drctry</code> in your research space <code>/mnt/research/Group</code>. Use <code>ls -ld</code> command to check the group ownership and the access permission of the directory\u00a0<code>Drctry</code> and the research space <code>Group</code> :</p> <pre><code>$ ls -ld /mnt/research/Group/Drctry /mnt/research/Group\ndrwxrwx--- 2 UserName Prmry 5464 Feb 27 11:34 /mnt/research/Group/Drctry\ndrwxrws--- 9 UserName Group 8192 Jul 10 15:34 /mnt/research/Group\n</code></pre> <p>where you can see their differences. For the group ownership, <code>Prmry</code> of the directory <code>Drctry</code> is different from <code>Group</code> of the research space\u00a0<code>/mnt/research/Group</code>. For the permission, <code>rwxrwx---</code> of the directory does not have the set-group-ID bit <code>rwxrws---</code> as the research space. The owner <code>UserName</code> of the directory can run the commands:</p> <pre><code>$ chgrp -R Group /mnt/research/Group/Drctry     # Change the group ownership to Group\n$ chmod g+s /mnt/research/Group/Drctry          # Set up set-group-ID bit\n</code></pre> <p>to change the two attributes of the directory. (A further instruction about file permission can be reviewed from the wiki page File Permissions on HPCC.) Once the settings are corrected:</p> <pre><code>$ ls -ld /mnt/research/Group/Drctry\ndrwxrws--- 2 UserName Group 5464 Feb 27 11:34 /mnt/research/Group/Drctry\n</code></pre> <p>the file transfer can proceed to the directory.\u00a0 </p> </li> <li>","title":"Make sure the directory to which files are copied has the same group ownership as the research space and has the set-group-ID bit."},{"location":"ufs18_File_Systems/#if-the-file-has-already-existed-its-group-ownership-needs-to-be-changed-to-the-group-of-the-research-space","text":"<p>For example, you try running a command to copy, transfer or write a file <code>foo</code> to a directory\u00a0<code>Drctry</code> of your research space. However, a file with the same file name <code>foo</code> has already existed in the directory <code>Drctry</code>. In order to make the command work, <code>foo</code> in the directory <code>Drctry</code> must have the same group ownership as the research space. Otherwise, the owner of the file can use <code>chgrp</code> command mentioned above to correct the group ownership or you have to rename or remove <code>foo</code> in the directory <code>Drctry</code>. </p> </li> <li>","title":"If the file has already existed, its group ownership needs to be changed to the group of the research space."},{"location":"ufs18_File_Systems/#if-the-file-is-going-to-be-created-users-primary-group-may-need-to-be-set-to-the-group-of-the-research-space","text":"<p>Users can use <code>newgrp</code> command to reset his primary group temporarily. For more information, please refer to Change Primary Group page.</p> </li> </ol>","title":"If the file is going to be created, user's primary group may need to be set to the group of the research space."},{"location":"ufs18_File_Systems/#samba-mapping-path-for-local-computer","text":"<p>Users mounting their home directories or research spaces will need to update their SMB/Samba/Windows File Sharing paths in their clients. To determine the mount path, log into HPCC, ssh to a development node and run:</p> <pre><code>show-samba-home\n</code></pre> <p>To determine the mount point for your research space, use the following command:</p> <pre><code>show-samba-research researchspacename\n</code></pre> <p>You may also use the powertools command to see all paths of your home and research spaces:</p> <p><pre><code>ml powertools                    # if powertools is not loaded\nshow-samba-paths\n</code></pre> To map your home or research directory, please refer to the Mapping HPC Drives with Samba page.</p>","title":"Samba mapping path for local computer"},{"location":"ufs18_File_Systems/#acl-for-gpfs","text":"<p>If you are using access control list (ACL), you will need to update them to NFSv4 ACLs. You will need to use the mmgetacl, mmputacl, and mmeditacl commands. Please refer to the GPFS Commands page for more details.</p>","title":"ACL for GPFS"},{"location":"using_DDD/","text":"<p>DDD stands for 'Data Display Debbuger'. It is a GUI front end of GDB, the GNU debugger. The main advantage of DDD over GDB is that DDD offers GUI. In this tutorial, we will learn about</p> <ul> <li>setting and removing breakpoints</li> <li>tracing through programs</li> <li>examining data at various points in execution.</li> </ul>","title":"Using DDD"},{"location":"using_DDD/#the-ddd-interface","text":"<p>When you start DDD, you would see a DDD window like this:</p> <p></p> <p>The DDD window consists of 4 sections:</p> <ul> <li>data window</li> <li>source window</li> <li>machine code window</li> <li>GDB console</li> </ul> <p>You can show/hide each of them in View menu.</p> <p>You can customize the DDD environment in Edit \u2192 Preferences menu.</p> <p>For example, to display line numbers in source window, Edit \u2192 Preferences \u2192 Source: check ' Display Source Line Numbers'</p>","title":"The DDD interface"},{"location":"using_DDD/#getting-started","text":"<p>To use DDD, we need a program to debug. Let's use the following code.</p> <p>debug_ex.c</p> <pre><code>#include &lt;stdio.h&gt;\n\nint main(int argc, char** argv){\n  for(int i = 0; i &lt; 10; i++){\n    int j = i*i;\n    printf(\"%d \", j);\n  }\n  printf(\"\\n\");\n}\n</code></pre>","title":"Getting started"},{"location":"using_DDD/#basic-steps","text":"<p>First, you need to compile this code with -g option to include the debug symbols such as</p> <pre><code>gcc -g debug_ex.c -o debug_ex\n</code></pre> <p>Now, run the DDD with an executable such as</p> <pre><code>ddd debug_ex\n</code></pre> <p>Even though you open an executable such as debug_ex, but the DDD will show the source file name such as debug_ex.c.</p> <p></p> <p>Breakpoints stop your program in the middle of running to examine the current state of variables and data structures.  You can continue from where you set the breakpoint to finish program execution. To set a breakpoint, double click to the left of the source line in the source window. A STOP icon will appear next to it. Click Run to start execution.</p> <p>Now, click 'Run' button or type 'run' on GDB console. The green arrow will appear as soon as you hit the breakpoint. The breakpoints you set can be deleted or disabled by right-clicking on the line just as before. Except this time, you'll choose either the \"disable breakpoint\" or \"delete breakpoint\" options. In order to set breakpoints in other files (ie, not in the main() function), choose the \"Open Source\" option from the File menu of DDD. The file dialog should appear. The figure shows that the program ran to the line number 5, and waits your input on line number 6. You can run the code a line by line with 'next' command (you can click the button, or just type on GDB console). To see the variable value, type 'print variable_name' oin GDB console. For example, 'print i' will show 'I' variable's value.</p> <p>To go to the next break point, click 'cont' button or type 'cont' on GDB console.</p> <p>When you find bugs, edit your source code in your editor of choice and recompile the code. Reload the new source code into DDD using the Source menu: Source \u2192 Reload Source.</p>","title":"Basic steps"},{"location":"using_DDD/#common-commands","text":"<p>DDD offers command buttons, but you can also type commands directly on GDB console.  </p>    command Description     help help documentation for topics and commands   help breakpoint Lists help information about breakpoints   break sets breakpoint   break line number Sets breakpoint at a line number   break function name Sets breakpoint at the begining of function name   enable, disable, delete/clear Enable, disable, or delete one or more breakpoints.   disable 3 Disables breakpoint number 3   clear line_number Clears breakpoint at line_number   delete 3 Deletes breakpoint number 3   delete Deletes all beakpoints   run Starts program running from the begining.   continue (or cont) Continues execution from the current line to the next breakpoint   step (or s) Execute next line(s) of program   step Executes one line of a program   step number Executes next number of lines of program   next (or n) Like step, but treats a function as a single line.   next Execute the next line   next number Executes next number of lines of program   until line_number Executes program until line number   quit quit DDD   list Lists program source code   condition Conditional breakpoints   print Display program values, results of expressions   whatis List type of an expression   whatis j Shows data type of expression 'j'   info Get information   info locals Shows local variables in current stack frame   info args Shows the argument variable of current stack frame   info break Show breakpoints   set Change values of variables, memory, registers   set x = 123*y Set variable x's value to 123*y","title":"Common commands"},{"location":"using_DDD/#examining-data","text":"<p>While the program is running, you may want to examine the contents of variables. You can do this by right-clicking on a variable name in the DDD window. Upon right-clicking, select \"Display\". If you want to display the value of a pointer. In this case, use the \"Display*\" menu item. Right-clicking on a variable name offers other capabilities such as print, lookup, what is (showing the data type), break, and clear.</p> <p>Instead of right-clicking, you can peek at memory contents also. To do that, click Data \u2192 Memory. The following window will pop up. </p>","title":"Examining data"},{"location":"using_DDD/#some-useful-resources","text":"<ul> <li>The official DDD Manual http://www.gnu.org/manual/ddd/html_mono/ddd.html</li> <li>A good debugging tutorial using DDD http://heather.cs.ucdavis.edu/~matloff/debug.html</li> </ul>","title":"Some useful resources"},{"location":"virtual_help_desk/","text":"<p>ICER offers virtual helpdesk office hours (every Monday and Thursday 1:00-2:00pm) online without walk-in. Users can reach us either through Microsoft Teams App or just a web browser.</p> <p>Please click on the ICER Help Desk link . It will take you to the launcher web site of Microsoft Teams:</p> <p></p> <p>You can now choose to use a web browser or Microsoft Teams to access our Help Desk channel.</p> <p>If you do not want to install and use Microsoft Teams, you can click on Use the web app instead to enter ICER Help Desk channel: </p> <p>If you would like to use Microsoft Teams but have not installed one in your computer yet, please click on Get the Teams app. If Microsoft Teams is installed already, you can click on Launch it now. If a \"Launch Appliction\" window pops out, choose Microsoft Teams  to open our Help Desk channel link:</p> <p></p> <p>Once you are in the channel, please ask your questions in the text bar located at the bottom of the window:</p> <p></p> <p>Click on the  button so we are able to see the message and help you. We can start a conversation and arrange a Zoom\u00a0   Zoom's Microsoft Teams integration has been set up to start or join an instant meeting right from our conversation. You may enter \"@zoom help\" or type \"@zoom\" in the text bar and click on\u00a0  Zoom to see a list of commands. To find out how to download or use Zoom, please visit the MSU Zoom page.</p>","title":"Virtual Help Desk by Microsoft Teams and Zoom"},{"location":"workshop_slides/","text":"<ul> <li>Introduction to HPCC (last update 7/2022, by Nanye Long): pdf</li> <li>Writing SLURM job scripts (last update 7/2022, by Nanye Long): pdf</li> <li>From PC to HPC (by Xiaoge Wang): pdf</li> </ul>","title":"Workshop slides"},{"location":"tags/","text":"<p>Docs pages organized by tag.</p>","title":"Tags"},{"location":"tags/#ros","text":"<ul> <li>LabNotebook ROS</li> </ul>","title":"ROS"},{"location":"tags/#visit","text":"<ul> <li>LabNotebook VisIt</li> </ul>","title":"VisIt"},{"location":"tags/#antismash","text":"<ul> <li>Lab Notebook: AntiSMASH</li> </ul>","title":"antismash"},{"location":"tags/#bactopia","text":"<ul> <li>LabNotebook Bactopia</li> </ul>","title":"bactopia"},{"location":"tags/#centos8","text":"<ul> <li>LabNotebook Using an alternate OS Singularity</li> </ul>","title":"centos8"},{"location":"tags/#conda","text":"<ul> <li>Lab Notebook: Singularity Overlays</li> <li>Lab Notebook: AntiSMASH</li> <li>LabNotebook Pymesh</li> </ul>","title":"conda"},{"location":"tags/#explanation","text":"<ul> <li>File Systems</li> </ul>","title":"explanation"},{"location":"tags/#how-to-guide","text":"<ul> <li>CUDA Example</li> <li>Docker</li> <li>Singularity: I. Introduction</li> </ul>","title":"how-to guide"},{"location":"tags/#lab-notebook","text":"<ul> <li>Lab Notebook: Singularity Overlays</li> <li>Lab Notebook: AntiSMASH</li> <li>LabNotebook Bactopia</li> <li>LabNotebook Pymesh</li> <li>LabNotebook ROS</li> <li>LabNotebook Using an alternate OS Singularity</li> <li>LabNotebook VisIt</li> <li>LabNotebook template</li> <li>Lab Notebooks</li> </ul>","title":"lab notebook"},{"location":"tags/#pymesh","text":"<ul> <li>LabNotebook Pymesh</li> </ul>","title":"pymesh"},{"location":"tags/#reference","text":"<ul> <li>Linux shells</li> <li>Quick start - web access</li> <li>SLURM - buyin information</li> </ul>","title":"reference"},{"location":"tags/#singularity","text":"<ul> <li>Lab Notebook: Singularity Overlays</li> <li>LabNotebook Using an alternate OS Singularity</li> </ul>","title":"singularity"},{"location":"tags/#tutorial","text":"<ul> <li>Connect to the HPCC</li> </ul>","title":"tutorial"}]}