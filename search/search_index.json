{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Access to the HPCC For potential users with an MSU NetID, accounts must be requested by a MSU tenure-track faculty member. Researchers at partner institutions should use the mechanism specified by their institution's agreement with MSU. For more information, see: Obtain an HPCC Account and on the ICER website . CPU and GPU Time Limits Non-buyin users are limited to 500,000 CPU hours (30,000,000 minutes) and 10,000 GPU hours (600,000 minutes) every year (from January 1st to December 31st). More information is available at Job Policies . Buy-in Options With each cluster purchase, ICER offers researchers the oppportunity to purchase buy-in nodes for priority access to the cluster. Questions? If you have questions and couldn\u2019t find answers in this documentation, please submit a ticket . Online Helpdesk Hours Monday and Thursday, 1-2pm at our ICER Public Help Desk Channel . More information about virtual support is available . HPCC Workshops and Training Our monthly workshops and our D2L training materials cover introductions to Linux, HPCC, and some popular software tools. Check out our training calendar for scheduled events and our course content available on Desire2Learn . Acknowledgements We encourage HPCC users to acknowledge ICER and MSU in publications based on work that was done with HPCC resources. A sample statement: \"This work was supported in part through computational resources and services provided by the Institute for Cyber-Enabled Research at Michigan State University.\" Let us know that we have been referenced, and we will link to your publication on our publication site , which will further increase the visibility of your work.","title":"Home"},{"location":"#getting-access-to-the-hpcc","text":"For potential users with an MSU NetID, accounts must be requested by a MSU tenure-track faculty member. Researchers at partner institutions should use the mechanism specified by their institution's agreement with MSU. For more information, see: Obtain an HPCC Account and on the ICER website .","title":"Getting Access to the HPCC"},{"location":"#cpu-and-gpu-time-limits","text":"Non-buyin users are limited to 500,000 CPU hours (30,000,000 minutes) and 10,000 GPU hours (600,000 minutes) every year (from January 1st to December 31st). More information is available at Job Policies .","title":"CPU and GPU Time Limits"},{"location":"#buy-in-options","text":"With each cluster purchase, ICER offers researchers the oppportunity to purchase buy-in nodes for priority access to the cluster.","title":"Buy-in Options"},{"location":"#questions","text":"If you have questions and couldn\u2019t find answers in this documentation, please submit a ticket .","title":"Questions?"},{"location":"#online-helpdesk-hours","text":"Monday and Thursday, 1-2pm at our ICER Public Help Desk Channel . More information about virtual support is available .","title":"Online Helpdesk Hours"},{"location":"#hpcc-workshops-and-training","text":"Our monthly workshops and our D2L training materials cover introductions to Linux, HPCC, and some popular software tools. Check out our training calendar for scheduled events and our course content available on Desire2Learn .","title":"HPCC Workshops and Training"},{"location":"#acknowledgements","text":"We encourage HPCC users to acknowledge ICER and MSU in publications based on work that was done with HPCC resources. A sample statement: \"This work was supported in part through computational resources and services provided by the Institute for Cyber-Enabled Research at Michigan State University.\" Let us know that we have been referenced, and we will link to your publication on our publication site , which will further increase the visibility of your work.","title":"Acknowledgements"},{"location":"1._Variables_-_Part_I/","text":"1. Variables - Part I The shell is a program that takes commands from the input device (usually, keyboard) and gives them to the operating system to perform. On most Linux system including HPC at MSU, sh works as the shell. Besides sh, other shells are available, but here, we will focus on sh. This tutorial assumes you have: minimal programming knowledge minimal Linux shell knowledge A first script Let's create a file 'first.sh' on the terminal using your favorite editor. If you rarely use any editor on Linux, this is a good chance to start using one of them ( Linux text editors ). first.sh 1 2 3 #!/bin/sh # This is a comment! echo Hello World # This is a comment, too! The first line tells Linux that the file is to be executed by /bin/bash. '#!' will be explained later. The second line begins with '#'. This special character makes the line as a comment, and it is ignored by the shell. The only exception is when the first line of the file starts with '#!' The third line runs a command echo, with two parameters/arguments 'Hello' and 'World'. The symbol '#' on line 3 makes the rest of the line as a comment. Now, run 'chmod 755 first.sh' to make the text file executable, then run './first.sh'. 1 2 3 $ chmod 755 first.sh $ ./first.sh Hello World Next, let's expand 'first.sh' with variables. var1.sh 1 2 3 #!/bin/bash MY_MESSAGE = \"Hello World\" echo $MY_MESSAGE This assigns the string 'Hello World' to the variable 'MY_MESSAGE' then echo command prints the value of the variable. Note that you need the quotes around the string. To use variables, '$' is required in front of variables. If you use 'echo MY_MESSAGE' in the above, it will print 'MY_MESSAGE' instead of 'Hello World'. The scope of the variable 'MY_MESSAGE\" is only inside of the script, and when the script finished the variable is empty (don't forget to use chmod 755 to make a script as an executable). 1 2 3 4 5 $ ./var1.sh Hello World $ echo $MY_MESSAGE $ In addition, if you use a variable without declaration, it returns empty string. There is no warning or error message. Let's create a shell script 'var2.sh'. var2.sh 1 2 3 4 #!/bin/sh echo \"MYVAR is: $MYVAR \" MYVAR = \"hi there\" echo \"MYVAR is: $MYVAR \" Then run the script. You can use chmod 755 to make var2.sh as an executable and run it as previous exampels or use bash command: 1 2 3 $ sh var2.sh MYVAR is: MYVAR is: hi there The first MYVAR is empty because it is not declared. The second MYVAR has the value we expected. The scope of the variables in a script is only inside the script. For example, MYVAR is only valid inside 'var2.sh' and when the script finishes, MYVAR is empty again. 1 2 3 4 5 $ ./var2.sh MYVAR is: MYVAR is: hi there $ echo $MYVAR $ You can declare variables with export command in a shell. Check the scope of variables. 1 2 3 4 5 6 7 $ MYVAR = \"hello there\" $ export MYVAR $ ./var2.sh MYVAR is: hello there MYVAR is: hi there $ echo $MYVAR hello there You can use variables in many ways. Here is one example. var3.sh 1 2 3 4 5 6 #!/bin/sh echo \"What is your name?\" read USER_NAME echo \"Hello $USER_NAME \" echo \"I will create a file called ${ USER_NAME } _file\" touch ${ USER_NAME } _file Let's run the script. 1 2 3 4 5 6 7 8 $ chmod 755 var3.sh $ ./var3.sh What is your name? ICER Hello ICER I will create a file called ICER_file $ls -l ICER_file -rw-r--r-- 1 choiyj staff 0 Jan 5 14 :08 ICER_file You would notice that we use curly braces for a file name. If you use '$USER_NAME_file' instead of '{USER_NAME}_file', the shell returns empty string because there is no variable called 'USER_NAME_file' in the script.","title":"1. Variables - Part I"},{"location":"1._Variables_-_Part_I/#1-variables-part-i","text":"The shell is a program that takes commands from the input device (usually, keyboard) and gives them to the operating system to perform. On most Linux system including HPC at MSU, sh works as the shell. Besides sh, other shells are available, but here, we will focus on sh. This tutorial assumes you have: minimal programming knowledge minimal Linux shell knowledge","title":"1. Variables - Part I"},{"location":"1._Variables_-_Part_I/#a-first-script","text":"Let's create a file 'first.sh' on the terminal using your favorite editor. If you rarely use any editor on Linux, this is a good chance to start using one of them ( Linux text editors ). first.sh 1 2 3 #!/bin/sh # This is a comment! echo Hello World # This is a comment, too! The first line tells Linux that the file is to be executed by /bin/bash. '#!' will be explained later. The second line begins with '#'. This special character makes the line as a comment, and it is ignored by the shell. The only exception is when the first line of the file starts with '#!' The third line runs a command echo, with two parameters/arguments 'Hello' and 'World'. The symbol '#' on line 3 makes the rest of the line as a comment. Now, run 'chmod 755 first.sh' to make the text file executable, then run './first.sh'. 1 2 3 $ chmod 755 first.sh $ ./first.sh Hello World Next, let's expand 'first.sh' with variables. var1.sh 1 2 3 #!/bin/bash MY_MESSAGE = \"Hello World\" echo $MY_MESSAGE This assigns the string 'Hello World' to the variable 'MY_MESSAGE' then echo command prints the value of the variable. Note that you need the quotes around the string. To use variables, '$' is required in front of variables. If you use 'echo MY_MESSAGE' in the above, it will print 'MY_MESSAGE' instead of 'Hello World'. The scope of the variable 'MY_MESSAGE\" is only inside of the script, and when the script finished the variable is empty (don't forget to use chmod 755 to make a script as an executable). 1 2 3 4 5 $ ./var1.sh Hello World $ echo $MY_MESSAGE $ In addition, if you use a variable without declaration, it returns empty string. There is no warning or error message. Let's create a shell script 'var2.sh'. var2.sh 1 2 3 4 #!/bin/sh echo \"MYVAR is: $MYVAR \" MYVAR = \"hi there\" echo \"MYVAR is: $MYVAR \" Then run the script. You can use chmod 755 to make var2.sh as an executable and run it as previous exampels or use bash command: 1 2 3 $ sh var2.sh MYVAR is: MYVAR is: hi there The first MYVAR is empty because it is not declared. The second MYVAR has the value we expected. The scope of the variables in a script is only inside the script. For example, MYVAR is only valid inside 'var2.sh' and when the script finishes, MYVAR is empty again. 1 2 3 4 5 $ ./var2.sh MYVAR is: MYVAR is: hi there $ echo $MYVAR $ You can declare variables with export command in a shell. Check the scope of variables. 1 2 3 4 5 6 7 $ MYVAR = \"hello there\" $ export MYVAR $ ./var2.sh MYVAR is: hello there MYVAR is: hi there $ echo $MYVAR hello there You can use variables in many ways. Here is one example. var3.sh 1 2 3 4 5 6 #!/bin/sh echo \"What is your name?\" read USER_NAME echo \"Hello $USER_NAME \" echo \"I will create a file called ${ USER_NAME } _file\" touch ${ USER_NAME } _file Let's run the script. 1 2 3 4 5 6 7 8 $ chmod 755 var3.sh $ ./var3.sh What is your name? ICER Hello ICER I will create a file called ICER_file $ls -l ICER_file -rw-r--r-- 1 choiyj staff 0 Jan 5 14 :08 ICER_file You would notice that we use curly braces for a file name. If you use '$USER_NAME_file' instead of '{USER_NAME}_file', the shell returns empty string because there is no variable called 'USER_NAME_file' in the script.","title":"A first script"},{"location":"2._Variables_-_Part_II/","text":"2. Variables - Part II Linux offers a set of pre-defined variables. These pre-defined variables contain useful information. The first set of variables are $0 ... $9 and $# . The variable $0 is the name of the program as it was called. For example, if you run 'example.sh' which has the variable $0, it will return 'example.sh'. $1 ... $9 are the first 9 additional parameters the script was called with. $* and $@ both will act the same unless they are enclosed in double quotes, \"\" . $@ special parameter takes the entire list and separates it into separate arguments. The variable $@ is all parameters $1 ... $any_number . The variables $* is similar but does not preserve any whitespace, and quoting, so \"File with spaces\" becomes \"File\" \"with\" \"spaces\". This is similar to the echo command. Let's do a hand on example. 1 2 3 4 5 6 #!/bin/sh echo \"Number of parameters from input: $# parameters\" echo \"My name is $0 \" echo \"My first parameter is $1 \" echo \"My second parameter is $2 \" echo \"All parameters are $@ \" Here is a sample run for the above script. 1 2 3 4 5 6 7 8 9 10 11 12 13 $ sh ./var4.sh Number of parameters from input: 0 parameters My name is ./var4.sh My first parameter is My second parameter is All parameters are $ sh ./var4.sh My lazy fox Number of parameters from input: 3 parameters My name is ./var4.sh My first parameter is My My second parameter is lazy All parameters are My lazy fox $# and $1 ... $9 are set by the shell. We can take more than 9 parameters by using the shift command. Look at the next example. 1 2 3 4 5 6 #!/bin/sh while [ \" $# \" -gt \"0\" ] do echo \"\\$1 is $1 \" shift done The backslash \\ character is used to mark $ so that it is not interpreted by the shell. This script uses shift until $# is down to zero. Here is a sample run for the above script. 1 2 3 4 5 6 7 8 9 10 $ sh ./test.sh The quick brown fox jumps over the lazy dog. $1 is The $1 is quick $1 is brown $1 is fox $1 is jumps $1 is over $1 is the $1 is lazy $1 is dog. We can write a script with $* to get a same result. 1 2 3 4 5 6 #!/bin/sh for TOKEN in $* do echo \\$ 1 is $TOKEN done Here, f or and do...done are loop commands which will be covered later. The $? variable represents the exit status of the previous command. Exit status is a numerical value returned by every command upon its completion. Most commands return 0 if they were successful, and 1 if they were unsuccessful. 1 2 3 4 5 6 7 #!/bin/sh for TOKEN in $* do echo $1 $TOKEN done echo $? Here is the result of the sample run. 1 2 3 4 5 6 7 8 9 10 11 $ sh ./test.sh The quick brown fox jumps over the lazy dog. $1 is The $1 is quick $1 is brown $1 is fox $1 is jumps $1 is over $1 is the $1 is lazy $1 is dog. 0","title":"2. Variables - Part II"},{"location":"2._Variables_-_Part_II/#2-variables-part-ii","text":"Linux offers a set of pre-defined variables. These pre-defined variables contain useful information. The first set of variables are $0 ... $9 and $# . The variable $0 is the name of the program as it was called. For example, if you run 'example.sh' which has the variable $0, it will return 'example.sh'. $1 ... $9 are the first 9 additional parameters the script was called with. $* and $@ both will act the same unless they are enclosed in double quotes, \"\" . $@ special parameter takes the entire list and separates it into separate arguments. The variable $@ is all parameters $1 ... $any_number . The variables $* is similar but does not preserve any whitespace, and quoting, so \"File with spaces\" becomes \"File\" \"with\" \"spaces\". This is similar to the echo command. Let's do a hand on example. 1 2 3 4 5 6 #!/bin/sh echo \"Number of parameters from input: $# parameters\" echo \"My name is $0 \" echo \"My first parameter is $1 \" echo \"My second parameter is $2 \" echo \"All parameters are $@ \" Here is a sample run for the above script. 1 2 3 4 5 6 7 8 9 10 11 12 13 $ sh ./var4.sh Number of parameters from input: 0 parameters My name is ./var4.sh My first parameter is My second parameter is All parameters are $ sh ./var4.sh My lazy fox Number of parameters from input: 3 parameters My name is ./var4.sh My first parameter is My My second parameter is lazy All parameters are My lazy fox $# and $1 ... $9 are set by the shell. We can take more than 9 parameters by using the shift command. Look at the next example. 1 2 3 4 5 6 #!/bin/sh while [ \" $# \" -gt \"0\" ] do echo \"\\$1 is $1 \" shift done The backslash \\ character is used to mark $ so that it is not interpreted by the shell. This script uses shift until $# is down to zero. Here is a sample run for the above script. 1 2 3 4 5 6 7 8 9 10 $ sh ./test.sh The quick brown fox jumps over the lazy dog. $1 is The $1 is quick $1 is brown $1 is fox $1 is jumps $1 is over $1 is the $1 is lazy $1 is dog. We can write a script with $* to get a same result. 1 2 3 4 5 6 #!/bin/sh for TOKEN in $* do echo \\$ 1 is $TOKEN done Here, f or and do...done are loop commands which will be covered later. The $? variable represents the exit status of the previous command. Exit status is a numerical value returned by every command upon its completion. Most commands return 0 if they were successful, and 1 if they were unsuccessful. 1 2 3 4 5 6 7 #!/bin/sh for TOKEN in $* do echo $1 $TOKEN done echo $? Here is the result of the sample run. 1 2 3 4 5 6 7 8 9 10 11 $ sh ./test.sh The quick brown fox jumps over the lazy dog. $1 is The $1 is quick $1 is brown $1 is fox $1 is jumps $1 is over $1 is the $1 is lazy $1 is dog. 0","title":"2. Variables - Part II"},{"location":"2018_cluster_resources/","text":"Cluster resources HPCC maintains several clusters. They are named according to the year of installation. Each cluster has very similar hardware with specific processors but has some variety in configuration, such as different coprocessors, memory capacity, or number of CPUs. However, with many different kinds of configurations, HPCC uses a single-queue system. Jobs submitted to our main queue can run on any possible nodes, unless there are specifications on cluster constraints. Users only have to specify resource requirements and our scheduler can assign your job to an appropriate cluster. All clusters currently run CentOS 7 and use the SLURM resource manager. They are connected to each other with scratch and home file systems through Infiniband. Your home, research space, and scratch space is available and identical on all nodes. The following table lists nodes that are currently available to run jobs; jobs can be submitted from any of our dev-nodes. Cluster_Type Node_Name Node_Count Processors Core Memory Disk_Size GPUs intel14 csm (11 nodes) & css (10 nodes) 21 Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz 20 240 GB 416 GB css-[002-003,020,023,032-035] 8 Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz 20 115 GB 416 GB css nodes 71 Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz 20 52 GB 416 GB intel14-k20 csn-[001-039] 39 Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz 20 115 GB 416 GB k20 (2) intel14-phi csp-[006,016-020,025-026] 8 Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz 20 115 GB 416 GB Phi card (2) intel14-xl qml-003 1 Four Intel Xeon CPU E7-8857 v2 @ 3.00GHz 48 969 GB 1.8 TB qml-[001-002,004] 3 Four Intel Xeon CPU E7-8857 v2 @ 3.00GHz 48 1.45 TB 897 GB qml-000 1 Four Intel Xeon CPU E7-8857 v2 @ 3.00GHz 48 2.93TB 1.1 TB qml-005 1 Eight Intel Xeon CPU E7-8857 v2 @ 3.00GHz 96 5.86 TB 1.8 TB intel16 lac-[250-253,256-261,302-317] 26 Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz 28 492 GB 190 GB lac-[224-225,228-248, 278-285,294-301] 39 Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz 28 240 GB 190 GB lac-[000-023,032-191,200-223, 254,255,276,277,318-341, 350-369,372,372-445] 313 Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz 28 115 GB 190 GB intel16-k80 lac-[024-031,080-087,136-143, 192-199,286-293,342-349] 48 Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz 28 240 GB 190 GB k80 (8) intel16-xl vim-[000,001] 2 Intel(R) Xeon(R) CPU E7-8867 v3 @ 2.50GHz 64 2.93 TB 860 GB vim-002 1 Intel(R) Xeon(R) CPU E7-8867 v4 @ 2.40GHz 144 5.86 TB 3.7 TB intel18 skl-[000-112] 113 Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz 40 83 GB 413 GB skl-[113-131] 19 Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz 40 178 GB 413 GB skl-[132-139,148-167] 28 Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz 40 367 GB 413 GB skl-[140-147] 8 Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz 40 745 GB 413 GB intel18-v100 nvl-[000-007] 8 Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz 40 367 GB 413 GB v100(8) amd20 amr-[000-101], amr-[137-209], amr-127, test-amr-[000-001] 178 AMD EPYC 7H12 Processor @2.595 GHz 128 493 GB 412 GB amr-[104-26], amr-[128-136] 32 AMD EPYC 7H12 Processor @2.595 GHz 128 996 GB 412 GB amr-[102-103] 2 AMD EPYC 7H12 Processor @2.595 GHz 128 2005 GB 412 GB amd20-v100 nvf-[000-020] 21 Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz 48 178 GB 412 GB v100s(4) nal-[000-002] 3 AMD 7713 @ 2.0 GHz 128 512 GB 1.92 TB A100(4) nif-[000-005] 6 Intel Xeon 8358 @ 2.6GHz 64 256 GB 1.92 TB A100(4)","title":"Cluster resources"},{"location":"2018_cluster_resources/#cluster-resources","text":"HPCC maintains several clusters. They are named according to the year of installation. Each cluster has very similar hardware with specific processors but has some variety in configuration, such as different coprocessors, memory capacity, or number of CPUs. However, with many different kinds of configurations, HPCC uses a single-queue system. Jobs submitted to our main queue can run on any possible nodes, unless there are specifications on cluster constraints. Users only have to specify resource requirements and our scheduler can assign your job to an appropriate cluster. All clusters currently run CentOS 7 and use the SLURM resource manager. They are connected to each other with scratch and home file systems through Infiniband. Your home, research space, and scratch space is available and identical on all nodes. The following table lists nodes that are currently available to run jobs; jobs can be submitted from any of our dev-nodes. Cluster_Type Node_Name Node_Count Processors Core Memory Disk_Size GPUs intel14 csm (11 nodes) & css (10 nodes) 21 Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz 20 240 GB 416 GB css-[002-003,020,023,032-035] 8 Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz 20 115 GB 416 GB css nodes 71 Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz 20 52 GB 416 GB intel14-k20 csn-[001-039] 39 Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz 20 115 GB 416 GB k20 (2) intel14-phi csp-[006,016-020,025-026] 8 Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz 20 115 GB 416 GB Phi card (2) intel14-xl qml-003 1 Four Intel Xeon CPU E7-8857 v2 @ 3.00GHz 48 969 GB 1.8 TB qml-[001-002,004] 3 Four Intel Xeon CPU E7-8857 v2 @ 3.00GHz 48 1.45 TB 897 GB qml-000 1 Four Intel Xeon CPU E7-8857 v2 @ 3.00GHz 48 2.93TB 1.1 TB qml-005 1 Eight Intel Xeon CPU E7-8857 v2 @ 3.00GHz 96 5.86 TB 1.8 TB intel16 lac-[250-253,256-261,302-317] 26 Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz 28 492 GB 190 GB lac-[224-225,228-248, 278-285,294-301] 39 Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz 28 240 GB 190 GB lac-[000-023,032-191,200-223, 254,255,276,277,318-341, 350-369,372,372-445] 313 Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz 28 115 GB 190 GB intel16-k80 lac-[024-031,080-087,136-143, 192-199,286-293,342-349] 48 Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz 28 240 GB 190 GB k80 (8) intel16-xl vim-[000,001] 2 Intel(R) Xeon(R) CPU E7-8867 v3 @ 2.50GHz 64 2.93 TB 860 GB vim-002 1 Intel(R) Xeon(R) CPU E7-8867 v4 @ 2.40GHz 144 5.86 TB 3.7 TB intel18 skl-[000-112] 113 Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz 40 83 GB 413 GB skl-[113-131] 19 Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz 40 178 GB 413 GB skl-[132-139,148-167] 28 Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz 40 367 GB 413 GB skl-[140-147] 8 Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz 40 745 GB 413 GB intel18-v100 nvl-[000-007] 8 Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz 40 367 GB 413 GB v100(8) amd20 amr-[000-101], amr-[137-209], amr-127, test-amr-[000-001] 178 AMD EPYC 7H12 Processor @2.595 GHz 128 493 GB 412 GB amr-[104-26], amr-[128-136] 32 AMD EPYC 7H12 Processor @2.595 GHz 128 996 GB 412 GB amr-[102-103] 2 AMD EPYC 7H12 Processor @2.595 GHz 128 2005 GB 412 GB amd20-v100 nvf-[000-020] 21 Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz 48 178 GB 412 GB v100s(4) nal-[000-002] 3 AMD 7713 @ 2.0 GHz 128 512 GB 1.92 TB A100(4) nif-[000-005] 6 Intel Xeon 8358 @ 2.6GHz 64 256 GB 1.92 TB A100(4)","title":"Cluster resources"},{"location":"3._Expansion/","text":"3. Expansion When we type a command with arguments/inputs and press the enter key, shell does several things to the arguments/input text before it actually carries out the command. This action is called expansion . With expansion, the arguments expands into something else before the shell acts on it with the command. Let's see an example with echo command. We already learned echo command in the previous section. As you know, echo prints out its text arguments on standard output. 1 2 $ echo hello world! hello world! Let's use echo with '*': 1 2 $ echo * hello.c hello.qsub hello.sb README Instead of print '*', it prints out all file names in the directory because shell expands '*' into something else before the echo command acts to the argument (in this case '*'). 1 2 3 4 5 $ ls hello.c hello.qsub hello.sb README $ echo h* hello.c hello.qsub hello.sb '\\~' is another special character with a special meaning. It expands into the name of the home directory of the user: 1 2 $ echo ~ $ temp_user_01 The shell allows arithmetic by expansion. Arithmetic expansion uses the form $((expression)) . Look at the example. 1 2 $ echo $(( 1 + 1 )) 2 Please keep in mind that arithmetic expansions allows only integers. Arithmetic expression can be nested, and spaces are allowed. 1 2 3 4 $ echo $(( 7 *( 2 + 2 ) )) 28 $ echo $(( 7 *( 2 + 2 )) ) 28 Brace expansion is useful when you write a shell script or batch script. The brace expression can contain a comma separated list of characters, strings, or integers. Here is a few examples. 1 2 3 4 5 6 7 8 9 10 11 $ echo srt- { a,b,c } -end srt-a-end srt-b-end srt-c-end $ echo number_ { 1 ..5 } number_1 number_2 number_3 number_4 number_5 $ echo { A..Z } A B C D E F G H I J K L M N O P Q R S T U V W X Y Z $ echo a { A { 1 ,5 } ,B { 6 ..10 }} b aA1b aA5b aB6b aB7b aB8b aB9b aB10b The next example is creating multiple directories with a brace expansion. 1 2 3 4 5 6 7 $ mkdir Photos $ cd Photos/ Photos$ mkdir { 2020 ..2021 } - { 01 ..12 } Photos$ ls 2020 -01/ 2020 -03/ 2020 -05/ 2020 -07/ 2020 -09/ 2020 -11/ 2021 -01/ 2021 -03/ 2021 -05/ 2021 -07/ 2021 -09/ 2021 -11/ 2020 -02/ 2020 -04/ 2020 -06/ 2020 -08/ 2020 -10/ 2020 -12/ 2021 -02/ 2021 -04/ 2021 -06/ 2021 -08/ 2021 -10/ 2021 -12/ Photos$ Expansion also allows us to use the output of a command. 1 2 $ echo $( ls | grep 2020 ) 2020 -01/ 2020 -02/ 2020 -03/ 2020 -04/ 2020 -05/ 2020 -06/ 2020 -07/ 2020 -08/ 2020 -09/ 2020 -10/ 2020 -11/ 2020 -12/ Next, let's learn how to control expansion. 1 2 3 4 5 $ echo This is a test This is a test $ echo The total is $100 .00 The total is 00 .00 In the first example, the shell removes extra space from the echo command's argument. In the second example, '$1' is interpreted the first input parameter which is not defined here, and therefore, it is replaced as empty string. With quoting, we can suppress unwanted expansions. First, let's learn about double quotes. If we place text inside double quotes, all special characters lose their special meaning, and treated as ordinary characters. However \"$\" \"\\\" (backslash) and \"`\" (back quote) are exceptions. 1 2 3 4 5 6 7 8 $ echo \"This is a test\" This is a test $ ls two words.txt ls: cannot access two: No such file or directory ls: cannot access words.txt: No such file or directory ( base ) dev-intel16-k80:shell$ ls \"two words.txt\" two words.txt If you want to suppress all expansions, you need to use single quotes. Next three examples show how quoting give different results. 1 2 3 4 5 6 7 $ echo text ~/*.txt { 1 ..5 } $( echo foo ) $(( 2 + 2 )) $( date ) text /mnt/home/user_name/hostfile.txt /mnt/home/choiyj/powertools.txt 1 2 3 4 5 foo 4 Tue Jan 19 15 :10:00 EST 2021 $ echo \"text ~/*.txt {1..5} $( echo foo ) $(( 2 + 2 )) $( date ) \" text ~/*.txt { 1 ..5 } foo 4 Tue Jan 19 15 :10:09 EST 2021 $ echo 'text ~/*.txt {1..5} $(echo foo) $((2+2)) $(date)' text ~/*.txt { 1 ..5 } $( echo foo ) $(( 2 + 2 )) $( date ) A backslash is useful when we wan to quote a single character. A backslash is called the escape character. Next example shows how quoting and an escape character work. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ echo The balance of $( date ) is $100 The balance of Tue Jan 19 15 :15:36 EST 2021 is 00 $ echo \"The balance of $( date ) is $100 \" The balance of Tue Jan 19 15 :15:48 EST 2021 is 00 $ echo 'The balance of $(date) is $100' The balance of $( date ) is $100 $ echo \"The balance of $( date ) is \\$100\" The balance of Tue Jan 19 15 :16:09 EST 2021 is $100 $ echo 'The balance of $(date) is \\$100' The balance of $( date ) is \\$ 100 The table show the most frequently used escape characters. Escape Character Name usage \\n newline Adding blank lines to text \\t tab Inserting horizontal tabs to text \\a alert Making the user terminal beep \\\\ backslash Inserting a backslash","title":"Expansion"},{"location":"3._Expansion/#3-expansion","text":"When we type a command with arguments/inputs and press the enter key, shell does several things to the arguments/input text before it actually carries out the command. This action is called expansion . With expansion, the arguments expands into something else before the shell acts on it with the command. Let's see an example with echo command. We already learned echo command in the previous section. As you know, echo prints out its text arguments on standard output. 1 2 $ echo hello world! hello world! Let's use echo with '*': 1 2 $ echo * hello.c hello.qsub hello.sb README Instead of print '*', it prints out all file names in the directory because shell expands '*' into something else before the echo command acts to the argument (in this case '*'). 1 2 3 4 5 $ ls hello.c hello.qsub hello.sb README $ echo h* hello.c hello.qsub hello.sb '\\~' is another special character with a special meaning. It expands into the name of the home directory of the user: 1 2 $ echo ~ $ temp_user_01 The shell allows arithmetic by expansion. Arithmetic expansion uses the form $((expression)) . Look at the example. 1 2 $ echo $(( 1 + 1 )) 2 Please keep in mind that arithmetic expansions allows only integers. Arithmetic expression can be nested, and spaces are allowed. 1 2 3 4 $ echo $(( 7 *( 2 + 2 ) )) 28 $ echo $(( 7 *( 2 + 2 )) ) 28 Brace expansion is useful when you write a shell script or batch script. The brace expression can contain a comma separated list of characters, strings, or integers. Here is a few examples. 1 2 3 4 5 6 7 8 9 10 11 $ echo srt- { a,b,c } -end srt-a-end srt-b-end srt-c-end $ echo number_ { 1 ..5 } number_1 number_2 number_3 number_4 number_5 $ echo { A..Z } A B C D E F G H I J K L M N O P Q R S T U V W X Y Z $ echo a { A { 1 ,5 } ,B { 6 ..10 }} b aA1b aA5b aB6b aB7b aB8b aB9b aB10b The next example is creating multiple directories with a brace expansion. 1 2 3 4 5 6 7 $ mkdir Photos $ cd Photos/ Photos$ mkdir { 2020 ..2021 } - { 01 ..12 } Photos$ ls 2020 -01/ 2020 -03/ 2020 -05/ 2020 -07/ 2020 -09/ 2020 -11/ 2021 -01/ 2021 -03/ 2021 -05/ 2021 -07/ 2021 -09/ 2021 -11/ 2020 -02/ 2020 -04/ 2020 -06/ 2020 -08/ 2020 -10/ 2020 -12/ 2021 -02/ 2021 -04/ 2021 -06/ 2021 -08/ 2021 -10/ 2021 -12/ Photos$ Expansion also allows us to use the output of a command. 1 2 $ echo $( ls | grep 2020 ) 2020 -01/ 2020 -02/ 2020 -03/ 2020 -04/ 2020 -05/ 2020 -06/ 2020 -07/ 2020 -08/ 2020 -09/ 2020 -10/ 2020 -11/ 2020 -12/ Next, let's learn how to control expansion. 1 2 3 4 5 $ echo This is a test This is a test $ echo The total is $100 .00 The total is 00 .00 In the first example, the shell removes extra space from the echo command's argument. In the second example, '$1' is interpreted the first input parameter which is not defined here, and therefore, it is replaced as empty string. With quoting, we can suppress unwanted expansions. First, let's learn about double quotes. If we place text inside double quotes, all special characters lose their special meaning, and treated as ordinary characters. However \"$\" \"\\\" (backslash) and \"`\" (back quote) are exceptions. 1 2 3 4 5 6 7 8 $ echo \"This is a test\" This is a test $ ls two words.txt ls: cannot access two: No such file or directory ls: cannot access words.txt: No such file or directory ( base ) dev-intel16-k80:shell$ ls \"two words.txt\" two words.txt If you want to suppress all expansions, you need to use single quotes. Next three examples show how quoting give different results. 1 2 3 4 5 6 7 $ echo text ~/*.txt { 1 ..5 } $( echo foo ) $(( 2 + 2 )) $( date ) text /mnt/home/user_name/hostfile.txt /mnt/home/choiyj/powertools.txt 1 2 3 4 5 foo 4 Tue Jan 19 15 :10:00 EST 2021 $ echo \"text ~/*.txt {1..5} $( echo foo ) $(( 2 + 2 )) $( date ) \" text ~/*.txt { 1 ..5 } foo 4 Tue Jan 19 15 :10:09 EST 2021 $ echo 'text ~/*.txt {1..5} $(echo foo) $((2+2)) $(date)' text ~/*.txt { 1 ..5 } $( echo foo ) $(( 2 + 2 )) $( date ) A backslash is useful when we wan to quote a single character. A backslash is called the escape character. Next example shows how quoting and an escape character work. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ echo The balance of $( date ) is $100 The balance of Tue Jan 19 15 :15:36 EST 2021 is 00 $ echo \"The balance of $( date ) is $100 \" The balance of Tue Jan 19 15 :15:48 EST 2021 is 00 $ echo 'The balance of $(date) is $100' The balance of $( date ) is $100 $ echo \"The balance of $( date ) is \\$100\" The balance of Tue Jan 19 15 :16:09 EST 2021 is $100 $ echo 'The balance of $(date) is \\$100' The balance of $( date ) is \\$ 100 The table show the most frequently used escape characters. Escape Character Name usage \\n newline Adding blank lines to text \\t tab Inserting horizontal tabs to text \\a alert Making the user terminal beep \\\\ backslash Inserting a backslash","title":"3. Expansion"},{"location":"4._Conditional_statements/","text":"4. Conditional statements if...fi/if...else...fi/if...elif...else...fi if...fi 1 2 3 4 if [ expression ] then Statement ( s ) to be executed if expression is true fi Example: 1 2 3 4 5 6 #!/bin/sh if [ $1 == $2 ] then echo \" $1 is equal to $2 \" fi This is the result of a sample run. 1 2 3 4 5 sh ./sample.sh is equal to $ sh ./test.sh 1 2 $ sh ./test.sh 12 12 12 is equal to 12 if...else...fi 1 2 3 4 5 6 if [ expression ] then Statement ( s ) to be executed if expression is true else Statement ( s ) to be executed if expression is not true fi Example: 1 2 3 4 5 6 7 8 #!/bin/sh if [ $1 == $2 ] then echo \" $1 is equal to $2 \" else echo \" $1 is not equal to $2 \" fi This is the result of a sample run. 1 2 3 4 5 6 7 8 $ sh ./test.sh is equal to $ sh ./test.sh 1 2 1 is not equal to 2 $ sh ./test.sh 12 12 12 is equal to 12 if...elif...else...fi 1 2 3 4 5 6 7 8 9 10 11 12 if [ expression 1 ] then Statement ( s ) to be executed if expression 1 is true elif [ expression 2 ] then Statement ( s ) to be executed if expression 2 is true elif [ expression 3 ] then Statement ( s ) to be executed if expression 3 is true else Statement ( s ) to be executed if no expression is true fi Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/sh if [ $1 == $2 ] then echo \" $1 is equal to $2 \" elif [ $1 -gt $2 ] then echo \" $1 is greater than $2 \" elif [ $1 -lt $2 ] then echo \" $1 is less than $2 \" else echo \"None of the condition met\" fi This is the result of a sample run. 1 2 3 4 5 6 7 8 9 #!/bin/sh sh ./test.sh is equal to $ sh ./test.sh 1 2 1 is less than 2 $ sh ./test.sh 12 2 12 is greater than 2 $ sh ./test.sh 12 12 12 is equal to 12 case...esac 1 2 3 case word in patterns ) commands ;; esac Example: 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/sh read -p \"Enter a number between 1 and 3 inclusive > \" character case $character in 1 ) echo \"You entered one.\" ;; 2 ) echo \"You entered two.\" ;; 3 ) echo \"You entered three.\" ;; * ) echo \"You did not enter a number between 1 and 3.\" esac This is the result of a sample run. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ sh test.sh Enter a number between 1 and 3 inclusive > 1 You entered one. $ bash test.sh Enter a number between 1 and 3 inclusive > 2 You entered two. $sh test.sh Enter a number between 1 and 3 inclusive > 3 You entered three. $sh test.sh Enter a number between 1 and 3 inclusive > 4 You did not enter a number between 1 and 3 .","title":"Conditional statements"},{"location":"4._Conditional_statements/#4-conditional-statements","text":"","title":"4. Conditional statements"},{"location":"4._Conditional_statements/#iffiifelsefiifelifelsefi","text":"","title":"if...fi/if...else...fi/if...elif...else...fi"},{"location":"4._Conditional_statements/#iffi","text":"1 2 3 4 if [ expression ] then Statement ( s ) to be executed if expression is true fi Example: 1 2 3 4 5 6 #!/bin/sh if [ $1 == $2 ] then echo \" $1 is equal to $2 \" fi This is the result of a sample run. 1 2 3 4 5 sh ./sample.sh is equal to $ sh ./test.sh 1 2 $ sh ./test.sh 12 12 12 is equal to 12","title":"if...fi"},{"location":"4._Conditional_statements/#ifelsefi","text":"1 2 3 4 5 6 if [ expression ] then Statement ( s ) to be executed if expression is true else Statement ( s ) to be executed if expression is not true fi Example: 1 2 3 4 5 6 7 8 #!/bin/sh if [ $1 == $2 ] then echo \" $1 is equal to $2 \" else echo \" $1 is not equal to $2 \" fi This is the result of a sample run. 1 2 3 4 5 6 7 8 $ sh ./test.sh is equal to $ sh ./test.sh 1 2 1 is not equal to 2 $ sh ./test.sh 12 12 12 is equal to 12","title":"if...else...fi"},{"location":"4._Conditional_statements/#ifelifelsefi","text":"1 2 3 4 5 6 7 8 9 10 11 12 if [ expression 1 ] then Statement ( s ) to be executed if expression 1 is true elif [ expression 2 ] then Statement ( s ) to be executed if expression 2 is true elif [ expression 3 ] then Statement ( s ) to be executed if expression 3 is true else Statement ( s ) to be executed if no expression is true fi Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/sh if [ $1 == $2 ] then echo \" $1 is equal to $2 \" elif [ $1 -gt $2 ] then echo \" $1 is greater than $2 \" elif [ $1 -lt $2 ] then echo \" $1 is less than $2 \" else echo \"None of the condition met\" fi This is the result of a sample run. 1 2 3 4 5 6 7 8 9 #!/bin/sh sh ./test.sh is equal to $ sh ./test.sh 1 2 1 is less than 2 $ sh ./test.sh 12 2 12 is greater than 2 $ sh ./test.sh 12 12 12 is equal to 12","title":"if...elif...else...fi"},{"location":"4._Conditional_statements/#caseesac","text":"1 2 3 case word in patterns ) commands ;; esac Example: 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/sh read -p \"Enter a number between 1 and 3 inclusive > \" character case $character in 1 ) echo \"You entered one.\" ;; 2 ) echo \"You entered two.\" ;; 3 ) echo \"You entered three.\" ;; * ) echo \"You did not enter a number between 1 and 3.\" esac This is the result of a sample run. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ sh test.sh Enter a number between 1 and 3 inclusive > 1 You entered one. $ bash test.sh Enter a number between 1 and 3 inclusive > 2 You entered two. $sh test.sh Enter a number between 1 and 3 inclusive > 3 You entered three. $sh test.sh Enter a number between 1 and 3 inclusive > 4 You did not enter a number between 1 and 3 .","title":"case...esac"},{"location":"5._Loops/","text":"5. Loops For loops 1 2 3 4 for [ expression ] do Statement ( s ) to be executed done Example: 1 2 3 4 5 #!/bin/sh for i in 1 2 3 4 5 do echo \"Looping ... number $i \" done This is the result of a sample run. 1 2 3 4 5 6 $ bash sample.sh Looping ... number 1 Looping ... number 2 Looping ... number 3 Looping ... number 4 Looping ... number 5 This script also gives a same results. Check the difference. 1 2 3 4 5 #!/bin/bash for (( i = 1 ; i< = 5 ; i += 1 )) do echo \"Looping ... number $i \" done While loops 1 2 3 4 while [ expression ] do Statement ( s ) to be executed done Example: 1 2 3 4 5 6 7 8 #!/bin/bash INPUT_STRING = hello while [ \" $INPUT_STRING \" ! = \"bye\" ] do echo \"Please type something in (bye to quit)\" read INPUT_STRING echo \"You typed: $INPUT_STRING \" done This is the result of a sample run. 1 2 3 4 5 6 7 8 9 10 11 $sh test.sh Please type something in ( bye to quit ) hello You typed: hello Please type something in ( bye to quit ) hi You typed: hi Please type something in ( bye to quit ) bye You typed: bye $","title":"5. Loops"},{"location":"5._Loops/#5-loops","text":"","title":"5. Loops"},{"location":"5._Loops/#for-loops","text":"1 2 3 4 for [ expression ] do Statement ( s ) to be executed done Example: 1 2 3 4 5 #!/bin/sh for i in 1 2 3 4 5 do echo \"Looping ... number $i \" done This is the result of a sample run. 1 2 3 4 5 6 $ bash sample.sh Looping ... number 1 Looping ... number 2 Looping ... number 3 Looping ... number 4 Looping ... number 5 This script also gives a same results. Check the difference. 1 2 3 4 5 #!/bin/bash for (( i = 1 ; i< = 5 ; i += 1 )) do echo \"Looping ... number $i \" done","title":"For loops"},{"location":"5._Loops/#while-loops","text":"1 2 3 4 while [ expression ] do Statement ( s ) to be executed done Example: 1 2 3 4 5 6 7 8 #!/bin/bash INPUT_STRING = hello while [ \" $INPUT_STRING \" ! = \"bye\" ] do echo \"Please type something in (bye to quit)\" read INPUT_STRING echo \"You typed: $INPUT_STRING \" done This is the result of a sample run. 1 2 3 4 5 6 7 8 9 10 11 $sh test.sh Please type something in ( bye to quit ) hello You typed: hello Please type something in ( bye to quit ) hi You typed: hi Please type something in ( bye to quit ) bye You typed: bye $","title":"While loops"},{"location":"ABySS/","text":"ABySS ABySS is a de novo , parallel, paired-end sequence assembler. It can run as an MPI job in the HPCC cluster. The latest version currently installed on the HPCC is 2.1.5, which can be loaded by 1 module load ABySS/2.1.5 You can optionally load other tools as needed, provided that they have been installed under the same toolchain environment as ABySS/2.1.5. For example, 1 module load BEDTools/2.27.1 SAMtools/1.9 BWA/0.7.17 is valid after you've loaded ABySS. A sample SLURM script is below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/bin/bash #SBATCH --job-name=abyss_test #SBATCH --nodes=4 #SBATCH --ntasks-per-node=2 #SBATCH --mem-per-cpu=5G #SBATCH --time=1:00:00 #SBATCH --output=%x-%j.SLURMout echo \"$SLURM_JOB_NODELIST\" module load ABySS/2.1.5 export OMPI_MCA_mpi_warn_on_fork=0 export OMPI_MCA_mpi_cuda_support=0 abyss-pe k=25 name=test in='/mnt/research/common-data/Bio/ABySS/test-data/reads1.fastq /mnt/research/common-data/Bio/ABySS/test-data/reads2.fastq' v=-v np=8 j=2 This script launches an MPI job by requesting 8 processes; they are distributed on 4 nodes ( --nodes=4 ) with two processes each ( --ntasks-per-node=2 ). Accordingly, in the abyss-pe command line, we specify np=8 . Regarding parameter j, the manual states The paired-end assembly stage is multithreaded, but must run on a single machine. The number of threads to use may be specified with the parameter j. The default value for j is the value of np. So, rather than using np as the default value for j, we set j = 2 which is the number of CPUs per node as requested (in this case \"task\" is equivalent to CPU). To submit the job, sbatch --constraint=\"[intel16|intel18]\" While the job is running, you may look at the SLURM output file, in this example, abyss_test-<job ID>.SLURMout , which has a lot of running log, including the following: Running on 8 processors 6: Running on host lac-391 0: Running on host lac-194 2: Running on host lac-225 4: Running on host lac-287 7: Running on host lac-391 3: Running on host lac-225 1: Running on host lac-194 5: Running on host lac-287","title":"ABySS"},{"location":"ABySS/#abyss","text":"ABySS is a de novo , parallel, paired-end sequence assembler. It can run as an MPI job in the HPCC cluster. The latest version currently installed on the HPCC is 2.1.5, which can be loaded by 1 module load ABySS/2.1.5 You can optionally load other tools as needed, provided that they have been installed under the same toolchain environment as ABySS/2.1.5. For example, 1 module load BEDTools/2.27.1 SAMtools/1.9 BWA/0.7.17 is valid after you've loaded ABySS. A sample SLURM script is below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/bin/bash #SBATCH --job-name=abyss_test #SBATCH --nodes=4 #SBATCH --ntasks-per-node=2 #SBATCH --mem-per-cpu=5G #SBATCH --time=1:00:00 #SBATCH --output=%x-%j.SLURMout echo \"$SLURM_JOB_NODELIST\" module load ABySS/2.1.5 export OMPI_MCA_mpi_warn_on_fork=0 export OMPI_MCA_mpi_cuda_support=0 abyss-pe k=25 name=test in='/mnt/research/common-data/Bio/ABySS/test-data/reads1.fastq /mnt/research/common-data/Bio/ABySS/test-data/reads2.fastq' v=-v np=8 j=2 This script launches an MPI job by requesting 8 processes; they are distributed on 4 nodes ( --nodes=4 ) with two processes each ( --ntasks-per-node=2 ). Accordingly, in the abyss-pe command line, we specify np=8 . Regarding parameter j, the manual states The paired-end assembly stage is multithreaded, but must run on a single machine. The number of threads to use may be specified with the parameter j. The default value for j is the value of np. So, rather than using np as the default value for j, we set j = 2 which is the number of CPUs per node as requested (in this case \"task\" is equivalent to CPU). To submit the job, sbatch --constraint=\"[intel16|intel18]\" While the job is running, you may look at the SLURM output file, in this example, abyss_test-<job ID>.SLURMout , which has a lot of running log, including the following: Running on 8 processors 6: Running on host lac-391 0: Running on host lac-194 2: Running on host lac-225 4: Running on host lac-287 7: Running on host lac-391 3: Running on host lac-225 1: Running on host lac-194 5: Running on host lac-287","title":"ABySS"},{"location":"AMD_Optimizing_CPU_Libraries_and_Compilers/","text":"AMD Optimizing CPU Libraries and Compilers With the purchase of amd20 cluster, we've installed AOCC (AMD Optimizing C/C++ Compiler) compiler system and AOCL (AMD Optimizing CPU Libraries) on the HPCC system. Here we introduce how to use the installed compilers and libraries. Based on LLVM 10.0 release, AMD compilers use the commands clang, clang++ and flang to compile c, c++ and fortran codes respectively. Users can simply load an AOCC module and use the command directly. To find out the version of AOCC installed in HPCC, please run the following command on a dev node. All modules should be able to load directly. 1 module spider AOCC AOCL are a set of numerical libraries specifically tuned for AMD EPYC processor family. The Libraries can work with either AOCC or GCC compilers. Users need to load a version of GCC or AOCC module and AOCL can be loaded. All available versions of AOCL can be found by running the following command on a dev node. Using AOCL on the new amd20 nodes is considered to provide better performance than using GCC compiled libraries (such as OpenBLAS, ScaLAPACK ...). User can find out how to use the libraries and the linkers from the documentation . 1 module spider AOCL Besides AOCC and AOCL, a version of OpenMPI compiled with AOCC (version 2.2.0) is also installed in case users would like to test MPI programs with AMD compilers. Users can simply load the aompi toolchain by running the following command on a dev node to load both AOCC and OpenMPI (version 4.0.3). 1 ml -* aompi An aoacl toolchain is also available and able to load the triple modules: AOCC, OpenMPI and AOCL at a time. To find out more information about AOCC or AOCL , Please check AMD Tools and SDKs .","title":"AMD Optimizing CPU Libraries and Compilers"},{"location":"AMD_Optimizing_CPU_Libraries_and_Compilers/#amd-optimizing-cpu-libraries-and-compilers","text":"With the purchase of amd20 cluster, we've installed AOCC (AMD Optimizing C/C++ Compiler) compiler system and AOCL (AMD Optimizing CPU Libraries) on the HPCC system. Here we introduce how to use the installed compilers and libraries. Based on LLVM 10.0 release, AMD compilers use the commands clang, clang++ and flang to compile c, c++ and fortran codes respectively. Users can simply load an AOCC module and use the command directly. To find out the version of AOCC installed in HPCC, please run the following command on a dev node. All modules should be able to load directly. 1 module spider AOCC AOCL are a set of numerical libraries specifically tuned for AMD EPYC processor family. The Libraries can work with either AOCC or GCC compilers. Users need to load a version of GCC or AOCC module and AOCL can be loaded. All available versions of AOCL can be found by running the following command on a dev node. Using AOCL on the new amd20 nodes is considered to provide better performance than using GCC compiled libraries (such as OpenBLAS, ScaLAPACK ...). User can find out how to use the libraries and the linkers from the documentation . 1 module spider AOCL Besides AOCC and AOCL, a version of OpenMPI compiled with AOCC (version 2.2.0) is also installed in case users would like to test MPI programs with AMD compilers. Users can simply load the aompi toolchain by running the following command on a dev node to load both AOCC and OpenMPI (version 4.0.3). 1 ml -* aompi An aoacl toolchain is also available and able to load the triple modules: AOCC, OpenMPI and AOCL at a time. To find out more information about AOCC or AOCL , Please check AMD Tools and SDKs .","title":"AMD Optimizing CPU Libraries and Compilers"},{"location":"ANSYS/","text":"ANSYS DRAFT Feb 2019 These instructions are for using Ansys on the current HPCC environment that uses Slurm scheduler. They may be helpful for those who are transitioning from our previous Moab/Torque/PBS scheduler. License Issues We are facing a temporary issue with Ansys licensing due to changes made over the many versions HPCC has installed. If you have license issues when Using Ansys, here is the work-around: 1. Log in to webrdp.msu.edu (see Web Site Access to HPCC ) or start an X11 terminal (with MobaXterm/Windows or XQuartz/Mac) (see Connect to HPCC System ). 2. SSH Connect to any development node, remembering to add -X options. 3. Start this program on any dev node. it launches a GUI /opt/software/ANSYS/19.2/ansys_inc/shared_files/licensing/lic_admin/anslic_admin On the left side of the window are three buttons. Click the button \"set License Preferences for User \\<username>\". A new window will open 5. select Release 19.2 in that new window and click OK 6. another window will open with tabs across the top and two options in the bottom that are the same for each tab. On the bottom, click the option for \"Use a seperate license for each application\". It doesn't matter which Tab you've slected (Solver/PrePost/etc). That setting should be the same for all tabs. 7. click OK, which closes that window. 8. In the original Ansys license utility, click File, and then \"exit\" to close it. This modifies the config file in your home directory. 9. Close any current sessions in which you running Ansys and start it again on any method (dev node, in 'salloc' interactive job etc). You should now be able to use the features you needed before. Guidelines for scheduling parallel (MPI) jobs Since moving to Slurm, batch scripts that had worked under Torque are no longer working. Here are some guidelines for requesting resources. Use --ntasks instead of nodes Note that -N or -\u2013nodes= will request that number of unique computers, but what most users want is the number of tasks across. Then in addition, use number of tasks instead of number of nodes for -t parameter to -t $SLURM_NTASKS Don't forget to request memory Request memory per task, and since the default is to have 1 cpu per task, you can request memory using --mem-per-cpu=1gb Create a temporary file for node list Inside the job, Fluent requires a file of a particular format ,and the slurm node file doesn't work. This seems to work 1 2 3 4 5 6 # create and save a unique temporary file FLUENTNODEFILE = ` mktemp ` # fill that tmpfile with node list Fluent can use scontrol show hostnames > $FLUENTNODEFILE # in your fluent command, use this parameter -cnf = $FLUENTNODEFILE Example fluent Job script (using Intel compiler). Increase tasks and memory as needed Ansys/Fluent job 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/usr/bin/bash --login # example 1 hour job with ntasks across any number of nodes # adjust the ram and tasks as needed #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=1 #SBATCH --ntasks=10 #SBATCH --mem-per-cpu=1gb # create host list NODEFILE = ` mktemp ` scontrol show hostnames > $NODEFILE # Load the ansys/cfx v19.2 module module load ANSYS # The Input file DEF_FILE = baseline.def # this file is something you have to provide! cfx5solve -def $DEF_FILE -parallel -par-dist $NODEFILE -start-method \"Platform MPI Distributed Parallel\" > cfx5.log After you have logged into a development nodes with an X11 terminal (or use the webrdp gateway as described above), You may run ANSYS tools in parallel and interactively as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # start a approximately 4 hour interactive job with 10 tasks. Adjust tasks and memory as needed # you'll have 4 hours to work. You must be in a X11 terminal for this to work salloc --ntasks = 10 --cpus-per-task = 1 --mem-per-cpu = 1gb --time = 3 :59:00 --x11 # wait for log-in and then... # load module ml intel ansys # this creates a temporary file and fills it with node list Fluent can use NODEFILE = ` mktemp ` scontrol show hostnames > $FLUENTNODEFILE #for example, run the workbench runwb2 # after running workbench you can start fluent directly # note we are using Intel mpi fluent 3ddp -t $SLURM_NTASKS -mpi = intel -cnf = $FLUENTNODEFILE -ssh CFX5 Solver This solver uses a different hosts file format for the par-dist parameter. The following uses an example Definition file provided by Ansys 19.2. The batch script will adapted the par-dist file depending on how you specify tasks and tasks-per-node (the example below does not specify tasks per node). Code is taken from https://secure.cci.rpi.edu/wiki/index.php?title=CFX . CFX5 Solver Example sbatch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 #!/usr/bin/bash --login # example 1 hour job with ntasks across any number of nodes # adjust the ram and tasks as needed #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=1 #SBATCH --ntasks=10 #SBATCH --mem-per-cpu=1gb module load ansys # codde adapts the hosts file depending on if you use multiple nodes and the tasks-per-node option. srun hostname -s > /tmp//hosts. $SLURM_JOB_ID if [ \"x $SLURM_NPROCS \" = \"x\" ] ; then if [ \"x $SLURM_NTASKS_PER_NODE \" = \"x\" ] ; then SLURM_NTASKS_PER_NODE = 1 fi SLURM_NPROCS = ` expr $SLURM_JOB_NUM_NODES \\* $SLURM_NTASKS_PER_NODE ` fi # use ssh instead of rsh export CFX5RSH = ssh # format the host list for cfx cfxHosts = ` tr '\\n' ',' < /tmp//hosts. $SLURM_JOB_ID ` # example file DEF = /opt/software/ANSYS/19.2/ansys_inc/v192/CFX/examples/StaticMixer.def # run the partitioner and solver cfx5solve -par -par-dist \" $cfxHosts \" -def $DEF -part $SLURM_NPROCS -start-method \"Platform MPI Distributed Parallel\" # cleanup rm /tmp/hosts. $SLURM_JOB_ID # output will be in a file named like StaticMixer_001.out and StaticMixer_001.res","title":"ANSYS"},{"location":"ANSYS/#ansys","text":"","title":"ANSYS"},{"location":"ANSYS/#draft-feb-2019","text":"These instructions are for using Ansys on the current HPCC environment that uses Slurm scheduler. They may be helpful for those who are transitioning from our previous Moab/Torque/PBS scheduler.","title":"DRAFT Feb 2019"},{"location":"ANSYS/#license-issues","text":"We are facing a temporary issue with Ansys licensing due to changes made over the many versions HPCC has installed. If you have license issues when Using Ansys, here is the work-around: 1. Log in to webrdp.msu.edu (see Web Site Access to HPCC ) or start an X11 terminal (with MobaXterm/Windows or XQuartz/Mac) (see Connect to HPCC System ). 2. SSH Connect to any development node, remembering to add -X options. 3. Start this program on any dev node. it launches a GUI /opt/software/ANSYS/19.2/ansys_inc/shared_files/licensing/lic_admin/anslic_admin On the left side of the window are three buttons. Click the button \"set License Preferences for User \\<username>\". A new window will open 5. select Release 19.2 in that new window and click OK 6. another window will open with tabs across the top and two options in the bottom that are the same for each tab. On the bottom, click the option for \"Use a seperate license for each application\". It doesn't matter which Tab you've slected (Solver/PrePost/etc). That setting should be the same for all tabs. 7. click OK, which closes that window. 8. In the original Ansys license utility, click File, and then \"exit\" to close it. This modifies the config file in your home directory. 9. Close any current sessions in which you running Ansys and start it again on any method (dev node, in 'salloc' interactive job etc). You should now be able to use the features you needed before.","title":"License Issues"},{"location":"ANSYS/#guidelines-for-scheduling-parallel-mpi-jobs","text":"Since moving to Slurm, batch scripts that had worked under Torque are no longer working. Here are some guidelines for requesting resources.","title":"Guidelines for scheduling parallel (MPI) jobs"},{"location":"ANSYS/#use-ntasks-instead-of-nodes","text":"Note that -N or -\u2013nodes= will request that number of unique computers, but what most users want is the number of tasks across. Then in addition, use number of tasks instead of number of nodes for -t parameter to -t $SLURM_NTASKS","title":"Use --ntasks instead of nodes"},{"location":"ANSYS/#dont-forget-to-request-memory","text":"Request memory per task, and since the default is to have 1 cpu per task, you can request memory using --mem-per-cpu=1gb","title":"Don't forget to request memory"},{"location":"ANSYS/#create-a-temporary-file-for-node-list","text":"Inside the job, Fluent requires a file of a particular format ,and the slurm node file doesn't work. This seems to work 1 2 3 4 5 6 # create and save a unique temporary file FLUENTNODEFILE = ` mktemp ` # fill that tmpfile with node list Fluent can use scontrol show hostnames > $FLUENTNODEFILE # in your fluent command, use this parameter -cnf = $FLUENTNODEFILE Example fluent Job script (using Intel compiler). Increase tasks and memory as needed Ansys/Fluent job 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/usr/bin/bash --login # example 1 hour job with ntasks across any number of nodes # adjust the ram and tasks as needed #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=1 #SBATCH --ntasks=10 #SBATCH --mem-per-cpu=1gb # create host list NODEFILE = ` mktemp ` scontrol show hostnames > $NODEFILE # Load the ansys/cfx v19.2 module module load ANSYS # The Input file DEF_FILE = baseline.def # this file is something you have to provide! cfx5solve -def $DEF_FILE -parallel -par-dist $NODEFILE -start-method \"Platform MPI Distributed Parallel\" > cfx5.log After you have logged into a development nodes with an X11 terminal (or use the webrdp gateway as described above), You may run ANSYS tools in parallel and interactively as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # start a approximately 4 hour interactive job with 10 tasks. Adjust tasks and memory as needed # you'll have 4 hours to work. You must be in a X11 terminal for this to work salloc --ntasks = 10 --cpus-per-task = 1 --mem-per-cpu = 1gb --time = 3 :59:00 --x11 # wait for log-in and then... # load module ml intel ansys # this creates a temporary file and fills it with node list Fluent can use NODEFILE = ` mktemp ` scontrol show hostnames > $FLUENTNODEFILE #for example, run the workbench runwb2 # after running workbench you can start fluent directly # note we are using Intel mpi fluent 3ddp -t $SLURM_NTASKS -mpi = intel -cnf = $FLUENTNODEFILE -ssh","title":"Create a temporary file for node list"},{"location":"ANSYS/#cfx5-solver","text":"This solver uses a different hosts file format for the par-dist parameter. The following uses an example Definition file provided by Ansys 19.2. The batch script will adapted the par-dist file depending on how you specify tasks and tasks-per-node (the example below does not specify tasks per node). Code is taken from https://secure.cci.rpi.edu/wiki/index.php?title=CFX . CFX5 Solver Example sbatch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 #!/usr/bin/bash --login # example 1 hour job with ntasks across any number of nodes # adjust the ram and tasks as needed #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=1 #SBATCH --ntasks=10 #SBATCH --mem-per-cpu=1gb module load ansys # codde adapts the hosts file depending on if you use multiple nodes and the tasks-per-node option. srun hostname -s > /tmp//hosts. $SLURM_JOB_ID if [ \"x $SLURM_NPROCS \" = \"x\" ] ; then if [ \"x $SLURM_NTASKS_PER_NODE \" = \"x\" ] ; then SLURM_NTASKS_PER_NODE = 1 fi SLURM_NPROCS = ` expr $SLURM_JOB_NUM_NODES \\* $SLURM_NTASKS_PER_NODE ` fi # use ssh instead of rsh export CFX5RSH = ssh # format the host list for cfx cfxHosts = ` tr '\\n' ',' < /tmp//hosts. $SLURM_JOB_ID ` # example file DEF = /opt/software/ANSYS/19.2/ansys_inc/v192/CFX/examples/StaticMixer.def # run the partitioner and solver cfx5solve -par -par-dist \" $cfxHosts \" -def $DEF -part $SLURM_NPROCS -start-method \"Platform MPI Distributed Parallel\" # cleanup rm /tmp/hosts. $SLURM_JOB_ID # output will be in a file named like StaticMixer_001.out and StaticMixer_001.res","title":"CFX5 Solver"},{"location":"Accessing_Repositories_with_SSH_Key-Based_Authentication/","text":"Accessing Repositories with SSH Key-Based Authentication Generating a SSH Keypair Please consult SSH Key-Based Authentication on how to generate a SSH keypair. Setting up Your SSH Agent Please consult SSH Key-Based Authentication on how to setup your SSH agent. Other Configuration Tweaks Currently, the HPCC default is to enable X11 forwarding when trying to connect to other machines via SSH. Your connections to vcs.icer.msu.edu can be sped up by overriding this default when connecting to it from HPCC machines. (Eventually, HPCC will override this at the system level, but for now you can do so on your own.) In your HPCC home directory, create a .ssh directory if it does not already exist. In that directory, place a file named config with the following contents: 1 2 3 Host gitlab.msu.edu ForwardX11 no ForwardX11Trusted no","title":"Accessing Repositories with SSH Key-Based Authentication"},{"location":"Accessing_Repositories_with_SSH_Key-Based_Authentication/#accessing-repositories-with-ssh-key-based-authentication","text":"","title":"Accessing Repositories with SSH Key-Based Authentication"},{"location":"Accessing_Repositories_with_SSH_Key-Based_Authentication/#generating-a-ssh-keypair","text":"Please consult SSH Key-Based Authentication on how to generate a SSH keypair.","title":"Generating a SSH Keypair"},{"location":"Accessing_Repositories_with_SSH_Key-Based_Authentication/#setting-up-your-ssh-agent","text":"Please consult SSH Key-Based Authentication on how to setup your SSH agent.","title":"Setting up Your SSH Agent"},{"location":"Accessing_Repositories_with_SSH_Key-Based_Authentication/#other-configuration-tweaks","text":"Currently, the HPCC default is to enable X11 forwarding when trying to connect to other machines via SSH. Your connections to vcs.icer.msu.edu can be sped up by overriding this default when connecting to it from HPCC machines. (Eventually, HPCC will override this at the system level, but for now you can do so on your own.) In your HPCC home directory, create a .ssh directory if it does not already exist. In that directory, place a file named config with the following contents: 1 2 3 Host gitlab.msu.edu ForwardX11 no ForwardX11Trusted no","title":"Other Configuration Tweaks"},{"location":"AlphaFold_installed_in_HPCC/","text":"AlphaFold installed in HPCC To use AlphaFold installed in HPCC, users can use the command: 1 [ UserName@dev-amd20-v100 ~ ] $ ml spider AlphaFold to find all versions installed in HPCC and use the command: 1 [ UserName@dev-amd20-v100 ~ ] $ ml spider AlphaFold/<version> to find how to load a specific AlphaFold version, where <version> is the version of AlphaFold to load. All AlphaFold versions use the same data structure and location /mnt/research/common-data/alphafold/database as mentioned in Alphafold via Singularity . Illegal memory address If CUDA_ERROR_ILLEGAL_ADDRESS or an illegal memory access was encountered while running AlphaFold, this is mostly due to not enough memory in GPU cards (from the python package \"jax\"). Please try to request the high memory GPU card A100 (79GB) to run your AlphaFold jobs. You can also set the environment variable: 1 export XLA_PYTHON_CLIENT_ALLOCATOR=platform to see if the allocated memory is enough or not. Please see GPU memory allocation for more information. AlphaFold example Users can get an example of AlphaFold to run on HPCC nodes. After log into HPCC and ssh to a dev node with GPU cards, users can run the powertools command: 1 [ UserName@dev-amd20-v100 ~ ] $ getexample AlphaFold to copy the example directory AlphaFold in the current directory. After you cd to the directory, you should be able to see the files inside: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [ UserName@dev-amd20-v100 ~ ] $ cd AlphaFold [ UserName@dev-amd20-v100 AlphaFold ] $ ls data.fasta README slurm_script.sb [ UserName@dev-amd20-v100 AlphaFold ] $ cat slurm_script.sb #!/bin/bash #SBATCH --job-name AlphaFold #SBATCH --time=12:00:00 #SBATCH --gres=gpu:4 #SBATCH --cpus-per-task=24 #SBATCH --mem=90GB #SBATCH --constraint=[intel18|amr|nvf|nal|nif] export NVIDIA_VISIBLE_DEVICES = \" ${ CUDA_VISIBLE_DEVICES } \" ml -* fosscuda/2020a AlphaFold/2.0.0 alphafold --fasta_paths = $PWD /data.fasta --output_dir = $PWD --preset = casp14 --max_template_date = 2020 -05-14 --model_names = model_1 scontrol show job ${ SLURM_JOBID } js -j ${ SLURM_JOBID } The job script file slurm_script.sb shows how to load the AlphaFold module and run the command alphafold . Since most of the specifications and variables have been set in the alphafold command script and the module file, 5 options: 1 2 3 4 5 --fasta_paths --output_dir --preset --max_template_date --model_names for the job are specified in the command line. Users can look into the AlphaFold documentation or use the commands: 1 [ UserName@dev-amd20-v100 AlphaFold ] $ alphafold --help to find out how to use all options after you load the AlphaFold module.","title":"AlphaFold on HPCC"},{"location":"AlphaFold_installed_in_HPCC/#alphafold-installed-in-hpcc","text":"To use AlphaFold installed in HPCC, users can use the command: 1 [ UserName@dev-amd20-v100 ~ ] $ ml spider AlphaFold to find all versions installed in HPCC and use the command: 1 [ UserName@dev-amd20-v100 ~ ] $ ml spider AlphaFold/<version> to find how to load a specific AlphaFold version, where <version> is the version of AlphaFold to load. All AlphaFold versions use the same data structure and location /mnt/research/common-data/alphafold/database as mentioned in Alphafold via Singularity . Illegal memory address If CUDA_ERROR_ILLEGAL_ADDRESS or an illegal memory access was encountered while running AlphaFold, this is mostly due to not enough memory in GPU cards (from the python package \"jax\"). Please try to request the high memory GPU card A100 (79GB) to run your AlphaFold jobs. You can also set the environment variable: 1 export XLA_PYTHON_CLIENT_ALLOCATOR=platform to see if the allocated memory is enough or not. Please see GPU memory allocation for more information.","title":"AlphaFold installed in HPCC"},{"location":"AlphaFold_installed_in_HPCC/#alphafold-example","text":"Users can get an example of AlphaFold to run on HPCC nodes. After log into HPCC and ssh to a dev node with GPU cards, users can run the powertools command: 1 [ UserName@dev-amd20-v100 ~ ] $ getexample AlphaFold to copy the example directory AlphaFold in the current directory. After you cd to the directory, you should be able to see the files inside: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [ UserName@dev-amd20-v100 ~ ] $ cd AlphaFold [ UserName@dev-amd20-v100 AlphaFold ] $ ls data.fasta README slurm_script.sb [ UserName@dev-amd20-v100 AlphaFold ] $ cat slurm_script.sb #!/bin/bash #SBATCH --job-name AlphaFold #SBATCH --time=12:00:00 #SBATCH --gres=gpu:4 #SBATCH --cpus-per-task=24 #SBATCH --mem=90GB #SBATCH --constraint=[intel18|amr|nvf|nal|nif] export NVIDIA_VISIBLE_DEVICES = \" ${ CUDA_VISIBLE_DEVICES } \" ml -* fosscuda/2020a AlphaFold/2.0.0 alphafold --fasta_paths = $PWD /data.fasta --output_dir = $PWD --preset = casp14 --max_template_date = 2020 -05-14 --model_names = model_1 scontrol show job ${ SLURM_JOBID } js -j ${ SLURM_JOBID } The job script file slurm_script.sb shows how to load the AlphaFold module and run the command alphafold . Since most of the specifications and variables have been set in the alphafold command script and the module file, 5 options: 1 2 3 4 5 --fasta_paths --output_dir --preset --max_template_date --model_names for the job are specified in the command line. Users can look into the AlphaFold documentation or use the commands: 1 [ UserName@dev-amd20-v100 AlphaFold ] $ alphafold --help to find out how to use all options after you load the AlphaFold module.","title":"AlphaFold example"},{"location":"Alphafold/","text":"Alphafold Alphafold via Singularity AlphaFold installed in HPCC .","title":"Alphafold"},{"location":"Alphafold/#alphafold","text":"","title":"Alphafold"},{"location":"Alphafold/#alphafold-via-singularity","text":"","title":"Alphafold via Singularity"},{"location":"Alphafold/#alphafold-installed-in-hpcc","text":".","title":"AlphaFold installed in HPCC"},{"location":"Alphafold_via_Singularity/","text":"Alphafold via Singularity Alphafold can be run via Singularity. Alphafold database is located in /mnt/research/common-data/alphafold/database . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 database \u251c\u2500\u2500 bfd \u2502 \u251c\u2500\u2500 bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt_a3m.ffdata \u2502 \u2514\u2500\u2500 bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt.tar.gz \u251c\u2500\u2500 mgnify \u2502 \u2514\u2500\u2500 mgy_clusters_2018_12.fa \u251c\u2500\u2500 params \u2502 \u251c\u2500\u2500 params_model_1.npz \u2502 \u251c\u2500\u2500 params_model_1_ptm.npz \u2502 \u251c\u2500\u2500 params_model_2.npz \u2502 \u251c\u2500\u2500 params_model_2_ptm.npz \u2502 \u251c\u2500\u2500 params_model_3.npz \u2502 \u251c\u2500\u2500 params_model_3_ptm.npz \u2502 \u251c\u2500\u2500 params_model_4.npz \u2502 \u251c\u2500\u2500 params_model_4_ptm.npz \u2502 \u251c\u2500\u2500 params_model_5.npz \u2502 \u2514\u2500\u2500 params_model_5_ptm.npz \u251c\u2500\u2500 pdb70 \u2502 \u251c\u2500\u2500 md5sum \u2502 \u251c\u2500\u2500 pdb70_a3m.ffdata \u2502 \u251c\u2500\u2500 pdb70_a3m.ffindex \u2502 \u251c\u2500\u2500 pdb70_clu.tsv \u2502 \u251c\u2500\u2500 pdb70_cs219.ffdata \u2502 \u251c\u2500\u2500 pdb70_cs219.ffindex \u2502 \u251c\u2500\u2500 pdb70_hhm.ffdata \u2502 \u251c\u2500\u2500 pdb70_hhm.ffindex \u2502 \u2514\u2500\u2500 pdb_filter.dat \u251c\u2500\u2500 pdb_mmcif \u2502 \u251c\u2500\u2500 mmcif_files \u2502 \u2514\u2500\u2500 obsolete.dat \u251c\u2500\u2500 small_bfd \u2502 \u2514\u2500\u2500 bfd-first_non_consensus_sequences.fasta \u251c\u2500\u2500 uniclust30 \u2502 \u2514\u2500\u2500 uniclust30_2018_08 \u2514\u2500\u2500 uniref90 \u251c\u2500\u2500 uniref90.fasta \u2514\u2500\u2500 uniref90.fasta.1.gz Before running Alphafold, you need to set 1 2 export ALPHAFOLD_DATA_PATH=\"/mnt/research/common-data/alphafold/database\" export ALPHAFOLD_MODELS=\"/mnt/research/common-data/alphafold/database/params\" To run alphafold, please use the following template (for more information about options/flags, please refer to https://github.com/deepmind/alphafold . In the script, input.fasta is your input data, and you need to set up output_dir. Since the command /usr/bin/hhsearch inside the container does not work on intel14 nodes ( Illegal instruction ), please use the SBATCH option --constraint in the job script. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 #!/bin/bash #SBATCH --job-name alphafold-run #SBATCH --time=08:00:00 #SBATCH --gres=gpu:1 #SBATCH --cpus-per-task=8 #SBATCH --mem=20G #SBATCH --constraint=\"[intel16|intel18|amd20]\" export ALPHAFOLD_DATA_PATH = \"/mnt/research/common-data/alphafold/database\" export ALPHAFOLD_MODELS = \"/mnt/research/common-data/alphafold/database/params\" singularity run --nv \\ -B $ALPHAFOLD_DATA_PATH :/data \\ -B $ALPHAFOLD_MODELS \\ -B .:/etc \\ --pwd /app/alphafold /opt/software/alphafold/2.0.0/alphafold.sif \\ --data_dir = /data \\ --output_dir = /mnt/gs18/scratch/users/my_id/alphafold/output \\ --fasta_paths = /mnt/gs18/scratch/users/my_id/alphafold/input.fasta \\ --uniref90_database_path = /data/uniref90/uniref90.fasta \\ --mgnify_database_path = /data/mgnify/mgy_clusters_2018_12.fa \\ --bfd_database_path = /data/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\ --uniclust30_database_path = /data/uniclust30/uniclust30_2018_08/uniclust30_2018_08 \\ --pdb70_database_path = /data/pdb70/pdb70 \\ --template_mmcif_dir = /data/pdb_mmcif/mmcif_files \\ --obsolete_pdbs_path = /data/pdb_mmcif/obsolete.dat \\ --max_template_date = 2020 -05-14 \\ --model_names = model_1 \\ --preset = casp14","title":"AlphaFold via Singularity"},{"location":"Alphafold_via_Singularity/#alphafold-via-singularity","text":"Alphafold can be run via Singularity. Alphafold database is located in /mnt/research/common-data/alphafold/database . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 database \u251c\u2500\u2500 bfd \u2502 \u251c\u2500\u2500 bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt_a3m.ffdata \u2502 \u2514\u2500\u2500 bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt.tar.gz \u251c\u2500\u2500 mgnify \u2502 \u2514\u2500\u2500 mgy_clusters_2018_12.fa \u251c\u2500\u2500 params \u2502 \u251c\u2500\u2500 params_model_1.npz \u2502 \u251c\u2500\u2500 params_model_1_ptm.npz \u2502 \u251c\u2500\u2500 params_model_2.npz \u2502 \u251c\u2500\u2500 params_model_2_ptm.npz \u2502 \u251c\u2500\u2500 params_model_3.npz \u2502 \u251c\u2500\u2500 params_model_3_ptm.npz \u2502 \u251c\u2500\u2500 params_model_4.npz \u2502 \u251c\u2500\u2500 params_model_4_ptm.npz \u2502 \u251c\u2500\u2500 params_model_5.npz \u2502 \u2514\u2500\u2500 params_model_5_ptm.npz \u251c\u2500\u2500 pdb70 \u2502 \u251c\u2500\u2500 md5sum \u2502 \u251c\u2500\u2500 pdb70_a3m.ffdata \u2502 \u251c\u2500\u2500 pdb70_a3m.ffindex \u2502 \u251c\u2500\u2500 pdb70_clu.tsv \u2502 \u251c\u2500\u2500 pdb70_cs219.ffdata \u2502 \u251c\u2500\u2500 pdb70_cs219.ffindex \u2502 \u251c\u2500\u2500 pdb70_hhm.ffdata \u2502 \u251c\u2500\u2500 pdb70_hhm.ffindex \u2502 \u2514\u2500\u2500 pdb_filter.dat \u251c\u2500\u2500 pdb_mmcif \u2502 \u251c\u2500\u2500 mmcif_files \u2502 \u2514\u2500\u2500 obsolete.dat \u251c\u2500\u2500 small_bfd \u2502 \u2514\u2500\u2500 bfd-first_non_consensus_sequences.fasta \u251c\u2500\u2500 uniclust30 \u2502 \u2514\u2500\u2500 uniclust30_2018_08 \u2514\u2500\u2500 uniref90 \u251c\u2500\u2500 uniref90.fasta \u2514\u2500\u2500 uniref90.fasta.1.gz Before running Alphafold, you need to set 1 2 export ALPHAFOLD_DATA_PATH=\"/mnt/research/common-data/alphafold/database\" export ALPHAFOLD_MODELS=\"/mnt/research/common-data/alphafold/database/params\" To run alphafold, please use the following template (for more information about options/flags, please refer to https://github.com/deepmind/alphafold . In the script, input.fasta is your input data, and you need to set up output_dir. Since the command /usr/bin/hhsearch inside the container does not work on intel14 nodes ( Illegal instruction ), please use the SBATCH option --constraint in the job script. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 #!/bin/bash #SBATCH --job-name alphafold-run #SBATCH --time=08:00:00 #SBATCH --gres=gpu:1 #SBATCH --cpus-per-task=8 #SBATCH --mem=20G #SBATCH --constraint=\"[intel16|intel18|amd20]\" export ALPHAFOLD_DATA_PATH = \"/mnt/research/common-data/alphafold/database\" export ALPHAFOLD_MODELS = \"/mnt/research/common-data/alphafold/database/params\" singularity run --nv \\ -B $ALPHAFOLD_DATA_PATH :/data \\ -B $ALPHAFOLD_MODELS \\ -B .:/etc \\ --pwd /app/alphafold /opt/software/alphafold/2.0.0/alphafold.sif \\ --data_dir = /data \\ --output_dir = /mnt/gs18/scratch/users/my_id/alphafold/output \\ --fasta_paths = /mnt/gs18/scratch/users/my_id/alphafold/input.fasta \\ --uniref90_database_path = /data/uniref90/uniref90.fasta \\ --mgnify_database_path = /data/mgnify/mgy_clusters_2018_12.fa \\ --bfd_database_path = /data/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\ --uniclust30_database_path = /data/uniclust30/uniclust30_2018_08/uniclust30_2018_08 \\ --pdb70_database_path = /data/pdb70/pdb70 \\ --template_mmcif_dir = /data/pdb_mmcif/mmcif_files \\ --obsolete_pdbs_path = /data/pdb_mmcif/obsolete.dat \\ --max_template_date = 2020 -05-14 \\ --model_names = model_1 \\ --preset = casp14","title":"Alphafold via Singularity"},{"location":"An_SSH_tunneling_via_multiple_hops/","text":"An SSH tunneling via multiple hops In some cases, you want to access dev nodes directly from your local machine (technically, you have to get through the gateway, but you don't have to ssh to a dev node manually with a tunneling). There are two ways to do that. Using ProxyJump Because we need to hop twice (your local machine -> gateway -> dev node), we need an ssh config file under .ssh directory of your local machine (the file name should be config ). The following is an example config file which defines intel18 and k80 dev node. With this file, you can connect a dev node from your local machine with 'ssh intel18' or 'ssh k80'. 1 2 3 4 5 6 7 8 9 10 11 12 13 Host gateway HostName gateway.hpcc.msu.edu User here_you_put_your_net_id Host intel18 HostName dev-intel18 User here_you_put_your_net_id ProxyJump gateway Host k80 HostName dev-intel16-k80 User here_you_put_your_net_id ProxyJump gateway config example Now you can just type the name of host to connect. Tip: With SSH Key-Based Authentication you don't have to type your password when you login. Using port forwarding Instead of using ProxyJump, you can use port forwarding. For this method, you need to open two terminals on your local machine. 1st terminal (left in the picture): type 1 ssh -L 1234:dev-intel18:22 <your_net_id>@gateway.hpcc.msu.edu You can change 1234 to any number larger than 1024 (1234 here is a port number you are using). You can change dev-intel18 to any dev-node name, but 22 (port number of dev node) should be remained. For example, 1 ssh -L 4321:dev-intel16:22 <your_net_id>@gateway.hpcc.msu.edu is also working. 2nd terminal (right in the picture): type 1 ssh -p 1234 <your_net_id>@localhost If it is the first time, it would request connection confirmation. type yes. Then you will arrive a dev-node on the 2nd terminal.","title":"SSH tunneling"},{"location":"An_SSH_tunneling_via_multiple_hops/#an-ssh-tunneling-via-multiple-hops","text":"In some cases, you want to access dev nodes directly from your local machine (technically, you have to get through the gateway, but you don't have to ssh to a dev node manually with a tunneling). There are two ways to do that.","title":"An SSH tunneling via multiple hops"},{"location":"An_SSH_tunneling_via_multiple_hops/#using-proxyjump","text":"Because we need to hop twice (your local machine -> gateway -> dev node), we need an ssh config file under .ssh directory of your local machine (the file name should be config ). The following is an example config file which defines intel18 and k80 dev node. With this file, you can connect a dev node from your local machine with 'ssh intel18' or 'ssh k80'. 1 2 3 4 5 6 7 8 9 10 11 12 13 Host gateway HostName gateway.hpcc.msu.edu User here_you_put_your_net_id Host intel18 HostName dev-intel18 User here_you_put_your_net_id ProxyJump gateway Host k80 HostName dev-intel16-k80 User here_you_put_your_net_id ProxyJump gateway config example Now you can just type the name of host to connect. Tip: With SSH Key-Based Authentication you don't have to type your password when you login.","title":"Using ProxyJump"},{"location":"An_SSH_tunneling_via_multiple_hops/#using-port-forwarding","text":"Instead of using ProxyJump, you can use port forwarding. For this method, you need to open two terminals on your local machine. 1st terminal (left in the picture): type 1 ssh -L 1234:dev-intel18:22 <your_net_id>@gateway.hpcc.msu.edu You can change 1234 to any number larger than 1024 (1234 here is a port number you are using). You can change dev-intel18 to any dev-node name, but 22 (port number of dev node) should be remained. For example, 1 ssh -L 4321:dev-intel16:22 <your_net_id>@gateway.hpcc.msu.edu is also working. 2nd terminal (right in the picture): type 1 ssh -p 1234 <your_net_id>@localhost If it is the first time, it would request connection confirmation. type yes. Then you will arrive a dev-node on the 2nd terminal.","title":"Using port forwarding"},{"location":"Application_Icons_on_Desktop/","text":"Application Icons on Desktop It is much easier to execute your favorite apps by clicking icons on the desktop just like using your Windows or Mac PC. We can certainly do this through Open OnDemand. There are some app icons already created in the directory /opt/software/OnDemand/Desktop-Icons , where you can see them by listing the folder: 1 2 3 4 $ ls /opt/software/OnDemand/Desktop-Icons ANSYS.desktop Dolphin.desktop GaussView.desktop Maestro.desktop rstudio.desktop tecplot.desktop VMD.desktop chromium-browser.desktop firefox.desktop GSEA.desktop MATLAB.desktop sas.desktop Terminal.desktop COMSOL.desktop Fluent.desktop Jupyter.desktop Nautilus.desktop Stata.desktop User ' s Anaconda3.deskto User can easily add the icons to their OnDemand interactive desktop by following the sections below. [ Use Command Lines ] [ Use Interactive Desktop ] [ Create App Icons ] A video instruction is also provided ( click to start ). Use Command Lines You can simply copy them to your desktop directory ~/Desktop . For example, if you would like to have MATLAB icon on your OnDemand desktop, you can run 1 2 $ mkdir -p ~/Desktop $ cp /opt/software/OnDemand/Desktop-Icons/MATLAB.desktop ~/Desktop/ Once the apps' desktop files are copied, request an Interactive Desktop session as mentioned in the Open OnDemand page. You should see the apps' icons on your desktop once you launch it: Use Interactive Desktop You can also request and start an Interactive Desktop session to copy the app icons from /opt/software/OnDemand/Desktop-Icons . Simply double-click the \" Trash \" or \" Computer \" icon on your desktop. It will pop out the file manager window. In the \" Location: \" place, please enter the directory /opt/software/OnDemand/Desktop-Icons . (If the \" Location: \" place does not allow any input, please click on ). It should show all app icons in the window. Right click on an app icon you would like to copy to your desktop. Choose \"Copy to\" and click \"Desktop\": The icon you choose will be copied to your desktop. Create App Icons If your favorite app icons are not in the directory, you can try to use one them as an example 1 2 3 4 5 6 7 8 9 $ cat /opt/software/OnDemand/Desktop-Icons/MATLAB.desktop [ Desktop Entry ] Type = Application Name = MATLAB Icon = /opt/software/OnDemand/images/matlab.png Exec = bash -c \"module load MATLAB/2018a; matlab -desktop\" Terminal = false GenericName = and modify it. Change the following contents 1 2 3 Name = <Software Name> Icon = <Location and File name of the Software Icon> Exec = <Commands to Run the Software> to your app's. Save the file with your app's file name in ~/Desktop directory. Every time you launch an Interactive Desktop session, the icon shows on your desktop. If you have any question, please let us know . We can help you to create one.","title":"Application Icons on Desktop"},{"location":"Application_Icons_on_Desktop/#application-icons-on-desktop","text":"It is much easier to execute your favorite apps by clicking icons on the desktop just like using your Windows or Mac PC. We can certainly do this through Open OnDemand. There are some app icons already created in the directory /opt/software/OnDemand/Desktop-Icons , where you can see them by listing the folder: 1 2 3 4 $ ls /opt/software/OnDemand/Desktop-Icons ANSYS.desktop Dolphin.desktop GaussView.desktop Maestro.desktop rstudio.desktop tecplot.desktop VMD.desktop chromium-browser.desktop firefox.desktop GSEA.desktop MATLAB.desktop sas.desktop Terminal.desktop COMSOL.desktop Fluent.desktop Jupyter.desktop Nautilus.desktop Stata.desktop User ' s Anaconda3.deskto User can easily add the icons to their OnDemand interactive desktop by following the sections below. [ Use Command Lines ] [ Use Interactive Desktop ] [ Create App Icons ] A video instruction is also provided ( click to start ).","title":"Application Icons on Desktop"},{"location":"Application_Icons_on_Desktop/#use-command-lines","text":"You can simply copy them to your desktop directory ~/Desktop . For example, if you would like to have MATLAB icon on your OnDemand desktop, you can run 1 2 $ mkdir -p ~/Desktop $ cp /opt/software/OnDemand/Desktop-Icons/MATLAB.desktop ~/Desktop/ Once the apps' desktop files are copied, request an Interactive Desktop session as mentioned in the Open OnDemand page. You should see the apps' icons on your desktop once you launch it:","title":"Use Command Lines"},{"location":"Application_Icons_on_Desktop/#use-interactive-desktop","text":"You can also request and start an Interactive Desktop session to copy the app icons from /opt/software/OnDemand/Desktop-Icons . Simply double-click the \" Trash \" or \" Computer \" icon on your desktop. It will pop out the file manager window. In the \" Location: \" place, please enter the directory /opt/software/OnDemand/Desktop-Icons . (If the \" Location: \" place does not allow any input, please click on ). It should show all app icons in the window. Right click on an app icon you would like to copy to your desktop. Choose \"Copy to\" and click \"Desktop\": The icon you choose will be copied to your desktop.","title":"Use Interactive Desktop"},{"location":"Application_Icons_on_Desktop/#create-app-icons","text":"If your favorite app icons are not in the directory, you can try to use one them as an example 1 2 3 4 5 6 7 8 9 $ cat /opt/software/OnDemand/Desktop-Icons/MATLAB.desktop [ Desktop Entry ] Type = Application Name = MATLAB Icon = /opt/software/OnDemand/images/matlab.png Exec = bash -c \"module load MATLAB/2018a; matlab -desktop\" Terminal = false GenericName = and modify it. Change the following contents 1 2 3 Name = <Software Name> Icon = <Location and File name of the Software Icon> Exec = <Commands to Run the Software> to your app's. Save the file with your app's file name in ~/Desktop directory. Every time you launch an Interactive Desktop session, the icon shows on your desktop. If you have any question, please let us know . We can help you to create one.","title":"Create App Icons"},{"location":"Aspera_bulk_file_transfer/","text":"Aspera bulk file transfer The Aspera Connect application (ascp) is a useful file transfer tool for downloading or uploading large files in bulk between the HPCC and data repository sites such as those operated by NCBI. In order to interact with a server via aspera, the remote host must be running the Aspera server. This tutorial will demonstrate how to install and use the command line version of Aspera to download files from the NCBI ftp site. You can only execute Aspera file transfers from gateway. Transfers on the dev-nodes will not work correctly. Go to https://www.ibm.com/products/aspera/downloads to download \"aspera connect\". Select the Linux OS and download aspera-connect-3.7.2.141527-linux-64.sh (version may change over time) to your home directory. Run chmod u+x aspera-connect-3.7.2.141527-linux-64.sh Run ./aspera-connect-3.7.2.141527-linux-64.sh The installation will then be located in ~/.aspera/connect/ ; and the command ascp is in ~/.aspera/connect/bin/ Example use: 1 ~/.aspera/connect/bin/ascp -T -k 1 -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh anonftp@ftp.ncbi.nlm.nih.gov:/refseq/uniprotkb ~/NCBI_data More instructions and examples can be found here .","title":"Aspera bulk file transfer"},{"location":"Aspera_bulk_file_transfer/#aspera-bulk-file-transfer","text":"The Aspera Connect application (ascp) is a useful file transfer tool for downloading or uploading large files in bulk between the HPCC and data repository sites such as those operated by NCBI. In order to interact with a server via aspera, the remote host must be running the Aspera server. This tutorial will demonstrate how to install and use the command line version of Aspera to download files from the NCBI ftp site. You can only execute Aspera file transfers from gateway. Transfers on the dev-nodes will not work correctly. Go to https://www.ibm.com/products/aspera/downloads to download \"aspera connect\". Select the Linux OS and download aspera-connect-3.7.2.141527-linux-64.sh (version may change over time) to your home directory. Run chmod u+x aspera-connect-3.7.2.141527-linux-64.sh Run ./aspera-connect-3.7.2.141527-linux-64.sh The installation will then be located in ~/.aspera/connect/ ; and the command ascp is in ~/.aspera/connect/bin/ Example use: 1 ~/.aspera/connect/bin/ascp -T -k 1 -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh anonftp@ftp.ncbi.nlm.nih.gov:/refseq/uniprotkb ~/NCBI_data More instructions and examples can be found here .","title":"Aspera bulk file transfer"},{"location":"Assembly_of_PacBio_long_reads_with_Canu/","text":"Assembly of PacBio long reads with Canu Introduction Canu is used for de novo assembly using long reads, as generated from PacBio or Oxford Nanopore technologies. It consists of three steps: read correction, read trimming and contig assembly. As of Sept 2021, we have the latest version 2.2 installed on the HPCC. You can load it by 1 2 3 module purge module load GCCcore/8.3.0 Java/11 Perl/5.30.0 gnuplot/5.2.8 export PATH=/opt/software/canu/canu-2.2/bin:$PATH Then, simply running canu will give you a good amount of help information. For example, at the bottom of the help document, we learn that canu supports three types of raw input data: 1 2 3 4 [technology] -pacbio <files> -nanopore <files> -pacbio-hifi <files> While canu can automate job submission using SLURM, we don't recommend this method. Therefore, please specify useGrid=false in the canu command to disable grid support. Users will write a job script manually, treating canu as an ordinary program. An example using PacBio reads The PacBio reads data we will be assembling is the same as the one used in the canu tutorial , which can be downloaded using the following command: 1 curl -L -o pacbio.fastq http://gembox.cbcb.umd.edu/mhap/raw/ecoli_p6_25x.filtered.fastq By default, the canu pipeline will correct the reads, trim the reads, and then assemble the reads to contigs. Minimally, you can run canu on a dev-node in the following way (we need to first load all necessary modules): 1 2 3 4 5 module purge module load GCCcore/8.3.0 Java/11 Perl/5.30.0 gnuplot/5.2.8 export PATH=/opt/software/canu/canu-2.2/bin:$PATH /bin/time -v canu -p ecoli -d ecoli-pacbio genomeSize=4.8m useGrid=false maxThreads=10 -pacbio pacbio.fastq > runCanu_2021-09-14.log 2>&1 & Above, pacbio.fastq is the input file, considered as raw and unprocessed reads. Coupled with -pacbio , canu knows which technology has generated these reads. -p : set the file name prefix of intermediate and output files; it's mandatory. -d : set assembly directory name for canu to run in. If not supplied, it'll run in the current directory. It is not possible to run two different assemblies in the same directory. genomeSize : in bases, with common prefixes allowed, such as 4.7m or 2.8g. canu uses it to determine coverage in the input reads. useGrid=false : make canu run on the local machine. maxThreads : the maximum number of threads that each task can use. Finally, we put time -v in front of the canu command in order to get resource usage, which will be shown at the end of the log file runCanu_2021-09-14.log . For example, Maximum resident set size (kbytes): 4113216 tells us that the maximum memory used during the process is about 4G. Percent of CPU this job got: 476% tells us we've used on average 5 CPUs. If we want to have canu run in the HPCC cluster, we can write a job script accordingly: 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash #SBATCH --job-name=canu_ecoli #SBATCH --cpus-per-task=10 #SBATCH --mem=10G #SBATCH --time=2:00:00 #SBATCH --output=%x-%j.SLURMout module purge module load GCCcore/8.3.0 Java/11 Perl/5.30.0 gnuplot/5.2.8 export PATH=/opt/software/canu/canu-2.2/bin:$PATH /bin/time -v canu -p ecoli -d ecoli-pacbio genomeSize=4.8m useGrid=false maxThreads=10 -pacbio pacbio.fastq > runCanu_2021-09-14.log 2>&1 The canu command is exactly the same as the one we run on the dev-node, except that the trailing & sign should be removed when it is within a job script. The primary output file for most users is the assembled contigs. In this example, it is ecoli-pacbio/ecoli.contigs.fasta under your current working directory. Refer to this page when you want to learn more about the output, such as the various statistics of the reads analyzed, as reported in the ecoli.report file. Notes To adjust default parameters, you need to consult canu parameter reference . The three steps (error correction, trimming and assembly) can be individually run. See this example . If your data is PacBio HiFi reads (i.e. CCS reads with predicted accuracy >= Q20 or 99%), you may want to use the option -pacbio-hifi rather than -pacbio . Canu will skip read correction and trimming in this case.","title":"Assembly of PacBio long reads with Canu"},{"location":"Assembly_of_PacBio_long_reads_with_Canu/#assembly-of-pacbio-long-reads-with-canu","text":"","title":"Assembly of PacBio long reads with Canu"},{"location":"Assembly_of_PacBio_long_reads_with_Canu/#introduction","text":"Canu is used for de novo assembly using long reads, as generated from PacBio or Oxford Nanopore technologies. It consists of three steps: read correction, read trimming and contig assembly. As of Sept 2021, we have the latest version 2.2 installed on the HPCC. You can load it by 1 2 3 module purge module load GCCcore/8.3.0 Java/11 Perl/5.30.0 gnuplot/5.2.8 export PATH=/opt/software/canu/canu-2.2/bin:$PATH Then, simply running canu will give you a good amount of help information. For example, at the bottom of the help document, we learn that canu supports three types of raw input data: 1 2 3 4 [technology] -pacbio <files> -nanopore <files> -pacbio-hifi <files> While canu can automate job submission using SLURM, we don't recommend this method. Therefore, please specify useGrid=false in the canu command to disable grid support. Users will write a job script manually, treating canu as an ordinary program.","title":"Introduction"},{"location":"Assembly_of_PacBio_long_reads_with_Canu/#an-example-using-pacbio-reads","text":"The PacBio reads data we will be assembling is the same as the one used in the canu tutorial , which can be downloaded using the following command: 1 curl -L -o pacbio.fastq http://gembox.cbcb.umd.edu/mhap/raw/ecoli_p6_25x.filtered.fastq By default, the canu pipeline will correct the reads, trim the reads, and then assemble the reads to contigs. Minimally, you can run canu on a dev-node in the following way (we need to first load all necessary modules): 1 2 3 4 5 module purge module load GCCcore/8.3.0 Java/11 Perl/5.30.0 gnuplot/5.2.8 export PATH=/opt/software/canu/canu-2.2/bin:$PATH /bin/time -v canu -p ecoli -d ecoli-pacbio genomeSize=4.8m useGrid=false maxThreads=10 -pacbio pacbio.fastq > runCanu_2021-09-14.log 2>&1 & Above, pacbio.fastq is the input file, considered as raw and unprocessed reads. Coupled with -pacbio , canu knows which technology has generated these reads. -p : set the file name prefix of intermediate and output files; it's mandatory. -d : set assembly directory name for canu to run in. If not supplied, it'll run in the current directory. It is not possible to run two different assemblies in the same directory. genomeSize : in bases, with common prefixes allowed, such as 4.7m or 2.8g. canu uses it to determine coverage in the input reads. useGrid=false : make canu run on the local machine. maxThreads : the maximum number of threads that each task can use. Finally, we put time -v in front of the canu command in order to get resource usage, which will be shown at the end of the log file runCanu_2021-09-14.log . For example, Maximum resident set size (kbytes): 4113216 tells us that the maximum memory used during the process is about 4G. Percent of CPU this job got: 476% tells us we've used on average 5 CPUs. If we want to have canu run in the HPCC cluster, we can write a job script accordingly: 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash #SBATCH --job-name=canu_ecoli #SBATCH --cpus-per-task=10 #SBATCH --mem=10G #SBATCH --time=2:00:00 #SBATCH --output=%x-%j.SLURMout module purge module load GCCcore/8.3.0 Java/11 Perl/5.30.0 gnuplot/5.2.8 export PATH=/opt/software/canu/canu-2.2/bin:$PATH /bin/time -v canu -p ecoli -d ecoli-pacbio genomeSize=4.8m useGrid=false maxThreads=10 -pacbio pacbio.fastq > runCanu_2021-09-14.log 2>&1 The canu command is exactly the same as the one we run on the dev-node, except that the trailing & sign should be removed when it is within a job script. The primary output file for most users is the assembled contigs. In this example, it is ecoli-pacbio/ecoli.contigs.fasta under your current working directory. Refer to this page when you want to learn more about the output, such as the various statistics of the reads analyzed, as reported in the ecoli.report file.","title":"An example using PacBio reads"},{"location":"Assembly_of_PacBio_long_reads_with_Canu/#notes","text":"To adjust default parameters, you need to consult canu parameter reference . The three steps (error correction, trimming and assembly) can be individually run. See this example . If your data is PacBio HiFi reads (i.e. CCS reads with predicted accuracy >= Q20 or 99%), you may want to use the option -pacbio-hifi rather than -pacbio . Canu will skip read correction and trimming in this case.","title":"Notes"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/","text":"BLAST/BLAST+ with Multiple Processors Overview It is possible to run BLAST or BLAST+ on the HPCC in multi-threaded mode. This is advantageous in that is allows users to leverage multiple processors to complete their BLAST searches, thereby decreasing compute time. To load BLAST or BLAST+ on the HPCC: 1 2 3 4 5 6 7 # Loading BLAST module purge module load BLAST/2.2.26-Linux_x86_64 # Loading BLAST+ module purge module load icc/2017.4.196-GCC-6.4.0-2.28 impi/2017.3.196 BLAST+/2.8.1-Python-2.7.14 Multi-Threading vs. MPI Multi-threaded BLAST runs enable the user to launch multiple worker threads on a single node. However, because standard BLAST and BLAST+ do not use distributed memory, you cannot accomplish multi-threaded runs across multiple nodes. Therefore, users executing multi-threaded BLAST or BLAST+ runs should not reserve more than one node, as this will reserve hardware resources that cannot be used. Job Submission Guidelines First, we need to differentiate between traditional NCBI BLAST and BLAST+. Traditional NCBI BLAST utilizes the \"-a #\" flag to specify the number of processors to use for the job (default is 1). BLAST+ uses the \"-num_threads #\" flag to specify the number of worker threads to use. Depending upon which type of BLAST you use, you will need to adjust your job submission script parameters accordingly. Traditional BLAST Using the \" -a \" flag in BLAST will specify the number of processors to use. To reserve the appropriate quantity of resources in your job submission script, you will need to reserve a number of cores equal to the value specified by the \" -a \" flag For example, if you used a command like: 1 blastall -p blastp -d swissprot -i prot.fasta -o test1.blast -e 0 .001 -a 4 You should specify something like the following in your SLURM job submission script: 1 #SBATCH --cpus-per-task=4 BLAST+ In contrast, BLAST+ uses the \" -num_threads \" flag to specify the number of worker threads to create. In order to specify the correct number of cores for the job, you will need to ADD ONE to the number of threads specified. This is to account for the number of worker threads, PLUS the main process thread. So if you used an equivalent BLAST+ command like: 1 blastn -task blastn -db swissprot -query prot.fasta -out test1.blast -evalue 0 .001 -num_threads 4 You should use the following in your SLURM script: 1 #SBATCH --cpus-per-task=5 BLASTDB The BLASTDB environmental variable tells BLAST or BLAST+ where to find your databases that can be searched. On the HPCC, we offer select BLAST-ready data sets for this purpose in a common read-only area. BLAST data sets can be accessed at: 1 /mnt/research/common-data/Bio/blastdb If you are using the FASTA sequences instead of nucleotide data sets, you need to augment the path above as follows: 1 /mnt/research/common-data/Bio/blastdb/FASTA For cluster jobs, you will need to set the value of BLASTDB in your job submission script, for example: 1 export BLASTDB = /mnt/research/common-data/Bio/blastdb:/mnt/research/common-data/Bio/blastdb/FASTA: $BLASTDB A Word About Memory In either case (BLAST or BLAST+) your requested memory (in the examples above, 4gb) will be divided amongst all of your task threads. Plan accordingly. BLAST data preparation Data downloaded from the NCBI website, or prepared by users can, in most cases, be easily converted for use with BLAST. This brief tutorial is designed to illustrate a fairly basic scenario where the user wants to download a set of FASTA sequences from the NCBI website and prepare them for BLAST-ing. Download The simplest way to do this is to note the link of the FASTA file, and use either the \"wget\" or \"curl\" command. For example: 1 wget ftp://ftp.ncbi.nih.gov/repository/UniGene/Triticum_aestivum/Ta.seq.all.gz or 1 curl -O ftp://ftp.ncbi.nih.gov/repository/UniGene/Triticum_aestivum/Ta.seq.all.gz This will download the file \"Ta.seq.all.gz\" into the current directory. Now unzip the file: 1 gunzip Ta.seq.all.gz This will leave a file called \"Ta.seq.all\" in your directory. Preparing the Indices To prepare the BLAST indices for nucleotides: 1 formatdb -i Ta.seq.all -p F The command above will produce several files, such as: 1 2 3 Ta.seq.all.fa.nhr Ta.seq.all.fa.nin Ta.seq.all.fa.nsq If you want to produce protein indices instead of, or in addition to nucleotides, run: 1 formatdb -i Ta.seq.all -p T In this case, this will produce the files: 1 2 3 Ta.seq.all.fa.phr Ta.seq.all.fa.pin Ta.seq.all.fa.psq You can verify whether your BLAST formatting was successful by looking at the \"formatdb.log\" file which should now be present in your directory.","title":"BLAST/BLAST+"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#blastblast-with-multiple-processors","text":"","title":"BLAST/BLAST+ with Multiple Processors"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#overview","text":"It is possible to run BLAST or BLAST+ on the HPCC in multi-threaded mode. This is advantageous in that is allows users to leverage multiple processors to complete their BLAST searches, thereby decreasing compute time. To load BLAST or BLAST+ on the HPCC: 1 2 3 4 5 6 7 # Loading BLAST module purge module load BLAST/2.2.26-Linux_x86_64 # Loading BLAST+ module purge module load icc/2017.4.196-GCC-6.4.0-2.28 impi/2017.3.196 BLAST+/2.8.1-Python-2.7.14","title":"Overview"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#multi-threading-vs-mpi","text":"Multi-threaded BLAST runs enable the user to launch multiple worker threads on a single node. However, because standard BLAST and BLAST+ do not use distributed memory, you cannot accomplish multi-threaded runs across multiple nodes. Therefore, users executing multi-threaded BLAST or BLAST+ runs should not reserve more than one node, as this will reserve hardware resources that cannot be used.","title":"Multi-Threading vs. MPI"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#job-submission-guidelines","text":"First, we need to differentiate between traditional NCBI BLAST and BLAST+. Traditional NCBI BLAST utilizes the \"-a #\" flag to specify the number of processors to use for the job (default is 1). BLAST+ uses the \"-num_threads #\" flag to specify the number of worker threads to use. Depending upon which type of BLAST you use, you will need to adjust your job submission script parameters accordingly.","title":"Job Submission Guidelines"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#traditional-blast","text":"Using the \" -a \" flag in BLAST will specify the number of processors to use. To reserve the appropriate quantity of resources in your job submission script, you will need to reserve a number of cores equal to the value specified by the \" -a \" flag For example, if you used a command like: 1 blastall -p blastp -d swissprot -i prot.fasta -o test1.blast -e 0 .001 -a 4 You should specify something like the following in your SLURM job submission script: 1 #SBATCH --cpus-per-task=4","title":"Traditional BLAST"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#blast","text":"In contrast, BLAST+ uses the \" -num_threads \" flag to specify the number of worker threads to create. In order to specify the correct number of cores for the job, you will need to ADD ONE to the number of threads specified. This is to account for the number of worker threads, PLUS the main process thread. So if you used an equivalent BLAST+ command like: 1 blastn -task blastn -db swissprot -query prot.fasta -out test1.blast -evalue 0 .001 -num_threads 4 You should use the following in your SLURM script: 1 #SBATCH --cpus-per-task=5","title":"BLAST+"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#blastdb","text":"The BLASTDB environmental variable tells BLAST or BLAST+ where to find your databases that can be searched. On the HPCC, we offer select BLAST-ready data sets for this purpose in a common read-only area. BLAST data sets can be accessed at: 1 /mnt/research/common-data/Bio/blastdb If you are using the FASTA sequences instead of nucleotide data sets, you need to augment the path above as follows: 1 /mnt/research/common-data/Bio/blastdb/FASTA For cluster jobs, you will need to set the value of BLASTDB in your job submission script, for example: 1 export BLASTDB = /mnt/research/common-data/Bio/blastdb:/mnt/research/common-data/Bio/blastdb/FASTA: $BLASTDB","title":"BLASTDB"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#a-word-about-memory","text":"In either case (BLAST or BLAST+) your requested memory (in the examples above, 4gb) will be divided amongst all of your task threads. Plan accordingly.","title":"A Word About Memory"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#blast-data-preparation","text":"Data downloaded from the NCBI website, or prepared by users can, in most cases, be easily converted for use with BLAST. This brief tutorial is designed to illustrate a fairly basic scenario where the user wants to download a set of FASTA sequences from the NCBI website and prepare them for BLAST-ing.","title":"BLAST data preparation"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#download","text":"The simplest way to do this is to note the link of the FASTA file, and use either the \"wget\" or \"curl\" command. For example: 1 wget ftp://ftp.ncbi.nih.gov/repository/UniGene/Triticum_aestivum/Ta.seq.all.gz or 1 curl -O ftp://ftp.ncbi.nih.gov/repository/UniGene/Triticum_aestivum/Ta.seq.all.gz This will download the file \"Ta.seq.all.gz\" into the current directory. Now unzip the file: 1 gunzip Ta.seq.all.gz This will leave a file called \"Ta.seq.all\" in your directory.","title":"Download"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#preparing-the-indices","text":"To prepare the BLAST indices for nucleotides: 1 formatdb -i Ta.seq.all -p F The command above will produce several files, such as: 1 2 3 Ta.seq.all.fa.nhr Ta.seq.all.fa.nin Ta.seq.all.fa.nsq If you want to produce protein indices instead of, or in addition to nucleotides, run: 1 formatdb -i Ta.seq.all -p T In this case, this will produce the files: 1 2 3 Ta.seq.all.fa.phr Ta.seq.all.fa.pin Ta.seq.all.fa.psq You can verify whether your BLAST formatting was successful by looking at the \"formatdb.log\" file which should now be present in your directory.","title":"Preparing the Indices"},{"location":"Basic_Mathematical_Library_Tests_on_AMD_EPYC_Processors/","text":"Basic Mathematical Library Tests on AMD EPYC Processors This test runs many calculations of sine, cosine and logarithm functions in parallel. Each of the calculations is independent from the others and finally they get summed up. The test is executed by a C program written with OpenMP multi-threading and compiled with different compilers and libraries. Three letters (A, G and I) followed by a digit (1 or 2) are used to specify different tests: Letters First Second Digit A AMD Compiler AMD basic mathematical Library 1: all threads running in one socket G GNU Compiler GNU basic mathematical Library (-lm) 2: threads evenly spread to two different sockets I Intel Compiler Intel basic mathematical Library (included in compiler) where the letter in the first and second position represents which compiler and basic mathematical library is in use respectively. The performance results are presented in the following figure: where all timing values were derived by the average of running ten times. As you can see in the figure, the performance of the parallel scaling is almost linear for all compilers and the scaling efficiency1 is strong (about 61% for AA1 and AA2). From the comparison of the timing results, Intel compiler with its library shows the best performance. However, GCC and AMD compilers with AMD basic mathematical library also performs well. In the results of 128 threads, the elapsed time of AMD compiler with AMD library are very closed to the time of Intel's. In the tests of spreading threads. we also find out all threads running on one socket has no difference from spreading them on two different sockets. The same C program was also compiled and run on an intel18 and an intel16 node. The timing of amd20 node with 128 threads is about 3 times faster than the performance of intel18 (with 40 threads) and 4.5 times faster than the performance of intel16 (with 28 threads). The decrease in timing is well prorated with the increase on thread number.","title":"Basic Mathematical Library Tests on AMD EPYC Processors"},{"location":"Basic_Mathematical_Library_Tests_on_AMD_EPYC_Processors/#basic-mathematical-library-tests-on-amd-epyc-processors","text":"This test runs many calculations of sine, cosine and logarithm functions in parallel. Each of the calculations is independent from the others and finally they get summed up. The test is executed by a C program written with OpenMP multi-threading and compiled with different compilers and libraries. Three letters (A, G and I) followed by a digit (1 or 2) are used to specify different tests: Letters First Second Digit A AMD Compiler AMD basic mathematical Library 1: all threads running in one socket G GNU Compiler GNU basic mathematical Library (-lm) 2: threads evenly spread to two different sockets I Intel Compiler Intel basic mathematical Library (included in compiler) where the letter in the first and second position represents which compiler and basic mathematical library is in use respectively. The performance results are presented in the following figure: where all timing values were derived by the average of running ten times. As you can see in the figure, the performance of the parallel scaling is almost linear for all compilers and the scaling efficiency1 is strong (about 61% for AA1 and AA2). From the comparison of the timing results, Intel compiler with its library shows the best performance. However, GCC and AMD compilers with AMD basic mathematical library also performs well. In the results of 128 threads, the elapsed time of AMD compiler with AMD library are very closed to the time of Intel's. In the tests of spreading threads. we also find out all threads running on one socket has no difference from spreading them on two different sockets. The same C program was also compiled and run on an intel18 and an intel16 node. The timing of amd20 node with 128 threads is about 3 times faster than the performance of intel18 (with 40 threads) and 4.5 times faster than the performance of intel16 (with 28 threads). The decrease in timing is well prorated with the increase on thread number.","title":"Basic Mathematical Library Tests on AMD EPYC Processors"},{"location":"Buy-In_Accounts_with_SLURM/","text":"Buy-In Accounts with SLURM If you want to have priority access to our clusters, you can purchase buy-in nodes . Users who run on buy-in nodes receive priority access to their nodes within 4 hours and are exempt from the 1 million CPU hour per year limit. Types of Partitions There are three types of partitions configured in SLURM. Name Purpose Buyin Partition (names vary) A partition is created for each buy-in account. Each buy-in partition includes all non-buy-in nodes, allowing buy-in jobs to span buy-in and non-buy-in nodes. These jobs get equal consideration for scheduling on non-buy-in nodes as jobs in general-long. When jobs submitted to these partitions request a wall time of four hours or less, they are also submitted to the general-short, enabling them to use other available buy-in nodes and ensuring they are scheduled as fast as possible. general-short This partition includes all nodes--buy-in and non-buy-in--and runs jobs that request a wall time of four hours or less. Jobs in this partition are considered for scheduling after jobs in buy-in partitions and the general-long partition. To prevent these jobs from being continuously bumped by general-long/buy-in jobs, they are also submitted to general-long. Jobs with a requested wall time of four hours or less are automatically submitted to this partition. general-long This partition includes non-buy-in nodes and allows jobs to run for up to seven days. Jobs with a requested wall time of four hours or less are also submitted to the general-short partition. general-long-bigmem This partition includes non-buy-in nodes with more memory and CPU cores than most nodes and allows jobs to run for up to seven days. Jobs requesting more than 256GB or 40 CPUs per node are automatically submitted to this partition. This partition ensures large jobs get priority access to large nodes over jobs that can run elsewhere. Jobs with a requested wall time of four hours or less are also submitted to the general-short partition. general-long-gpu This partition contains non-buy-in nodes with GPUs and allows jobs to run for up to seven days. Jobs requesting GPUs are automatically submitted to the partition. This partition ensures jobs requesting GPUs get priority access to nodes with GPUs over jobs not requesting GPUs. Jobs with a requested wall time of four hours or less are also submitted to the general-short partition. In most cases, you will not need to specify a partition when submitting a job. Given a specified (or default) account, SLURM's job submit plugin will assemble the partition list automatically. Default Account When submitting a job without specifying an account, your default account is used. You can check your default account using the buyin_status power tool. 1 2 3 4 5 6 $ module load powertools $ buyin_status -l User: fordste5 Accounts: test1_classres test1 classres Default: classres User fordste5 has a default account of classres. If fordste5 submits a job without specifying an account, it will be submitted to the classres partition. If the job requested four hours or less of wall time, it will be submitted to the general-short partition as well. Jobs submitted by users with a default account of 'general' will be queued in the general-long and general-short partitions. Default Account Wall Time<=4 Hours Wall Time<=4 Hours and >=256GB or 40CPU Wall Time<=4 Hours and GPUs Reqested Wall Time>4 Hours Wall Time>4 Hours and >=256GB or 40CPU Wall Time>4 Hours and GPUs general general-long,general-short general-long-bigmem,general-short general-long-gpu,general-short general-long general-long-bigmem general-long-gpu <buyin> <buyin> , general-long, general-short <buyin> , general-long-bigmem, general-short <buyin> , general-long-gpu, general-short <buyin> , general-long <buyin> , general-long-bigmem <buyin> , general-long-gpu where <buyin> is the name of buyin account. Using Non-Default Account You can see what accounts you have access to using the buyin_status powertool: 1 2 3 4 5 6 $ module load powertools $ buyin_status -l User: fordste5 Accounts: test1_classres test1 classres Default: classres This output shows that fordste5 is a member of the test1_classres, test1, and classres buy-in accounts. Both test1 and classres correspond to partitions with the same name. Because fordste5 has a default account of classres, if they want to submit jobs to test1, they will have to explicitly specify the test1 account at submission. This is done by using either using the '-A test1' option for srun/sbatch/salloc, or by adding the '#SBATCH -A test1' directive to their batch script. Checking the Status of Buy-Ins The status of buy-ins can be viewed with the buyin_status powertool. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ module load powertools $ buyin_status -a classres User: fordste5 Accounts: test1_classres test1 classres Default: classres Buyin: classres JOBID STATE USER CPUS PRIORITY TIME_LIMIT START_TIME 17401 RUNNING fordste5 10 101 1:00:00 2018-08-09T12:55:17 17409 RUNNING changc81 4 100 1:00:00 2018-08-09T12:57:54 Partition: classres lac-421 (down*) csn-020 (down*) csp-018 (down*) css-034 (down*) css-035 (down*) css-079 (down*) css-080 (down*) css-033 (allocated) JOBID ACCOUNT USER CPUS TIME TIME_LEFT 17402 general fordste5 10 4:36 55:24 17401 classres fordste5 10 4:36 55:24 This tool will list all jobs that are queued for a buy-in account, all nodes associated with the buy-in account\u2013including node status\u2013and a list of jobs running on each node. Because jobs with a wall time of four hours or less can run on any buy-in node, it is possible to see jobs running on buy-in nodes from other accounts. Buy-in partitions have the same name as a given buy-in account Managing Buy-In Account Membership SLURM allows for the configuration of account coordinators. Account coordinators can add and remove users to accounts that they coordinate. Buy-in account owners are configured as coordinators of their buy-in accounts and can request that other users also be added as coordinators. If you had a buy-in account configured in the old Moab scheduler, account coordinators for your SLURM buy-in account were copied from the managers listed on the Moab account. View what users have access to your buy-in account: 1 $ sacctmgr show association where account = <account> Add a user to your buy-in account: 1 $ sacctmgr add user name = <userid> account = <account> Remove a user from your buy-in account: 1 $ sacctmgr delete user <userid> where account = <account>","title":"Buyin accounts"},{"location":"Buy-In_Accounts_with_SLURM/#buy-in-accounts-with-slurm","text":"If you want to have priority access to our clusters, you can purchase buy-in nodes . Users who run on buy-in nodes receive priority access to their nodes within 4 hours and are exempt from the 1 million CPU hour per year limit.","title":"Buy-In Accounts with SLURM"},{"location":"Buy-In_Accounts_with_SLURM/#types-of-partitions","text":"There are three types of partitions configured in SLURM. Name Purpose Buyin Partition (names vary) A partition is created for each buy-in account. Each buy-in partition includes all non-buy-in nodes, allowing buy-in jobs to span buy-in and non-buy-in nodes. These jobs get equal consideration for scheduling on non-buy-in nodes as jobs in general-long. When jobs submitted to these partitions request a wall time of four hours or less, they are also submitted to the general-short, enabling them to use other available buy-in nodes and ensuring they are scheduled as fast as possible. general-short This partition includes all nodes--buy-in and non-buy-in--and runs jobs that request a wall time of four hours or less. Jobs in this partition are considered for scheduling after jobs in buy-in partitions and the general-long partition. To prevent these jobs from being continuously bumped by general-long/buy-in jobs, they are also submitted to general-long. Jobs with a requested wall time of four hours or less are automatically submitted to this partition. general-long This partition includes non-buy-in nodes and allows jobs to run for up to seven days. Jobs with a requested wall time of four hours or less are also submitted to the general-short partition. general-long-bigmem This partition includes non-buy-in nodes with more memory and CPU cores than most nodes and allows jobs to run for up to seven days. Jobs requesting more than 256GB or 40 CPUs per node are automatically submitted to this partition. This partition ensures large jobs get priority access to large nodes over jobs that can run elsewhere. Jobs with a requested wall time of four hours or less are also submitted to the general-short partition. general-long-gpu This partition contains non-buy-in nodes with GPUs and allows jobs to run for up to seven days. Jobs requesting GPUs are automatically submitted to the partition. This partition ensures jobs requesting GPUs get priority access to nodes with GPUs over jobs not requesting GPUs. Jobs with a requested wall time of four hours or less are also submitted to the general-short partition. In most cases, you will not need to specify a partition when submitting a job. Given a specified (or default) account, SLURM's job submit plugin will assemble the partition list automatically.","title":"Types of Partitions"},{"location":"Buy-In_Accounts_with_SLURM/#default-account","text":"When submitting a job without specifying an account, your default account is used. You can check your default account using the buyin_status power tool. 1 2 3 4 5 6 $ module load powertools $ buyin_status -l User: fordste5 Accounts: test1_classres test1 classres Default: classres User fordste5 has a default account of classres. If fordste5 submits a job without specifying an account, it will be submitted to the classres partition. If the job requested four hours or less of wall time, it will be submitted to the general-short partition as well. Jobs submitted by users with a default account of 'general' will be queued in the general-long and general-short partitions. Default Account Wall Time<=4 Hours Wall Time<=4 Hours and >=256GB or 40CPU Wall Time<=4 Hours and GPUs Reqested Wall Time>4 Hours Wall Time>4 Hours and >=256GB or 40CPU Wall Time>4 Hours and GPUs general general-long,general-short general-long-bigmem,general-short general-long-gpu,general-short general-long general-long-bigmem general-long-gpu <buyin> <buyin> , general-long, general-short <buyin> , general-long-bigmem, general-short <buyin> , general-long-gpu, general-short <buyin> , general-long <buyin> , general-long-bigmem <buyin> , general-long-gpu where <buyin> is the name of buyin account.","title":"Default Account"},{"location":"Buy-In_Accounts_with_SLURM/#using-non-default-account","text":"You can see what accounts you have access to using the buyin_status powertool: 1 2 3 4 5 6 $ module load powertools $ buyin_status -l User: fordste5 Accounts: test1_classres test1 classres Default: classres This output shows that fordste5 is a member of the test1_classres, test1, and classres buy-in accounts. Both test1 and classres correspond to partitions with the same name. Because fordste5 has a default account of classres, if they want to submit jobs to test1, they will have to explicitly specify the test1 account at submission. This is done by using either using the '-A test1' option for srun/sbatch/salloc, or by adding the '#SBATCH -A test1' directive to their batch script.","title":"Using Non-Default Account"},{"location":"Buy-In_Accounts_with_SLURM/#checking-the-status-of-buy-ins","text":"The status of buy-ins can be viewed with the buyin_status powertool. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ module load powertools $ buyin_status -a classres User: fordste5 Accounts: test1_classres test1 classres Default: classres Buyin: classres JOBID STATE USER CPUS PRIORITY TIME_LIMIT START_TIME 17401 RUNNING fordste5 10 101 1:00:00 2018-08-09T12:55:17 17409 RUNNING changc81 4 100 1:00:00 2018-08-09T12:57:54 Partition: classres lac-421 (down*) csn-020 (down*) csp-018 (down*) css-034 (down*) css-035 (down*) css-079 (down*) css-080 (down*) css-033 (allocated) JOBID ACCOUNT USER CPUS TIME TIME_LEFT 17402 general fordste5 10 4:36 55:24 17401 classres fordste5 10 4:36 55:24 This tool will list all jobs that are queued for a buy-in account, all nodes associated with the buy-in account\u2013including node status\u2013and a list of jobs running on each node. Because jobs with a wall time of four hours or less can run on any buy-in node, it is possible to see jobs running on buy-in nodes from other accounts. Buy-in partitions have the same name as a given buy-in account","title":"Checking the Status of Buy-Ins"},{"location":"Buy-In_Accounts_with_SLURM/#managing-buy-in-account-membership","text":"SLURM allows for the configuration of account coordinators. Account coordinators can add and remove users to accounts that they coordinate. Buy-in account owners are configured as coordinators of their buy-in accounts and can request that other users also be added as coordinators. If you had a buy-in account configured in the old Moab scheduler, account coordinators for your SLURM buy-in account were copied from the managers listed on the Moab account. View what users have access to your buy-in account: 1 $ sacctmgr show association where account = <account> Add a user to your buy-in account: 1 $ sacctmgr add user name = <userid> account = <account> Remove a user from your buy-in account: 1 $ sacctmgr delete user <userid> where account = <account>","title":"Managing Buy-In Account Membership"},{"location":"COMSOL%202/","text":"COMSOL To use COMSOL with SLURM you should use the slurm option '-mpibootstrap slurm' and omit using srun. Sample run line: 1 comsol batch -mpibootstrap slurm -inputfile batchin.mph -outputfile batchout.mph -batchlog batchlog.out","title":"COMSOL"},{"location":"COMSOL%202/#comsol","text":"To use COMSOL with SLURM you should use the slurm option '-mpibootstrap slurm' and omit using srun. Sample run line: 1 comsol batch -mpibootstrap slurm -inputfile batchin.mph -outputfile batchout.mph -batchlog batchlog.out","title":"COMSOL"},{"location":"COMSOL/","text":"COMSOL To use COMSOL with SLURM you should use the slurm option '-mpibootstrap slurm' and omit using srun. Sample run line: 1 comsol batch -mpibootstrap slurm -inputfile batchin.mph -outputfile batchout.mph -batchlog batchlog.out","title":"COMSOL"},{"location":"COMSOL/#comsol","text":"To use COMSOL with SLURM you should use the slurm option '-mpibootstrap slurm' and omit using srun. Sample run line: 1 comsol batch -mpibootstrap slurm -inputfile batchin.mph -outputfile batchout.mph -batchlog batchlog.out","title":"COMSOL"},{"location":"CUDA_Example/","tags":["how-to guide"],"text":"CUDA Example There are several CUDA versions installed in HPCC. Users can use the command: 1 [ username@dev-amd20-v100 ~ ] $ ml spider CUDA to see all installed versions. In order to compile a CUDA code, users might also need to load a GNU or Intel compiler (for C and Fortran). A toolchain which combines CUDA, C and Fortran compilers can be found by using the command: 1 [ username@dev-amd20-v100 ~ ] $ ml spider gcccuda for GNU and CUDA compilers and 1 [ username@dev-amd20-v100 ~ ] $ ml spider iccifortcuda for Intel and CUDA compilers. You can load one of the versions directly to do your CUDA code compilation. GPU Capabilities Please be aware that GPU K80 and K20 cards are not able to run normally with CUDA versions higher than 11.0. Simple CUDA example Users can get a simple CUDA code to do the compilation. After log into HPCC and ssh to a dev node with GPU cards, users can run the powertools command: 1 [ username@dev-amd20-v100 ~ ] $ getexample helloCUDA to copy the CUDA example directory \" helloCUDA \" in the current directory. After you cd to the directory, you should be able to see the CUDA code file: \" Hello.cu \": 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 [ username@dev-amd20-v100 ~ ] $ cd helloCUDA [ username@dev-amd20-v100 ~ ] $ ls Hello.cu Hello-CUDA.sb README run.sh [ username@dev-amd20-v100 ~ ] $ cat Hello.cu #include <stdio.h> #define NUM_BLOCKS 16 #define BLOCK_WIDTH 1 __global__ void hello () { printf ( \"Hello world! I'm a thread in block %d\\n\" , blockIdx.x ) ; } int main ( int argc,char **argv ) { // launch the kernel hello <<< NUM_BLOCKS, BLOCK_WIDTH>>> () ; // force the printf () s to flush cudaDeviceSynchronize () ; printf ( \"That's all!\\n\" ) ; return 0 ; } The CUDA code defines 16 blocks with one GPU core in each block. Each core runs one thread to print out \" Hello world! \" and the block number of the core. Totally 16 threads are executed in parallel. We can simply compile the code by \" nvcc \" command after a version of gcccuda toolchain is loaded: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [ username@dev-amd20-v100 ~ ] $ ml -* gcccuda/2019b [ username@dev-amd20-v100 ~ ] $ nvcc Hello.cu -o Hello_CUDA [ username@dev-amd20-v100 ~ ] $ ./Hello_CUDA Hello world! I 'm a thread in block 5 Hello world! I' m a thread in block 12 Hello world! I 'm a thread in block 11 Hello world! I' m a thread in block 0 Hello world! I 'm a thread in block 6 Hello world! I' m a thread in block 1 Hello world! I 'm a thread in block 7 Hello world! I' m a thread in block 13 Hello world! I 'm a thread in block 14 Hello world! I' m a thread in block 2 Hello world! I 'm a thread in block 8 Hello world! I' m a thread in block 4 Hello world! I 'm a thread in block 10 Hello world! I' m a thread in block 3 Hello world! I 'm a thread in block 9 Hello world! I' m a thread in block 15 That ' s all! Once it is compiled successfully, the execution of the binary \" Hello_CUDA \" on a GPU node should give the output as above.","title":"CUDA Example"},{"location":"CUDA_Example/#cuda-example","text":"There are several CUDA versions installed in HPCC. Users can use the command: 1 [ username@dev-amd20-v100 ~ ] $ ml spider CUDA to see all installed versions. In order to compile a CUDA code, users might also need to load a GNU or Intel compiler (for C and Fortran). A toolchain which combines CUDA, C and Fortran compilers can be found by using the command: 1 [ username@dev-amd20-v100 ~ ] $ ml spider gcccuda for GNU and CUDA compilers and 1 [ username@dev-amd20-v100 ~ ] $ ml spider iccifortcuda for Intel and CUDA compilers. You can load one of the versions directly to do your CUDA code compilation. GPU Capabilities Please be aware that GPU K80 and K20 cards are not able to run normally with CUDA versions higher than 11.0.","title":"CUDA Example"},{"location":"CUDA_Example/#simple-cuda-example","text":"Users can get a simple CUDA code to do the compilation. After log into HPCC and ssh to a dev node with GPU cards, users can run the powertools command: 1 [ username@dev-amd20-v100 ~ ] $ getexample helloCUDA to copy the CUDA example directory \" helloCUDA \" in the current directory. After you cd to the directory, you should be able to see the CUDA code file: \" Hello.cu \": 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 [ username@dev-amd20-v100 ~ ] $ cd helloCUDA [ username@dev-amd20-v100 ~ ] $ ls Hello.cu Hello-CUDA.sb README run.sh [ username@dev-amd20-v100 ~ ] $ cat Hello.cu #include <stdio.h> #define NUM_BLOCKS 16 #define BLOCK_WIDTH 1 __global__ void hello () { printf ( \"Hello world! I'm a thread in block %d\\n\" , blockIdx.x ) ; } int main ( int argc,char **argv ) { // launch the kernel hello <<< NUM_BLOCKS, BLOCK_WIDTH>>> () ; // force the printf () s to flush cudaDeviceSynchronize () ; printf ( \"That's all!\\n\" ) ; return 0 ; } The CUDA code defines 16 blocks with one GPU core in each block. Each core runs one thread to print out \" Hello world! \" and the block number of the core. Totally 16 threads are executed in parallel. We can simply compile the code by \" nvcc \" command after a version of gcccuda toolchain is loaded: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [ username@dev-amd20-v100 ~ ] $ ml -* gcccuda/2019b [ username@dev-amd20-v100 ~ ] $ nvcc Hello.cu -o Hello_CUDA [ username@dev-amd20-v100 ~ ] $ ./Hello_CUDA Hello world! I 'm a thread in block 5 Hello world! I' m a thread in block 12 Hello world! I 'm a thread in block 11 Hello world! I' m a thread in block 0 Hello world! I 'm a thread in block 6 Hello world! I' m a thread in block 1 Hello world! I 'm a thread in block 7 Hello world! I' m a thread in block 13 Hello world! I 'm a thread in block 14 Hello world! I' m a thread in block 2 Hello world! I 'm a thread in block 8 Hello world! I' m a thread in block 4 Hello world! I 'm a thread in block 10 Hello world! I' m a thread in block 3 Hello world! I 'm a thread in block 9 Hello world! I' m a thread in block 15 That ' s all! Once it is compiled successfully, the execution of the binary \" Hello_CUDA \" on a GPU node should give the output as above.","title":"Simple CUDA example"},{"location":"Change_Primary_Group/","text":"Change primary group Generally, HPCC users need to request root privilege through the ticket system to change their primary group permanently. However, a user can also use newgrp command to change his primary group temporarily to any one of his groups. In this page, we will introduce Primary Group Change on Command Line To change a user's primary group with a command line, simply run 1 [ username@dev-intel18 CurrentPath ] $ newgrp <Group Name> where <Group Name> has to be one of the your group names. To find out all your group names, please run groups command: 1 2 [ username@dev-intel18 CurrentPath ] $ groups chemistry VMD g09 BiCEP education-data ParaView where the primary group is \" chemistry \", the first group name in the results. After the execution of newgrp command, a new shell session is created and the current environment, including the current working directory remains the same. 1 2 3 [ username@dev-intel18 CurrentPath ] $ newgrp g09 [ username@dev-intel18 CurrentPath ] $ groups g09 chemistry VMD BiCEP education-data ParaView where the primary group is changed to g09. To leave the session, just run \" exit \" command. It will go back to the previous session with the original primary group ( chemistry ). If a user would like to start the shell session with a new primary group as though the user just logged in. The optional \" - \" flag can be used 1 2 3 [ username@dev-intel18 CurrentPath ] $ newgrp - g09 [ username@dev-intel18 ~ ] $ groups g09 chemistry VMD BiCEP education-data ParaView where the user's environment is reinitialized and the shell session starts from his home directory. Some users might have the issue to lose the header in front of prompt after executing the command: 1 2 3 [ username@dev-intel14-phi CurrentPath ] $ newgrp g09 bash-4.2$ groups g09 chemistry VMD BiCEP education-data ParaView Please try source the default bashrc file to get it back. 1 2 bash-4.2$ source /etc/bashrc [ changc81@dev-intel14-phi MyCurrentPath ] $ Please notice! If a job is submitted in the session of a new primary group, the environment with the new primary group will be used in the job running by default instead of the original one. For more information, please check the row of --export in the table of List of Job Specifications page. Primary Group Change in Job Script Many HPCC users have more than one research spaces. In a job running, the user might need his primary group set to be the group of the research space where the output files are. In order to do this, the job script can use newgrp from the beginning of the command lines. For example, an original job script may look like: 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash --login #SBATCH --time=00:10:00 # limit of wall clock time - how long the job will run (same as -t) #SBATCH --ntasks=5 # number of tasks - how many tasks (nodes) that you require (same as -n) #SBATCH --mem-per-cpu=2G # memory required per allocated CPU (or core) - amount of memory (in bytes) module purge module load GCC/6.4.0-2.28 OpenMPI ### load necessary modules, e.g. srun -n 5 <executable> ### call your executable (similar to mpirun) scontrol show job $SLURM_JOB_ID ### write job information to output file If the job script is submitted in a research space, adding a few lines can make the primary group the same as the group of the research space: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bash --login #SBATCH --time=00:10:00 # limit of wall clock time - how long the job will run (same as -t) #SBATCH --ntasks=5 # number of tasks - how many tasks (nodes) that you require (same as -n) #SBATCH --mem-per-cpu=2G # memory required per allocated CPU (or core) - amount of memory (in bytes) GroupName = $( readlink -f $PWD | awk -F \"/\" '{ if ($2==\"mnt\" && $3==\"research\" && $4!=\"\") system(\"ls -ld /mnt/research/\"$4\"|cut -d \\\" \\\" -f 4\") }' ) newgrp ${ GroupName } << EOS module purge module load GCC/6.4.0-2.28 OpenMPI ### load necessary modules, e.g. srun -n 5 <executable> ### call your executable (similar to mpirun) scontrol show job $SLURM_JOB_ID ### write job information to output file EOS where the group name of the research space for job running is automatically determined in line 7. You can also directly set the variable \" GroupName \" to be the group name of your research space: 1 GroupName = <group name of your research space> in line 7 if you are sure about jobs running in the research space. After the group name of the research space is given, the command lines between line 9 and 16 (inside the two \" EOS \") will be run under the environment of the new primary group ( ${GroupName} ). Automatic Primary Group Change after Login If users need to change their primary group constantly, they can use the powertools script PrimaryGroup. Users can add a command line \"source /opt/software/powertools/doc/PrimaryGroup \" in the beginning of the file \\~/.bashrc: ~/.bashrc 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash source /opt/software/powertools/doc/PrimaryGroup <Group Name> # Add this line to change primary group automatically # Source global definitions if [ -f /etc/bashrc ] ; then . /etc/bashrc fi ... ... where <Group Name> is one of his group names. This also provides a flexibility that user's primary group can be changed to any of his groups at any time without requesting root privilege. Please notice! The powertools script PrimaryGroup only works on dev nodes. It does not work on gateway or compute nodes. Users need to ssh to a dev node to see the effect. After this command line is added in the ~/.bashrc file, the command newgrp with the optional \" - \" flag will not work. Users can still use newgrp without \" - \" flag.","title":"Change primary group"},{"location":"Change_Primary_Group/#change-primary-group","text":"Generally, HPCC users need to request root privilege through the ticket system to change their primary group permanently. However, a user can also use newgrp command to change his primary group temporarily to any one of his groups. In this page, we will introduce","title":"Change primary group"},{"location":"Change_Primary_Group/#primary-group-change-on-command-line","text":"To change a user's primary group with a command line, simply run 1 [ username@dev-intel18 CurrentPath ] $ newgrp <Group Name> where <Group Name> has to be one of the your group names. To find out all your group names, please run groups command: 1 2 [ username@dev-intel18 CurrentPath ] $ groups chemistry VMD g09 BiCEP education-data ParaView where the primary group is \" chemistry \", the first group name in the results. After the execution of newgrp command, a new shell session is created and the current environment, including the current working directory remains the same. 1 2 3 [ username@dev-intel18 CurrentPath ] $ newgrp g09 [ username@dev-intel18 CurrentPath ] $ groups g09 chemistry VMD BiCEP education-data ParaView where the primary group is changed to g09. To leave the session, just run \" exit \" command. It will go back to the previous session with the original primary group ( chemistry ). If a user would like to start the shell session with a new primary group as though the user just logged in. The optional \" - \" flag can be used 1 2 3 [ username@dev-intel18 CurrentPath ] $ newgrp - g09 [ username@dev-intel18 ~ ] $ groups g09 chemistry VMD BiCEP education-data ParaView where the user's environment is reinitialized and the shell session starts from his home directory. Some users might have the issue to lose the header in front of prompt after executing the command: 1 2 3 [ username@dev-intel14-phi CurrentPath ] $ newgrp g09 bash-4.2$ groups g09 chemistry VMD BiCEP education-data ParaView Please try source the default bashrc file to get it back. 1 2 bash-4.2$ source /etc/bashrc [ changc81@dev-intel14-phi MyCurrentPath ] $ Please notice! If a job is submitted in the session of a new primary group, the environment with the new primary group will be used in the job running by default instead of the original one. For more information, please check the row of --export in the table of List of Job Specifications page.","title":"Primary Group Change on Command Line"},{"location":"Change_Primary_Group/#primary-group-change-in-job-script","text":"Many HPCC users have more than one research spaces. In a job running, the user might need his primary group set to be the group of the research space where the output files are. In order to do this, the job script can use newgrp from the beginning of the command lines. For example, an original job script may look like: 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash --login #SBATCH --time=00:10:00 # limit of wall clock time - how long the job will run (same as -t) #SBATCH --ntasks=5 # number of tasks - how many tasks (nodes) that you require (same as -n) #SBATCH --mem-per-cpu=2G # memory required per allocated CPU (or core) - amount of memory (in bytes) module purge module load GCC/6.4.0-2.28 OpenMPI ### load necessary modules, e.g. srun -n 5 <executable> ### call your executable (similar to mpirun) scontrol show job $SLURM_JOB_ID ### write job information to output file If the job script is submitted in a research space, adding a few lines can make the primary group the same as the group of the research space: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bash --login #SBATCH --time=00:10:00 # limit of wall clock time - how long the job will run (same as -t) #SBATCH --ntasks=5 # number of tasks - how many tasks (nodes) that you require (same as -n) #SBATCH --mem-per-cpu=2G # memory required per allocated CPU (or core) - amount of memory (in bytes) GroupName = $( readlink -f $PWD | awk -F \"/\" '{ if ($2==\"mnt\" && $3==\"research\" && $4!=\"\") system(\"ls -ld /mnt/research/\"$4\"|cut -d \\\" \\\" -f 4\") }' ) newgrp ${ GroupName } << EOS module purge module load GCC/6.4.0-2.28 OpenMPI ### load necessary modules, e.g. srun -n 5 <executable> ### call your executable (similar to mpirun) scontrol show job $SLURM_JOB_ID ### write job information to output file EOS where the group name of the research space for job running is automatically determined in line 7. You can also directly set the variable \" GroupName \" to be the group name of your research space: 1 GroupName = <group name of your research space> in line 7 if you are sure about jobs running in the research space. After the group name of the research space is given, the command lines between line 9 and 16 (inside the two \" EOS \") will be run under the environment of the new primary group ( ${GroupName} ).","title":"Primary Group Change in Job Script"},{"location":"Change_Primary_Group/#automatic-primary-group-change-after-login","text":"If users need to change their primary group constantly, they can use the powertools script PrimaryGroup. Users can add a command line \"source /opt/software/powertools/doc/PrimaryGroup \" in the beginning of the file \\~/.bashrc: ~/.bashrc 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash source /opt/software/powertools/doc/PrimaryGroup <Group Name> # Add this line to change primary group automatically # Source global definitions if [ -f /etc/bashrc ] ; then . /etc/bashrc fi ... ... where <Group Name> is one of his group names. This also provides a flexibility that user's primary group can be changed to any of his groups at any time without requesting root privilege. Please notice! The powertools script PrimaryGroup only works on dev nodes. It does not work on gateway or compute nodes. Users need to ssh to a dev node to see the effect. After this command line is added in the ~/.bashrc file, the command newgrp with the optional \" - \" flag will not work. Users can still use newgrp without \" - \" flag.","title":"Automatic Primary Group Change after Login"},{"location":"Check-pointing_with_longjob/","text":"Check-pointing with longjob \"Check-pointing\" is a technique for stopping a process, saving it's state to disk in a way that the process can be restarted. Some Application include this feature, and we encourage users of the HPCC developing their own code to consider implementing checkpointing by writing result variables to disk at regular intervals and including functions to read those variables in and restart. However if your program does not include checkpointing, you may consider using \"Berkeley Lab Checkpoint-Restart\" or BLCR feature that installed on our nodes. BLCR is a forceful way to checkpoint a process but if there are no other options, it may work. It does not work for multinode (e.g. MPI) jobs or those that depend on temporary files (e.g. often written to /tmp). Dirk Colbry, former HPCC Director created a new Powertool to use BLCR called \"longjob\" It requires a few modifications to your submission script. In addition to running jobs longer than a week, using longjob with a four hour walltime has the following advantages: Run jobs with unknown walltimes Run jobs on the buy-in nodes (which requires 4 hours or less walltime) Enables robustness of long jobs due to hardware failure Run jobs up to a maintenance window without having to wait for that window to complete The following are instructions for trying out longjob on our system. First, you start with a a basic submission script. For example, consider the following simple submission script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #!/bin/bash -login #PBS -l nodes=1:ppn=1,walltime=168:00:00,mem=2gb,feature=intel16 #PBS -j oe #PBS -m ae cd $PBS_O_WORKDIR srcdir = ${ PBS_O_WORKDIR } /bin/ WORK = /mnt/scratch/ ${ USER } /KineticSN/ ${ PBS_JOBID } mkdir -p ${ WORK } # Copy files to work directory cp -r $srcdir /* $WORK / #Move to the working directory cd $WORK #Run my program ./SimulationTest -scattering_flag 0 -weak_reaction_flag 0 -outputVisData 100 ret = $? qstat -f ${ PBS_JOBID } exit $ret To get longjob to work, the following modificaitons need to be made: Adjust the walltime to be shorter (I suggest 4 hours or less). Wrap all setup-code that only needs to be run once in an if statement that checks for the checkpoint file (checkfile.blcr). This will ensure that the setup-code only runs the first time the script is run because the first time the script is run there should not be a checkpoint file. add the \"longjob\" command before the command in the submission script that you want to checkpoint. load the powertools module and turn on aliases. i.e. add the following lines of code to the script: shopt -s expand_aliases module load use.cus powertools Set the following enviornment variables as appropriate for your job: BLCR_WAIT_SEC number of seconds the job should wait before checkpointing and restarting. (should be less than your walltime, default is 3 hours and 55 minutes). PBS_JOBSCRIPT (required) the path and name of the jobscript to use in the restart. Typically this is the same as your main jobscript and by default you can always add the following line: export PBS_JOBSCRIPT=\"$0\" BLCR_OUTPUT name of the main standardout/standarderr file (Default is output.txt) BLCR_CHECKFILE name of the checkpoint file (Default is checkfile.blcr) The following is a modified example script with the changes: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #!/bin/bash -login #PBS -l nodes=1:ppn=1,walltime=04:00:00,mem=2gb,feature=intel16 #PBS -j oe #PBS -m ae shopt -s expand_aliases cd $PBS_O_WORKDIR module load powertools # 4 hours * 60 minutes * 6 seconds - 60 seconds * 5 minutes export BLCR_WAIT_SEC = $(( 4 * 60 * 60 - 60 * 5 )) export PBS_JOBSCRIPT = \" $0 \" echo \"Waiting ${ BLCR_WAIT_SEC } seconds to run ${ PBS_JOBSCRIPT } \" if [ ! -f checkfile.blcr ] then srcdir = ${ PBS_O_WORKDIR } /bin/ WORK = /mnt/scratch/ ${ USER } /KineticSN/ ${ PBS_JOBID } mkdir -p ${ WORK } # Copy files to work directory cp -r $srcdir /* $WORK / #Run main simulation program cd $WORK fi longjob ./SimulationTest -scattering_flag 0 -weak_reaction_flag 0 -outputVisData 100 ret = $? qstat -f ${ PBS_JOBID } exit $ret If everything works as expected, you should be able to qsub the above file and it will resubmit itself every four hours until the job completes. Note, this is rough code, not completely tested and does work in all cases. For example, one case that could propose a problem is if the main program gets caught in a loop and never exits, in this case the code will keep submitting itself indefinitely. Note that in addition you can't include output redirection as you'd expect, that is a command like \"myprogram.py > myoutput.txt\" and \"longjog myprogram.py > myoutput.py\" is not the same (the redirection here applies to longjob, not your program).","title":"Check-pointing with longjob"},{"location":"Check-pointing_with_longjob/#check-pointing-with-longjob","text":"\"Check-pointing\" is a technique for stopping a process, saving it's state to disk in a way that the process can be restarted. Some Application include this feature, and we encourage users of the HPCC developing their own code to consider implementing checkpointing by writing result variables to disk at regular intervals and including functions to read those variables in and restart. However if your program does not include checkpointing, you may consider using \"Berkeley Lab Checkpoint-Restart\" or BLCR feature that installed on our nodes. BLCR is a forceful way to checkpoint a process but if there are no other options, it may work. It does not work for multinode (e.g. MPI) jobs or those that depend on temporary files (e.g. often written to /tmp). Dirk Colbry, former HPCC Director created a new Powertool to use BLCR called \"longjob\" It requires a few modifications to your submission script. In addition to running jobs longer than a week, using longjob with a four hour walltime has the following advantages: Run jobs with unknown walltimes Run jobs on the buy-in nodes (which requires 4 hours or less walltime) Enables robustness of long jobs due to hardware failure Run jobs up to a maintenance window without having to wait for that window to complete The following are instructions for trying out longjob on our system. First, you start with a a basic submission script. For example, consider the following simple submission script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #!/bin/bash -login #PBS -l nodes=1:ppn=1,walltime=168:00:00,mem=2gb,feature=intel16 #PBS -j oe #PBS -m ae cd $PBS_O_WORKDIR srcdir = ${ PBS_O_WORKDIR } /bin/ WORK = /mnt/scratch/ ${ USER } /KineticSN/ ${ PBS_JOBID } mkdir -p ${ WORK } # Copy files to work directory cp -r $srcdir /* $WORK / #Move to the working directory cd $WORK #Run my program ./SimulationTest -scattering_flag 0 -weak_reaction_flag 0 -outputVisData 100 ret = $? qstat -f ${ PBS_JOBID } exit $ret To get longjob to work, the following modificaitons need to be made: Adjust the walltime to be shorter (I suggest 4 hours or less). Wrap all setup-code that only needs to be run once in an if statement that checks for the checkpoint file (checkfile.blcr). This will ensure that the setup-code only runs the first time the script is run because the first time the script is run there should not be a checkpoint file. add the \"longjob\" command before the command in the submission script that you want to checkpoint. load the powertools module and turn on aliases. i.e. add the following lines of code to the script: shopt -s expand_aliases module load use.cus powertools Set the following enviornment variables as appropriate for your job: BLCR_WAIT_SEC number of seconds the job should wait before checkpointing and restarting. (should be less than your walltime, default is 3 hours and 55 minutes). PBS_JOBSCRIPT (required) the path and name of the jobscript to use in the restart. Typically this is the same as your main jobscript and by default you can always add the following line: export PBS_JOBSCRIPT=\"$0\" BLCR_OUTPUT name of the main standardout/standarderr file (Default is output.txt) BLCR_CHECKFILE name of the checkpoint file (Default is checkfile.blcr) The following is a modified example script with the changes: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #!/bin/bash -login #PBS -l nodes=1:ppn=1,walltime=04:00:00,mem=2gb,feature=intel16 #PBS -j oe #PBS -m ae shopt -s expand_aliases cd $PBS_O_WORKDIR module load powertools # 4 hours * 60 minutes * 6 seconds - 60 seconds * 5 minutes export BLCR_WAIT_SEC = $(( 4 * 60 * 60 - 60 * 5 )) export PBS_JOBSCRIPT = \" $0 \" echo \"Waiting ${ BLCR_WAIT_SEC } seconds to run ${ PBS_JOBSCRIPT } \" if [ ! -f checkfile.blcr ] then srcdir = ${ PBS_O_WORKDIR } /bin/ WORK = /mnt/scratch/ ${ USER } /KineticSN/ ${ PBS_JOBID } mkdir -p ${ WORK } # Copy files to work directory cp -r $srcdir /* $WORK / #Run main simulation program cd $WORK fi longjob ./SimulationTest -scattering_flag 0 -weak_reaction_flag 0 -outputVisData 100 ret = $? qstat -f ${ PBS_JOBID } exit $ret If everything works as expected, you should be able to qsub the above file and it will resubmit itself every four hours until the job completes. Note, this is rough code, not completely tested and does work in all cases. For example, one case that could propose a problem is if the main program gets caught in a loop and never exits, in this case the code will keep submitting itself indefinitely. Note that in addition you can't include output redirection as you'd expect, that is a command like \"myprogram.py > myoutput.txt\" and \"longjog myprogram.py > myoutput.py\" is not the same (the redirection here applies to longjob, not your program).","title":"Check-pointing with longjob"},{"location":"Checkpoint_with_DMTCP/","text":"Checkpoint with DMTCP Note To run DMTCP correctly, the limit of stack size could not be \"unlimited\". To set it to a number n, run \"ulimit -s n\". For example, \"ulimit -s 8192\" would set the stack size as 8192. User could also add it into .bashrc file. It should also be set in the job batch script, as seen below. DMTCP (Distributed MultiThreaded Checkpointing) transparently checkpoints a single-host or distributed computation in user-space. It supports MPI (various implementations), OpenMP, MATLAB, Python, Perl, R, and many programming languages and shell scripting languages. DMTCP works under Linux with no modifications to the application binaries. It can be used by users (no root privilege needed). DMTCP allows one to checkpoint to disk a distributed computation and later restart from a checkpoint, or even migrate the processes by moving the checkpoint files to another host prior to restarting. There is one DMTCP coordinator for each computation that you wish to checkpoint. A DMTCP coordinator process is started on one host. Application binaries are started under the dmtcp_launch command, causing them to connect to the coordinator upon startup. As threads are spawned, child processes are forked, remote processes are spawned via ssh, libraries are dynamically loaded, DMTCP transparently and automatically tracks them. To checkpoint, use dmtcp_coordinator command to start checkpointing. To restart from a checkpoint, use dmtcp_restart . By default, DMTCP uses gzip to compress the checkpoint images. This can be turned off. This will be faster, and if your memory is dominated by incompressible data, this can be helpful. gzip can add seconds for large checkpoint images. Typically, checkpoint and restart is less than one second without gzip. To run a program with checkpointing usually involves following 4 steps (may need more option settings for special cases) : Start dmtcp_coordinator $ dmtcp_coordinator --daemon --exit-on-last $@ 1>/dev/null 2>&1 #run coordinator as daemon in background Launch program with dmtcp_launch $ dmtcp_launch ./a.out # launch ./a.out To checkpoint, run dmtcp_command with checkpointing option. It will generate a set of checkpointing image files (file type: .dmtcp) and a shell script for restart. $ dmtcp_command --bcheckpoint # checkpointing Restart: Creating a checkpoint causes the dmtcp_coordinator to write a script, dmtcp_restart_script.sh, along with a checkpoint file (file type: .dmtcp) for each client process. The simplest way to restart a previously checkpointed computation is: $ ./dmtcp_restart_script.sh # restart using script $ dmtcp_restart ckpt_*.dmtcp # Alternatively, if all processes were on the same processor, and there were no .dmtcp files prior to this checkpoint: Following is the sample script longjob.sb that using DMTCP for checkpointing a long job so that the job could be run as a sequence of short walltime jobs. To obtain the complete example, run \"module load powertools; getexample dmtcp_longjob\". 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 #!/bin/bash -login ## resource requests for task: #SBATCH -J count-longjob # Job Name #SBATCH --time=00:06:00 # Note that 6 min is not enough to complete the job. It enough for checkpointing and resubmit job #SBATCH -N 1 -c 1 --mem=20MB # requested resource #SBATCH --constraint=lac # user could add other requests as usual. # set a limited stack size so DMTCP could work ulimit -s 8192 # current working directory shuld have source code dmtcp1.c cd ${ SLURM_SUBMIT_DIR } # this script file name. This script may be resubmit multiple times until job completed export SLURM_JOBSCRIPT = \"longjob.sb\" ######################## start dmtcp_coordinator ####################### fname = port. $SLURM_JOBID # to store port number dmtcp_coordinator --daemon --exit-on-last -p 0 --port-file $fname $@ 1 >/dev/null 2 > & 1 # start coordinater h = ` hostname ` # get coordinator's host name p = ` cat $fname ` # get coordinator's port number export DMTCP_COORD_HOST = $h # save coordinators host info in an environment variable export DMTCP_COORD_PORT = $p # save coordinators port info in an environment variable #rm $fname # uncommand following lines to print out some information if user wish #echo \"coordinator is on host $DMTCP_COORD_HOST \" #echo \"port number is $DMTCP_COORD_PORT \" #echo \" working directory: ${SLURM_SUBMIT_DIR} \" #echo \" job script is $SLURM_JOBSCRIPT \" ####################### BODY of the JOB ###################### # prepare work environment of the job module swap GNU/6.4.0-2.28 GCC/4.9.2 # build the program if executable file does not exist if [ ! -f count.exe ] then cc count.c -o count.exe fi # run the program count.exe. # To run interactively: # $ ./count.exe n num.odd 1> num.even # it will count to number n and generate 2 files: # num.odd contains all the odd number; # num.even contains all the even number. # To run with DMTCP, use dmtcp commamds. # if first time launch, use \"dmtcp_launch\" # otherwise use \"dmtcp_restart\" # set checkpoint interval. This script would wait after dmtcp_launch # the job for the interval (in seconds), then do start the checkpoint. export CKPT_WAIT_SEC = $(( 3 * 60 )) # checkpointing when program runs for 3 min # Launch or restart the execution if [ ! -f ckpt_*.dmtcp ] # if no ckpt file exists, it is first time run, use dmtcp_launch then # first time run, use dmtcp_launch to start the job and run on background */ dmtcp_launch -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --rm --ckpt-open-files ./count.exe 800 num.odd 1 > num.even 10 > & - 11 > & - & #wait for an inverval of checkpoint seconds to start checkpointing sleep $CKPT_WAIT_SEC # start checkpointing dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --ckpt-open-files --bcheckpoint # kill the running job after checkpointing dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --quit # resubmit the job sbatch $SLURM_JOBSCRIPT else # it is a restart run # restart job with checkpoint files ckpt_*.dmtcp and run in background dmtcp_restart -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT ckpt_*.dmtcp 1 > num.even & # wait for a checkpoint interval to start checkpointing sleep $CKPT_WAIT_SEC # if program is still running, do the checkpoint and resubmit if dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT -s 1 >/dev/null 2 > & 1 then # clean up old ckpt files before start checkpointing rm -r ckpt_*.dmtcp # checkpointing the job dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --ckpt-open-files -bc # kill the running program and quit dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --quit # resubmit this script to slurm sbatch $SLURM_JOBSCRIPT else echo \"job finished\" fi fi # show the job status info scontrol show job $SLURM_JOB_ID","title":"Checkpoint with DMTCP"},{"location":"Checkpoint_with_DMTCP/#checkpoint-with-dmtcp","text":"Note To run DMTCP correctly, the limit of stack size could not be \"unlimited\". To set it to a number n, run \"ulimit -s n\". For example, \"ulimit -s 8192\" would set the stack size as 8192. User could also add it into .bashrc file. It should also be set in the job batch script, as seen below. DMTCP (Distributed MultiThreaded Checkpointing) transparently checkpoints a single-host or distributed computation in user-space. It supports MPI (various implementations), OpenMP, MATLAB, Python, Perl, R, and many programming languages and shell scripting languages. DMTCP works under Linux with no modifications to the application binaries. It can be used by users (no root privilege needed). DMTCP allows one to checkpoint to disk a distributed computation and later restart from a checkpoint, or even migrate the processes by moving the checkpoint files to another host prior to restarting. There is one DMTCP coordinator for each computation that you wish to checkpoint. A DMTCP coordinator process is started on one host. Application binaries are started under the dmtcp_launch command, causing them to connect to the coordinator upon startup. As threads are spawned, child processes are forked, remote processes are spawned via ssh, libraries are dynamically loaded, DMTCP transparently and automatically tracks them. To checkpoint, use dmtcp_coordinator command to start checkpointing. To restart from a checkpoint, use dmtcp_restart . By default, DMTCP uses gzip to compress the checkpoint images. This can be turned off. This will be faster, and if your memory is dominated by incompressible data, this can be helpful. gzip can add seconds for large checkpoint images. Typically, checkpoint and restart is less than one second without gzip. To run a program with checkpointing usually involves following 4 steps (may need more option settings for special cases) : Start dmtcp_coordinator $ dmtcp_coordinator --daemon --exit-on-last $@ 1>/dev/null 2>&1 #run coordinator as daemon in background Launch program with dmtcp_launch $ dmtcp_launch ./a.out # launch ./a.out To checkpoint, run dmtcp_command with checkpointing option. It will generate a set of checkpointing image files (file type: .dmtcp) and a shell script for restart. $ dmtcp_command --bcheckpoint # checkpointing Restart: Creating a checkpoint causes the dmtcp_coordinator to write a script, dmtcp_restart_script.sh, along with a checkpoint file (file type: .dmtcp) for each client process. The simplest way to restart a previously checkpointed computation is: $ ./dmtcp_restart_script.sh # restart using script $ dmtcp_restart ckpt_*.dmtcp # Alternatively, if all processes were on the same processor, and there were no .dmtcp files prior to this checkpoint: Following is the sample script longjob.sb that using DMTCP for checkpointing a long job so that the job could be run as a sequence of short walltime jobs. To obtain the complete example, run \"module load powertools; getexample dmtcp_longjob\". 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 #!/bin/bash -login ## resource requests for task: #SBATCH -J count-longjob # Job Name #SBATCH --time=00:06:00 # Note that 6 min is not enough to complete the job. It enough for checkpointing and resubmit job #SBATCH -N 1 -c 1 --mem=20MB # requested resource #SBATCH --constraint=lac # user could add other requests as usual. # set a limited stack size so DMTCP could work ulimit -s 8192 # current working directory shuld have source code dmtcp1.c cd ${ SLURM_SUBMIT_DIR } # this script file name. This script may be resubmit multiple times until job completed export SLURM_JOBSCRIPT = \"longjob.sb\" ######################## start dmtcp_coordinator ####################### fname = port. $SLURM_JOBID # to store port number dmtcp_coordinator --daemon --exit-on-last -p 0 --port-file $fname $@ 1 >/dev/null 2 > & 1 # start coordinater h = ` hostname ` # get coordinator's host name p = ` cat $fname ` # get coordinator's port number export DMTCP_COORD_HOST = $h # save coordinators host info in an environment variable export DMTCP_COORD_PORT = $p # save coordinators port info in an environment variable #rm $fname # uncommand following lines to print out some information if user wish #echo \"coordinator is on host $DMTCP_COORD_HOST \" #echo \"port number is $DMTCP_COORD_PORT \" #echo \" working directory: ${SLURM_SUBMIT_DIR} \" #echo \" job script is $SLURM_JOBSCRIPT \" ####################### BODY of the JOB ###################### # prepare work environment of the job module swap GNU/6.4.0-2.28 GCC/4.9.2 # build the program if executable file does not exist if [ ! -f count.exe ] then cc count.c -o count.exe fi # run the program count.exe. # To run interactively: # $ ./count.exe n num.odd 1> num.even # it will count to number n and generate 2 files: # num.odd contains all the odd number; # num.even contains all the even number. # To run with DMTCP, use dmtcp commamds. # if first time launch, use \"dmtcp_launch\" # otherwise use \"dmtcp_restart\" # set checkpoint interval. This script would wait after dmtcp_launch # the job for the interval (in seconds), then do start the checkpoint. export CKPT_WAIT_SEC = $(( 3 * 60 )) # checkpointing when program runs for 3 min # Launch or restart the execution if [ ! -f ckpt_*.dmtcp ] # if no ckpt file exists, it is first time run, use dmtcp_launch then # first time run, use dmtcp_launch to start the job and run on background */ dmtcp_launch -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --rm --ckpt-open-files ./count.exe 800 num.odd 1 > num.even 10 > & - 11 > & - & #wait for an inverval of checkpoint seconds to start checkpointing sleep $CKPT_WAIT_SEC # start checkpointing dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --ckpt-open-files --bcheckpoint # kill the running job after checkpointing dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --quit # resubmit the job sbatch $SLURM_JOBSCRIPT else # it is a restart run # restart job with checkpoint files ckpt_*.dmtcp and run in background dmtcp_restart -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT ckpt_*.dmtcp 1 > num.even & # wait for a checkpoint interval to start checkpointing sleep $CKPT_WAIT_SEC # if program is still running, do the checkpoint and resubmit if dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT -s 1 >/dev/null 2 > & 1 then # clean up old ckpt files before start checkpointing rm -r ckpt_*.dmtcp # checkpointing the job dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --ckpt-open-files -bc # kill the running program and quit dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --quit # resubmit this script to slurm sbatch $SLURM_JOBSCRIPT else echo \"job finished\" fi fi # show the job status info scontrol show job $SLURM_JOB_ID","title":"Checkpoint with DMTCP"},{"location":"Cluster_amd20_with_AMD_CPUs/","text":"Cluster amd20 with AMD CPUs In 2020, the HPCC purchased a new cluster amd20 powered by AMD EPYC processors. Each amd20 node has total 128 cores and at least 0.5 TB RAM. Each core has a base clock speed 2.6 GHz, up to 3.3 GHz. Please check the following sections for how to use the test nodes and the node performance from our test results for user reference. Running Jobs on amd20 Test Nodes Basic Mathematical Library Tests on AMD EPYC Processors AMD Optimizing CPU Libraries and Compilers Basic architecture information The AMD CPU contains 2 sockets with CPU packages with 4 NUMA nodes in each socket, and 16 cores in each NUMA node. Each NUMA core contains two \"Core Complex Dies\", which have two four-core \"Core-Complex\" modules. Each four-core Core-Complex shares a 16 MB L3 cache. A 7002 series processor (via AMD) A Core-Complex (via AMD) logical diagram. A 1 TB amd20 node. Generally, cores within the same L3 cache have the lowest latency, followed by in-NUMA cores, other cores on the same socket, and cores on the other socket are slower. The Slurm job scheduler on the HPCC will try to keep within the same NUMA node by default. One option to try during testing is to use OpenMP within the L3 node, and MPI for everything else. When using newer versions of OpenMPI, you can use the following argument with OMP_NUM_THREADS=4 to distribute one 4-thread rank per L3: 1 mpirun -map-by ppr:1:l3cache:pe = 4 --bind-to core In testing, the Intel Compiler and MKL toolchain works well if you \u201cexport MKL_DEBUG_CPU_TYPE=5\u201d and compile for AVX2 instead of AVX512. If you\u2019re doing single-node scaling work, be aware of memory bandwidth; on these nodes, HPL scales from 1-96 cores linearly but only 3.5 -> 4 TF from 96->128 when going from 3 cores per L3 to 4. Each node has a 100 gigabit HDR100 connection. There are 52-56 nodes per switch: AMD20 network topology. You can figure out by using scontrol show node and looking at 1 2 3 4 5 6 7 $ sinfo -n amr- [ 000 -209 ] -o %n,%b HOSTNAMES,ACTIVE_FEATURES amr-188,amr,gbe,amd20,ib,rack-S21 amr-186,amr,gbe,amd20,ib,rack-S21 amr-000,amr,gbe,amd20,ib,rack-Z21 amr-001,amr,gbe,amd20,ib,rack-Z21 amr-002,amr,gbe,amd20,ib,rack-Z21 Z21/Y21 are on the same switch, etc. Resources: High Performance Computing: Tuning Guide for AMD EPYC\u2122 7002 Series Processors Compiler Options Quick Ref Guide for AMD EPYC 7xx2 Series Processors.pdf","title":"Cluster amd20 with AMD CPUs"},{"location":"Cluster_amd20_with_AMD_CPUs/#cluster-amd20-with-amd-cpus","text":"In 2020, the HPCC purchased a new cluster amd20 powered by AMD EPYC processors. Each amd20 node has total 128 cores and at least 0.5 TB RAM. Each core has a base clock speed 2.6 GHz, up to 3.3 GHz. Please check the following sections for how to use the test nodes and the node performance from our test results for user reference.","title":"Cluster amd20 with AMD CPUs"},{"location":"Cluster_amd20_with_AMD_CPUs/#running-jobs-on-amd20-test-nodes","text":"","title":"Running Jobs on amd20 Test Nodes"},{"location":"Cluster_amd20_with_AMD_CPUs/#basic-mathematical-library-tests-on-amd-epyc-processors","text":"","title":"Basic Mathematical Library Tests on AMD EPYC Processors"},{"location":"Cluster_amd20_with_AMD_CPUs/#amd-optimizing-cpu-libraries-and-compilers","text":"Basic architecture information The AMD CPU contains 2 sockets with CPU packages with 4 NUMA nodes in each socket, and 16 cores in each NUMA node. Each NUMA core contains two \"Core Complex Dies\", which have two four-core \"Core-Complex\" modules. Each four-core Core-Complex shares a 16 MB L3 cache. A 7002 series processor (via AMD) A Core-Complex (via AMD) logical diagram. A 1 TB amd20 node. Generally, cores within the same L3 cache have the lowest latency, followed by in-NUMA cores, other cores on the same socket, and cores on the other socket are slower. The Slurm job scheduler on the HPCC will try to keep within the same NUMA node by default. One option to try during testing is to use OpenMP within the L3 node, and MPI for everything else. When using newer versions of OpenMPI, you can use the following argument with OMP_NUM_THREADS=4 to distribute one 4-thread rank per L3: 1 mpirun -map-by ppr:1:l3cache:pe = 4 --bind-to core In testing, the Intel Compiler and MKL toolchain works well if you \u201cexport MKL_DEBUG_CPU_TYPE=5\u201d and compile for AVX2 instead of AVX512. If you\u2019re doing single-node scaling work, be aware of memory bandwidth; on these nodes, HPL scales from 1-96 cores linearly but only 3.5 -> 4 TF from 96->128 when going from 3 cores per L3 to 4. Each node has a 100 gigabit HDR100 connection. There are 52-56 nodes per switch: AMD20 network topology. You can figure out by using scontrol show node and looking at 1 2 3 4 5 6 7 $ sinfo -n amr- [ 000 -209 ] -o %n,%b HOSTNAMES,ACTIVE_FEATURES amr-188,amr,gbe,amd20,ib,rack-S21 amr-186,amr,gbe,amd20,ib,rack-S21 amr-000,amr,gbe,amd20,ib,rack-Z21 amr-001,amr,gbe,amd20,ib,rack-Z21 amr-002,amr,gbe,amd20,ib,rack-Z21 Z21/Y21 are on the same switch, etc. Resources: High Performance Computing: Tuning Guide for AMD EPYC\u2122 7002 Series Processors Compiler Options Quick Ref Guide for AMD EPYC 7xx2 Series Processors.pdf","title":"AMD Optimizing CPU Libraries and Compilers"},{"location":"Collaborative_Research/","text":"Collaborative research Research space To support collaborative research on campus, a PI may request a shared research space where files and softwares can be shared. Software installation request We suggest the following two steps when you are in need of a particular software package. Go to /opt/software/ on a dev-node. This is where all software packages are installed. Check if your desired software and specific versions are there. Please note there could be some name discrepancy caused by switch between lower and upper case letters. Take BBMap for example, all the versions available can be viewed by \" ls -l /opt/software/BBMap \". Send us a ticket if the software is totally absent in /opt/software or your desired version is missing. If the software and the version are both present in /opt/software , go to step 2. Follow the instruction to try loading software/version. If you can't find the software using module spider , send us a ticket. If you can find the software but not the desired version, send us a ticket too. Using our Academic Research Consulting Services ICER Academic Research Consulting service (ARCS) is to meet the computationally complex needs of the MSU research community. Research groups in need of software and computational workflow development services, dedicated training programs, as well as those that require additional scientific expertise in areas such as bioinformatics, time-series analysis, and computational modeling may request long-term collaborations with ARCS research consultants. ICER ARCS consultants are PhD-level researchers with domain-specific computational expertise and experience at R1 institutions. ARCS collaborations require funding and may be funded internally through startup grants or unit-level support or externally by grant support or industrial partnerships. More information can be found here .","title":"Collaborative research"},{"location":"Collaborative_Research/#collaborative-research","text":"","title":"Collaborative research"},{"location":"Collaborative_Research/#research-space","text":"To support collaborative research on campus, a PI may request a shared research space where files and softwares can be shared.","title":"Research space"},{"location":"Collaborative_Research/#software-installation-request","text":"We suggest the following two steps when you are in need of a particular software package. Go to /opt/software/ on a dev-node. This is where all software packages are installed. Check if your desired software and specific versions are there. Please note there could be some name discrepancy caused by switch between lower and upper case letters. Take BBMap for example, all the versions available can be viewed by \" ls -l /opt/software/BBMap \". Send us a ticket if the software is totally absent in /opt/software or your desired version is missing. If the software and the version are both present in /opt/software , go to step 2. Follow the instruction to try loading software/version. If you can't find the software using module spider , send us a ticket. If you can find the software but not the desired version, send us a ticket too.","title":"Software installation request"},{"location":"Collaborative_Research/#using-our-academic-research-consulting-services","text":"ICER Academic Research Consulting service (ARCS) is to meet the computationally complex needs of the MSU research community. Research groups in need of software and computational workflow development services, dedicated training programs, as well as those that require additional scientific expertise in areas such as bioinformatics, time-series analysis, and computational modeling may request long-term collaborations with ARCS research consultants. ICER ARCS consultants are PhD-level researchers with domain-specific computational expertise and experience at R1 institutions. ARCS collaborations require funding and may be funded internally through startup grants or unit-level support or externally by grant support or industrial partnerships. More information can be found here .","title":"Using our Academic Research Consulting Services"},{"location":"Common_Module_Commands/","text":"Common Module Commands A module manages environment variables needed to load a particular piece of software. The login process loads a few modules by default for HPCC users. The modules include the GNU compilers, the OpenMPI communication library, MATLAB and R. To see a list of modules that are currently loaded: 1 module list To search what modules are available to be loaded: 1 module avail To see the entire list of all modules: 1 module spider To find out if a particular software is in the list of all modules: 1 module spider <software_name> To check how to load a particular version of a software 1 module spider <software_name>/<version> To load a module: 1 module load <module_name> To unload a module: 1 module unload <module_name> To swap a module: 1 module swap GNU Intel To see what environment variables would be set/changed if you load a specific module: 1 module show <module_name> To load a modulefile in a non-standard directory: 1 2 module use <path_to_module> module load <modulename> For more information about Lmod module system, please refer to the documentation web site .","title":"Common Module Commands"},{"location":"Common_Module_Commands/#common-module-commands","text":"A module manages environment variables needed to load a particular piece of software. The login process loads a few modules by default for HPCC users. The modules include the GNU compilers, the OpenMPI communication library, MATLAB and R. To see a list of modules that are currently loaded: 1 module list To search what modules are available to be loaded: 1 module avail To see the entire list of all modules: 1 module spider To find out if a particular software is in the list of all modules: 1 module spider <software_name> To check how to load a particular version of a software 1 module spider <software_name>/<version> To load a module: 1 module load <module_name> To unload a module: 1 module unload <module_name> To swap a module: 1 module swap GNU Intel To see what environment variables would be set/changed if you load a specific module: 1 module show <module_name> To load a modulefile in a non-standard directory: 1 2 module use <path_to_module> module load <modulename> For more information about Lmod module system, please refer to the documentation web site .","title":"Common Module Commands"},{"location":"Compilers_and_Libraries/","text":"Compilers and Libraries Compilers MPI Libraries Math Libraries Build Commands Compilers HPCC has various compilers installed, which can be used to create OpenMP, pthreads, MPI, hybrid, and serial programs. To view the version of compilers available, please use the module avail command. We presently offer GNU (default), Intel, PGI, CUDA, g95. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [ ongbw@dev-amd09 ~ ] $ module avail GNU ------------------------------------------------- /opt/software/modulefiles/compilers -------------------------------------------------- GNU/4.4.5 ( default ) [ ongbw@dev-amd09 ~ ] $ module avail Intel ------------------------------------------------- /opt/software/modulefiles/compilers -------------------------------------------------- Intel/12.0.0.084 ( default ) Intel/12.1.0.233 Intel/12.1.2.273 [ ongbw@dev-amd09 ~ ] $ module avail PGI ------------------------------------------------- /opt/software/modulefiles/compilers -------------------------------------------------- PGI/11.10 ( default ) [ ongbw@dev-amd09 ~ ] $ module avail g95 ------------------------------------------------- /opt/software/modulefiles/compilers -------------------------------------------------- g95/0.93 [ ongbw@dev-amd09 ~ ] $ module avail CUDA ------------------------------------------------- /opt/software/modulefiles/compilers -------------------------------------------------- CUDA/4.0 ( default ) MPI Libraries HPCC also has various MPI libraries installed. These include: OpenMPI (default), MVAPICH, IMPI, MPICH, MPICH2. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [ ongbw@dev-amd09 ~ ] $ module avail OpenMPI ------------------------------------------- /opt/software/modulefiles/GNU/4.4.5/modulefiles -------------------------------------------- OpenMPI/1.4.3 ( default ) OpenMPI/1.4.4 OpenMPI/1.5.3 [ ongbw@dev-amd09 ~ ] $ module avail MVAPICH ------------------------------------------- /opt/software/modulefiles/GNU/4.4.5/modulefiles -------------------------------------------- MVAPICH/1.1 ( default ) MVAPICH2/1.7 MVAPICH2/1.7-r5078 ( default ) MVAPICH2/1.8-a1p1 [ ongbw@dev-amd09 ~ ] $ module avail IMPI ------------------------------------------- /opt/software/modulefiles/GNU/4.4.5/modulefiles -------------------------------------------- IMPI/4.0.1.007 ( default ) [ ongbw@dev-amd09 ~ ] $ module avail MPICH ------------------------------------------- /opt/software/modulefiles/GNU/4.4.5/modulefiles -------------------------------------------- MPICH2/1.4.1p1 ( default ) [ ongbw@dev-amd09 ~ ] $ module avail MPICH2 ------------------------------------------- /opt/software/modulefiles/GNU/4.4.5/modulefiles -------------------------------------------- MPICH2/1.4.1p1 ( default ) Math Libraries HPCC has many common libraries available, including MKL, FFTW, ACML, BLAS, ScaLaPACK, LaPACK, PETSc. The module names and available link flags are summarized in the following table. Additionally, some code samples are available as indicated by typing: 1 2 module load powertools getexample <example name> Library Name Module Name Link Flags Powertool Examples ACML ACML -lamcl -lacml_mp BLAS BLAS -lblas -lcblas -lfblas -lxblas FFTW FFTW -lfftw3 -lfftw3_omp -lfftw3_mpi -lfftw3_threads fftw LaPACK LAPACK -llapack -ltmglib MKL MKL -lmkl_blacs -lfftw3xc_intel -lfftw2xc_intel -lmkl_blas95_lp64 -lmkl_core -lmkl_lapack95_lp64 -lmkl_scalapack_lp64 -lmkl_solver_lp64 MKL_Example PETSc PETSc -lfblas -lflapack -lpetsc libtriangle.a ScaLaPACK ScaLAPACK -lscalapack Build Commands The commands and flags for the various compilers are summarized in the table below: Compiler Module Name Serial OpenMP Pthreads MPI Hybrid Intel Fortran Intel ifort myserial.f ifort -fopenmp myopenmp.f ifort -pthread mypthread.f mpif90 mympi.f mpif90 -fopenmp myhybrid.f Intel C Intel icc myserial.c icc -fopenmp myopenmp.c icc -pthread mypthread.c mpicc mympi.c mpicc -fopenmp myhybrid.c Intel C++ Intel icpc myserial.cpp icpc -fopenmp myopenmp.cpp icpc -pthread mypthread.cpp mpic++ mympi.cpp mpic++ -fopenmp myhybrid.cpp GNU Fortran GNU gfortran myserial.f90 gfortran -openmp myopenmp.f90 gfortran -pthread mypthread.f90 mpif90 mympi.f mpif90 -openmp myhybrid.f GNU C GNU gcc myserial.c gcc -fopenmp myopenmp.c gcc -pthread mypthread.c mpicc mympi.c mpicc -openmp myhybrid.c GNU C++ GNU g++ myserial.cpp g++ -fopenmp myopenmp.cpp g++ -pthread mypthread.cpp mpic++ mympi.cpp mpic++ -openmp myhybrid.cpp PGI Fortran PGI pgif77 myserial.f pgf77 -mp myopenmp.f pg77 -pthread mypthread.f mpif77 mympi.f mpif77 -mp myhybrid.f pgf90 myserial.f90 pgf90 -mp myopenmp.f90 pgf90 -pthread mypthread.f90 mpif90 mympi.f mpif90 -mp myhybrid.f PGI C PGI pgcc myserial.c pgcc -mp myopenmp.c pgcc -pthread mypthread.c mpicc mympi.c mpicc -mp myhybrid.c PGI C++ PGI pgCC myserial.cpp pgCC -mp myopenmp.cpp pgCC -pthreads mypthread.cpp mpic++ mympi.cpp mpic++ -mp myhybrid.cpp g95 g95 g95 myserial.f90 not supported g95 -pthread mypthread.f90 mpif90 mympi.f not available","title":"Compilers and Libraries"},{"location":"Compilers_and_Libraries/#compilers-and-libraries","text":"Compilers MPI Libraries Math Libraries Build Commands","title":"Compilers and Libraries"},{"location":"Compilers_and_Libraries/#compilers","text":"HPCC has various compilers installed, which can be used to create OpenMP, pthreads, MPI, hybrid, and serial programs. To view the version of compilers available, please use the module avail command. We presently offer GNU (default), Intel, PGI, CUDA, g95. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [ ongbw@dev-amd09 ~ ] $ module avail GNU ------------------------------------------------- /opt/software/modulefiles/compilers -------------------------------------------------- GNU/4.4.5 ( default ) [ ongbw@dev-amd09 ~ ] $ module avail Intel ------------------------------------------------- /opt/software/modulefiles/compilers -------------------------------------------------- Intel/12.0.0.084 ( default ) Intel/12.1.0.233 Intel/12.1.2.273 [ ongbw@dev-amd09 ~ ] $ module avail PGI ------------------------------------------------- /opt/software/modulefiles/compilers -------------------------------------------------- PGI/11.10 ( default ) [ ongbw@dev-amd09 ~ ] $ module avail g95 ------------------------------------------------- /opt/software/modulefiles/compilers -------------------------------------------------- g95/0.93 [ ongbw@dev-amd09 ~ ] $ module avail CUDA ------------------------------------------------- /opt/software/modulefiles/compilers -------------------------------------------------- CUDA/4.0 ( default )","title":"Compilers"},{"location":"Compilers_and_Libraries/#mpi-libraries","text":"HPCC also has various MPI libraries installed. These include: OpenMPI (default), MVAPICH, IMPI, MPICH, MPICH2. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [ ongbw@dev-amd09 ~ ] $ module avail OpenMPI ------------------------------------------- /opt/software/modulefiles/GNU/4.4.5/modulefiles -------------------------------------------- OpenMPI/1.4.3 ( default ) OpenMPI/1.4.4 OpenMPI/1.5.3 [ ongbw@dev-amd09 ~ ] $ module avail MVAPICH ------------------------------------------- /opt/software/modulefiles/GNU/4.4.5/modulefiles -------------------------------------------- MVAPICH/1.1 ( default ) MVAPICH2/1.7 MVAPICH2/1.7-r5078 ( default ) MVAPICH2/1.8-a1p1 [ ongbw@dev-amd09 ~ ] $ module avail IMPI ------------------------------------------- /opt/software/modulefiles/GNU/4.4.5/modulefiles -------------------------------------------- IMPI/4.0.1.007 ( default ) [ ongbw@dev-amd09 ~ ] $ module avail MPICH ------------------------------------------- /opt/software/modulefiles/GNU/4.4.5/modulefiles -------------------------------------------- MPICH2/1.4.1p1 ( default ) [ ongbw@dev-amd09 ~ ] $ module avail MPICH2 ------------------------------------------- /opt/software/modulefiles/GNU/4.4.5/modulefiles -------------------------------------------- MPICH2/1.4.1p1 ( default )","title":"MPI Libraries"},{"location":"Compilers_and_Libraries/#math-libraries","text":"HPCC has many common libraries available, including MKL, FFTW, ACML, BLAS, ScaLaPACK, LaPACK, PETSc. The module names and available link flags are summarized in the following table. Additionally, some code samples are available as indicated by typing: 1 2 module load powertools getexample <example name> Library Name Module Name Link Flags Powertool Examples ACML ACML -lamcl -lacml_mp BLAS BLAS -lblas -lcblas -lfblas -lxblas FFTW FFTW -lfftw3 -lfftw3_omp -lfftw3_mpi -lfftw3_threads fftw LaPACK LAPACK -llapack -ltmglib MKL MKL -lmkl_blacs -lfftw3xc_intel -lfftw2xc_intel -lmkl_blas95_lp64 -lmkl_core -lmkl_lapack95_lp64 -lmkl_scalapack_lp64 -lmkl_solver_lp64 MKL_Example PETSc PETSc -lfblas -lflapack -lpetsc libtriangle.a ScaLaPACK ScaLAPACK -lscalapack","title":"Math Libraries"},{"location":"Compilers_and_Libraries/#build-commands","text":"The commands and flags for the various compilers are summarized in the table below: Compiler Module Name Serial OpenMP Pthreads MPI Hybrid Intel Fortran Intel ifort myserial.f ifort -fopenmp myopenmp.f ifort -pthread mypthread.f mpif90 mympi.f mpif90 -fopenmp myhybrid.f Intel C Intel icc myserial.c icc -fopenmp myopenmp.c icc -pthread mypthread.c mpicc mympi.c mpicc -fopenmp myhybrid.c Intel C++ Intel icpc myserial.cpp icpc -fopenmp myopenmp.cpp icpc -pthread mypthread.cpp mpic++ mympi.cpp mpic++ -fopenmp myhybrid.cpp GNU Fortran GNU gfortran myserial.f90 gfortran -openmp myopenmp.f90 gfortran -pthread mypthread.f90 mpif90 mympi.f mpif90 -openmp myhybrid.f GNU C GNU gcc myserial.c gcc -fopenmp myopenmp.c gcc -pthread mypthread.c mpicc mympi.c mpicc -openmp myhybrid.c GNU C++ GNU g++ myserial.cpp g++ -fopenmp myopenmp.cpp g++ -pthread mypthread.cpp mpic++ mympi.cpp mpic++ -openmp myhybrid.cpp PGI Fortran PGI pgif77 myserial.f pgf77 -mp myopenmp.f pg77 -pthread mypthread.f mpif77 mympi.f mpif77 -mp myhybrid.f pgf90 myserial.f90 pgf90 -mp myopenmp.f90 pgf90 -pthread mypthread.f90 mpif90 mympi.f mpif90 -mp myhybrid.f PGI C PGI pgcc myserial.c pgcc -mp myopenmp.c pgcc -pthread mypthread.c mpicc mympi.c mpicc -mp myhybrid.c PGI C++ PGI pgCC myserial.cpp pgCC -mp myopenmp.cpp pgCC -pthreads mypthread.cpp mpic++ mympi.cpp mpic++ -mp myhybrid.cpp g95 g95 g95 myserial.f90 not supported g95 -pthread mypthread.f90 mpif90 mympi.f not available","title":"Build Commands"},{"location":"Connect_over_SSH_with_VS_Code/","text":"Connect over SSH with VS Code VS Code allows you to work on remote servers from your local machine. prerequistes Install VS Code (Click HERE ) ~/.ssh/config Because we need to hop twice (your local machine -> gateway -> dev node), we need an ssh config file. The following is an example config which defines intel18 and k80 dev node. With this file, you can connect dev node from your local machine with 'ssh intel18' or 'ssh k80'. 1 2 3 4 5 6 7 8 9 10 11 12 13 Host gateway HostName gateway.hpcc.msu.edu User your_net_id Host intel18 HostName dev-intel18 User your_net_id ProxyJump gateway Host k80 HostName dev-intel16-k80 User your_net_id ProxyJump gateway Next, we need an extension program for VS Code (Click HERE ) When the extension is installed, press F1, and select 'Remote-SSH: Connect to Host...' Then select intel18 or k80. A new VS Code window will pop up. When you are successfully connected, bottom left of the VS Code will show server information such as 'SSH: intel18'. Now that you are connected, you can run commands and codes from VS Code. When you are connected to the HPC, all files created through VS Code will be saved to the HPC, not on your local machine. To test the ability to run remote code, let's create a Python file 'hello.py' with VS Code. Click 'New file' and create a file with the following contest to the file: 1 print('hello!') Now, open a terminal from 'Terminal' menu from VS Code (or by pressing the key sequence CTRL+Shift+` and run the code. 1 $ python hello.py Once you have connected the server, click the Remote Explorer to see the list of servers. Click the icon to connect to host in a new window. Right click the name to see the option.","title":"SSH connection via VS Code"},{"location":"Connect_over_SSH_with_VS_Code/#connect-over-ssh-with-vs-code","text":"VS Code allows you to work on remote servers from your local machine.","title":"Connect over SSH with VS Code"},{"location":"Connect_over_SSH_with_VS_Code/#prerequistes","text":"Install VS Code (Click HERE ) ~/.ssh/config Because we need to hop twice (your local machine -> gateway -> dev node), we need an ssh config file. The following is an example config which defines intel18 and k80 dev node. With this file, you can connect dev node from your local machine with 'ssh intel18' or 'ssh k80'. 1 2 3 4 5 6 7 8 9 10 11 12 13 Host gateway HostName gateway.hpcc.msu.edu User your_net_id Host intel18 HostName dev-intel18 User your_net_id ProxyJump gateway Host k80 HostName dev-intel16-k80 User your_net_id ProxyJump gateway Next, we need an extension program for VS Code (Click HERE ) When the extension is installed, press F1, and select 'Remote-SSH: Connect to Host...' Then select intel18 or k80. A new VS Code window will pop up. When you are successfully connected, bottom left of the VS Code will show server information such as 'SSH: intel18'. Now that you are connected, you can run commands and codes from VS Code. When you are connected to the HPC, all files created through VS Code will be saved to the HPC, not on your local machine. To test the ability to run remote code, let's create a Python file 'hello.py' with VS Code. Click 'New file' and create a file with the following contest to the file: 1 print('hello!') Now, open a terminal from 'Terminal' menu from VS Code (or by pressing the key sequence CTRL+Shift+` and run the code. 1 $ python hello.py Once you have connected the server, click the Remote Explorer to see the list of servers. Click the icon to connect to host in a new window. Right click the name to see the option.","title":"prerequistes"},{"location":"Connect_over_SSH_with_atom/","text":"Connet over SSH with Atom Atom does not support ProxyJump or port forwarding as VS Code. However, you can use rsync gateway, because Atom downloads the file from the server and edits on a local machine, and uploads it. For remote file editing, you need to install \"remote-ftp\" package. File \u2192 Setting Install \u2192 searching \"Remote-FTP\", and Install Configuration: Package \u2192 Remote FTP \u2192 Create SFTP config file Edit config files (You need to change, host, user, pass and maybe remote. host: rsync.hpcc.msu.edu, user, pass, and remote: your net it and password, and your home dir path) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"protocol\": \"sftp\", \"host\": \"rsync.hpcc.msu.edu\", // \"port\": 22, \"user\": \"your_net_id\", // your net_id is here \"pass\": \"your_pass_word\", // your password is here \"promptForPass\": false, \"remote\": \"/\", // default is root (/). You would like to change it to your home dir path such as /mnt/home/your_net_id or research path \"local\": \"\", \"agent\": \"\", \"privatekey\": \"\", \"passphrase\": \"\", \"hosthash\": \"\", \"ignorehost\": true, \"connTimeout\": 10000, \"keepalive\": 10000, \"keyboardInteractive\": false, \"keyboardInteractiveForPass\": false, \"remoteCommand\": \"\", \"remoteShell\": \"\", \"watch\": [], \"watchTimeout\": 500 } Menu activation: Package \u2192 Remote FTP \u2192 Toggle Click remote, and connect Connected.","title":"SSH connection via Atom"},{"location":"Connect_over_SSH_with_atom/#connet-over-ssh-with-atom","text":"Atom does not support ProxyJump or port forwarding as VS Code. However, you can use rsync gateway, because Atom downloads the file from the server and edits on a local machine, and uploads it. For remote file editing, you need to install \"remote-ftp\" package. File \u2192 Setting Install \u2192 searching \"Remote-FTP\", and Install Configuration: Package \u2192 Remote FTP \u2192 Create SFTP config file Edit config files (You need to change, host, user, pass and maybe remote. host: rsync.hpcc.msu.edu, user, pass, and remote: your net it and password, and your home dir path) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"protocol\": \"sftp\", \"host\": \"rsync.hpcc.msu.edu\", // \"port\": 22, \"user\": \"your_net_id\", // your net_id is here \"pass\": \"your_pass_word\", // your password is here \"promptForPass\": false, \"remote\": \"/\", // default is root (/). You would like to change it to your home dir path such as /mnt/home/your_net_id or research path \"local\": \"\", \"agent\": \"\", \"privatekey\": \"\", \"passphrase\": \"\", \"hosthash\": \"\", \"ignorehost\": true, \"connTimeout\": 10000, \"keepalive\": 10000, \"keyboardInteractive\": false, \"keyboardInteractiveForPass\": false, \"remoteCommand\": \"\", \"remoteShell\": \"\", \"watch\": [], \"watchTimeout\": 500 } Menu activation: Package \u2192 Remote FTP \u2192 Toggle Click remote, and connect Connected.","title":"Connet over SSH with Atom"},{"location":"Connect_over_SSH_with_editors/","text":"Connect over SSH with editors Connect over SSH with VS Code Connect over SSH with Atom","title":"Connect over SSH with editors"},{"location":"Connect_over_SSH_with_editors/#connect-over-ssh-with-editors","text":"Connect over SSH with VS Code Connect over SSH with Atom","title":"Connect over SSH with editors"},{"location":"Connect_to_HPCC_System/","tags":["tutorial"],"text":"Connect to the HPCC Connect to the gateway Open the SSH client in your local computer and run ssh -XY <username>@hpcc.msu.edu , where <username> should be your HPCC account name. SSH will prompt for your password. Please type the password of your MSU NetID (the password will be invisible). Upon a successful login, you will see a welcome message and current usage load for each development/dev-node. Our gateway is for entrance to the HPCC system only . Connect further to a dev-node for your program running and other tasks. See the next section. NOTE: If you have difficulty in X11 forwarding (e.g. receiving an error message \"Warning: untrusted X11 forwarding setup failed: xauth key data not generated\"), please check your local machine's \"config\" file ( $HOME/.ssh/config ) setting. It should look like 1 2 3 4 5 Host * ForwardAgent yes ForwardX11 yes ForwardX11Trusted yes XAuthLocation /opt/X11/bin/xauth SSH to a dev-node To access a dev-node from gateway, pick one and type \" ssh -X node-name \" (if you don't need to launch a GUI program, omit -X ). See here for dev-node usage and policy. Test X-Windows If your local computer has a X-Windows client installed (such as Xquatz, or mobaxterm ...) and you log into the nodes with the -X option, you can test if GUI will work by running xeyes on the command line. If everything is set correctly, you should see a pop-up window containing a pair of eyes.","title":"Connect to the HPCC"},{"location":"Connect_to_HPCC_System/#connect-to-the-hpcc","text":"","title":"Connect to the HPCC"},{"location":"Connect_to_HPCC_System/#connect-to-the-gateway","text":"Open the SSH client in your local computer and run ssh -XY <username>@hpcc.msu.edu , where <username> should be your HPCC account name. SSH will prompt for your password. Please type the password of your MSU NetID (the password will be invisible). Upon a successful login, you will see a welcome message and current usage load for each development/dev-node. Our gateway is for entrance to the HPCC system only . Connect further to a dev-node for your program running and other tasks. See the next section. NOTE: If you have difficulty in X11 forwarding (e.g. receiving an error message \"Warning: untrusted X11 forwarding setup failed: xauth key data not generated\"), please check your local machine's \"config\" file ( $HOME/.ssh/config ) setting. It should look like 1 2 3 4 5 Host * ForwardAgent yes ForwardX11 yes ForwardX11Trusted yes XAuthLocation /opt/X11/bin/xauth","title":"Connect to the gateway"},{"location":"Connect_to_HPCC_System/#ssh-to-a-dev-node","text":"To access a dev-node from gateway, pick one and type \" ssh -X node-name \" (if you don't need to launch a GUI program, omit -X ). See here for dev-node usage and policy.","title":"SSH to a dev-node"},{"location":"Connect_to_HPCC_System/#test-x-windows","text":"If your local computer has a X-Windows client installed (such as Xquatz, or mobaxterm ...) and you log into the nodes with the -X option, you can test if GUI will work by running xeyes on the command line. If everything is set correctly, you should see a pop-up window containing a pair of eyes.","title":"Test X-Windows"},{"location":"Containers_Docker_and_Singularity_/","text":"Containers (Docker and Singularity) Docker Singularity: I. Introduction Singularity: II. Running a container on HPC Singularity: III. Advanced skills","title":"Containers (Docker and Singularity)"},{"location":"Containers_Docker_and_Singularity_/#containers-docker-and-singularity","text":"Docker Singularity: I. Introduction Singularity: II. Running a container on HPC Singularity: III. Advanced skills","title":"Containers (Docker and Singularity)"},{"location":"Display_Compute_Nodes_and_Job_Partitions_by_sinfo_command/","text":"Display Compute Nodes and Job Partitions by sinfo command Information of Compute Nodes If you would like to run a job with a lot of resources, it is a good idea to check available resources, such as how many cores and how much memory for each node and how many nodes available, so the job will not wait for too much time. Users can use SLURM command sinfo to get a list of nodes controlled by the job scheduler. Such as, running the command sinfo -N -r -l , where the specifications -N for showing nodes, -r for showing nodes only responsive to SLURM and -l for long description are used. However, for each node, sinfo displays all possible partitions and causes repetitive information. Here, the powertools command node_status can be used to display much better results: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 $ node_status # powertools command Wed Apr 22 11 :14:40 EDT 2020 NodeName Account State CPU ( Load:Aloc Idl:Tot ) Mem ( Aval:Tot ) Mb GPU ( I:T ) Reason ---------------------------------------------------------------------------------------------------------- csm-001 general ALLOCATED 13 .61: 20 0 : 20 45186 : 246640 N/A csm-002 albrecht MIXED 10 .14: 15 5 : 20 1072 : 246640 N/A csm-003 colej ALLOCATED 7 .45: 20 0 : 20 50032 : 246640 N/A ...... csn-005 general MIXED 9 .92: 12 8 : 20 16160 : 118012 k20 ( 0 :2 ) ...... cs* = > 33 .3% ( buyin ) 91 .4% ( 162 ) 43 .6%: 59 .5% ( 3240 ) 69 .9% ( 17 .0Tb ) 97 % ( 78 ) Usage% ( Total ) ...... ...... lac-078 general MIXED 11 .38: 8 20 : 28 69884 : 118012 N/A lac-079 ptg ALLOCATED 22 .37: 28 0 : 28 15612 : 118012 N/A lac-080 merzjrke MIXED 2 .48: 16 12 : 28 50032 : 246640 k80 ( 0 :8 ) ...... ...... vim-002 ccg MIXED 66 .14: 63 81 :144 5427008 :6145856 N/A intel14 = > 34 .5% ( buyin ) 91 .7% ( 168 ) 47 .8%: 62 .7% ( 3576 ) 60 .1% ( 31 .1Tb ) 97 % ( 78 ) Usage% ( Total ) intel16 = > 69 .0% ( buyin ) 98 .8% ( 429 ) 55 .2%: 65 .1% ( 12200 ) 76 .6% ( 79 .9Tb ) 70 % ( 384 ) Usage% ( Total ) intel18 = > 63 .6% ( buyin ) 99 .4% ( 176 ) 45 .8%: 55 .8% ( 7040 ) 77 .1% ( 31 .3Tb ) 55 % ( 64 ) Usage% ( Total ) Summary = > 60 .3% ( buyin ) 97 .4% ( 773 ) 51 .2%: 61 .9% ( 22816 ) 73 .1% ( 142Tb ) 72 % ( 526 ) Usage% ( Total The result shows many items about the compute nodes, including node names, buy-in Accounts, node states, CPU cores, memory, GPU and the reason to cause the node unavailable. The result of node_status is a good reference to find out how many nodes available for your jobs. If you need more information of a particular node, you can use scontrol show node -a command to see a whole list of the node: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ scontrol show node -a skl-166 NodeName = skl-166 Arch = x86_64 CoresPerSocket = 20 CPUAlloc = 0 CPUTot = 40 CPULoad = 0 .01 AvailableFeatures = skl,gbe,intel18,ib,edr18 ActiveFeatures = skl,gbe,intel18,ib,edr18 Gres =( null ) NodeAddr = skl-166 NodeHostName = skl-166 Version = 18 .08 OS = Linux 3 .10.0-693.21.1.el7.x86_64 #1 SMP Wed Mar 7 19:03:37 UTC 2018 RealMemory = 376162 AllocMem = 0 FreeMem = 382562 Sockets = 2 Boards = 1 State = DOWN ThreadsPerCore = 1 TmpDisk = 174080 Weight = 103 Owner = N/A MCS_label = N/A Partitions = general-short,general-short-18,general-long,general-long-18,qian-18,nvl-benchmark-18,piermaro-18,vmante-18,liulab-18,devolab-18,tsangm-18,plzbuyin-18,chenlab-18,shadeash-colej-18,allenmc-18,cmse-18,seiswei-18,niederhu-18,daylab-18,junlin-18,mitchmcg-18,pollyhsu-18,davidroy-18,yueqibuyin-18,eisenlohr-18 BootTime = 2019 -02-11T15:07:38 SlurmdStartTime = 2019 -02-11T15:08:44 CfgTRES = cpu = 40 ,mem = 376162M,billing = 57176 AllocTRES = CapWatts = n/a CurrentWatts = 0 LowestJoules = 0 ConsumedJoules = 0 ExtSensorsJoules = n/s ExtSensorsWatts = 0 ExtSensorsTemp = n/s Reason = Currently being imaged [ fordste5@2019-02-11T09:49:30 ] SLURM Partitions for Jobs One of the important setups of a node is to determine what kind of jobs can run on it. For example, if a node is a buy-in node, only jobs with walltime equal to or less than 4 hours can run for a non-buyin users. We can check the summary of all partitions with -s specification: 1 2 3 4 5 6 $ sinfo -s PARTITION AVAIL TIMELIMIT NODES ( A/I/O/T ) NODELIST general-short up 4 :00:00 729 /26/16/771 csm- [ 001 -005,007-010,017-022 ] ,csn- [ 001 -039 ] ,csp- [ 006 -007,016-020,025-026 ] ,css- [ 001 -003,007-012,014,016-020,023,032-036,038-045,047-050,052-067,071-072,074-076,079-085,087-095,097-103,106-109,111-127 ] ,lac- [ 000 -225,228-247,250-261,276-369,372,374-445 ] ,nvl- [ 000 -007 ] ,qml- [ 000 -005 ] ,skl- [ 000 -167 ] ,vim- [ 000 -002 ] general-long up 7 -00:00:00 269 /0/8/277 csm-001,csn-020,csp- [ 006 -007,016-018,020,025 ] ,css- [ 008 -012,014,016-019,023,032,034-036,038-045,047-050,052-066,071,075-076,079-080,083,087-089,092-095,097-099,107,118,121,124,126 ] ,lac- [ 038 -044,078,123,209,217,225,228,230-235,246-247,276-284,300-301,336-339,353-360,363-364,372,374-399,401-420,422-445 ] ,skl- [ 023 ,026-112 ] general-long-bigmem up 7 -00:00:00 17 /0/0/17 lac- [ 252 -253,306 ] ,qml- [ 000 ,005 ] ,skl- [ 143 -147,162-167 ] ,vim-001 general-long-gpu up 7 -00:00:00 46 /12/0/58 csn- [ 001 -019,021-036 ] ,lac- [ 030 ,087,137,143,192-199,287-290,292-293,342,348 ] ,nvl- [ 005 -007 ] where the list of job partitions and their setup for walltime limit and nodes are shown. More detailed information for each job partition can also be found by -p specification: 1 2 3 4 5 6 7 $ sinfo -p general-long -r -l Mon Jul 13 12 :22:16 2020 PARTITION AVAIL TIMELIMIT JOB_SIZE ROOT OVERSUBS GROUPS NODES STATE NODELIST general-long up 7 -00:00:00 1 -infinite no NO all 2 draining lac- [ 231 ,247 ] general-long up 7 -00:00:00 1 -infinite no NO all 1 drained css-053 general-long up 7 -00:00:00 1 -infinite no NO all 217 mixed csm-001,csp- [ 006 ,017-018,020,025 ] ,css- [ 010 ,018-019,023,032,034-035,038,044,047-049,052,055-056,061-066,075,088-089,098-099,107,118,126 ] ,lac- [ 038 -044,078,123,209,217,225,228,230,232,234-235,276-280,282-284,300-301,336-337,339,353-360,363,372,374-382,384-399,401-420,423,427-445 ] ,skl- [ 023 ,026,028-029,031,033-034,036-042,044-046,048,050-067,069-079,081-094,096-106,108-112 ] general-long up 7 -00:00:00 1 -infinite no NO all 50 allocated csn-020,csp-016,css- [ 008 -009,011,016-017,036,039-043,045,050,054,057-060,083,087,092-095,097,121,124 ] ,lac- [ 233 ,246,281,338,364,383,422,424-426 ] ,skl- [ 027 ,030,032,035,043,047,049,068,080,095,107 ] Users can also show nodes only allowed for specific job partitions by using -N and -p : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ sinfo -N -l -r -p general-short,general-long Mon Jul 13 12 :25:58 2020 NODELIST NODES PARTITION STATE CPUS S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON csm-001 1 general-short mixed 20 2 :10:1 246640 174080 101 gbe,ib,i none csm-001 1 general-long mixed 20 2 :10:1 246640 174080 101 gbe,ib,i none csm-002 1 general-short mixed 20 2 :10:1 246640 174080 101 gbe,ib,i none csm-003 1 general-short mixed 20 2 :10:1 246640 174080 101 gbe,ib,i none csm-004 1 general-short mixed 20 2 :10:1 246640 174080 101 gbe,ib,i none csm-005 1 general-short mixed 20 2 :10:1 246640 174080 101 gbe,ib,i none ... ... skl-166 1 general-short mixed 40 2 :20:1 376162 174080 103 skl,gbe, none skl-167 1 general-short mixed 40 2 :20:1 376162 174080 103 skl,gbe, none vim-000 1 general-short mixed 64 4 :16:1 306780 174080 102 gbe,inte none vim-001 1 general-short mixed 64 4 :16:1 306780 174080 102 gbe,inte none vim-002 1 general-short allocated 144 8 :18:1 614585 174080 102 gbe,inte none For a complete instruction of sinfo , please refer to the SLURM web page .","title":"SLURM - node status and job partition"},{"location":"Display_Compute_Nodes_and_Job_Partitions_by_sinfo_command/#display-compute-nodes-and-job-partitions-by-sinfo-command","text":"","title":"Display Compute Nodes and Job Partitions by sinfo command"},{"location":"Display_Compute_Nodes_and_Job_Partitions_by_sinfo_command/#information-of-compute-nodes","text":"If you would like to run a job with a lot of resources, it is a good idea to check available resources, such as how many cores and how much memory for each node and how many nodes available, so the job will not wait for too much time. Users can use SLURM command sinfo to get a list of nodes controlled by the job scheduler. Such as, running the command sinfo -N -r -l , where the specifications -N for showing nodes, -r for showing nodes only responsive to SLURM and -l for long description are used. However, for each node, sinfo displays all possible partitions and causes repetitive information. Here, the powertools command node_status can be used to display much better results: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 $ node_status # powertools command Wed Apr 22 11 :14:40 EDT 2020 NodeName Account State CPU ( Load:Aloc Idl:Tot ) Mem ( Aval:Tot ) Mb GPU ( I:T ) Reason ---------------------------------------------------------------------------------------------------------- csm-001 general ALLOCATED 13 .61: 20 0 : 20 45186 : 246640 N/A csm-002 albrecht MIXED 10 .14: 15 5 : 20 1072 : 246640 N/A csm-003 colej ALLOCATED 7 .45: 20 0 : 20 50032 : 246640 N/A ...... csn-005 general MIXED 9 .92: 12 8 : 20 16160 : 118012 k20 ( 0 :2 ) ...... cs* = > 33 .3% ( buyin ) 91 .4% ( 162 ) 43 .6%: 59 .5% ( 3240 ) 69 .9% ( 17 .0Tb ) 97 % ( 78 ) Usage% ( Total ) ...... ...... lac-078 general MIXED 11 .38: 8 20 : 28 69884 : 118012 N/A lac-079 ptg ALLOCATED 22 .37: 28 0 : 28 15612 : 118012 N/A lac-080 merzjrke MIXED 2 .48: 16 12 : 28 50032 : 246640 k80 ( 0 :8 ) ...... ...... vim-002 ccg MIXED 66 .14: 63 81 :144 5427008 :6145856 N/A intel14 = > 34 .5% ( buyin ) 91 .7% ( 168 ) 47 .8%: 62 .7% ( 3576 ) 60 .1% ( 31 .1Tb ) 97 % ( 78 ) Usage% ( Total ) intel16 = > 69 .0% ( buyin ) 98 .8% ( 429 ) 55 .2%: 65 .1% ( 12200 ) 76 .6% ( 79 .9Tb ) 70 % ( 384 ) Usage% ( Total ) intel18 = > 63 .6% ( buyin ) 99 .4% ( 176 ) 45 .8%: 55 .8% ( 7040 ) 77 .1% ( 31 .3Tb ) 55 % ( 64 ) Usage% ( Total ) Summary = > 60 .3% ( buyin ) 97 .4% ( 773 ) 51 .2%: 61 .9% ( 22816 ) 73 .1% ( 142Tb ) 72 % ( 526 ) Usage% ( Total The result shows many items about the compute nodes, including node names, buy-in Accounts, node states, CPU cores, memory, GPU and the reason to cause the node unavailable. The result of node_status is a good reference to find out how many nodes available for your jobs. If you need more information of a particular node, you can use scontrol show node -a command to see a whole list of the node: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ scontrol show node -a skl-166 NodeName = skl-166 Arch = x86_64 CoresPerSocket = 20 CPUAlloc = 0 CPUTot = 40 CPULoad = 0 .01 AvailableFeatures = skl,gbe,intel18,ib,edr18 ActiveFeatures = skl,gbe,intel18,ib,edr18 Gres =( null ) NodeAddr = skl-166 NodeHostName = skl-166 Version = 18 .08 OS = Linux 3 .10.0-693.21.1.el7.x86_64 #1 SMP Wed Mar 7 19:03:37 UTC 2018 RealMemory = 376162 AllocMem = 0 FreeMem = 382562 Sockets = 2 Boards = 1 State = DOWN ThreadsPerCore = 1 TmpDisk = 174080 Weight = 103 Owner = N/A MCS_label = N/A Partitions = general-short,general-short-18,general-long,general-long-18,qian-18,nvl-benchmark-18,piermaro-18,vmante-18,liulab-18,devolab-18,tsangm-18,plzbuyin-18,chenlab-18,shadeash-colej-18,allenmc-18,cmse-18,seiswei-18,niederhu-18,daylab-18,junlin-18,mitchmcg-18,pollyhsu-18,davidroy-18,yueqibuyin-18,eisenlohr-18 BootTime = 2019 -02-11T15:07:38 SlurmdStartTime = 2019 -02-11T15:08:44 CfgTRES = cpu = 40 ,mem = 376162M,billing = 57176 AllocTRES = CapWatts = n/a CurrentWatts = 0 LowestJoules = 0 ConsumedJoules = 0 ExtSensorsJoules = n/s ExtSensorsWatts = 0 ExtSensorsTemp = n/s Reason = Currently being imaged [ fordste5@2019-02-11T09:49:30 ]","title":"Information of Compute Nodes"},{"location":"Display_Compute_Nodes_and_Job_Partitions_by_sinfo_command/#slurm-partitions-for-jobs","text":"One of the important setups of a node is to determine what kind of jobs can run on it. For example, if a node is a buy-in node, only jobs with walltime equal to or less than 4 hours can run for a non-buyin users. We can check the summary of all partitions with -s specification: 1 2 3 4 5 6 $ sinfo -s PARTITION AVAIL TIMELIMIT NODES ( A/I/O/T ) NODELIST general-short up 4 :00:00 729 /26/16/771 csm- [ 001 -005,007-010,017-022 ] ,csn- [ 001 -039 ] ,csp- [ 006 -007,016-020,025-026 ] ,css- [ 001 -003,007-012,014,016-020,023,032-036,038-045,047-050,052-067,071-072,074-076,079-085,087-095,097-103,106-109,111-127 ] ,lac- [ 000 -225,228-247,250-261,276-369,372,374-445 ] ,nvl- [ 000 -007 ] ,qml- [ 000 -005 ] ,skl- [ 000 -167 ] ,vim- [ 000 -002 ] general-long up 7 -00:00:00 269 /0/8/277 csm-001,csn-020,csp- [ 006 -007,016-018,020,025 ] ,css- [ 008 -012,014,016-019,023,032,034-036,038-045,047-050,052-066,071,075-076,079-080,083,087-089,092-095,097-099,107,118,121,124,126 ] ,lac- [ 038 -044,078,123,209,217,225,228,230-235,246-247,276-284,300-301,336-339,353-360,363-364,372,374-399,401-420,422-445 ] ,skl- [ 023 ,026-112 ] general-long-bigmem up 7 -00:00:00 17 /0/0/17 lac- [ 252 -253,306 ] ,qml- [ 000 ,005 ] ,skl- [ 143 -147,162-167 ] ,vim-001 general-long-gpu up 7 -00:00:00 46 /12/0/58 csn- [ 001 -019,021-036 ] ,lac- [ 030 ,087,137,143,192-199,287-290,292-293,342,348 ] ,nvl- [ 005 -007 ] where the list of job partitions and their setup for walltime limit and nodes are shown. More detailed information for each job partition can also be found by -p specification: 1 2 3 4 5 6 7 $ sinfo -p general-long -r -l Mon Jul 13 12 :22:16 2020 PARTITION AVAIL TIMELIMIT JOB_SIZE ROOT OVERSUBS GROUPS NODES STATE NODELIST general-long up 7 -00:00:00 1 -infinite no NO all 2 draining lac- [ 231 ,247 ] general-long up 7 -00:00:00 1 -infinite no NO all 1 drained css-053 general-long up 7 -00:00:00 1 -infinite no NO all 217 mixed csm-001,csp- [ 006 ,017-018,020,025 ] ,css- [ 010 ,018-019,023,032,034-035,038,044,047-049,052,055-056,061-066,075,088-089,098-099,107,118,126 ] ,lac- [ 038 -044,078,123,209,217,225,228,230,232,234-235,276-280,282-284,300-301,336-337,339,353-360,363,372,374-382,384-399,401-420,423,427-445 ] ,skl- [ 023 ,026,028-029,031,033-034,036-042,044-046,048,050-067,069-079,081-094,096-106,108-112 ] general-long up 7 -00:00:00 1 -infinite no NO all 50 allocated csn-020,csp-016,css- [ 008 -009,011,016-017,036,039-043,045,050,054,057-060,083,087,092-095,097,121,124 ] ,lac- [ 233 ,246,281,338,364,383,422,424-426 ] ,skl- [ 027 ,030,032,035,043,047,049,068,080,095,107 ] Users can also show nodes only allowed for specific job partitions by using -N and -p : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ sinfo -N -l -r -p general-short,general-long Mon Jul 13 12 :25:58 2020 NODELIST NODES PARTITION STATE CPUS S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON csm-001 1 general-short mixed 20 2 :10:1 246640 174080 101 gbe,ib,i none csm-001 1 general-long mixed 20 2 :10:1 246640 174080 101 gbe,ib,i none csm-002 1 general-short mixed 20 2 :10:1 246640 174080 101 gbe,ib,i none csm-003 1 general-short mixed 20 2 :10:1 246640 174080 101 gbe,ib,i none csm-004 1 general-short mixed 20 2 :10:1 246640 174080 101 gbe,ib,i none csm-005 1 general-short mixed 20 2 :10:1 246640 174080 101 gbe,ib,i none ... ... skl-166 1 general-short mixed 40 2 :20:1 376162 174080 103 skl,gbe, none skl-167 1 general-short mixed 40 2 :20:1 376162 174080 103 skl,gbe, none vim-000 1 general-short mixed 64 4 :16:1 306780 174080 102 gbe,inte none vim-001 1 general-short mixed 64 4 :16:1 306780 174080 102 gbe,inte none vim-002 1 general-short allocated 144 8 :18:1 614585 174080 102 gbe,inte none For a complete instruction of sinfo , please refer to the SLURM web page .","title":"SLURM Partitions for Jobs"},{"location":"Docker/","text":"Docker What is Docker? Docker is a tool to make it easier to create, deploy and run application by using containers. Containers allow developers to package up an application with all of the dependencies such as libraries and tools, and deploy it as one package. The application will run on most OS machines (Mac/Windows/Linux) regardless of any customized settings. Overall, this page covers how you can running on the development environments using Docker containers. Docker installation Testing Docker installation Running Docker containers from prebuilt images Build Docker images which contain your own code Docker images Building your first Docker image Creating working directory Python script Dockerfile Build the image Run your image Docker and Jupyter notebook Docker installation Docker supports various OS (Mac/Windows/Linux) and, installation of Docker and running on your OS is very easy. For installation, click here: Mac / Windows / Linux . Note If you are a Windows user, please make sure you have shared your drive . You have to log into Docker to use images from Docker repository. Testing Docker installation When you installed Docker, test your Docker installation by running the following command: 1 2 $ docker --version Docker version 19 .03.8, build afacb8b When you run docker command without --version , you would see whole options available with docker. Alternatively, you can test your installation by running the following (you have to log into Docker to use this test): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 $ docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world 0e03bdcc26d7: Pull complete Digest: sha256:6a65f928fb91fcfbc963f7aa6d57c8eeb426ad9a20c7ee045538ef34847f44f1 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1 . The Docker client contacted the Docker daemon. 2 . The Docker daemon pulled the \"hello-world\" image from the Docker Hub. ( amd64 ) 3 . The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4 . The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ Running Docker containers from prebuilt images Now, you setup everything, and it is time to use Docker seriously. You will run a container from Alpine Linux image on your system and will learn run docker command. However, you first would like to know what containers and images are , and the difference between containers and an images. Images : The file system and configuration of applications which are created and distributed by developers. Of course, you can create and distribute images. Containers : Running instances of Docker images. You can have many containers for a same image. Now you know what containers and images are, and let's run docker run alpine ls -l command in your terminal. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ docker run alpine ls -l Unable to find image 'alpine:latest' locally latest: Pulling from library/alpine cbdbe7a5bc2a: Pull complete Digest: sha256:9a839e63dad54c3a6d1834e29692c8492d93f90c59c978c1ed79109ea4fb9a54 Status: Downloaded newer image for alpine:latest total 56 drwxr-xr-x 2 root root 4096 Apr 23 06 :25 bin drwxr-xr-x 5 root root 340 May 26 17 :11 dev drwxr-xr-x 1 root root 4096 May 26 17 :11 etc drwxr-xr-x 2 root root 4096 Apr 23 06 :25 home drwxr-xr-x 5 root root 4096 Apr 23 06 :25 lib drwxr-xr-x 5 root root 4096 Apr 23 06 :25 media drwxr-xr-x 2 root root 4096 Apr 23 06 :25 mnt drwxr-xr-x 2 root root 4096 Apr 23 06 :25 opt dr-xr-xr-x 187 root root 0 May 26 17 :11 proc drwx------ 2 root root 4096 Apr 23 06 :25 root drwxr-xr-x 2 root root 4096 Apr 23 06 :25 run drwxr-xr-x 2 root root 4096 Apr 23 06 :25 sbin drwxr-xr-x 2 root root 4096 Apr 23 06 :25 srv dr-xr-xr-x 12 root root 0 May 26 17 :11 sys drwxrwxrwt 2 root root 4096 Apr 23 06 :25 tmp drwxr-xr-x 7 root root 4096 Apr 23 06 :25 usr drwxr-xr-x 12 root root 4096 Apr 23 06 :25 var ( base ) Hyperion:~ yongjunchoi$ When you run docker run alpine ls -l command, it searchs \"alpine:latest\" image from your system first. If your system has it (i.e. if you downloaded it previously), Docker uses that image. If your system does not have that image, then Docker fetches the \"alpine:latest\" image from the Docker repository first, and save it in your system, and then runs a container from the saved image. docker run alpine starts a container, and ls -l is a command which is fed to the container, so Docker starts the given command and results show up. To see a list of all images on your system, you can use docker images command. 1 2 3 $ docker images alpine latest f70734b6a266 4 weeks ago 5 .61MB hello-world latest bf756fb1ae65 4 months ago 13 .3kB Next, let's try another command. 1 2 $ docker run alpine echo \"Hello world\" Hello world In this case, Docker ran the echo command in your \"alpine\" container, and then exited it. Exit means the container is terminated after running the command. Let's try another command. 1 $ docker run alpine sh It seems nothing happened. In fact, docker ran sh command in you alpine container, and exited it. If you want to be inside the container shell, you need to use docker run -it alpine sh . -it mean interactive and allocating a pseudo-TTY. You can find more help on run command with docker run --help . Let's run docker run -it alpine sh . 1 2 3 4 5 6 $ docker run -it alpine sh / # ls bin dev etc home lib media mnt opt proc root run sbin srv sys tmp usr var / # uname -a Linux c1552c9b6cf0 4 .19.76-linuxkit #1 SMP Fri Apr 3 15:53:26 UTC 2020 x86_64 Linux / # exit You are inside of the container shell and you can try out a few commands like ls and uname -a and others. To quit the container, type exit on the terminal. If you use exit command, the container is terminated. If you want to keep the container active, then you can use keys ctrl p , and then ctrl q (You don't have to press these two keys simultaneously). If you want to go back into the container, you can type docker attach \\<container_id> , such as docker attach c1552c9b6cf0 . You can find container id with docker ps -all . This command will be explained next. Now, let's learn on the docker ps command which shows you all containers that are currently running. 1 2 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS You can not see any container because no containers are running. To see a list of all containers that you ran, use docker ps --all . You can see that STATUS says that all containers exited. 1 2 3 4 5 6 $ docker ps --all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c1552c9b6cf0 alpine \"sh\" 6 minutes ago Exited ( 0 ) 2 minutes ago wonderful_cori 5de22ab86f2a alpine \"echo 'Hello world'\" 18 minutes ago Exited ( 0 ) 18 minutes ago goofy_visvesvaraya df35ee7df7e3 alpine \"ls -l\" 31 minutes ago Exited ( 0 ) 31 minutes ago fervent_gould 6dbe999044b4 hello-world \"/hello\" 3 hours ago Exited ( 0 ) 3 hours ago When Docker containers are created, the Docker system automatically assign a universally unique identifier (UUID) number to each container to avoid any naming conflicts. CONTAINER ID is a shortform of the UUID. You can assign names to your Docker containers when you run them, using --name flags. In addition, you can rename your Docker container's name with rename command. For example, let's rename \"wonderful_cori\" to \"my_container\" with docker rename command. 1 2 3 4 5 6 $ docker rename wonderful_cori my_container $ docker ps --all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c1552c9b6cf0 alpine \"sh\" 10 minutes ago Exited ( 0 ) 6 minutes ago my_container 5de22ab86f2a alpine \"echo 'Hello world'\" 22 minutes ago Exited ( 0 ) 22 minutes ago goofy_visvesvaraya df35ee7df7e3 alpine \"ls -l\" 35 minutes ago Exited ( 0 ) 35 minutes ago fervent_gould Build Docker images which contain your own code Now, you are ready to use Docker to create your own applications! First, you will learn more about Docker images. Then you will build your own image and use that image to run an application on your local machine. Docker images Docker images are basis of containers. IN the above example, you pulled the alpine image from the repository and ran a container based on that image. To see the list of images that are available on your local machine, run docker images command. 1 2 3 4 5 6 7 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE alpine latest f70734b6a266 4 weeks ago 5 .61MB hello-world latest bf756fb1ae65 4 months ago 13 .3kB centos/python-36-centos7 latest 070f320fe348 2 weeks ago 698MB centos 7 b5b4d78bc90c 2 weeks ago 203MB ... The TAG refers to a particular snapshot of the image and the ID is the corresponding UUID of the image. Images can have multiple versions. When you do not assign a specific version number, the client defaults to latest. If you want a specific version of the image, you can use docker pull command as follows: 1 $ docker pull centos:7 You can search for images from a repository (for example, the following is Docker hub for centos ) or directly from command line using docker search . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 $ docker search centos NAME DESCRIPTION STARS OFFICIAL AUTOMATED centos The official build of CentOS. 6014 [ OK ] ansible/centos7-ansible Ansible on Centos7 129 [ OK ] consol/centos-xfce-vnc Centos container with \"headless\" VNC session\u2026 115 [ OK ] jdeathe/centos-ssh OpenSSH / Supervisor / EPEL/IUS/SCL Repos - \u2026 114 [ OK ] centos/mysql-57-centos7 MySQL 5 .7 SQL database server 76 imagine10255/centos6-lnmp-php56 centos6-lnmp-php56 58 [ OK ] tutum/centos Simple CentOS docker image with SSH access 46 centos/postgresql-96-centos7 PostgreSQL is an advanced Object-Relational \u2026 44 kinogmt/centos-ssh CentOS with SSH 29 [ OK ] pivotaldata/centos-gpdb-dev CentOS image for GPDB development. Tag names\u2026 12 guyton/centos6 From official centos6 container with full up\u2026 10 [ OK ] centos/tools Docker image that has systems administration\u2026 6 [ OK ] drecom/centos-ruby centos ruby 6 [ OK ] pivotaldata/centos Base centos, freshened up a little with a Do\u2026 4 pivotaldata/centos-mingw Using the mingw toolchain to cross-compile t\u2026 3 darksheer/centos Base Centos Image -- Updated hourly 3 [ OK ] mamohr/centos-java Oracle Java 8 Docker image based on Centos 7 3 [ OK ] pivotaldata/centos-gcc-toolchain CentOS with a toolchain, but unaffiliated wi\u2026 3 miko2u/centos6 CentOS6 \u65e5\u672c\u8a9e\u74b0\u5883 2 [ OK ] blacklabelops/centos CentOS Base Image! Built and Updates Daily! 1 [ OK ] indigo/centos-maven Vanilla CentOS 7 with Oracle Java Developmen\u2026 1 [ OK ] mcnaughton/centos-base centos base image 1 [ OK ] pivotaldata/centos7-dev CentosOS 7 image for GPDB development 0 smartentry/centos centos with smartentry 0 [ OK ] pivotaldata/centos6.8-dev CentosOS 6 .8 image for GPDB development 0 Building your first Docker image In this section, you will build a simple Docker image with writing a Dockerfile, and run it. For this purpose, we will create a Python script, and a Dockerfile. Creating working directory Let's create a working directory where you will make following files: hello.py , Dockerfile 1 2 3 $ cd ~ $ mkdir my_first_Docker_image $ cd my_first_Docker_image Python script Create hello.py file with the following content. 1 2 print ( \"Hello world!\" ) print ( \"This is my 1st Docker image!\" ) Dockerfile A Dockerfile is a text file which has a list of commands that the Docker calls while creating an image. The Dockerfile is similar to a job batch file, and contains all information that Docker needs to know to to run the application package. In my_first_Docker_image directory, create a file, called Dockerfile, which has content as below. 1 2 3 4 5 6 7 8 9 10 11 # our base image. The latest version will be pulled. FROM alpine # install python and pip RUN apk add --update py3-pip # copy files required to run COPY hello.py /usr/src/my_app/ # run the application CMD python3 /usr/src/my_app/hello.py Now, let's learn the meaning of the each line. The first line means that we will use alpine Linux as a base image. No version is specified, so the latest version will be pulled. Use FROM keyword. 1 FROM alpine Next, Python pip package is installed. Use RUN keyword. 1 RUN apk add --update py3-pip Next, copy the file to the image. /usr/src/my_app will be created while the file is copied. Use COPY keyword. 1 COPY hello.py /usr/src/my_app/ The last step is run the application with CMD keyword. CMD tells the container what the container should do by default when it is started. 1 CMD python3 /usr/src/my_app/hello.py Build the image Now you are ready to build your first Docker image. docker build command will do most of the work. However, before you run docker build command, check again if you log in to Docker. To build the image, use the following command. 1 $ docker build -t my_first_image . The client will pull all necessary images and create your image. If everything goes well, your image is ready to be used! Run docker images command to see if your image my_first_image is shown. Run your image When you successfully create your Docker image, test it by starting a new container form the image. 1 $ docker run my_first_image If everything went well, you would see this message. 1 2 Hello world! This is my 1st Docker image! Docker and Jupyter notebook Now, we will build a Docker image which deploy a Jupyter notebook. First, let's check Jupyter images on Docker Hub. We will use minimal-notebook. 1 2 3 4 5 6 7 8 9 10 11 $docker search jupyter NAME DESCRIPTION STARS OFFICIAL AUTOMATED jupyter/datascience-notebook Jupyter Notebook Data Science Stack from htt\u2026 666 jupyter/all-spark-notebook Jupyter Notebook Python, Scala, R, Spark, Me\u2026 301 jupyterhub/jupyterhub JupyterHub: multi-user Jupyter notebook serv\u2026 248 [ OK ] jupyter/scipy-notebook Jupyter Notebook Scientific Python Stack fro\u2026 241 jupyter/tensorflow-notebook Jupyter Notebook Scientific Python Stack w/ \u2026 218 jupyter/pyspark-notebook Jupyter Notebook Python, Spark, Mesos Stack \u2026 157 jupyter/base-notebook Small base image for Jupyter Notebook stacks\u2026 106 jupyter/minimal-notebook Minimal Jupyter Notebook Stack from https://\u2026 105 ... Let's start by creating a directory my_note_book . Copy hello.py , which we used for the Python image, to my_note_book dir. Then create a Dockerfile under the my_note_book directory and add content to is as the following. 1 2 3 4 5 6 7 8 # base image FROM jupyter/base-notebook # copy files COPY hello.py /home/my_note_book/ # the port number the container should expose EXPOSE 8888 The last part is specifying the port number which needs to be exposed. The default port for Jupyter is 8888, and therefore, we will expose that port. Now, build the image using the following command: 1 2 3 4 5 6 7 8 9 10 11 12 $ docker build -t mynotebook . Sending build context to Docker daemon 4 .096kB Step 1 /3 : FROM jupyter/base-notebook ---> 7ea955290e01 Step 2 /3 : COPY model.py /home/my_note_book_dir/ ---> 9a1dd638a667 Step 3 /3 : EXPOSE 8888 ---> Running in a955eb421dde Removing intermediate container a955eb421dde ---> 8d3f7447955c Successfully built 8d3f7447955c Successfully tagged mynotebook:latest Now, everything is ready. You can run the image using the Docker run command. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ docker run -p 8888 :8888 mynotebook Executing the command: jupyter notebook [ I 20 :26:03.538 NotebookApp ] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret [ I 20 :26:04.144 NotebookApp ] JupyterLab extension loaded from /opt/conda/lib/python3.7/site-packages/jupyterlab [ I 20 :26:04.144 NotebookApp ] JupyterLab application directory is /opt/conda/share/jupyter/lab [ I 20 :26:04.146 NotebookApp ] Serving notebooks from local directory: /home/jovyan [ I 20 :26:04.147 NotebookApp ] The Jupyter Notebook is running at: [ I 20 :26:04.147 NotebookApp ] http://c47b22cd65be:8888/?token = a53b2303380fa4a2e92cc50441ad121a08d6e39dea726fd9 [ I 20 :26:04.147 NotebookApp ] or http://127.0.0.1:8888/?token = a53b2303380fa4a2e92cc50441ad121a08d6e39dea726fd9 [ I 20 :26:04.147 NotebookApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . [ C 20 :26:04.151 NotebookApp ] To access the notebook, open this file in a browser: file:///home/.local/share/jupyter/runtime/nbserver-6-open.html Or copy and paste one of these URLs: http://c47b22cd65be:8888/?token = a53b2303380fa4a2e92cc50441ad121a08d6e39dea726fd9 or http://127.0.0.1:8888/?token = a53b2303380fa4a2e92cc50441ad121a08d6e39dea726fd9 [ I 20 :26:11.151 NotebookApp ] 302 GET /?token = a53b2303380fa4a2e92cc50441ad121a08d6e39dea726fd9 ( 172 .17.0.1 ) 0 .65ms","title":"Docker"},{"location":"Docker/#docker","text":"What is Docker? Docker is a tool to make it easier to create, deploy and run application by using containers. Containers allow developers to package up an application with all of the dependencies such as libraries and tools, and deploy it as one package. The application will run on most OS machines (Mac/Windows/Linux) regardless of any customized settings. Overall, this page covers how you can running on the development environments using Docker containers. Docker installation Testing Docker installation Running Docker containers from prebuilt images Build Docker images which contain your own code Docker images Building your first Docker image Creating working directory Python script Dockerfile Build the image Run your image Docker and Jupyter notebook","title":"Docker"},{"location":"Docker/#docker-installation","text":"Docker supports various OS (Mac/Windows/Linux) and, installation of Docker and running on your OS is very easy. For installation, click here: Mac / Windows / Linux . Note If you are a Windows user, please make sure you have shared your drive . You have to log into Docker to use images from Docker repository.","title":"Docker installation"},{"location":"Docker/#testing-docker-installation","text":"When you installed Docker, test your Docker installation by running the following command: 1 2 $ docker --version Docker version 19 .03.8, build afacb8b When you run docker command without --version , you would see whole options available with docker. Alternatively, you can test your installation by running the following (you have to log into Docker to use this test): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 $ docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world 0e03bdcc26d7: Pull complete Digest: sha256:6a65f928fb91fcfbc963f7aa6d57c8eeb426ad9a20c7ee045538ef34847f44f1 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1 . The Docker client contacted the Docker daemon. 2 . The Docker daemon pulled the \"hello-world\" image from the Docker Hub. ( amd64 ) 3 . The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4 . The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/","title":"Testing Docker installation"},{"location":"Docker/#running-docker-containers-from-prebuilt-images","text":"Now, you setup everything, and it is time to use Docker seriously. You will run a container from Alpine Linux image on your system and will learn run docker command. However, you first would like to know what containers and images are , and the difference between containers and an images. Images : The file system and configuration of applications which are created and distributed by developers. Of course, you can create and distribute images. Containers : Running instances of Docker images. You can have many containers for a same image. Now you know what containers and images are, and let's run docker run alpine ls -l command in your terminal. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ docker run alpine ls -l Unable to find image 'alpine:latest' locally latest: Pulling from library/alpine cbdbe7a5bc2a: Pull complete Digest: sha256:9a839e63dad54c3a6d1834e29692c8492d93f90c59c978c1ed79109ea4fb9a54 Status: Downloaded newer image for alpine:latest total 56 drwxr-xr-x 2 root root 4096 Apr 23 06 :25 bin drwxr-xr-x 5 root root 340 May 26 17 :11 dev drwxr-xr-x 1 root root 4096 May 26 17 :11 etc drwxr-xr-x 2 root root 4096 Apr 23 06 :25 home drwxr-xr-x 5 root root 4096 Apr 23 06 :25 lib drwxr-xr-x 5 root root 4096 Apr 23 06 :25 media drwxr-xr-x 2 root root 4096 Apr 23 06 :25 mnt drwxr-xr-x 2 root root 4096 Apr 23 06 :25 opt dr-xr-xr-x 187 root root 0 May 26 17 :11 proc drwx------ 2 root root 4096 Apr 23 06 :25 root drwxr-xr-x 2 root root 4096 Apr 23 06 :25 run drwxr-xr-x 2 root root 4096 Apr 23 06 :25 sbin drwxr-xr-x 2 root root 4096 Apr 23 06 :25 srv dr-xr-xr-x 12 root root 0 May 26 17 :11 sys drwxrwxrwt 2 root root 4096 Apr 23 06 :25 tmp drwxr-xr-x 7 root root 4096 Apr 23 06 :25 usr drwxr-xr-x 12 root root 4096 Apr 23 06 :25 var ( base ) Hyperion:~ yongjunchoi$ When you run docker run alpine ls -l command, it searchs \"alpine:latest\" image from your system first. If your system has it (i.e. if you downloaded it previously), Docker uses that image. If your system does not have that image, then Docker fetches the \"alpine:latest\" image from the Docker repository first, and save it in your system, and then runs a container from the saved image. docker run alpine starts a container, and ls -l is a command which is fed to the container, so Docker starts the given command and results show up. To see a list of all images on your system, you can use docker images command. 1 2 3 $ docker images alpine latest f70734b6a266 4 weeks ago 5 .61MB hello-world latest bf756fb1ae65 4 months ago 13 .3kB Next, let's try another command. 1 2 $ docker run alpine echo \"Hello world\" Hello world In this case, Docker ran the echo command in your \"alpine\" container, and then exited it. Exit means the container is terminated after running the command. Let's try another command. 1 $ docker run alpine sh It seems nothing happened. In fact, docker ran sh command in you alpine container, and exited it. If you want to be inside the container shell, you need to use docker run -it alpine sh . -it mean interactive and allocating a pseudo-TTY. You can find more help on run command with docker run --help . Let's run docker run -it alpine sh . 1 2 3 4 5 6 $ docker run -it alpine sh / # ls bin dev etc home lib media mnt opt proc root run sbin srv sys tmp usr var / # uname -a Linux c1552c9b6cf0 4 .19.76-linuxkit #1 SMP Fri Apr 3 15:53:26 UTC 2020 x86_64 Linux / # exit You are inside of the container shell and you can try out a few commands like ls and uname -a and others. To quit the container, type exit on the terminal. If you use exit command, the container is terminated. If you want to keep the container active, then you can use keys ctrl p , and then ctrl q (You don't have to press these two keys simultaneously). If you want to go back into the container, you can type docker attach \\<container_id> , such as docker attach c1552c9b6cf0 . You can find container id with docker ps -all . This command will be explained next. Now, let's learn on the docker ps command which shows you all containers that are currently running. 1 2 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS You can not see any container because no containers are running. To see a list of all containers that you ran, use docker ps --all . You can see that STATUS says that all containers exited. 1 2 3 4 5 6 $ docker ps --all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c1552c9b6cf0 alpine \"sh\" 6 minutes ago Exited ( 0 ) 2 minutes ago wonderful_cori 5de22ab86f2a alpine \"echo 'Hello world'\" 18 minutes ago Exited ( 0 ) 18 minutes ago goofy_visvesvaraya df35ee7df7e3 alpine \"ls -l\" 31 minutes ago Exited ( 0 ) 31 minutes ago fervent_gould 6dbe999044b4 hello-world \"/hello\" 3 hours ago Exited ( 0 ) 3 hours ago When Docker containers are created, the Docker system automatically assign a universally unique identifier (UUID) number to each container to avoid any naming conflicts. CONTAINER ID is a shortform of the UUID. You can assign names to your Docker containers when you run them, using --name flags. In addition, you can rename your Docker container's name with rename command. For example, let's rename \"wonderful_cori\" to \"my_container\" with docker rename command. 1 2 3 4 5 6 $ docker rename wonderful_cori my_container $ docker ps --all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c1552c9b6cf0 alpine \"sh\" 10 minutes ago Exited ( 0 ) 6 minutes ago my_container 5de22ab86f2a alpine \"echo 'Hello world'\" 22 minutes ago Exited ( 0 ) 22 minutes ago goofy_visvesvaraya df35ee7df7e3 alpine \"ls -l\" 35 minutes ago Exited ( 0 ) 35 minutes ago fervent_gould","title":"Running Docker containers from prebuilt images"},{"location":"Docker/#build-docker-images-which-contain-your-own-code","text":"Now, you are ready to use Docker to create your own applications! First, you will learn more about Docker images. Then you will build your own image and use that image to run an application on your local machine.","title":"Build Docker images which contain your own code"},{"location":"Docker/#docker-images","text":"Docker images are basis of containers. IN the above example, you pulled the alpine image from the repository and ran a container based on that image. To see the list of images that are available on your local machine, run docker images command. 1 2 3 4 5 6 7 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE alpine latest f70734b6a266 4 weeks ago 5 .61MB hello-world latest bf756fb1ae65 4 months ago 13 .3kB centos/python-36-centos7 latest 070f320fe348 2 weeks ago 698MB centos 7 b5b4d78bc90c 2 weeks ago 203MB ... The TAG refers to a particular snapshot of the image and the ID is the corresponding UUID of the image. Images can have multiple versions. When you do not assign a specific version number, the client defaults to latest. If you want a specific version of the image, you can use docker pull command as follows: 1 $ docker pull centos:7 You can search for images from a repository (for example, the following is Docker hub for centos ) or directly from command line using docker search . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 $ docker search centos NAME DESCRIPTION STARS OFFICIAL AUTOMATED centos The official build of CentOS. 6014 [ OK ] ansible/centos7-ansible Ansible on Centos7 129 [ OK ] consol/centos-xfce-vnc Centos container with \"headless\" VNC session\u2026 115 [ OK ] jdeathe/centos-ssh OpenSSH / Supervisor / EPEL/IUS/SCL Repos - \u2026 114 [ OK ] centos/mysql-57-centos7 MySQL 5 .7 SQL database server 76 imagine10255/centos6-lnmp-php56 centos6-lnmp-php56 58 [ OK ] tutum/centos Simple CentOS docker image with SSH access 46 centos/postgresql-96-centos7 PostgreSQL is an advanced Object-Relational \u2026 44 kinogmt/centos-ssh CentOS with SSH 29 [ OK ] pivotaldata/centos-gpdb-dev CentOS image for GPDB development. Tag names\u2026 12 guyton/centos6 From official centos6 container with full up\u2026 10 [ OK ] centos/tools Docker image that has systems administration\u2026 6 [ OK ] drecom/centos-ruby centos ruby 6 [ OK ] pivotaldata/centos Base centos, freshened up a little with a Do\u2026 4 pivotaldata/centos-mingw Using the mingw toolchain to cross-compile t\u2026 3 darksheer/centos Base Centos Image -- Updated hourly 3 [ OK ] mamohr/centos-java Oracle Java 8 Docker image based on Centos 7 3 [ OK ] pivotaldata/centos-gcc-toolchain CentOS with a toolchain, but unaffiliated wi\u2026 3 miko2u/centos6 CentOS6 \u65e5\u672c\u8a9e\u74b0\u5883 2 [ OK ] blacklabelops/centos CentOS Base Image! Built and Updates Daily! 1 [ OK ] indigo/centos-maven Vanilla CentOS 7 with Oracle Java Developmen\u2026 1 [ OK ] mcnaughton/centos-base centos base image 1 [ OK ] pivotaldata/centos7-dev CentosOS 7 image for GPDB development 0 smartentry/centos centos with smartentry 0 [ OK ] pivotaldata/centos6.8-dev CentosOS 6 .8 image for GPDB development 0","title":"Docker images"},{"location":"Docker/#building-your-first-docker-image","text":"In this section, you will build a simple Docker image with writing a Dockerfile, and run it. For this purpose, we will create a Python script, and a Dockerfile.","title":"Building your first Docker image"},{"location":"Docker/#creating-working-directory","text":"Let's create a working directory where you will make following files: hello.py , Dockerfile 1 2 3 $ cd ~ $ mkdir my_first_Docker_image $ cd my_first_Docker_image","title":"Creating working directory"},{"location":"Docker/#python-script","text":"Create hello.py file with the following content. 1 2 print ( \"Hello world!\" ) print ( \"This is my 1st Docker image!\" )","title":"Python script"},{"location":"Docker/#dockerfile","text":"A Dockerfile is a text file which has a list of commands that the Docker calls while creating an image. The Dockerfile is similar to a job batch file, and contains all information that Docker needs to know to to run the application package. In my_first_Docker_image directory, create a file, called Dockerfile, which has content as below. 1 2 3 4 5 6 7 8 9 10 11 # our base image. The latest version will be pulled. FROM alpine # install python and pip RUN apk add --update py3-pip # copy files required to run COPY hello.py /usr/src/my_app/ # run the application CMD python3 /usr/src/my_app/hello.py Now, let's learn the meaning of the each line. The first line means that we will use alpine Linux as a base image. No version is specified, so the latest version will be pulled. Use FROM keyword. 1 FROM alpine Next, Python pip package is installed. Use RUN keyword. 1 RUN apk add --update py3-pip Next, copy the file to the image. /usr/src/my_app will be created while the file is copied. Use COPY keyword. 1 COPY hello.py /usr/src/my_app/ The last step is run the application with CMD keyword. CMD tells the container what the container should do by default when it is started. 1 CMD python3 /usr/src/my_app/hello.py","title":"Dockerfile"},{"location":"Docker/#build-the-image","text":"Now you are ready to build your first Docker image. docker build command will do most of the work. However, before you run docker build command, check again if you log in to Docker. To build the image, use the following command. 1 $ docker build -t my_first_image . The client will pull all necessary images and create your image. If everything goes well, your image is ready to be used! Run docker images command to see if your image my_first_image is shown.","title":"Build the image"},{"location":"Docker/#run-your-image","text":"When you successfully create your Docker image, test it by starting a new container form the image. 1 $ docker run my_first_image If everything went well, you would see this message. 1 2 Hello world! This is my 1st Docker image!","title":"Run your image"},{"location":"Docker/#docker-and-jupyter-notebook","text":"Now, we will build a Docker image which deploy a Jupyter notebook. First, let's check Jupyter images on Docker Hub. We will use minimal-notebook. 1 2 3 4 5 6 7 8 9 10 11 $docker search jupyter NAME DESCRIPTION STARS OFFICIAL AUTOMATED jupyter/datascience-notebook Jupyter Notebook Data Science Stack from htt\u2026 666 jupyter/all-spark-notebook Jupyter Notebook Python, Scala, R, Spark, Me\u2026 301 jupyterhub/jupyterhub JupyterHub: multi-user Jupyter notebook serv\u2026 248 [ OK ] jupyter/scipy-notebook Jupyter Notebook Scientific Python Stack fro\u2026 241 jupyter/tensorflow-notebook Jupyter Notebook Scientific Python Stack w/ \u2026 218 jupyter/pyspark-notebook Jupyter Notebook Python, Spark, Mesos Stack \u2026 157 jupyter/base-notebook Small base image for Jupyter Notebook stacks\u2026 106 jupyter/minimal-notebook Minimal Jupyter Notebook Stack from https://\u2026 105 ... Let's start by creating a directory my_note_book . Copy hello.py , which we used for the Python image, to my_note_book dir. Then create a Dockerfile under the my_note_book directory and add content to is as the following. 1 2 3 4 5 6 7 8 # base image FROM jupyter/base-notebook # copy files COPY hello.py /home/my_note_book/ # the port number the container should expose EXPOSE 8888 The last part is specifying the port number which needs to be exposed. The default port for Jupyter is 8888, and therefore, we will expose that port. Now, build the image using the following command: 1 2 3 4 5 6 7 8 9 10 11 12 $ docker build -t mynotebook . Sending build context to Docker daemon 4 .096kB Step 1 /3 : FROM jupyter/base-notebook ---> 7ea955290e01 Step 2 /3 : COPY model.py /home/my_note_book_dir/ ---> 9a1dd638a667 Step 3 /3 : EXPOSE 8888 ---> Running in a955eb421dde Removing intermediate container a955eb421dde ---> 8d3f7447955c Successfully built 8d3f7447955c Successfully tagged mynotebook:latest Now, everything is ready. You can run the image using the Docker run command. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ docker run -p 8888 :8888 mynotebook Executing the command: jupyter notebook [ I 20 :26:03.538 NotebookApp ] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret [ I 20 :26:04.144 NotebookApp ] JupyterLab extension loaded from /opt/conda/lib/python3.7/site-packages/jupyterlab [ I 20 :26:04.144 NotebookApp ] JupyterLab application directory is /opt/conda/share/jupyter/lab [ I 20 :26:04.146 NotebookApp ] Serving notebooks from local directory: /home/jovyan [ I 20 :26:04.147 NotebookApp ] The Jupyter Notebook is running at: [ I 20 :26:04.147 NotebookApp ] http://c47b22cd65be:8888/?token = a53b2303380fa4a2e92cc50441ad121a08d6e39dea726fd9 [ I 20 :26:04.147 NotebookApp ] or http://127.0.0.1:8888/?token = a53b2303380fa4a2e92cc50441ad121a08d6e39dea726fd9 [ I 20 :26:04.147 NotebookApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . [ C 20 :26:04.151 NotebookApp ] To access the notebook, open this file in a browser: file:///home/.local/share/jupyter/runtime/nbserver-6-open.html Or copy and paste one of these URLs: http://c47b22cd65be:8888/?token = a53b2303380fa4a2e92cc50441ad121a08d6e39dea726fd9 or http://127.0.0.1:8888/?token = a53b2303380fa4a2e92cc50441ad121a08d6e39dea726fd9 [ I 20 :26:11.151 NotebookApp ] 302 GET /?token = a53b2303380fa4a2e92cc50441ad121a08d6e39dea726fd9 ( 172 .17.0.1 ) 0 .65ms","title":"Docker and Jupyter notebook"},{"location":"Environment_Variables/","text":"Environment Variables The following is a list of correspondence between Torque and SLURM environment variables: Description Torque SLURM Variable The ID of the job PBS_JOBID SLURM_JOB_ID Job array ID (index) number PBS_ARRAYID SLURM_ARRAY_TASK_ID Directory where the submission command was executed PBS_O_WORKDIR SLURM_SUBMIT_DIR Name of the job PBS_JOBNAME SLURM_JOB_NAME List of nodes allocated to the job PBS_NODEFILE SLURM_JOB_NODELIST Number of Processors Per Node requeste PBS_NUM_PPN SLURM_JOB_CPUS_PER_NODE Total number of cores requested PBS_NP SLURM_NTASKS * SLURM_CPUS_PER_TASK Total number of nodes requested PBS_NUM_NODES SLURM_NTASKS Current Host of PBS job PBS_O_HOST SLURM_SUBMIT_HOST","title":"Environment variables"},{"location":"Environment_Variables/#environment-variables","text":"The following is a list of correspondence between Torque and SLURM environment variables: Description Torque SLURM Variable The ID of the job PBS_JOBID SLURM_JOB_ID Job array ID (index) number PBS_ARRAYID SLURM_ARRAY_TASK_ID Directory where the submission command was executed PBS_O_WORKDIR SLURM_SUBMIT_DIR Name of the job PBS_JOBNAME SLURM_JOB_NAME List of nodes allocated to the job PBS_NODEFILE SLURM_JOB_NODELIST Number of Processors Per Node requeste PBS_NUM_PPN SLURM_JOB_CPUS_PER_NODE Total number of cores requested PBS_NP SLURM_NTASKS * SLURM_CPUS_PER_TASK Total number of nodes requested PBS_NUM_NODES SLURM_NTASKS Current Host of PBS job PBS_O_HOST SLURM_SUBMIT_HOST","title":"Environment Variables"},{"location":"Example_SLURM_scripts/","text":"Example SLURM scripts Three examples are given in this pdf tutorial Example 1: single node, single core Example 2: single node, multiple threads Example 3: multiple nodes (e.g., a MPI program)","title":"Example job scripts"},{"location":"Example_SLURM_scripts/#example-slurm-scripts","text":"Three examples are given in this pdf tutorial Example 1: single node, single core Example 2: single node, multiple threads Example 3: multiple nodes (e.g., a MPI program)","title":"Example SLURM scripts"},{"location":"File-Permission-in-Research-Space_34963746.html/","text":"Teaching : File Permission in Research Space A user with account name User1 is not able to access a directory Dirct in his research space Group1 . The following is the result of the ls command: 1 2 3 4 5 6 7 8 [User1@dev-intel14-k20 ~]$ ls -la /mnt/research/Group1 total 98 drwxrwS--- 3 ProjInvs Group1 8192 Aug 6 08:53 . drwxr-xr-x 391 root root 0 Sep 9 07:34 .. -rwx------ 1 User2 Group2 4299 Jul 2 2018 file1 -rwx------ 1 User3 Group3 2452 Jul 2 2018 file2 drwxrwS--- 2 User2 Group2 8192 May 22 11:31 Dirct -rw-rw-r-- 1 User1 Group1 263 Aug 6 08:54 file3 Q: How to make User1 able to access the directory Dirct ? A: Since User2 is the owner of the directory, User2 can run a command to change the group ownership: 1 [User2@dev-node ~]$ chgrp -R Group1 /mnt/research/Group1/Dirct Q: In order for all group users able to access files and directories in the research space, what should they do? A: They should run the following commands: Change the group ownership of all files and directories to the research group Group1 : 1 [UserID@dev-node ~]$ chgrp -R Group1 /mnt/research/Group1/ 2>/dev/null Open the permissions of all files and directories to be readable ( r ) and writable ( w ) to group users: 1 [UserID@dev-node ~]$ chmod -R g+rw /mnt/research/Group1/ 2>/dev/null Make all directories sticky to the group ownership for any file generated inside (turn on group sticky bits): 1 [UserID@dev-node ~]$ chmod g+s $(find /mnt/research/Group1/ -type d -user $USER 2>/dev/null) Group users should not copy files to their research space with preserving the ownership, such as using command \" cp -p ... \".","title":"File Permission in Research Space 34963746.html"},{"location":"File-Permission-in-Research-Space_34963746.html/#teaching-file-permission-in-research-space","text":"A user with account name User1 is not able to access a directory Dirct in his research space Group1 . The following is the result of the ls command: 1 2 3 4 5 6 7 8 [User1@dev-intel14-k20 ~]$ ls -la /mnt/research/Group1 total 98 drwxrwS--- 3 ProjInvs Group1 8192 Aug 6 08:53 . drwxr-xr-x 391 root root 0 Sep 9 07:34 .. -rwx------ 1 User2 Group2 4299 Jul 2 2018 file1 -rwx------ 1 User3 Group3 2452 Jul 2 2018 file2 drwxrwS--- 2 User2 Group2 8192 May 22 11:31 Dirct -rw-rw-r-- 1 User1 Group1 263 Aug 6 08:54 file3 Q: How to make User1 able to access the directory Dirct ? A: Since User2 is the owner of the directory, User2 can run a command to change the group ownership: 1 [User2@dev-node ~]$ chgrp -R Group1 /mnt/research/Group1/Dirct Q: In order for all group users able to access files and directories in the research space, what should they do? A: They should run the following commands: Change the group ownership of all files and directories to the research group Group1 : 1 [UserID@dev-node ~]$ chgrp -R Group1 /mnt/research/Group1/ 2>/dev/null Open the permissions of all files and directories to be readable ( r ) and writable ( w ) to group users: 1 [UserID@dev-node ~]$ chmod -R g+rw /mnt/research/Group1/ 2>/dev/null Make all directories sticky to the group ownership for any file generated inside (turn on group sticky bits): 1 [UserID@dev-node ~]$ chmod g+s $(find /mnt/research/Group1/ -type d -user $USER 2>/dev/null) Group users should not copy files to their research space with preserving the ownership, such as using command \" cp -p ... \".","title":"Teaching : File Permission in Research Space"},{"location":"File_Permissions_on_HPCC/","text":"File Permissions on HPCC The HPCC offers several different types of storage for users. All of these filesystems make use of standard UNIX file permissions. Understanding how standard UNIX permissions and ownership works is an important way to control access to your files. UNIX users and groups Every user has a unique username on HPCC systems. This is typically your MSU NetID. Every user is also a member of at least one group. This group is typically the department the user is in (such as cse or plb). An user can be a member of additional groups. To see what groups you are a member of, run the groups command. If you feel you are in the wrong group, please contact HPCC staff. UNIX file ownership Every file and directory has two sets of ownership, the user and the group. The user owner is normally set to the user that created the file. Normally, the user owner of a file or directory is the only user that is able to change permissions or group ownership. The group owner of a file or directory allows a user owner to grant permissions to a group of users for a particular file or directory. The user owner of a file can change the group ownership of a file to any group that they are a member of. Any file created by a user normally defaults to group owner being set to the user's primary group, unless the user or directory owner has changed the behavior (using procedures described below.) The three types of basic UNIX permissions Read Read permission on a file allows the contents of a file to be read. The read permission, when applied to a directory, allows the contents of a directory to be listed. Referred to as \"r\" in the output of the ls -l command. Write Write permission on a file allows the file to be modified or deleted. Write permissions in a directory allow the creation of additional files in that directory. Referred to as \"w\" in the output of the ls -l command. Execute The execute permission allows a file to be run as an executable. When applied to a directory it allows traversal of that directory: the ablility to access files or subdirectories in that directory. Referred to as \"x\" in the output of the ls -l command. Displaying permissions of files and directories To display permissions in the current directory, run: 1 ls -l You can also display the permissions of an individual file or directory by running: 1 ls -ld filename For example, you can check the permissions of your home directory: 1 ls -ld ~ Applying these to the three types of users In the normal UNIX security model, there are three levels that are evaluated when considering file or directory access: user owner, group owner, and everyone else on the system. These types are typically referred to as user ( u ), group ( g ) and other ( o ). Only the owner of a file or a directory is allowed to change its permissions or the group name (to one of his groups). To change user permissions (in this case, add all permissions), run the following command: 1 chmod u+rwx FileName Note that any file you create will already have the \"rw\" permission for your user account. However to have a program script able to be run from the command line, you need to change the 'execute' permission 1 chmod u+x FileName Group and other permissions can also be altered: To allow anyone in the group that owns the file to be able to read that file, change the group read permission: 1 chmod g+r FileName To allow anyone in the group to read and write the file, you can change the read and write permission 1 chmod g+wr FileName If you have a file that is currently read and writeable by the group (g+wr) and you want to make it private, remove those permissions: 1 chmod g-rw FileName To add the ability for other users to write to a file or directory (this allows all users on the HPC to see read this file if it's in a shared folder which we don't recommend). 1 chmod o+w FileName Change group name: To change the group ownership of a file or a directory, simply run 1 chgrp <GroupName> <FileName> where <GroupName> is the group name which you would like to change to and <FileName> is the name and path of the file or directory. Working with non-primary groups and permissions If you have more than one group associated with your account, you can switch group owns the files created by default with the newgrp command: newgrp myothergroup . If you need to do this frequently, you can contact HPCC staff to change your primary group. You can also change the default group for new files created in a directory by setting the set-group-ID setting. The /mnt/research HPCC Research file share spaces have this setting set by default. To set the set-group-ID bit on a directory: 1 chmod g+s DirectoryName To remove the set-group-ID bit on a directory: 1 chmod g-s DirectoryName Other special permissions There are other group permissions beyond the scope of this document, primarily the set-user-ID bit and the \"sticky\" bit. For more information about special permissions, please review the GNU documentation, available on any HPCC system: 1 info chmod Filesystem-specific differences Home Your home directory has default permissions that allow only you to have access. Other users , whether they are in your primary group or not, are not allowed access to the contents of your home directory by default. If you wish to allow other users access to your home directory, you will need to change permissions on it. To allow every member of a group access to read your home directory, use: 1 chmod g+rx ~ To allow every user outside your UNIX group to read your home directory, use: 1 chmod o+rx ~ To allow world-wide read access to your home directory 1 chmod a+rx ~ Scratch Directories are created as private to you by default. If you do not wish this to be the case, you can use the technique for sharing directories (see below). TMPDIR space Directories are creates as world-readable by default, but the scheduler deletes the contents of $TMPDIR after a job exits. If you require additional security for this temporary space, manually setting the permissions of $TMPDIR is necessary. Here is an example to mimic the security of home directory space: 1 chmod go-rwx $TMPDIR Sharing a single directory inside your home directory If you wish to share only a single directory in your home directory and keep all other contents private, you can use the following techinque: 1 2 3 4 5 6 7 8 9 # create the shared folder cd ~ mkdir shared chmod o+rwx shared # create a shared file in the shared folder echo \"hello, iCER\" > shared/sharefile.txt chmod o+rw shared/sharefile.txt # anyone can read this file using cat /mnt/home/<netid>/shared/sharefile.txt You can use the same technique for your $SCRATCH folder to share folders on that. Note if there are other directories above your shared directory (e.g. it's a sub-sub-directory like \\~/project/data/shared), then every directory in the path will need the execute bit set for everyone. Other resources This just covers the basics of UNIX file permissions. Here are some other resources for more in-depth information: Software Carpentry - Permissions The Linux Cookbook, 2nd ed., Chapter 9 https://www.computerhope.com/unix/uumask.htm","title":"File permissions"},{"location":"File_Permissions_on_HPCC/#file-permissions-on-hpcc","text":"The HPCC offers several different types of storage for users. All of these filesystems make use of standard UNIX file permissions. Understanding how standard UNIX permissions and ownership works is an important way to control access to your files.","title":"File Permissions on HPCC"},{"location":"File_Permissions_on_HPCC/#unix-users-and-groups","text":"Every user has a unique username on HPCC systems. This is typically your MSU NetID. Every user is also a member of at least one group. This group is typically the department the user is in (such as cse or plb). An user can be a member of additional groups. To see what groups you are a member of, run the groups command. If you feel you are in the wrong group, please contact HPCC staff.","title":"UNIX users and groups"},{"location":"File_Permissions_on_HPCC/#unix-file-ownership","text":"Every file and directory has two sets of ownership, the user and the group. The user owner is normally set to the user that created the file. Normally, the user owner of a file or directory is the only user that is able to change permissions or group ownership. The group owner of a file or directory allows a user owner to grant permissions to a group of users for a particular file or directory. The user owner of a file can change the group ownership of a file to any group that they are a member of. Any file created by a user normally defaults to group owner being set to the user's primary group, unless the user or directory owner has changed the behavior (using procedures described below.)","title":"UNIX file ownership"},{"location":"File_Permissions_on_HPCC/#the-three-types-of-basic-unix-permissions","text":"","title":"The three types of basic UNIX permissions"},{"location":"File_Permissions_on_HPCC/#read","text":"Read permission on a file allows the contents of a file to be read. The read permission, when applied to a directory, allows the contents of a directory to be listed. Referred to as \"r\" in the output of the ls -l command.","title":"Read"},{"location":"File_Permissions_on_HPCC/#write","text":"Write permission on a file allows the file to be modified or deleted. Write permissions in a directory allow the creation of additional files in that directory. Referred to as \"w\" in the output of the ls -l command.","title":"Write"},{"location":"File_Permissions_on_HPCC/#execute","text":"The execute permission allows a file to be run as an executable. When applied to a directory it allows traversal of that directory: the ablility to access files or subdirectories in that directory. Referred to as \"x\" in the output of the ls -l command.","title":"Execute"},{"location":"File_Permissions_on_HPCC/#displaying-permissions-of-files-and-directories","text":"To display permissions in the current directory, run: 1 ls -l You can also display the permissions of an individual file or directory by running: 1 ls -ld filename For example, you can check the permissions of your home directory: 1 ls -ld ~","title":"Displaying permissions of files and directories"},{"location":"File_Permissions_on_HPCC/#applying-these-to-the-three-types-of-users","text":"In the normal UNIX security model, there are three levels that are evaluated when considering file or directory access: user owner, group owner, and everyone else on the system. These types are typically referred to as user ( u ), group ( g ) and other ( o ). Only the owner of a file or a directory is allowed to change its permissions or the group name (to one of his groups).","title":"Applying these to the three types of users"},{"location":"File_Permissions_on_HPCC/#to-change-user-permissions-in-this-case-add-all-permissions-run-the-following-command","text":"1 chmod u+rwx FileName Note that any file you create will already have the \"rw\" permission for your user account. However to have a program script able to be run from the command line, you need to change the 'execute' permission 1 chmod u+x FileName","title":"To change user permissions (in this case, add all permissions), run the following command:"},{"location":"File_Permissions_on_HPCC/#group-and-other-permissions-can-also-be-altered","text":"To allow anyone in the group that owns the file to be able to read that file, change the group read permission: 1 chmod g+r FileName To allow anyone in the group to read and write the file, you can change the read and write permission 1 chmod g+wr FileName If you have a file that is currently read and writeable by the group (g+wr) and you want to make it private, remove those permissions: 1 chmod g-rw FileName To add the ability for other users to write to a file or directory (this allows all users on the HPC to see read this file if it's in a shared folder which we don't recommend). 1 chmod o+w FileName","title":"Group and other permissions can also be altered:"},{"location":"File_Permissions_on_HPCC/#change-group-name","text":"To change the group ownership of a file or a directory, simply run 1 chgrp <GroupName> <FileName> where <GroupName> is the group name which you would like to change to and <FileName> is the name and path of the file or directory.","title":"Change group name:"},{"location":"File_Permissions_on_HPCC/#working-with-non-primary-groups-and-permissions","text":"If you have more than one group associated with your account, you can switch group owns the files created by default with the newgrp command: newgrp myothergroup . If you need to do this frequently, you can contact HPCC staff to change your primary group. You can also change the default group for new files created in a directory by setting the set-group-ID setting. The /mnt/research HPCC Research file share spaces have this setting set by default. To set the set-group-ID bit on a directory: 1 chmod g+s DirectoryName To remove the set-group-ID bit on a directory: 1 chmod g-s DirectoryName","title":"Working with non-primary groups and permissions"},{"location":"File_Permissions_on_HPCC/#other-special-permissions","text":"There are other group permissions beyond the scope of this document, primarily the set-user-ID bit and the \"sticky\" bit. For more information about special permissions, please review the GNU documentation, available on any HPCC system: 1 info chmod","title":"Other special permissions"},{"location":"File_Permissions_on_HPCC/#filesystem-specific-differences","text":"","title":"Filesystem-specific differences"},{"location":"File_Permissions_on_HPCC/#home","text":"Your home directory has default permissions that allow only you to have access. Other users , whether they are in your primary group or not, are not allowed access to the contents of your home directory by default. If you wish to allow other users access to your home directory, you will need to change permissions on it. To allow every member of a group access to read your home directory, use: 1 chmod g+rx ~ To allow every user outside your UNIX group to read your home directory, use: 1 chmod o+rx ~ To allow world-wide read access to your home directory 1 chmod a+rx ~","title":"Home"},{"location":"File_Permissions_on_HPCC/#scratch","text":"Directories are created as private to you by default. If you do not wish this to be the case, you can use the technique for sharing directories (see below).","title":"Scratch"},{"location":"File_Permissions_on_HPCC/#tmpdir-space","text":"Directories are creates as world-readable by default, but the scheduler deletes the contents of $TMPDIR after a job exits. If you require additional security for this temporary space, manually setting the permissions of $TMPDIR is necessary. Here is an example to mimic the security of home directory space: 1 chmod go-rwx $TMPDIR","title":"TMPDIR space"},{"location":"File_Permissions_on_HPCC/#sharing-a-single-directory-inside-your-home-directory","text":"If you wish to share only a single directory in your home directory and keep all other contents private, you can use the following techinque: 1 2 3 4 5 6 7 8 9 # create the shared folder cd ~ mkdir shared chmod o+rwx shared # create a shared file in the shared folder echo \"hello, iCER\" > shared/sharefile.txt chmod o+rw shared/sharefile.txt # anyone can read this file using cat /mnt/home/<netid>/shared/sharefile.txt You can use the same technique for your $SCRATCH folder to share folders on that. Note if there are other directories above your shared directory (e.g. it's a sub-sub-directory like \\~/project/data/shared), then every directory in the path will need the execute bit set for everyone.","title":"Sharing a single directory inside your home directory"},{"location":"File_Permissions_on_HPCC/#other-resources","text":"This just covers the basics of UNIX file permissions. Here are some other resources for more in-depth information: Software Carpentry - Permissions The Linux Cookbook, 2nd ed., Chapter 9 https://www.computerhope.com/unix/uumask.htm","title":"Other resources"},{"location":"File_Systems/","tags":["explanation"],"text":"File Systems HPCC provides six types of file storage. They are referred to here as HOME, RESEARCH, SCRATCH, FLASH (ffs17), LOCAL and RAMDISK. This article addresses the differences between these file storage systems from a hardware and software point of view. The primary usage, allocation and application cases for each storage type are also detailed to help users select the appropriate file systems for running their job. Home Space Research Space Scratch File Systems Local File Systems Guidelines for Choosing File Storage and I/O Sensitive Data on the HPCC ## Attachments: [Picture1.jpg](attachments/11895667/11895686.jpg) (image/jpeg)","title":"File Systems"},{"location":"File_Systems/#file-systems","text":"HPCC provides six types of file storage. They are referred to here as HOME, RESEARCH, SCRATCH, FLASH (ffs17), LOCAL and RAMDISK. This article addresses the differences between these file storage systems from a hardware and software point of view. The primary usage, allocation and application cases for each storage type are also detailed to help users select the appropriate file systems for running their job.","title":"File Systems"},{"location":"File_Systems/#home-space","text":"","title":"Home Space"},{"location":"File_Systems/#research-space","text":"","title":"Research Space"},{"location":"File_Systems/#scratch-file-systems","text":"","title":"Scratch File Systems"},{"location":"File_Systems/#local-file-systems","text":"","title":"Local File Systems"},{"location":"File_Systems/#guidelines-for-choosing-file-storage-and-io","text":"","title":"Guidelines for Choosing File Storage and I/O"},{"location":"File_Systems/#sensitive-data-on-the-hpcc","text":"## Attachments: [Picture1.jpg](attachments/11895667/11895686.jpg) (image/jpeg)","title":"Sensitive Data on the HPCC"},{"location":"File_and_Data_Transfer/","text":"File and Data Transfer File transfer Mapping HPC drives with Samba Mapping HPC drives with SSHFS SFTP Mapping on HPCC file systems Transferring data with Globus Science DMZ Rclone - rsync for cloud storage","title":"File and Data Transfer"},{"location":"File_and_Data_Transfer/#file-and-data-transfer","text":"","title":"File and Data Transfer"},{"location":"File_and_Data_Transfer/#file-transfer","text":"","title":"File transfer"},{"location":"File_and_Data_Transfer/#mapping-hpc-drives-with-samba","text":"","title":"Mapping HPC drives with Samba"},{"location":"File_and_Data_Transfer/#mapping-hpc-drives-with-sshfs","text":"","title":"Mapping HPC drives with SSHFS"},{"location":"File_and_Data_Transfer/#sftp-mapping-on-hpcc-file-systems","text":"","title":"SFTP Mapping on HPCC file systems"},{"location":"File_and_Data_Transfer/#transferring-data-with-globus","text":"","title":"Transferring data with Globus"},{"location":"File_and_Data_Transfer/#science-dmz","text":"","title":"Science DMZ"},{"location":"File_and_Data_Transfer/#rclone-rsync-for-cloud-storage","text":"","title":"Rclone - rsync for cloud storage"},{"location":"File_transfer/","text":"File transfer This document highlights several simple methods to transfer files to the HPCC home and research directories. There are two main systems for copying files. First, simply \" hpcc.msu.edu \" which is our main log-in gateway. It can be used for file transfer but may have high traffic, is not meant for demanding file transfers (very large files or many files), and cannot access the scratch disk (/mnt/ls15/scratch or /mnt/scratch). We offer a second gateway designed for file transfer, and does have access to the scratch file system with the host name \" rsync.hpcc.msu.edu \". Therefore we highly recommend you use rsync.hpcc.msu.edu for the 'host' or 'server' in all examples below, as it's the only way to transfer files from scratch. Note that while it's named for the popular unix 'rsync' command, it can be used for sftp or scp as well. Using FileZilla for Mac and Windows Using Linux commands Using FileZilla for Mac and Windows FileZilla is a GUI application. Download and install the appropriate (free) Filezilla client from https://filezilla-project.org/download.php?show_all=1 and select your operating system version. Mac users will have to 'unzip' the file and move the application into your Applications folder. To use, launch the program. In the top dialog boxes, enter: (Host) rsync.hpcc.msu.edu (Username) <your username> (Password) <your password> (Port) 22 Then click connect or quickconnect. The first time you use this, you will have to accept the host certificate. Once connected, the left column displays files on your local computer, the right column displays files on hpcc. You can select the appropriate directories by double clicking through each tree. Files can be dragged and dropped from one column to the next. By dragging files from the left column to the right, you are uploading files to HPCC from your local computer. By dragging files from the right column to the left, you can download files from HPCC to your local computer. Using Linux commands A number of different command-line utilities are available to OS X and Linux users. Each of them has its own advantages. Basic file copy (scp) A simple command for transferring files between the cluster and another host is scp. To copy a file from a local directory to file space on the cluster, use a line like 1 scp example.txt username@rsync.hpcc.msu.edu:example_copy.txt This will copy the file named example.txt in the local host's home directory to the user's home directory on the cluster, with the copy having the name example_copy.txt. Leaving the space after the colon blank gives the new file the same name as the original. Note: To transfer a file name with spaces you must put a backslash before each space in your file name, i.e. scp \"My File Name\" username@hpcc.msu.edu:\"My\\ File\\ Name\" . To copy a file from the cluster to your local directory, 1 scp username@rsync.hpcc.msu.edu:example.txt ./example_copy.txt will copy the file named example.txt from the user's home directory on the cluster to the home directory of the local host, naming the new file example_copy.txt. Leaving the space after the slash blank gives the new file the same name as the original. The -r option can be used to copy entire directories recursively. Synchronize directories (rsync) If you are an advanced LINUX/Mac user, there is a wonderful little utility that makes mirroring directories simple. The syntax looks very similar to scp. To mirror <local_dir> on my local computer to <hpcc_dir> on hpcc, the following command can be issued. 1 rsync -ave ssh <local_dir> username@rsync.hpcc.msu.edu:<hpcc_dir> In the above command, rsync will scan through both directories. If any files in the <local_dir> are newer, they will be uploaded to <hpcc_dir> . (It is also possible to get rsync to upload ALL different files, regardless of which is newer). To mirror the HPCC directory to your local system, call 1 rsync -ave ssh username@rsync.hpcc.msu.edu:<hpcc_dir> <local_dir> Please use rsync command with the option --chmod=Dg+s to transfer files from a local computer to your research space . See the following example: 1 rsync -ave ssh TestDir --chmod=Dg+s <username>@rsync.hpcc.msu.edu:/mnt/research/<GroupName>/ !!! Note: the first time you use rsync, you might want to add the -n flag to do a dry run before any files are copied. Interactive file copy (sftp) When preforming several data transfers between hosts, the sftp command may be preferable, as it allows the user to work interactively. Running 1 sftp username@rsync.hpcc.msu.edu from a local host establishes a connection between that host and the cluster. Both hosts can be navigated. For the local file system, lcd changes to the specified directory, lpwd prints the working directory, and lls prints a list of files in the current directory. For the remote file system, the same three commands are available, minus the leading \"l.\" Also available are commands to change permissions, rename files, and manipulate directories on the remote host. The two key commands are get <file> , which copies the file in the remote working directory to the local working directory, and put <file> , which copies the file in the local working directory to the remote working directory. The quit command closes the connection between hosts. Copy file from Internet (wget) Wget is a simple command useful for copying files from the Internet to a user's file space on the cluster. Submitting the line 1 wget http://www.examplesite.com/examplefile.txt downloads examplefile.txt to the user's working directory.","title":"File transfer"},{"location":"File_transfer/#file-transfer","text":"This document highlights several simple methods to transfer files to the HPCC home and research directories. There are two main systems for copying files. First, simply \" hpcc.msu.edu \" which is our main log-in gateway. It can be used for file transfer but may have high traffic, is not meant for demanding file transfers (very large files or many files), and cannot access the scratch disk (/mnt/ls15/scratch or /mnt/scratch). We offer a second gateway designed for file transfer, and does have access to the scratch file system with the host name \" rsync.hpcc.msu.edu \". Therefore we highly recommend you use rsync.hpcc.msu.edu for the 'host' or 'server' in all examples below, as it's the only way to transfer files from scratch. Note that while it's named for the popular unix 'rsync' command, it can be used for sftp or scp as well. Using FileZilla for Mac and Windows Using Linux commands","title":"File transfer"},{"location":"File_transfer/#using-filezilla-for-mac-and-windows","text":"FileZilla is a GUI application. Download and install the appropriate (free) Filezilla client from https://filezilla-project.org/download.php?show_all=1 and select your operating system version. Mac users will have to 'unzip' the file and move the application into your Applications folder. To use, launch the program. In the top dialog boxes, enter: (Host) rsync.hpcc.msu.edu (Username) <your username> (Password) <your password> (Port) 22 Then click connect or quickconnect. The first time you use this, you will have to accept the host certificate. Once connected, the left column displays files on your local computer, the right column displays files on hpcc. You can select the appropriate directories by double clicking through each tree. Files can be dragged and dropped from one column to the next. By dragging files from the left column to the right, you are uploading files to HPCC from your local computer. By dragging files from the right column to the left, you can download files from HPCC to your local computer.","title":"Using FileZilla for Mac and Windows"},{"location":"File_transfer/#using-linux-commands","text":"A number of different command-line utilities are available to OS X and Linux users. Each of them has its own advantages. Basic file copy (scp) A simple command for transferring files between the cluster and another host is scp. To copy a file from a local directory to file space on the cluster, use a line like 1 scp example.txt username@rsync.hpcc.msu.edu:example_copy.txt This will copy the file named example.txt in the local host's home directory to the user's home directory on the cluster, with the copy having the name example_copy.txt. Leaving the space after the colon blank gives the new file the same name as the original. Note: To transfer a file name with spaces you must put a backslash before each space in your file name, i.e. scp \"My File Name\" username@hpcc.msu.edu:\"My\\ File\\ Name\" . To copy a file from the cluster to your local directory, 1 scp username@rsync.hpcc.msu.edu:example.txt ./example_copy.txt will copy the file named example.txt from the user's home directory on the cluster to the home directory of the local host, naming the new file example_copy.txt. Leaving the space after the slash blank gives the new file the same name as the original. The -r option can be used to copy entire directories recursively. Synchronize directories (rsync) If you are an advanced LINUX/Mac user, there is a wonderful little utility that makes mirroring directories simple. The syntax looks very similar to scp. To mirror <local_dir> on my local computer to <hpcc_dir> on hpcc, the following command can be issued. 1 rsync -ave ssh <local_dir> username@rsync.hpcc.msu.edu:<hpcc_dir> In the above command, rsync will scan through both directories. If any files in the <local_dir> are newer, they will be uploaded to <hpcc_dir> . (It is also possible to get rsync to upload ALL different files, regardless of which is newer). To mirror the HPCC directory to your local system, call 1 rsync -ave ssh username@rsync.hpcc.msu.edu:<hpcc_dir> <local_dir> Please use rsync command with the option --chmod=Dg+s to transfer files from a local computer to your research space . See the following example: 1 rsync -ave ssh TestDir --chmod=Dg+s <username>@rsync.hpcc.msu.edu:/mnt/research/<GroupName>/ !!! Note: the first time you use rsync, you might want to add the -n flag to do a dry run before any files are copied. Interactive file copy (sftp) When preforming several data transfers between hosts, the sftp command may be preferable, as it allows the user to work interactively. Running 1 sftp username@rsync.hpcc.msu.edu from a local host establishes a connection between that host and the cluster. Both hosts can be navigated. For the local file system, lcd changes to the specified directory, lpwd prints the working directory, and lls prints a list of files in the current directory. For the remote file system, the same three commands are available, minus the leading \"l.\" Also available are commands to change permissions, rename files, and manipulate directories on the remote host. The two key commands are get <file> , which copies the file in the remote working directory to the local working directory, and put <file> , which copies the file in the local working directory to the remote working directory. The quit command closes the connection between hosts. Copy file from Internet (wget) Wget is a simple command useful for copying files from the Internet to a user's file space on the cluster. Submitting the line 1 wget http://www.examplesite.com/examplefile.txt downloads examplefile.txt to the user's working directory.","title":"Using Linux commands"},{"location":"Frequently_Asked_Questions_FAQ_/","text":"Frequently Asked Questions (FAQ) What is my HPCC user name/password? If you are affiliated with MSU, then your MSU NetID is your user name, and your NetID password is your HPCC password. This is the same as those for all the MSU online services. An HPCC account must be requested by an MSU faculty member at https://contact.icer.msu.edu/account Can I reset my password on the HPCC because my login got denied after multiple failed attempts? No. The authentication on the HPCC is directly tied to MSU. You will need to request a password reset at https://netid.msu.edu/netid/password/index.html I used to be able to connect to the HPCC server, but now I can't. Why? There can be multiple reasons for this, such as system downtime (so please check the ICER blog first). Another common reason is account expiry. The HPCC periodically disables users who are no longer affiliated with the university or registered with a class for which the instructor has created temporary student accounts. To re-activate your HPCC account, please have your PI submit a sponsoring form at https://contact.icer.msu.edu/sponsoredrenewal Can you keep me posted on the current status of the HPCC? Yes. Users are encouraged to follow the HPCC Announcements blog to keep updated on the status of HPCC (such as scheduled downtimes and urgent notices). I am looking for help to troubleshoot my problem. How do I share my code/files with you? We do not go to your directory to view files or test your code for that matter. Please send your files along with your reply to the ticket email. Is there any limit per user on using the HPCC resources? Limit on running a program on a dev-node : 2 CPU hours . If you are running a multi-threaded program, the wall time limit would be (roughly) 2 hours divided by the number of threads. Limit on file counts : 1 million files for each of the home, research and scratch spaces. Limit on storage size : each user has up to 1 TB of storage for free (for each of the home and research directories); beyond 1 TB, the cost is $125 per TB per year. For scratch space (i.e. /mnt/scratch/<your_user_name> ), 50 TB is the maximum and cannot be increased further (users will need to archive/delete files when this limit has been exceeded). Limit on cluster usage : 1) the longest wall time you can request is 7 days ; 2) the maximal number of CPU cores you can use is 1040 at any one time; 3) the maximal number of jobs that can be queued or running is 1000 (except in the scavenger queue ); 4) non-buyin users have a maximum of 1 million CPU hours per year . I would like to know more about the dev-node limit. When you connect to any of the HPCC's dev-nodes, you will see the following message: processes on development nodes are limited to two hours of CPU time. The two hour CPU time limit is for each process you run on that dev-node. If one process uses CPU time greater than 2 hours, then only that process will be killed. You can, however, still connect to that dev-node, and run another process. Additionally, if your process uses 100% CPU (1 core), it will be terminated in two hours. If your process uses 200% CPU (2 cores), it will be terminated in one hour, and so on. How do I check my cpu time usage? Run the command cputime . You can also run sreport to get the information with date such as sreport user top user= start=2021-01-06 -t hour . How do I get my storage usage data? Run command \"quota\". You can't write new files if your quota has been used up . I have a buyin account, do I need to specify it when I submit jobs? No. When submitting a job without specifying an account, your default account is used. You can check your default account using the \"buyin_status -l\" command; buyin user's default is their buyin account. We recommend you read this if you have purchased buyin nodes. What is HPCC's data backup policy? We back up data in users' home and research directories, not in their scratch spaces. You will have 24 hourly backups for the previous 24 hour period. For previous days however, we will provide daily backups only. Daily backups are performed at 12 AM Eastern Time and retained for 60 days. My files in the scratch space are gone. Files in scratch are automatically purged if the last modification time is older than 45 days. Note that the scratch spaces are not intended for long-term storage. Files saved in scratch have no back-up . I can't transfer files from/to my scratch space. If you use as the hostname, you are connecting to the gateway (login) node which has no scratch mounted. You should connect to in this case. It is a dedicated server for file transfer. Do you support running GPU jobs? Yes. There are three GPU dev-nodes and a series of compute nodes in the cluster; see Cluster resources . Why did I get an \"Illegal Instruction\" error? This is usually because a program was compiled on a newer CPU architecture (e.g., intel18) but then run on an older one (e.g., intel14). Our system has a range of CPUs, and the newest versions support new instructions not available on the older CPUs. One short-term fix is to run programs on the same CPU that they were compiled on. Based on our experience, this error has occurred only on intel14 nodes and therefore you need to avoid them. That is, for dev-node testing, pick one from dev-intel16, dev-intel16-k80 and dev-intel18. For job submission, add #SBATCH--constraint=\"[intel16|intel18]\" in your SLURM script. How do I use Python on the HPCC? There are two methods: users can install their own version of Python with Anaconda or use the versions of Python installed on the HPCC system. See here . I tried to use python matplotlib to plot, but got an error of \"No module named '_tkinter'\" If you use the default python module ( /opt/software/Python/3.6.4-foss-2018a/bin/python ) on a dev-node, you need to load the Tkinter module before using python in order to proceed without errors. Run: module load Tkinter/3.6.4-Python-3.6.4 I have a Python conflict. What should I do to resolve it? Upon login to a dev-node, a default module list will load automatically. Since Python/3.6.4 is included in the list, it can interfere with a user's conda environment. As a consequence, your program may not be able to find packages installed in your conda environment even if it has been activated. In other words, the program still picks up Python/3.6.4 in the module system. The solution is to run module unload Python before activating the conda environment. How do I deactivate Conda base environment? Many users have reported that after a local installation of Anaconda on the HPCC, their login prompt changes to something starting with (base) -bash-4.2$ . This is because conda activates the default environment, base , upon startup. To disable this behavior, which often results in conflicts with system defaults, users can run the following command: 1 conda config --set auto_activate_base False Why did my \"module load\" command output errors? There are many reasons that errors occur when you try loading a module. However, the most common cause is that you have forgotten to run module purge . Sometimes, module spider can also fail to find the module. Most likely it's because your personal module cache is out of date. To clear it, run rm -r ~/.lmod.d/.cache . I want to install software packages, what should I do? See here for instructions. Please note that we encourage users to install software on their own, if possible. The HPCC has provided numerous versions of compilers and libraries which should accommodate the vast majority of software across different fields. If you are thinking of requesting the system-wide installation of a piece of software, we strongly recommend you check the following factors when submitting a request for software installation: (1) How popular is the software? If it is not a popular software, are there other users on HPCC who would also be using it? If you are the only one using it, we would recommend it be installed in your home directory. (2) What type of license agreement does the software have? Some software licenses may restrict use even when they are free. Examples include software with export control, specific end-user license agreement, etc. When software licenses restrict use, we typically recommend the user directly make an agreement with the software provider to obtain and install it in their home directory. If it will be used by a group of people, HPCC system administrators can help with setting up the group access in compliance with the license agreement. (3) Is the software well maintained and up-to-date? If the software you wish to install is legacy software or is not being well maintained, chances are its installation will require an older version of its dependencies as well. The effort to install this software may then be greater than the effort required to find an up-to-date software with the same, similar, or even better functionality. It may be time to consider transitioning to using a newer software. What does the message \"Nodes required for job are DOWN, DRAINED or reserved for jobs in higher priority partitions\" mean after my job is submitted? Once a job is submitted the scheduler adds it to the calculations and continues to update the status of the job as the system works. The status for a job will reflect the current state of the scheduler, so you will see this message update once the scheduler has found a place to put the job. There are always some nodes which are down or drained in the cluster due to normal maintenance, but the \"reserved for jobs in higher priority partitions\" is the important part, and simply indicates that the scheduler has not yet found a time to schedule the job. This will update as the scheduler continues to function. Can I use HPC through web browsers? Yes, we provide Open OnDemand, a web portal for easy web access to the HPCC. Check out this tutorial . What should I do when I cannot load modules? See How to find and load software modules . I have issues with copying files to my HPCC research space. Many users have reported problems copying or transferring files to their research space. Although their research space still has plenty of space, they still get the following error message: failed to ... Disk quota exceeded This problem may occur because the folders which you copy or transfer files to have incorrect group ownership or no set-group-ID. Please read this for more instructions. What is powertools? The powertools module is a collection of software tools and examples that allows researchers to better utilize HPC systems. Powertools was created to help advanced users use the HPCC more effectively. To learn more about powertools, run the command powertools . I want copy files from/to my MS One Drive/Google Drive. Rclone is currently installed on the HPCC. This software supports research in the cloud and helps HPCC users to sync files and directories between MSU\u2019s HPCC and their cloud storage, including OneDrive and Google Drive. Please refer to Rclone How to check the HPCC node usage? Users can see this information by simply running the node_status command on any dev node. We also offer a web-based dashboard at https://icer.msu.edu/dashboard . Does HPCC offer a cheaper long-term archiving plan? We do not. However, MSU offers the Data Storage Finder ( https://data-storage-finder.tech.msu.edu , on-campus only). There are several possible options for data archiving.","title":"FAQ"},{"location":"Frequently_Asked_Questions_FAQ_/#frequently-asked-questions-faq","text":"","title":"Frequently Asked Questions (FAQ)"},{"location":"Frequently_Asked_Questions_FAQ_/#what-is-my-hpcc-user-namepassword","text":"If you are affiliated with MSU, then your MSU NetID is your user name, and your NetID password is your HPCC password. This is the same as those for all the MSU online services. An HPCC account must be requested by an MSU faculty member at https://contact.icer.msu.edu/account","title":"What is my HPCC user name/password?"},{"location":"Frequently_Asked_Questions_FAQ_/#can-i-reset-my-password-on-the-hpcc-because-my-login-got-denied-after-multiple-failed-attempts","text":"No. The authentication on the HPCC is directly tied to MSU. You will need to request a password reset at https://netid.msu.edu/netid/password/index.html","title":"Can I reset my password on the HPCC because my login got denied after multiple failed attempts?"},{"location":"Frequently_Asked_Questions_FAQ_/#i-used-to-be-able-to-connect-to-the-hpcc-server-but-now-i-cant-why","text":"There can be multiple reasons for this, such as system downtime (so please check the ICER blog first). Another common reason is account expiry. The HPCC periodically disables users who are no longer affiliated with the university or registered with a class for which the instructor has created temporary student accounts. To re-activate your HPCC account, please have your PI submit a sponsoring form at https://contact.icer.msu.edu/sponsoredrenewal","title":"I used to be able to connect to the HPCC server, but now I can't. Why?"},{"location":"Frequently_Asked_Questions_FAQ_/#can-you-keep-me-posted-on-the-current-status-of-the-hpcc","text":"Yes. Users are encouraged to follow the HPCC Announcements blog to keep updated on the status of HPCC (such as scheduled downtimes and urgent notices).","title":"Can you keep me posted on the current status of the HPCC?"},{"location":"Frequently_Asked_Questions_FAQ_/#i-am-looking-for-help-to-troubleshoot-my-problem-how-do-i-share-my-codefiles-with-you","text":"We do not go to your directory to view files or test your code for that matter. Please send your files along with your reply to the ticket email.","title":"I am looking for help to troubleshoot my problem. How do I share my code/files with you?"},{"location":"Frequently_Asked_Questions_FAQ_/#is-there-any-limit-per-user-on-using-the-hpcc-resources","text":"Limit on running a program on a dev-node : 2 CPU hours . If you are running a multi-threaded program, the wall time limit would be (roughly) 2 hours divided by the number of threads. Limit on file counts : 1 million files for each of the home, research and scratch spaces. Limit on storage size : each user has up to 1 TB of storage for free (for each of the home and research directories); beyond 1 TB, the cost is $125 per TB per year. For scratch space (i.e. /mnt/scratch/<your_user_name> ), 50 TB is the maximum and cannot be increased further (users will need to archive/delete files when this limit has been exceeded). Limit on cluster usage : 1) the longest wall time you can request is 7 days ; 2) the maximal number of CPU cores you can use is 1040 at any one time; 3) the maximal number of jobs that can be queued or running is 1000 (except in the scavenger queue ); 4) non-buyin users have a maximum of 1 million CPU hours per year .","title":"Is there any limit per user on using the HPCC resources?"},{"location":"Frequently_Asked_Questions_FAQ_/#i-would-like-to-know-more-about-the-dev-node-limit","text":"When you connect to any of the HPCC's dev-nodes, you will see the following message: processes on development nodes are limited to two hours of CPU time. The two hour CPU time limit is for each process you run on that dev-node. If one process uses CPU time greater than 2 hours, then only that process will be killed. You can, however, still connect to that dev-node, and run another process. Additionally, if your process uses 100% CPU (1 core), it will be terminated in two hours. If your process uses 200% CPU (2 cores), it will be terminated in one hour, and so on.","title":"I would like to know more about the dev-node limit."},{"location":"Frequently_Asked_Questions_FAQ_/#how-do-i-check-my-cpu-time-usage","text":"Run the command cputime . You can also run sreport to get the information with date such as sreport user top user= start=2021-01-06 -t hour .","title":"How do I check my cpu time usage?"},{"location":"Frequently_Asked_Questions_FAQ_/#how-do-i-get-my-storage-usage-data","text":"Run command \"quota\". You can't write new files if your quota has been used up .","title":"How do I get my storage usage data?"},{"location":"Frequently_Asked_Questions_FAQ_/#i-have-a-buyin-account-do-i-need-to-specify-it-when-i-submit-jobs","text":"No. When submitting a job without specifying an account, your default account is used. You can check your default account using the \"buyin_status -l\" command; buyin user's default is their buyin account. We recommend you read this if you have purchased buyin nodes.","title":"I have a buyin account, do I need to specify it when I submit jobs?"},{"location":"Frequently_Asked_Questions_FAQ_/#what-is-hpccs-data-backup-policy","text":"We back up data in users' home and research directories, not in their scratch spaces. You will have 24 hourly backups for the previous 24 hour period. For previous days however, we will provide daily backups only. Daily backups are performed at 12 AM Eastern Time and retained for 60 days.","title":"What is HPCC's data backup policy?"},{"location":"Frequently_Asked_Questions_FAQ_/#my-files-in-the-scratch-space-are-gone","text":"Files in scratch are automatically purged if the last modification time is older than 45 days. Note that the scratch spaces are not intended for long-term storage. Files saved in scratch have no back-up .","title":"My files in the scratch space are gone."},{"location":"Frequently_Asked_Questions_FAQ_/#i-cant-transfer-files-fromto-my-scratch-space","text":"If you use as the hostname, you are connecting to the gateway (login) node which has no scratch mounted. You should connect to in this case. It is a dedicated server for file transfer.","title":"I can't transfer files from/to my scratch space."},{"location":"Frequently_Asked_Questions_FAQ_/#do-you-support-running-gpu-jobs","text":"Yes. There are three GPU dev-nodes and a series of compute nodes in the cluster; see Cluster resources .","title":"Do you support running GPU jobs?"},{"location":"Frequently_Asked_Questions_FAQ_/#why-did-i-get-an-illegal-instruction-error","text":"This is usually because a program was compiled on a newer CPU architecture (e.g., intel18) but then run on an older one (e.g., intel14). Our system has a range of CPUs, and the newest versions support new instructions not available on the older CPUs. One short-term fix is to run programs on the same CPU that they were compiled on. Based on our experience, this error has occurred only on intel14 nodes and therefore you need to avoid them. That is, for dev-node testing, pick one from dev-intel16, dev-intel16-k80 and dev-intel18. For job submission, add #SBATCH--constraint=\"[intel16|intel18]\" in your SLURM script.","title":"Why did I get an \"Illegal Instruction\" error?"},{"location":"Frequently_Asked_Questions_FAQ_/#how-do-i-use-python-on-the-hpcc","text":"There are two methods: users can install their own version of Python with Anaconda or use the versions of Python installed on the HPCC system. See here .","title":"How do I use Python on the HPCC?"},{"location":"Frequently_Asked_Questions_FAQ_/#i-tried-to-use-python-matplotlib-to-plot-but-got-an-error-of-no-module-named-_tkinter","text":"If you use the default python module ( /opt/software/Python/3.6.4-foss-2018a/bin/python ) on a dev-node, you need to load the Tkinter module before using python in order to proceed without errors. Run: module load Tkinter/3.6.4-Python-3.6.4","title":"I tried to use python matplotlib to plot, but got an error of \"No module named '_tkinter'\""},{"location":"Frequently_Asked_Questions_FAQ_/#i-have-a-python-conflict-what-should-i-do-to-resolve-it","text":"Upon login to a dev-node, a default module list will load automatically. Since Python/3.6.4 is included in the list, it can interfere with a user's conda environment. As a consequence, your program may not be able to find packages installed in your conda environment even if it has been activated. In other words, the program still picks up Python/3.6.4 in the module system. The solution is to run module unload Python before activating the conda environment.","title":"I have a Python conflict. What should I do to resolve it?"},{"location":"Frequently_Asked_Questions_FAQ_/#how-do-i-deactivate-conda-base-environment","text":"Many users have reported that after a local installation of Anaconda on the HPCC, their login prompt changes to something starting with (base) -bash-4.2$ . This is because conda activates the default environment, base , upon startup. To disable this behavior, which often results in conflicts with system defaults, users can run the following command: 1 conda config --set auto_activate_base False","title":"How do I deactivate Conda base environment?"},{"location":"Frequently_Asked_Questions_FAQ_/#why-did-my-module-load-command-output-errors","text":"There are many reasons that errors occur when you try loading a module. However, the most common cause is that you have forgotten to run module purge . Sometimes, module spider can also fail to find the module. Most likely it's because your personal module cache is out of date. To clear it, run rm -r ~/.lmod.d/.cache .","title":"Why did my \"module load\" command output errors?"},{"location":"Frequently_Asked_Questions_FAQ_/#i-want-to-install-software-packages-what-should-i-do","text":"See here for instructions. Please note that we encourage users to install software on their own, if possible. The HPCC has provided numerous versions of compilers and libraries which should accommodate the vast majority of software across different fields. If you are thinking of requesting the system-wide installation of a piece of software, we strongly recommend you check the following factors when submitting a request for software installation: (1) How popular is the software? If it is not a popular software, are there other users on HPCC who would also be using it? If you are the only one using it, we would recommend it be installed in your home directory. (2) What type of license agreement does the software have? Some software licenses may restrict use even when they are free. Examples include software with export control, specific end-user license agreement, etc. When software licenses restrict use, we typically recommend the user directly make an agreement with the software provider to obtain and install it in their home directory. If it will be used by a group of people, HPCC system administrators can help with setting up the group access in compliance with the license agreement. (3) Is the software well maintained and up-to-date? If the software you wish to install is legacy software or is not being well maintained, chances are its installation will require an older version of its dependencies as well. The effort to install this software may then be greater than the effort required to find an up-to-date software with the same, similar, or even better functionality. It may be time to consider transitioning to using a newer software.","title":"I want to install software packages, what should I do?"},{"location":"Frequently_Asked_Questions_FAQ_/#what-does-the-message-nodes-required-for-job-are-down-drained-or-reserved-for-jobs-in-higher-priority-partitions-mean-after-my-job-is-submitted","text":"Once a job is submitted the scheduler adds it to the calculations and continues to update the status of the job as the system works. The status for a job will reflect the current state of the scheduler, so you will see this message update once the scheduler has found a place to put the job. There are always some nodes which are down or drained in the cluster due to normal maintenance, but the \"reserved for jobs in higher priority partitions\" is the important part, and simply indicates that the scheduler has not yet found a time to schedule the job. This will update as the scheduler continues to function.","title":"What does the message \"Nodes required for job are DOWN, DRAINED or reserved for jobs in higher priority partitions\" mean after my job is submitted?"},{"location":"Frequently_Asked_Questions_FAQ_/#can-i-use-hpc-through-web-browsers","text":"Yes, we provide Open OnDemand, a web portal for easy web access to the HPCC. Check out this tutorial .","title":"Can I use HPC through web browsers?"},{"location":"Frequently_Asked_Questions_FAQ_/#what-should-i-do-when-i-cannot-load-modules","text":"See How to find and load software modules .","title":"What should I do when I cannot load modules?"},{"location":"Frequently_Asked_Questions_FAQ_/#i-have-issues-with-copying-files-to-my-hpcc-research-space","text":"Many users have reported problems copying or transferring files to their research space. Although their research space still has plenty of space, they still get the following error message: failed to ... Disk quota exceeded This problem may occur because the folders which you copy or transfer files to have incorrect group ownership or no set-group-ID. Please read this for more instructions.","title":"I have issues with copying files to my HPCC research space."},{"location":"Frequently_Asked_Questions_FAQ_/#what-is-powertools","text":"The powertools module is a collection of software tools and examples that allows researchers to better utilize HPC systems. Powertools was created to help advanced users use the HPCC more effectively. To learn more about powertools, run the command powertools .","title":"What is powertools?"},{"location":"Frequently_Asked_Questions_FAQ_/#i-want-copy-files-fromto-my-ms-one-drivegoogle-drive","text":"Rclone is currently installed on the HPCC. This software supports research in the cloud and helps HPCC users to sync files and directories between MSU\u2019s HPCC and their cloud storage, including OneDrive and Google Drive. Please refer to Rclone","title":"I want copy files from/to my MS One Drive/Google Drive."},{"location":"Frequently_Asked_Questions_FAQ_/#how-to-check-the-hpcc-node-usage","text":"Users can see this information by simply running the node_status command on any dev node. We also offer a web-based dashboard at https://icer.msu.edu/dashboard .","title":"How to check the HPCC node usage?"},{"location":"Frequently_Asked_Questions_FAQ_/#does-hpcc-offer-a-cheaper-long-term-archiving-plan","text":"We do not. However, MSU offers the Data Storage Finder ( https://data-storage-finder.tech.msu.edu , on-campus only). There are several possible options for data archiving.","title":"Does HPCC offer a cheaper long-term archiving plan?"},{"location":"GATK4/","text":"GATK4 Be sure to read this Quick Start before using GATK4. In particular, note the following statement from the developers: Once you have downloaded and unzipped the package (named gatk-[version] ), you will find four files inside the resulting directory: gatk gatk-package-[version]-local.jar gatk-package-[version]-spark.jar README.md Now you may ask, why are there two jars? As the names suggest, gatk-package-[version]-spark.jar is the jar for running Spark tools on a Spark cluster, while gatk-package-[version]-local.jar is the jar that is used for everything else (including running Spark tools \"locally\", i.e. on a regular server or cluster). So does that mean you have to specify which one you want to run each time? Nope! See the gatk file in there? That's an executable wrapper script that you invoke and that will choose the appropriate jar for you based on the rest of your command line. You could still invoke a specific jar if you wanted, but using gatk is easier, and it will also take care of setting some parameters that you would otherwise have to specify manually. On the HPCC, after login to a dev-node, run: module load GATK/4.0.5.1-Python-3.6.4 . As a tip, if you happen to run a module purge command in the middle of your work, and want to go back to the original login environment, please type the command: exec bash -l A simple test on the HPCC is provided below. 1 2 module load GATK/4.0.5.1-Python-3.6.4 gatk --java-options \"-Xmx8G\" HaplotypeCaller -R /opt/software/GATK/3.3-0-Java-1.7.0_80/resources/exampleFASTA.fasta -I /opt/software/GATK/3.3-0-Java-1.7.0_80/resources/exampleBAM.bam -O gatk_test.vcf","title":"GATK4"},{"location":"GATK4/#gatk4","text":"Be sure to read this Quick Start before using GATK4. In particular, note the following statement from the developers: Once you have downloaded and unzipped the package (named gatk-[version] ), you will find four files inside the resulting directory: gatk gatk-package-[version]-local.jar gatk-package-[version]-spark.jar README.md Now you may ask, why are there two jars? As the names suggest, gatk-package-[version]-spark.jar is the jar for running Spark tools on a Spark cluster, while gatk-package-[version]-local.jar is the jar that is used for everything else (including running Spark tools \"locally\", i.e. on a regular server or cluster). So does that mean you have to specify which one you want to run each time? Nope! See the gatk file in there? That's an executable wrapper script that you invoke and that will choose the appropriate jar for you based on the rest of your command line. You could still invoke a specific jar if you wanted, but using gatk is easier, and it will also take care of setting some parameters that you would otherwise have to specify manually. On the HPCC, after login to a dev-node, run: module load GATK/4.0.5.1-Python-3.6.4 . As a tip, if you happen to run a module purge command in the middle of your work, and want to go back to the original login environment, please type the command: exec bash -l A simple test on the HPCC is provided below. 1 2 module load GATK/4.0.5.1-Python-3.6.4 gatk --java-options \"-Xmx8G\" HaplotypeCaller -R /opt/software/GATK/3.3-0-Java-1.7.0_80/resources/exampleFASTA.fasta -I /opt/software/GATK/3.3-0-Java-1.7.0_80/resources/exampleBAM.bam -O gatk_test.vcf","title":"GATK4"},{"location":"Gaussian_Access/","text":"Gaussian Access To obtain access to this software on HPCC complete and sign the appropriate Gaussian Confidentiality Agreement. Please ensure that all required sections are filled out legibly or else the form will be returned to you. Research Group Leader Form \u2014 Only one form needs to be completed per research group. User/Researcher Form \u2014 Each user of the software needs to complete this form. Please ensure that your group leader has completed the Research Group Leader form before submitting this. Coursework Form \u2014 This form is to be completed by the each student enrolled in an MSU course. Please note that the course instructor should have completed the Research Group Leader Form above. Access ends after the class is completed. Email the completed form to general@rt.hpcc.msu.edu . Please make the subject of your email, \"Gaussian Confidentiality Agreement.\" Additionally, the original form must be mailed to the ICER office: Biomedical & Physical Sciences Building, Room 1440, 567 Wilson Rd, East Lansing, MI 48824. Your completed agreement will then be sent to Gaussian for the final approval. Once approved, the HPCC System Admin team will act upon the request and grant access. Note: Typically this process typically takes about two weeks from the date of signing the Confidentiality Agreement to Gaussian giving approval of the software so we ask that you please be patient. Without a personally signed Confidentiality Agreement, you will not be granted access to Gaussian.","title":"Gaussian access"},{"location":"Gaussian_Access/#gaussian-access","text":"To obtain access to this software on HPCC complete and sign the appropriate Gaussian Confidentiality Agreement. Please ensure that all required sections are filled out legibly or else the form will be returned to you. Research Group Leader Form \u2014 Only one form needs to be completed per research group. User/Researcher Form \u2014 Each user of the software needs to complete this form. Please ensure that your group leader has completed the Research Group Leader form before submitting this. Coursework Form \u2014 This form is to be completed by the each student enrolled in an MSU course. Please note that the course instructor should have completed the Research Group Leader Form above. Access ends after the class is completed. Email the completed form to general@rt.hpcc.msu.edu . Please make the subject of your email, \"Gaussian Confidentiality Agreement.\" Additionally, the original form must be mailed to the ICER office: Biomedical & Physical Sciences Building, Room 1440, 567 Wilson Rd, East Lansing, MI 48824. Your completed agreement will then be sent to Gaussian for the final approval. Once approved, the HPCC System Admin team will act upon the request and grant access. Note: Typically this process typically takes about two weeks from the date of signing the Confidentiality Agreement to Gaussian giving approval of the software so we ask that you please be patient. Without a personally signed Confidentiality Agreement, you will not be granted access to Gaussian.","title":"Gaussian Access"},{"location":"Gaussian_Job_Script/","text":"Gaussian Job Script Here is a simple job script g16.sb for running Gaussian job g16.com : g16.sb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/bash --login #SBATCH \u2013-job-name=GaussianJob #SBATCH \u2013-ntasks=1 #SBATCH \u2013-cpus-per-task=4 #SBATCH --mem=7G #SBATCH \u2013-time=00:10:00 InputFile = g16.com OutputFile = g16.log module load Gaussian/g16 powertools # GAUSS_SCRDIR=<your preferred Gaussian scratch space> # mkdir -p ${GAUSS_SCRDIR} g16 < ${ InputFile } > ${ OutputFile } ### write job information to SLURM output file scontrol show job $SLURM_JOB_ID # Print out resource usage js -j $SLURM_JOB_ID ### powetools command where the Gaussian input file g16.com can be found from the previous section Running Gaussian by Command Lines . For the resource request ( #SBATCH lines) above, since Gaussian can only run parallely with shared memory in HPCC system, only 1 task (with 1 node) is requested in line 2. The number of CPUs requested in line 3 is the same as the setting of \" %NProcShared \" ( =4 ) in the Gaussian input file. The memory request in line 4 should be larger than the setting of \" %Mem \" ( =5GB ) in the Gaussian input file in case the job runs out of memory. Please also make sure the walltime request in line 5 is longer enough to finish the job. In the command lines, you need to make sure Gaussian/g16 is loaded as in line 10. If you would like to use scratch directory other than /mnt/scratch/$USER for the Gaussian scratch files, you could set up a different one with line 11 and 12. The calculation of the Gaussian job is executed in line 14 with input file g16.com and output file g16.log . Once the calculation is done, line 17 and 20 will be executed to print out the job information and resource usage respectively to the SLURM output file ( with file name: slurm-<JobID>.out ).","title":"Gaussian job script"},{"location":"Gaussian_Job_Script/#gaussian-job-script","text":"Here is a simple job script g16.sb for running Gaussian job g16.com : g16.sb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/bash --login #SBATCH \u2013-job-name=GaussianJob #SBATCH \u2013-ntasks=1 #SBATCH \u2013-cpus-per-task=4 #SBATCH --mem=7G #SBATCH \u2013-time=00:10:00 InputFile = g16.com OutputFile = g16.log module load Gaussian/g16 powertools # GAUSS_SCRDIR=<your preferred Gaussian scratch space> # mkdir -p ${GAUSS_SCRDIR} g16 < ${ InputFile } > ${ OutputFile } ### write job information to SLURM output file scontrol show job $SLURM_JOB_ID # Print out resource usage js -j $SLURM_JOB_ID ### powetools command where the Gaussian input file g16.com can be found from the previous section Running Gaussian by Command Lines . For the resource request ( #SBATCH lines) above, since Gaussian can only run parallely with shared memory in HPCC system, only 1 task (with 1 node) is requested in line 2. The number of CPUs requested in line 3 is the same as the setting of \" %NProcShared \" ( =4 ) in the Gaussian input file. The memory request in line 4 should be larger than the setting of \" %Mem \" ( =5GB ) in the Gaussian input file in case the job runs out of memory. Please also make sure the walltime request in line 5 is longer enough to finish the job. In the command lines, you need to make sure Gaussian/g16 is loaded as in line 10. If you would like to use scratch directory other than /mnt/scratch/$USER for the Gaussian scratch files, you could set up a different one with line 11 and 12. The calculation of the Gaussian job is executed in line 14 with input file g16.com and output file g16.log . Once the calculation is done, line 17 and 20 will be executed to print out the job information and resource usage respectively to the SLURM output file ( with file name: slurm-<JobID>.out ).","title":"Gaussian Job Script"},{"location":"Gaussian_Job_with_Checkpointing_Run/","text":"Gaussian Job with Checkpointing Run For running a large system with Gaussian, it usually takes a long time and many resources to complete. It is a good idea to set up checkpointing so the calculation can keep going in case of any interruption due to walltime limit or possible system malfunction. Checkpointing function can save a snapshot of a Gaussian running state so it can restart from the previous calculation. Users can also divide a long-time job into many 4-hour short jobs since jobs with walltime less than or equal to 4 hours can use the buy-in nodes (55% of all nodes) in HPCC. In order to have an appropriate checkpointing run with Gaussian, an unified read-write file setting ( %RWF ) should be in the Link 0 section of the input file. An example water.gjf is in the following: water.gjf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 %NProcShared = 2 %Mem = 3GB %RWF = water.rwf %NoSave %chk = water.chk #P opt b3lyp/aug-cc-pVTZ water molecules 0 1 O -2.12123400 1 .99409800 -1.27381200 H 1 .52438600 0 .53672100 0 .67508800 H 1 .76493000 -0.81527300 -0.18137000 O -1.12977500 -0.31430400 -0.37860700 H -1.76492800 -0.81528500 0 .18137200 H -1.52439700 0 .53670800 -0.67510100 O 2 .89125300 -1.69896600 -1.06351900 O 1 .12976700 -0.31428900 0 .37859300 H 2 .99568600 -1.73945400 -2.01677200 H 3 .39746100 -2.42787400 -0.69708600 O -2.89123000 -1.69896400 1 .06353700 H -2.99563400 -1.73945600 2 .01679300 H -2.43456700 2 .07972500 -2.17761600 H -2.58174600 2 .66131900 -0.75942800 H -3.39743400 -2.42788000 0 .69711700 The input file requests geometry optimization of 5 water molecules with a very large basis set aug-cc-pVTZ. It will take about 25 CPU hours to finish the whole calculation. We have the setting on %RWF which specifies water.rwf file for checkpointing function besides water.chk file. Since the specification %RWF is placed before %NoSave line, the rwf file will be deleted if the calculation is normally completed without any error. In order to have several restart running after first running stops, we can build a restart Gaussian input file restart.gjf simply as restart.gjf 1 2 3 4 5 6 %NProcShared = 2 %Mem = 3GB %RWF = waters.rwf %NoSave %chk = waters.chk #P Restart Since all information about the calculation is recorded in the rwf file, a line with \"Restart\" is enough for Gaussian to restart from the previous job. This restart input file can also be created by the commands: 1 2 $ grep '^%' waters.gjf > restart.gjf $ echo -e '#P Restart\\n' >> restart.gjf where we simply \" grep \" the lines starting with \" % \" sign in water.gjf and put them to the Gaussian restart file with \" #P Restart \" line in the end. Now we need a job script to submit the Gaussian calculation. The script needs to keep submitting jobs to restart the previous calculation until it is completed. Here is a job script water.sb which can do the work: water.sb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #SBATCH \u2013-job-name=LongJob #SBATCH \u2013-ntasks=1 #SBATCH \u2013-cpus-per-task=2 #SBATCH --mem=5G #SBATCH \u2013-time=04:00:00 module load Gaussian/g16 powertools OutputFile = \"water- ${ SLURM_JOBID } .log\" # Gaussian output file name for each job # How many seconds before end of job to submit another BeforeEnd = 300 # 5 minutes # The background script to keep job submission until calculation is completed ( sleep $(( 4 * 60 * 60 - BeforeEnd )) # sleep until the time before end of job js -j ${ SLURM_JOBID } # print out resource usage cat ${ OutputFile } >> water.log # collect Gaussian outputs into one file echo -e \"\\n\\n====== Gaussian calculation on job ${ SLURM_JOBID } stops. ======\\n\\n\" >> water.log echo \"The Gaussian calculation has not completed. Submit another job to keep doing it.\" sbatch water.sb # submit another job scancel ${ SLURM_JOBID } ) & # job stops if g16 command is not finished # Whether this is a restart job or not if [ -f water.rwf ] && [ -f water.chk ] ; then InputFile = \"restart.gjf\" else InputFile = \"water.gjf\" fi g16 < ${ InputFile } > ${ OutputFile } # The following commands are not executed unless g16 command is completed. # Print out resource usage js -j $SLURM_JOB_ID ### powetools command cat ${ OutputFile } >> water.log echo -e \"\\n\\n====== Gaussian calculation is completed on job ${ SLURM_JOBID } . ======\\n\\n\" >> water.log where a background script in (---)& from line 14 to 20 is added to keep submitting job s . Once the job is started, the background script is running at the same time as the foreground script. The background script is in sleep for 3 hours and 55 minutes first. During this time, the foreground script runs the Gaussian calculation or restart the previous calculation if the checkpointing files ( water.rwf and water.chk ) exist. After 5 minutes before the end of the job, the background is awake to print out the resource usage and Gaussian output. It submits another job and stop the current running job in line 19 and 20 if the g16 command in line 29 is not completed. If the g16 command is finished before the background is awake, the job will keep executing all command lines after line 30 and finish. There will be no more job submitted. Since the rwf file usually takes a lot of file space, it is suggested to run checkpointing jobs in scratch space in case your home or research space is over quota. Users can create a directory in their scratch space. Copy all files ( water,gjf , restart.gjf and water.sb ) and submit their job script there. Please check your job running frequently. Make sure to copy necessary files back to home or research directory from time to time since files on scratch will be purged if they have not been modified for 45 days.","title":"Gaussian job with checkpointing"},{"location":"Gaussian_Job_with_Checkpointing_Run/#gaussian-job-with-checkpointing-run","text":"For running a large system with Gaussian, it usually takes a long time and many resources to complete. It is a good idea to set up checkpointing so the calculation can keep going in case of any interruption due to walltime limit or possible system malfunction. Checkpointing function can save a snapshot of a Gaussian running state so it can restart from the previous calculation. Users can also divide a long-time job into many 4-hour short jobs since jobs with walltime less than or equal to 4 hours can use the buy-in nodes (55% of all nodes) in HPCC. In order to have an appropriate checkpointing run with Gaussian, an unified read-write file setting ( %RWF ) should be in the Link 0 section of the input file. An example water.gjf is in the following: water.gjf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 %NProcShared = 2 %Mem = 3GB %RWF = water.rwf %NoSave %chk = water.chk #P opt b3lyp/aug-cc-pVTZ water molecules 0 1 O -2.12123400 1 .99409800 -1.27381200 H 1 .52438600 0 .53672100 0 .67508800 H 1 .76493000 -0.81527300 -0.18137000 O -1.12977500 -0.31430400 -0.37860700 H -1.76492800 -0.81528500 0 .18137200 H -1.52439700 0 .53670800 -0.67510100 O 2 .89125300 -1.69896600 -1.06351900 O 1 .12976700 -0.31428900 0 .37859300 H 2 .99568600 -1.73945400 -2.01677200 H 3 .39746100 -2.42787400 -0.69708600 O -2.89123000 -1.69896400 1 .06353700 H -2.99563400 -1.73945600 2 .01679300 H -2.43456700 2 .07972500 -2.17761600 H -2.58174600 2 .66131900 -0.75942800 H -3.39743400 -2.42788000 0 .69711700 The input file requests geometry optimization of 5 water molecules with a very large basis set aug-cc-pVTZ. It will take about 25 CPU hours to finish the whole calculation. We have the setting on %RWF which specifies water.rwf file for checkpointing function besides water.chk file. Since the specification %RWF is placed before %NoSave line, the rwf file will be deleted if the calculation is normally completed without any error. In order to have several restart running after first running stops, we can build a restart Gaussian input file restart.gjf simply as restart.gjf 1 2 3 4 5 6 %NProcShared = 2 %Mem = 3GB %RWF = waters.rwf %NoSave %chk = waters.chk #P Restart Since all information about the calculation is recorded in the rwf file, a line with \"Restart\" is enough for Gaussian to restart from the previous job. This restart input file can also be created by the commands: 1 2 $ grep '^%' waters.gjf > restart.gjf $ echo -e '#P Restart\\n' >> restart.gjf where we simply \" grep \" the lines starting with \" % \" sign in water.gjf and put them to the Gaussian restart file with \" #P Restart \" line in the end. Now we need a job script to submit the Gaussian calculation. The script needs to keep submitting jobs to restart the previous calculation until it is completed. Here is a job script water.sb which can do the work: water.sb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #SBATCH \u2013-job-name=LongJob #SBATCH \u2013-ntasks=1 #SBATCH \u2013-cpus-per-task=2 #SBATCH --mem=5G #SBATCH \u2013-time=04:00:00 module load Gaussian/g16 powertools OutputFile = \"water- ${ SLURM_JOBID } .log\" # Gaussian output file name for each job # How many seconds before end of job to submit another BeforeEnd = 300 # 5 minutes # The background script to keep job submission until calculation is completed ( sleep $(( 4 * 60 * 60 - BeforeEnd )) # sleep until the time before end of job js -j ${ SLURM_JOBID } # print out resource usage cat ${ OutputFile } >> water.log # collect Gaussian outputs into one file echo -e \"\\n\\n====== Gaussian calculation on job ${ SLURM_JOBID } stops. ======\\n\\n\" >> water.log echo \"The Gaussian calculation has not completed. Submit another job to keep doing it.\" sbatch water.sb # submit another job scancel ${ SLURM_JOBID } ) & # job stops if g16 command is not finished # Whether this is a restart job or not if [ -f water.rwf ] && [ -f water.chk ] ; then InputFile = \"restart.gjf\" else InputFile = \"water.gjf\" fi g16 < ${ InputFile } > ${ OutputFile } # The following commands are not executed unless g16 command is completed. # Print out resource usage js -j $SLURM_JOB_ID ### powetools command cat ${ OutputFile } >> water.log echo -e \"\\n\\n====== Gaussian calculation is completed on job ${ SLURM_JOBID } . ======\\n\\n\" >> water.log where a background script in (---)& from line 14 to 20 is added to keep submitting job s . Once the job is started, the background script is running at the same time as the foreground script. The background script is in sleep for 3 hours and 55 minutes first. During this time, the foreground script runs the Gaussian calculation or restart the previous calculation if the checkpointing files ( water.rwf and water.chk ) exist. After 5 minutes before the end of the job, the background is awake to print out the resource usage and Gaussian output. It submits another job and stop the current running job in line 19 and 20 if the g16 command in line 29 is not completed. If the g16 command is finished before the background is awake, the job will keep executing all command lines after line 30 and finish. There will be no more job submitted. Since the rwf file usually takes a lot of file space, it is suggested to run checkpointing jobs in scratch space in case your home or research space is over quota. Users can create a directory in their scratch space. Copy all files ( water,gjf , restart.gjf and water.sb ) and submit their job script there. Please check your job running frequently. Make sure to copy necessary files back to home or research directory from time to time since files on scratch will be purged if they have not been modified for 45 days.","title":"Gaussian Job with Checkpointing Run"},{"location":"Gaussian_on_HPCC/","text":"Gaussian on HPCC Here is the slides pdf for the Gaussian workshop.","title":"Gaussian workshop slides"},{"location":"Gaussian_on_HPCC/#gaussian-on-hpcc","text":"Here is the slides pdf for the Gaussian workshop.","title":"Gaussian on HPCC"},{"location":"Guidelines_for_Choosing_File_Storage_and_I_O/","text":"Guidelines for Choosing File Storage and I/O HOME, RESEARCH and SCRATCH are referred to as networked file systems. Each node must go through the network switch to access these spaces. /tmp and /mnt/local are locally accessible in the hard drive of each node. The space is not affected by the network and has larger size compared with the RAMDISK /dev/shm which is located inside the node\u2019s RAM. However, /dev/shm is the closest storage location for files. Files stored here take up some of the node\u2019s memory space. The table below provides detailed information about each type of storage on HPCC. ($USER is your login username and GROUP is your research group name). Please use the table below to choose which file system is best for your job. The two columns from the left are the locations with system automatic backup. The three columns from the right are the locations with system automatic purge. The column in between shows the location where it is often considered and treated by users who requested it as the same as HOME or RESEARCH space, but it is NOT automatically backuped! HOME RESEARCH nodr portion of HOME/RESEARCH SCRATCH LOCAL RAMDISK Primary Private files or data storage for each user Shared files or data storage for group users same as the standard HOME/RESEARCH Temporary large files or data storage for users and groups Temporary small files or data usage for job running same as LOCAL with very fast I/O Access location Automatic login $HOME or /mnt/home/$USER /mnt/research/GROUP /mnt/ufs18/nodr $SCRATCH or /mnt/scratch/$USER /mnt/local or /tmp (at each node) $TMPDIR (used in a job as /tmp/local/$SLURM_JOBID) /dev/shm (at each node) Size 50GB upto 1TB, 1 million files, ($125/year for each additional TB) upto 1TB and 1 million files ($125/year for each additional TB) as a portion of HOME or RESEARCH by user's request. No limit on the number of files 50TB and 1 million files ~400GB for intel14, ~170GB for intel16, ~400GB for intel18 \u00bd of RAM Command to check quota quota quota quota quota #SBATCH --tmp=20gb to reserve 20gb in $TMPDIR. \u00bd of the memory requested by job Backup Yes Yes No No No No Purge policy No No No Yes. (Files not accessed or modified for more than 45 days may be removed) Yes (at completion of job) Yes (RAM may be reused by other jobs) I/O Best Practice low I/O using single or multiple nodes Same as HOME same as HOME or RESEARCH heavy I/O on files of large size using single or multiple nodes frequent I/O operations on many files in one node frequent and fast I/O operations on small files in one node Careful with Watch for quota. Avoid heavy parallel I/O. Same as HOME. In addition, need to set umask or file permission so files can be shared in group. Be aware of no automatic backup. May need to do backup manually by user. Avoid frequent I/O on many small files(< 1MB), such as untarring a tar file to create many small files in a short time. Move files to HOME or RESEARCH before purge period elapses. Need to copy or move files to HOME or RESEARCH before job completes.Only local access available. Users are not able to store files in one node and gain I/O access to them from other nodes. Same as LOCAL. Request extra memory in your job script so you'll have enough space for file storage.","title":"Guidelines for choosing file systems and I/O"},{"location":"Guidelines_for_Choosing_File_Storage_and_I_O/#guidelines-for-choosing-file-storage-and-io","text":"HOME, RESEARCH and SCRATCH are referred to as networked file systems. Each node must go through the network switch to access these spaces. /tmp and /mnt/local are locally accessible in the hard drive of each node. The space is not affected by the network and has larger size compared with the RAMDISK /dev/shm which is located inside the node\u2019s RAM. However, /dev/shm is the closest storage location for files. Files stored here take up some of the node\u2019s memory space. The table below provides detailed information about each type of storage on HPCC. ($USER is your login username and GROUP is your research group name). Please use the table below to choose which file system is best for your job. The two columns from the left are the locations with system automatic backup. The three columns from the right are the locations with system automatic purge. The column in between shows the location where it is often considered and treated by users who requested it as the same as HOME or RESEARCH space, but it is NOT automatically backuped! HOME RESEARCH nodr portion of HOME/RESEARCH SCRATCH LOCAL RAMDISK Primary Private files or data storage for each user Shared files or data storage for group users same as the standard HOME/RESEARCH Temporary large files or data storage for users and groups Temporary small files or data usage for job running same as LOCAL with very fast I/O Access location Automatic login $HOME or /mnt/home/$USER /mnt/research/GROUP /mnt/ufs18/nodr $SCRATCH or /mnt/scratch/$USER /mnt/local or /tmp (at each node) $TMPDIR (used in a job as /tmp/local/$SLURM_JOBID) /dev/shm (at each node) Size 50GB upto 1TB, 1 million files, ($125/year for each additional TB) upto 1TB and 1 million files ($125/year for each additional TB) as a portion of HOME or RESEARCH by user's request. No limit on the number of files 50TB and 1 million files ~400GB for intel14, ~170GB for intel16, ~400GB for intel18 \u00bd of RAM Command to check quota quota quota quota quota #SBATCH --tmp=20gb to reserve 20gb in $TMPDIR. \u00bd of the memory requested by job Backup Yes Yes No No No No Purge policy No No No Yes. (Files not accessed or modified for more than 45 days may be removed) Yes (at completion of job) Yes (RAM may be reused by other jobs) I/O Best Practice low I/O using single or multiple nodes Same as HOME same as HOME or RESEARCH heavy I/O on files of large size using single or multiple nodes frequent I/O operations on many files in one node frequent and fast I/O operations on small files in one node Careful with Watch for quota. Avoid heavy parallel I/O. Same as HOME. In addition, need to set umask or file permission so files can be shared in group. Be aware of no automatic backup. May need to do backup manually by user. Avoid frequent I/O on many small files(< 1MB), such as untarring a tar file to create many small files in a short time. Move files to HOME or RESEARCH before purge period elapses. Need to copy or move files to HOME or RESEARCH before job completes.Only local access available. Users are not able to store files in one node and gain I/O access to them from other nodes. Same as LOCAL. Request extra memory in your job script so you'll have enough space for file storage.","title":"Guidelines for Choosing File Storage and I/O"},{"location":"HPCC-Job-Submission-Workflow_40337480.html/","text":"Teaching : HPCC Job Submission Workflow Goal: Understand basic workflow for submitting jobs to the clusters. Task: Run hello.c on one core and one compute node. Login to HPCC Load the powertools module. Create a copy the helloworld getexample in your current directory. Change into the helloworld directory created. View the submission script in the helloworld directory. Compile the hello C program on the development node. Submit the script to the SLURM scheduler. Check the queue for your job. Examine your job Check the output of your job View the output of your job Answer Expand source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 #Login to HPCC ssh -XY msu_netid@hpcc.msu.edu ssh dev-intel18 #Load powertools module module load powertools #Copy the helloworld getexample getexample helloworld ls -l #Change into the helloworld directory cd helloworld ls -l #View the submission script in the helloworld directory cat hello.sb #Compile the C program <hello> on the development node gcc hello.c -o hello #Submit the jobscript hello.sb to the SLURM scheduler sbatch hello.sb #Check the queue for your job qstat \u2013u $USER #Examine your job scontrol <job_id> #View the output of your job less <slurm-jobid.out>","title":"HPCC Job Submission Workflow 40337480.html"},{"location":"HPCC-Job-Submission-Workflow_40337480.html/#teaching-hpcc-job-submission-workflow","text":"Goal: Understand basic workflow for submitting jobs to the clusters. Task: Run hello.c on one core and one compute node. Login to HPCC Load the powertools module. Create a copy the helloworld getexample in your current directory. Change into the helloworld directory created. View the submission script in the helloworld directory. Compile the hello C program on the development node. Submit the script to the SLURM scheduler. Check the queue for your job. Examine your job Check the output of your job View the output of your job Answer Expand source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 #Login to HPCC ssh -XY msu_netid@hpcc.msu.edu ssh dev-intel18 #Load powertools module module load powertools #Copy the helloworld getexample getexample helloworld ls -l #Change into the helloworld directory cd helloworld ls -l #View the submission script in the helloworld directory cat hello.sb #Compile the C program <hello> on the development node gcc hello.c -o hello #Submit the jobscript hello.sb to the SLURM scheduler sbatch hello.sb #Check the queue for your job qstat \u2013u $USER #Examine your job scontrol <job_id> #View the output of your job less <slurm-jobid.out>","title":"Teaching : HPCC Job Submission Workflow"},{"location":"HPCC_File_Systems_Overview/","text":"HPCC File Systems Overview HPCC provides a variety of secure file storage options for research data and fast connections for high-speed file communication (I/O). Users have access to replicated high capacity storage that can be shared among group members or remain private to each user. Additionally, high performance storage is available for temporarily staging data that needs to be access quickly. Moreover, each compute node also has an attached local disk for data intensive jobs. Storage options include: ufs18: Home Directory All users are given a home directory to store your research located /mnt/home/$USER on every node, where the environment variable $USER stores the login account name. Each new user starts with a 50 GB limit for file storage space and each space can contains less than 1 million files. You can request and increase this up to 1TB by completing the Quota Increase Request form. Storage space greater than 1TB is available for an annual fee, paid through a MSU financial account. Home directories are automatically backed-up except the files saved in nodr space. To access file backup, please submit a ticket. Please mention the paths of the folders or filenames with the time frame you would like to be restored. For more information about checking quota and increasing limit on number of files, please see ufs18 file system . ufs18: Research Space A 1 TB block of space to be shared among members of a research group may be obtained if a MSU PI can provide adequate justification. Additional space may also be purchased . Files on this system, located at /mnt/research/[groupname], are also backed up (except the files saved in nodr space). Both /mnt/home and /mnt/research are stored in /mnt/ufs18. To understand the quota setting on research space, please refer to ufs18 file system . Local Disk: /tmp, /mnt/local & /dev/shm Each cluster node has it's own disk drive, called the \"local\" disk, and between 100-300 GB of \"scratch\" space is available. This is local to each node, not networked and hence is not accessible from other nodes (e.g. for multi-node jobs). This space is transient, volatile storage optimized for smaller-scale I/O. It is available as a folder /mnt/local in which you can create folders during your job, but also as a special folder created for the job and in the $TMPDIR environment variable. Files in $TMPDIR are only available during the job, so your script must copy files before the job ends. Files may be stored for a maximum of 8 days on /mnt/local. This space is regularly and routinely erased to ensure a maximum amount of free space for users. gs18 & ls15: Scratch Space In addition to home and research files, we have file systems for temporary work that we call \"scratch\" available to users. Two Scratch File Systems are installed in the HPC system. gs18 is based on the IBM General Parallel File System (GPFS) and located at /mnt/gs18 with a capacity of 834 TB. ls15 uses Lustre technology and locates at /mnt/ls15 with a capacity of 1.9 PB. Research groups can also have their scratch folders available. If you would like a folder for your research group please request one. The parallel file systems are designed for heavy reading and writing of large files (I/O). Both of them are connected to all compute nodes with infiniband and can sustain very high data transfer rates. For many types of jobs, it's much faster than using your home or research folders. However, jobs that read and write many small files are not suitable for ls15 system. For those type of jobs you can explore using $TMPDIR (/mnt/local) or /mnt/ffs17 (Flash Drive). Please contact us for more information if you find the Scratch file system is too slow for your work. Unlike the home and research directories, the scratch spaces are temporary file systems and not intended for long-term storage so there is no back-up . Files are automatically purged after 45 days if they are not modified. The quota of the space for each user is limited to total 50TB and less than 1 Million files. Because the scratch space is intended for job running, it cannot be accessed from the gateway node, only the development nodes and file transfer gateway. That is, after you log in to the gateway (ssh hpcc.msu.edu ) and then try to access scratch you will receive an error \"No such file or directory.\" To access scratch, please connect to a development node (e.g. ssh dev-intel16). Your scratch folder is also available to all of the compute nodes and hence your jobs. To be able to transfer files on your scratch folder to/from hpcc, you may use our file transfer gateway, which has the host name **rsync.hpcc.msu.edu. See Transferring Files to the HPCC for more information. Exceeding your quota will 'lock out' your directory and you will not be able to delete or create new files in your folder. To request additional space visit: https://contact.icer.msu.edu/contact . fs-07: /opt/software & /opt/modules This is where HPCC installs application software (binaries and code) and module files. It is mostly readable and executable for all users. Additional Storage Options For MSU researchers, up to 1TB of secure storage can be allocated per PI for free. Backup of data files are stored off-site. Additional storage can be purchased at an annual rate of $125/TB. To make a storage increase request greater > 1TB, please complete the Increase Digital File Storage Form . To find out how to use the above file systems and which one to choose. please refer to the file systems page .","title":"HPCC File Systems Overview"},{"location":"HPCC_File_Systems_Overview/#hpcc-file-systems-overview","text":"HPCC provides a variety of secure file storage options for research data and fast connections for high-speed file communication (I/O). Users have access to replicated high capacity storage that can be shared among group members or remain private to each user. Additionally, high performance storage is available for temporarily staging data that needs to be access quickly. Moreover, each compute node also has an attached local disk for data intensive jobs. Storage options include:","title":"HPCC File Systems Overview"},{"location":"HPCC_File_Systems_Overview/#ufs18-home-directory","text":"All users are given a home directory to store your research located /mnt/home/$USER on every node, where the environment variable $USER stores the login account name. Each new user starts with a 50 GB limit for file storage space and each space can contains less than 1 million files. You can request and increase this up to 1TB by completing the Quota Increase Request form. Storage space greater than 1TB is available for an annual fee, paid through a MSU financial account. Home directories are automatically backed-up except the files saved in nodr space. To access file backup, please submit a ticket. Please mention the paths of the folders or filenames with the time frame you would like to be restored. For more information about checking quota and increasing limit on number of files, please see ufs18 file system .","title":"ufs18: Home Directory"},{"location":"HPCC_File_Systems_Overview/#ufs18-research-space","text":"A 1 TB block of space to be shared among members of a research group may be obtained if a MSU PI can provide adequate justification. Additional space may also be purchased . Files on this system, located at /mnt/research/[groupname], are also backed up (except the files saved in nodr space). Both /mnt/home and /mnt/research are stored in /mnt/ufs18. To understand the quota setting on research space, please refer to ufs18 file system .","title":"ufs18: Research Space"},{"location":"HPCC_File_Systems_Overview/#local-disk-tmp-mntlocal-devshm","text":"Each cluster node has it's own disk drive, called the \"local\" disk, and between 100-300 GB of \"scratch\" space is available. This is local to each node, not networked and hence is not accessible from other nodes (e.g. for multi-node jobs). This space is transient, volatile storage optimized for smaller-scale I/O. It is available as a folder /mnt/local in which you can create folders during your job, but also as a special folder created for the job and in the $TMPDIR environment variable. Files in $TMPDIR are only available during the job, so your script must copy files before the job ends. Files may be stored for a maximum of 8 days on /mnt/local. This space is regularly and routinely erased to ensure a maximum amount of free space for users.","title":"Local Disk: /tmp, /mnt/local &amp; /dev/shm"},{"location":"HPCC_File_Systems_Overview/#gs18-ls15-scratch-space","text":"In addition to home and research files, we have file systems for temporary work that we call \"scratch\" available to users. Two Scratch File Systems are installed in the HPC system. gs18 is based on the IBM General Parallel File System (GPFS) and located at /mnt/gs18 with a capacity of 834 TB. ls15 uses Lustre technology and locates at /mnt/ls15 with a capacity of 1.9 PB. Research groups can also have their scratch folders available. If you would like a folder for your research group please request one. The parallel file systems are designed for heavy reading and writing of large files (I/O). Both of them are connected to all compute nodes with infiniband and can sustain very high data transfer rates. For many types of jobs, it's much faster than using your home or research folders. However, jobs that read and write many small files are not suitable for ls15 system. For those type of jobs you can explore using $TMPDIR (/mnt/local) or /mnt/ffs17 (Flash Drive). Please contact us for more information if you find the Scratch file system is too slow for your work. Unlike the home and research directories, the scratch spaces are temporary file systems and not intended for long-term storage so there is no back-up . Files are automatically purged after 45 days if they are not modified. The quota of the space for each user is limited to total 50TB and less than 1 Million files. Because the scratch space is intended for job running, it cannot be accessed from the gateway node, only the development nodes and file transfer gateway. That is, after you log in to the gateway (ssh hpcc.msu.edu ) and then try to access scratch you will receive an error \"No such file or directory.\" To access scratch, please connect to a development node (e.g. ssh dev-intel16). Your scratch folder is also available to all of the compute nodes and hence your jobs. To be able to transfer files on your scratch folder to/from hpcc, you may use our file transfer gateway, which has the host name **rsync.hpcc.msu.edu. See Transferring Files to the HPCC for more information. Exceeding your quota will 'lock out' your directory and you will not be able to delete or create new files in your folder. To request additional space visit: https://contact.icer.msu.edu/contact .","title":"gs18 &amp; ls15: Scratch Space"},{"location":"HPCC_File_Systems_Overview/#fs-07-optsoftware-optmodules","text":"This is where HPCC installs application software (binaries and code) and module files. It is mostly readable and executable for all users.","title":"fs-07: /opt/software &amp; /opt/modules"},{"location":"HPCC_File_Systems_Overview/#additional-storage-options","text":"For MSU researchers, up to 1TB of secure storage can be allocated per PI for free. Backup of data files are stored off-site. Additional storage can be purchased at an annual rate of $125/TB. To make a storage increase request greater > 1TB, please complete the Increase Digital File Storage Form . To find out how to use the above file systems and which one to choose. please refer to the file systems page .","title":"Additional Storage Options"},{"location":"HPC_Concept/","text":"HPC Concept Why Use HPCC A comparison between your personal computer (PC) and ICER's HPCC Parallel Computing An introduction to three basic parallel models: Shared Memory, Distributed Memory and Hybrid HPC Glossary Definitions for commonly used HPC terminology","title":"HPC concepts"},{"location":"HPC_Concept/#hpc-concept","text":"","title":"HPC Concept"},{"location":"HPC_Concept/#why-use-hpcc","text":"A comparison between your personal computer (PC) and ICER's HPCC","title":"Why Use HPCC"},{"location":"HPC_Concept/#parallel-computing","text":"An introduction to three basic parallel models: Shared Memory, Distributed Memory and Hybrid","title":"Parallel Computing"},{"location":"HPC_Concept/#hpc-glossary","text":"Definitions for commonly used HPC terminology","title":"HPC Glossary"},{"location":"HPC_Glossary/","text":"HPC Glossary Program \u2013 code stored on a computer intended to fulfill a certain task There are many types of programs: Part of the operating system and help computer function Fulfill a particular job are called applications Typically stored on disk ( g. , hard drive) A program needs memory and various operating system resources ( g., peripheral interfaces) to run Operating System \u2013 manages all resources needed for a program ( e.g. , macOS) Process \u2013 program with all necessary resources loaded into memory When a program is run, it is loaded into memory which makes it accessible for processing by the computer\u2019s central processing unit (CPU) There can be multiple instances of a single program, and each instance of that running program is a process Each process has a separate memory address space, which means that a process runs independently and is isolated from other processes This independence of processes is valuable so that a problem with one process cannot cause havoc with another process Central processing unit (CPU) - logical hardware unit capable of executing a single process ( i.e. , gets instructions then performs calculations) Made up of: Processor is a device that processes program instructions to manipulate data Socket is an array of pins that connect processor to motherboard Individual CPU processors now contain multiple cores for more efficient multi-tasking and parallel computing Core is the smallest hardware unit capable of performing a processing task Ex: dual-core processor has two cores Thread (of execution) is the smallest set of programmed instructions that can be managed independently by an operation system. In general, one thread is handled by one core. As video gaming popularity increased, so did the need for more computing power. To accomplish this a CPU can work with a graphics processing unit (GPU) , usually found on a graphics card docked into the motherboard, to quickly render high-resolution images and video concurrently. A GPU gets its intense computing power from hundreds of smaller cores capable of crunch application data in parallel. Multiple GPUs can be installed on one graphics card or multiple graphics card can be installed in one node to further improve computation power through parallelism. After GPUs became popular for gaming, they were made fully programmable to be useful in processing big science data. The resulting general-purpose graphics processing unit (GPGPU) is used extensively in supercomputing to increase speed and improve analysis of scientific data. Node is a single computer comprised of 1+ CPUs, memory, network interfaces, etc. Cluster is a group of nodes networked together so that a program can run on them in parallel Parallel computing is an umbrella term describing the use of multiple computers or computers made up of multiple processors in combination to solve a single problem Within HPCC there are different types of nodes: Gateway nodes are nodes used to enter the computer system: Login - login and non-intensive compute tasks ( e. , moving files) rsync - data transfer to/from HPCC Development (dev) nodes are nodes used to navigate file systems and compile, test, and schedule heavy computational tasks ( e. , jobs) Compute nodes are clusters that perform scheduled jobs Accelerator nodes are nodes equipped with accelerated cards ( g. , GPU or phi nodes) Secure Shell (SSH) - network protocols and implementing suite of utilities that provide a secure way to access and execute commands on a remote computer over an unsecured network Remote synchronization (rsync) - software utility for Linux systems that efficiently sync files and directories between two hosts or machines making ideal for transferring large files File system \u2013 tree-like directory organization for storing many files For more information on these terms, check out the following videos: What is ICER\u2019s HPCC (11min) HPCC System Layout (7min)","title":"HPC Glossary"},{"location":"HPC_Glossary/#hpc-glossary","text":"Program \u2013 code stored on a computer intended to fulfill a certain task There are many types of programs: Part of the operating system and help computer function Fulfill a particular job are called applications Typically stored on disk ( g. , hard drive) A program needs memory and various operating system resources ( g., peripheral interfaces) to run Operating System \u2013 manages all resources needed for a program ( e.g. , macOS) Process \u2013 program with all necessary resources loaded into memory When a program is run, it is loaded into memory which makes it accessible for processing by the computer\u2019s central processing unit (CPU) There can be multiple instances of a single program, and each instance of that running program is a process Each process has a separate memory address space, which means that a process runs independently and is isolated from other processes This independence of processes is valuable so that a problem with one process cannot cause havoc with another process Central processing unit (CPU) - logical hardware unit capable of executing a single process ( i.e. , gets instructions then performs calculations) Made up of: Processor is a device that processes program instructions to manipulate data Socket is an array of pins that connect processor to motherboard Individual CPU processors now contain multiple cores for more efficient multi-tasking and parallel computing Core is the smallest hardware unit capable of performing a processing task Ex: dual-core processor has two cores Thread (of execution) is the smallest set of programmed instructions that can be managed independently by an operation system. In general, one thread is handled by one core. As video gaming popularity increased, so did the need for more computing power. To accomplish this a CPU can work with a graphics processing unit (GPU) , usually found on a graphics card docked into the motherboard, to quickly render high-resolution images and video concurrently. A GPU gets its intense computing power from hundreds of smaller cores capable of crunch application data in parallel. Multiple GPUs can be installed on one graphics card or multiple graphics card can be installed in one node to further improve computation power through parallelism. After GPUs became popular for gaming, they were made fully programmable to be useful in processing big science data. The resulting general-purpose graphics processing unit (GPGPU) is used extensively in supercomputing to increase speed and improve analysis of scientific data. Node is a single computer comprised of 1+ CPUs, memory, network interfaces, etc. Cluster is a group of nodes networked together so that a program can run on them in parallel Parallel computing is an umbrella term describing the use of multiple computers or computers made up of multiple processors in combination to solve a single problem Within HPCC there are different types of nodes: Gateway nodes are nodes used to enter the computer system: Login - login and non-intensive compute tasks ( e. , moving files) rsync - data transfer to/from HPCC Development (dev) nodes are nodes used to navigate file systems and compile, test, and schedule heavy computational tasks ( e. , jobs) Compute nodes are clusters that perform scheduled jobs Accelerator nodes are nodes equipped with accelerated cards ( g. , GPU or phi nodes) Secure Shell (SSH) - network protocols and implementing suite of utilities that provide a secure way to access and execute commands on a remote computer over an unsecured network Remote synchronization (rsync) - software utility for Linux systems that efficiently sync files and directories between two hosts or machines making ideal for transferring large files File system \u2013 tree-like directory organization for storing many files","title":"HPC Glossary"},{"location":"HPC_Glossary/#for-more-information-on-these-terms-check-out-the-following-videos","text":"What is ICER\u2019s HPCC (11min) HPCC System Layout (7min)","title":"For more information on these terms, check out the following videos:"},{"location":"HPC_Tutorial_Series/","text":"HPC Tutorial Series Connect over SSH with editors Containers (Docker and Singularity) Learning the Shell Linux Command Line Interface for Beginners I Linux Command Line Interface for Beginners II Makefile Python on HPC Regular Expressions Using DDD","title":"HPC Tutorial Series"},{"location":"HPC_Tutorial_Series/#hpc-tutorial-series","text":"","title":"HPC Tutorial Series"},{"location":"HPC_Tutorial_Series/#connect-over-ssh-with-editors","text":"","title":"Connect over SSH with editors"},{"location":"HPC_Tutorial_Series/#containers-docker-and-singularity","text":"","title":"Containers (Docker and Singularity)"},{"location":"HPC_Tutorial_Series/#learning-the-shell","text":"","title":"Learning the Shell"},{"location":"HPC_Tutorial_Series/#linux-command-line-interface-for-beginners-i","text":"","title":"Linux Command Line Interface for Beginners I"},{"location":"HPC_Tutorial_Series/#linux-command-line-interface-for-beginners-ii","text":"","title":"Linux Command Line Interface for Beginners II"},{"location":"HPC_Tutorial_Series/#makefile","text":"","title":"Makefile"},{"location":"HPC_Tutorial_Series/#python-on-hpc","text":"","title":"Python on HPC"},{"location":"HPC_Tutorial_Series/#regular-expressions","text":"","title":"Regular Expressions"},{"location":"HPC_Tutorial_Series/#using-ddd","text":"","title":"Using DDD"},{"location":"HPC_s_entire_layout_at_ICER/","text":"HPC's entire layout at ICER Users can use MSU's HPCC resources by first connecting to gateway gateway.hpcc.msu.edu gateway and rsync are the only two nodes directly accessible to the internet. \"gateway\" is not meant for running software, connecting to scratch space or compute nodes. \"rsync\" is majorly for file transfer and able to connect to scratch but not able to access to compute nodes. From the gateway node, users can connect to any development node to compile their jobs or and run short tests. Each development node runs the same operating system and The AMD64/Intel64 instruction set. Compiled software should be \"binary compatible\" across the cluster, if you do not compile with architecture-specific tuning (e.g. -march or -x.) For example, if you compile your code on dev-intel14, it can mostly run on the intel16 cluster. A detailed description of our hardware can be found on the page of 2018 Cluster Resources .","title":"HPCC layout"},{"location":"HPC_s_entire_layout_at_ICER/#hpcs-entire-layout-at-icer","text":"Users can use MSU's HPCC resources by first connecting to gateway gateway.hpcc.msu.edu gateway and rsync are the only two nodes directly accessible to the internet. \"gateway\" is not meant for running software, connecting to scratch space or compute nodes. \"rsync\" is majorly for file transfer and able to connect to scratch but not able to access to compute nodes. From the gateway node, users can connect to any development node to compile their jobs or and run short tests. Each development node runs the same operating system and The AMD64/Intel64 instruction set. Compiled software should be \"binary compatible\" across the cluster, if you do not compile with architecture-specific tuning (e.g. -march or -x.) For example, if you compile your code on dev-intel14, it can mostly run on the intel16 cluster. A detailed description of our hardware can be found on the page of 2018 Cluster Resources .","title":"HPC's entire layout at ICER"},{"location":"HTSeq%202/","text":"HTSeq HTSeq is a Python package for analysis of high-throughput sequencing data. It's available upon a fresh login to a dev-node. For example, you can call command htseq-count directly without needing to load specific modules. This is because HTSeq is installed to the default Python distribution (you can run module list to see the list of modules loaded by default). If you happen to run a module purge command in the middle of your work, you need to load the default Python back again in order to use HTSeq. To do so: 1 2 module purge module load GCC/6.4.0-2.28 OpenMPI/2.1.2 Python/3.6.4 Tip: if you want to go back to the original shell environment immediately after your login, you can run: exec bash -l","title":"HTSeq"},{"location":"HTSeq%202/#htseq","text":"HTSeq is a Python package for analysis of high-throughput sequencing data. It's available upon a fresh login to a dev-node. For example, you can call command htseq-count directly without needing to load specific modules. This is because HTSeq is installed to the default Python distribution (you can run module list to see the list of modules loaded by default). If you happen to run a module purge command in the middle of your work, you need to load the default Python back again in order to use HTSeq. To do so: 1 2 module purge module load GCC/6.4.0-2.28 OpenMPI/2.1.2 Python/3.6.4 Tip: if you want to go back to the original shell environment immediately after your login, you can run: exec bash -l","title":"HTSeq"},{"location":"HTSeq/","text":"HTSeq HTSeq is a Python package for analysis of high-throughput sequencing data. It's available upon a fresh login to a dev-node. For example, you can call command htseq-count directly without needing to load specific modules. This is because HTSeq is installed to the default Python distribution (you can run module list to see the list of modules loaded by default). If you happen to run a module purge command in the middle of your work, you need to load the default Python back again in order to use HTSeq. To do so: 1 2 module purge module load GCC/6.4.0-2.28 OpenMPI/2.1.2 Python/3.6.4 Tip: if you want to go back to the original shell environment immediately after your login, you can run: exec bash -l","title":"HTSeq"},{"location":"HTSeq/#htseq","text":"HTSeq is a Python package for analysis of high-throughput sequencing data. It's available upon a fresh login to a dev-node. For example, you can call command htseq-count directly without needing to load specific modules. This is because HTSeq is installed to the default Python distribution (you can run module list to see the list of modules loaded by default). If you happen to run a module purge command in the middle of your work, you need to load the default Python back again in order to use HTSeq. To do so: 1 2 module purge module load GCC/6.4.0-2.28 OpenMPI/2.1.2 Python/3.6.4 Tip: if you want to go back to the original shell environment immediately after your login, you can run: exec bash -l","title":"HTSeq"},{"location":"Home_Space/","text":"Home Space Each user account is given a home space for personal file storage. By default, It is set only accessible to the user and located at /mnt/home/$USER for every node, where $USER is the environment variable of the user's login name. It is often referred as \u201chome directory\u201d since this is the beginning directory after login of any HPCC node. Every home space starts with a 50 GB limit for file storage space and can not contain more than 1 million files. You can request and increase this up to 1TB by completing Quota Increase Request form. Storage space greater than 1TB is available for an annual fee paid through a MSU financial account. Users can find the fee and submit their request by Large Quota Increase Request form. To find out the quota and used space of your home directory, please check Space quota section. If you would like to store more than 1 million files in your home space, please refer to the following section Limit on number of files . All home directories are stored in the IBM General Parallel File System (GPFS) mounted on /mnt/ufs18. It is automatically backed-up except files saved in nodr space. To restore any file from backup, please submit a ticket and let us know the paths to the files or the directory with the time frame you would like them restored. To learn more information, please check the documentation of ufs18 file system . For the system security and user data privacy, we recommend that users do NOT open HOME directory access permission to others. When you report an issue about files saved in HOME, please attach them to your message for reference. ICER staffs do not access any files or directories in your home directory. Incorrect Used Space Currently our home file system check quota function will sometimes cause a user's directory over the quota due to incorrect calculation of used space . If you see this please open a ticket and we will work with you to temporarily increase your quota. We continue to work with our vendor to correct this issue. Space quota The only way to get quota information of home space is to run the command quota : 1 2 3 4 5 $ quota home directory: Space Space Space Space Files Files Files Files Quota Used Available % Used Quota Used Available % Used ----------------------------------------------------------------------------------------------- /mnt/home/ $USER 50G 32G 18G 64 % 1048576 432525 616051 59 % where all file spaces accessible to the user are listed, including home, research, scratch and ffs17. In each space, the information of quota, usage and availablility on space size and number of files can be found. If \"Free\" or \"Available\" is a negative value (such as \"Space Available\" in home directory of the above example), the usage is over the quota. Please remove, transfer or compress some files so the used space or the file count can be lower than the \"Quota\" value. Since GPFS uses a different compression algorithm, you may notice higher space size after files are copied to the ufs18 file system. Actual disk usage (du) different from quota results The new file system has a smallest file block size of 64k. This means that files between 2K and 64K will occupy 64K of space. This causes space usage inflated greatly for users with large amounts of small files. One suggested solution would be to compress many small files into one large file (larger than 64K). in this way, the number of files can also be reduced. If you still have any difficulty, a temporarily larger quota can be requested if your quota is at 1T with many small files. Limit on number of files Besides the quota on the size of space, users are also limited to 1 million files in their home or research directory. We need to set this limit because, with a great number of files, the file system will spend too much time on back-up to be able to function normally. If possible, users can compress many files into one to reduce the file number. If users do not wish to have the limit, they can request to have part of their home (or research space) moved under nodr space ( /mnt/ufs18/nodr/ or /mnt/ufs18/nodr/research/ ) where there is no limit on the file count yet no back-up on the files either. Users will be responsible for their own back-up in the nodr space. By default, one half of the space quota is assigned to the requested nodr space. The quota of the original space under /mnt/home/ (or /mnt/research/ ) is then donwsized to its half so the total space quota remains the same. A different size of nodr space can also be assigned according to user's request. Once this space is created, the path and the quota information can be found by the quota command mentioned above. All home space files are periodically, automatically backed up (except those files that a user has opted to store in a specially requested nodr space). To access file backups, please submit a help ticket containing the file paths and the period i.e. , the time frame, from which the files should be restored.","title":"Home space"},{"location":"Home_Space/#home-space","text":"Each user account is given a home space for personal file storage. By default, It is set only accessible to the user and located at /mnt/home/$USER for every node, where $USER is the environment variable of the user's login name. It is often referred as \u201chome directory\u201d since this is the beginning directory after login of any HPCC node. Every home space starts with a 50 GB limit for file storage space and can not contain more than 1 million files. You can request and increase this up to 1TB by completing Quota Increase Request form. Storage space greater than 1TB is available for an annual fee paid through a MSU financial account. Users can find the fee and submit their request by Large Quota Increase Request form. To find out the quota and used space of your home directory, please check Space quota section. If you would like to store more than 1 million files in your home space, please refer to the following section Limit on number of files . All home directories are stored in the IBM General Parallel File System (GPFS) mounted on /mnt/ufs18. It is automatically backed-up except files saved in nodr space. To restore any file from backup, please submit a ticket and let us know the paths to the files or the directory with the time frame you would like them restored. To learn more information, please check the documentation of ufs18 file system . For the system security and user data privacy, we recommend that users do NOT open HOME directory access permission to others. When you report an issue about files saved in HOME, please attach them to your message for reference. ICER staffs do not access any files or directories in your home directory.","title":"Home Space"},{"location":"Home_Space/#incorrect-used-space","text":"Currently our home file system check quota function will sometimes cause a user's directory over the quota due to incorrect calculation of used space . If you see this please open a ticket and we will work with you to temporarily increase your quota. We continue to work with our vendor to correct this issue.","title":"Incorrect Used Space"},{"location":"Home_Space/#space-quota","text":"The only way to get quota information of home space is to run the command quota : 1 2 3 4 5 $ quota home directory: Space Space Space Space Files Files Files Files Quota Used Available % Used Quota Used Available % Used ----------------------------------------------------------------------------------------------- /mnt/home/ $USER 50G 32G 18G 64 % 1048576 432525 616051 59 % where all file spaces accessible to the user are listed, including home, research, scratch and ffs17. In each space, the information of quota, usage and availablility on space size and number of files can be found. If \"Free\" or \"Available\" is a negative value (such as \"Space Available\" in home directory of the above example), the usage is over the quota. Please remove, transfer or compress some files so the used space or the file count can be lower than the \"Quota\" value. Since GPFS uses a different compression algorithm, you may notice higher space size after files are copied to the ufs18 file system.","title":"Space quota"},{"location":"Home_Space/#actual-disk-usage-du-different-from-quota-results","text":"The new file system has a smallest file block size of 64k. This means that files between 2K and 64K will occupy 64K of space. This causes space usage inflated greatly for users with large amounts of small files. One suggested solution would be to compress many small files into one large file (larger than 64K). in this way, the number of files can also be reduced. If you still have any difficulty, a temporarily larger quota can be requested if your quota is at 1T with many small files.","title":"Actual disk usage (du) different from quota results"},{"location":"Home_Space/#limit-on-number-of-files","text":"Besides the quota on the size of space, users are also limited to 1 million files in their home or research directory. We need to set this limit because, with a great number of files, the file system will spend too much time on back-up to be able to function normally. If possible, users can compress many files into one to reduce the file number. If users do not wish to have the limit, they can request to have part of their home (or research space) moved under nodr space ( /mnt/ufs18/nodr/ or /mnt/ufs18/nodr/research/ ) where there is no limit on the file count yet no back-up on the files either. Users will be responsible for their own back-up in the nodr space. By default, one half of the space quota is assigned to the requested nodr space. The quota of the original space under /mnt/home/ (or /mnt/research/ ) is then donwsized to its half so the total space quota remains the same. A different size of nodr space can also be assigned according to user's request. Once this space is created, the path and the quota information can be found by the quota command mentioned above. All home space files are periodically, automatically backed up (except those files that a user has opted to store in a specially requested nodr space). To access file backups, please submit a help ticket containing the file paths and the period i.e. , the time frame, from which the files should be restored.","title":"Limit on number of files"},{"location":"How_Jobs_are_Scheduled/","text":"How Jobs are Scheduled SLURM schedules jobs in two ways: the main scheduler and the backfill scheduler. The main scheduler constantly tries to start high priority jobs. The backfill scheduler considers all jobs, and starts any jobs that won't defer the start time of a higher priority job. Scheduler Function When it Runs Run Time Main Launches high priority jobs that can start immediately. Stops evaluating jobs once it encounters a job that cannot be started. About every 2 seconds 0.08-2 seconds Backfill Evaluates the entire queue. Launches jobs that won't interfere with the start time of a higher priority job. Sets jobs' StartTime and SchedNodeList . 20 seconds after the last backfill cycle completes 2-15+ minutes StartTime and SchedNodeList The backfill scheduler sets the StartTime and SchedNodeList parameters on jobs that can start within the next 7 days. These parameters can be viewed in the output of \"scontrol show job <jobid> \". StartTime estimates when a job will start and SchedNodeList shows the nodes this job might start on. StartTime is only an estimate. These values are updated every time the backfill scheduler runs and may change as running jobs complete and new jobs are submitted. Minimum Job Requirements to Avoid Deferment Jobs must meet certain criteria before the backfill scheduler will avoid potentially deferring them through starting lower priority jobs. These thresholds allow the backfill scheduler to cycle faster and maintain high system utilization. Criteria Minimum Description Priority 3000 Jobs require a minimum priority of 3000 is require to avoid potential deferment in scheduling. Buy-in account jobs are never below this threshold. Age 30 minutes Jobs must be queued for at least 30 minutes to avoid potential deferment in scheduling. This applies to all jobs. Job Priority Factors A jobs priority is determined by a combination of several priority factors. Age, size, fairshare, and whether it was submitted to a buy-in account all contribute to the job\u2019s priority. Priority Factor Description Maximum Contribution to Priority Age Starts at zero at job submission, then increases linearly to a maximum of 60000 after 30 days 60000 after 30 days Fairshare Starts at 60000 and decreases and users' recent usage goes up. Usage for this calculation is decayed 50% each day 60000 for no recent cluster usage Size Scales linearly with the amount of CPU and memory requested by a job. 100 per CPU, 20 per GB. 52000+ depending on memory requested QOS Adds 3000 to buy-in jobs to ensure they are always above backfill schedulers minimum priority for reserving resources 3000","title":"How Jobs are Scheduled"},{"location":"How_Jobs_are_Scheduled/#how-jobs-are-scheduled","text":"SLURM schedules jobs in two ways: the main scheduler and the backfill scheduler. The main scheduler constantly tries to start high priority jobs. The backfill scheduler considers all jobs, and starts any jobs that won't defer the start time of a higher priority job. Scheduler Function When it Runs Run Time Main Launches high priority jobs that can start immediately. Stops evaluating jobs once it encounters a job that cannot be started. About every 2 seconds 0.08-2 seconds Backfill Evaluates the entire queue. Launches jobs that won't interfere with the start time of a higher priority job. Sets jobs' StartTime and SchedNodeList . 20 seconds after the last backfill cycle completes 2-15+ minutes","title":"How Jobs are Scheduled"},{"location":"How_Jobs_are_Scheduled/#starttime-and-schednodelist","text":"The backfill scheduler sets the StartTime and SchedNodeList parameters on jobs that can start within the next 7 days. These parameters can be viewed in the output of \"scontrol show job <jobid> \". StartTime estimates when a job will start and SchedNodeList shows the nodes this job might start on. StartTime is only an estimate. These values are updated every time the backfill scheduler runs and may change as running jobs complete and new jobs are submitted.","title":"StartTime and SchedNodeList"},{"location":"How_Jobs_are_Scheduled/#minimum-job-requirements-to-avoid-deferment","text":"Jobs must meet certain criteria before the backfill scheduler will avoid potentially deferring them through starting lower priority jobs. These thresholds allow the backfill scheduler to cycle faster and maintain high system utilization. Criteria Minimum Description Priority 3000 Jobs require a minimum priority of 3000 is require to avoid potential deferment in scheduling. Buy-in account jobs are never below this threshold. Age 30 minutes Jobs must be queued for at least 30 minutes to avoid potential deferment in scheduling. This applies to all jobs.","title":"Minimum Job Requirements to Avoid Deferment"},{"location":"How_Jobs_are_Scheduled/#job-priority-factors","text":"A jobs priority is determined by a combination of several priority factors. Age, size, fairshare, and whether it was submitted to a buy-in account all contribute to the job\u2019s priority. Priority Factor Description Maximum Contribution to Priority Age Starts at zero at job submission, then increases linearly to a maximum of 60000 after 30 days 60000 after 30 days Fairshare Starts at 60000 and decreases and users' recent usage goes up. Usage for this calculation is decayed 50% each day 60000 for no recent cluster usage Size Scales linearly with the amount of CPU and memory requested by a job. 100 per CPU, 20 per GB. 52000+ depending on memory requested QOS Adds 3000 to buy-in jobs to ensure they are always above backfill schedulers minimum priority for reserving resources 3000","title":"Job Priority Factors"},{"location":"How_to_find_and_load_software_modules/","text":"How to find and load software modules General search using module spider To search for a particular software module (say ABC), you would run 1 module spider ABC # can also be abc, ABc... since the name is case-insensitive Once you find it, and want to load a specific version (say 1.1.1), run 1 module spider ABC/1.1.1 # should only be ABC The resulting output information will tell you what prerequisites modules are needed before loading your ABC/1.1.1 . You don't need to know the full name of the software. See below (note the third case about PCRE). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 $ module spider sam -------------------------- SAMtools: -------------------------- Description: SAM Tools provide various utilities for manipulating alignments in the SAM format, including sorting, merging, indexing and generating alignments in a per-position format. Versions: SAMtools/0.1.19 SAMtools/1.5 SAMtools/1.7 SAMtools/1.8 SAMtools/1.9 -------------------------- For detailed information about a specific \"SAMtools\" module (including how to load the modules) use the module's full name. For example: $ module spider SAMtools/1.7 -------------------------- $ module spider amtool -------------------------- BamTools: -------------------------- Description: BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files. Versions: BamTools/2.4.1 BamTools/2.5.1 -------------------------- SAMtools: -------------------------- Description: SAM Tools provide various utilities for manipulating alignments in the SAM format, including sorting, merging, indexing and generating alignments in a per-position format. Versions: SAMtools/0.1.19 SAMtools/1.5 SAMtools/1.7 SAMtools/1.8 SAMtools/1.9 $ module spider PCRE -------------------------- PCRE: -------------------------- Description: The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. Versions: PCRE/8.38 PCRE/8.39 PCRE/8.40 PCRE/8.41 Other possible modules matches: PCRE2 -------------------------- To find other possible module matches execute: $ module -r spider '.*PCRE.*' -------------------------- Example of loading R In this example, we are looking for available versions of R. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ ssh dev-intel18 $ module spider R ----------- R: ----------- Description: R is a free software environment for statistical computing and graphics. Versions: R/3.3.1 ... ... R/4.0.2 R/4.0.3 R/4.1.0 R/4.1.2 Suppose we want to load R/4.1.2, and in order to know how, we run 1 2 3 4 5 6 7 8 9 $ module spider R/4.1.2 Description: R is a free software environment for statistical computing and graphics. You will need to load all module(s) on any one of the lines below before the \"R/4.1.2\" module is available to load. GCC/11.2.0 OpenMPI/4.1.1 Above output tells us to load two prerequisites (GCC/11.2.0 and OpenMPI/4.1.1) before loading R. Therefore, we'll run 1 2 3 module purge # a MUST-HAVE module load GCC/11.2.0 OpenMPI/4.1.1 module load R/4.1.2 You are all set. Note that \"module purge\" is always needed before you start loading a different software module. Advanced skill: Searching for modules in module file hierarchy using \"module avail\" If you start with a minimum set of loaded modules (most commonly a compiler-MPI pair, or a compiler alone), and want to know what software packages are available to load in the current MODULEPATH (run echo $MODULEPATH to see the paths), you can run module avail . module avail is different from module spider above (which lists all possible modules, not just the modules that can be seen in the current MODULEPATH ). To check availability of a particular module, use module avail keyword. If the keyword (such as \"openmpi\") is long and distinct, the search result would normally be clean. However, for \"R\" for example, a single letter that can appear in almost any module names, we need to use regular expression to fully specify the pattern when running module avail. See below. 1 2 3 4 5 6 7 8 9 10 11 $ module purge $ module load GCC/8.3.0 OpenMPI/3.1.4 $ module avail # a large amount of output will be printed on your screen and they are omitted here. # Now we search for available R versions under the current GCC-OpenMPI pair. $ module -r avail '^R$' --------------------------------------- /opt/modules/MPI/GCC/8.3.0/OpenMPI/3.1.4 ---------------------------- R/3.6.2 R/3.6.3 R/4.0.2.bak R/4.0.2.test R/4.0.2 R/4.1.0 (D) Where: D: Default Module Troubleshooting Sometimes, \" module spider \" doesn't work because your personal module cache is out of date. To clear it, do rm -r ~/.lmod.d/.cache","title":"Loading software modules"},{"location":"How_to_find_and_load_software_modules/#how-to-find-and-load-software-modules","text":"","title":"How to find and load software modules"},{"location":"How_to_find_and_load_software_modules/#general-search-using-module-spider","text":"To search for a particular software module (say ABC), you would run 1 module spider ABC # can also be abc, ABc... since the name is case-insensitive Once you find it, and want to load a specific version (say 1.1.1), run 1 module spider ABC/1.1.1 # should only be ABC The resulting output information will tell you what prerequisites modules are needed before loading your ABC/1.1.1 . You don't need to know the full name of the software. See below (note the third case about PCRE). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 $ module spider sam -------------------------- SAMtools: -------------------------- Description: SAM Tools provide various utilities for manipulating alignments in the SAM format, including sorting, merging, indexing and generating alignments in a per-position format. Versions: SAMtools/0.1.19 SAMtools/1.5 SAMtools/1.7 SAMtools/1.8 SAMtools/1.9 -------------------------- For detailed information about a specific \"SAMtools\" module (including how to load the modules) use the module's full name. For example: $ module spider SAMtools/1.7 -------------------------- $ module spider amtool -------------------------- BamTools: -------------------------- Description: BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files. Versions: BamTools/2.4.1 BamTools/2.5.1 -------------------------- SAMtools: -------------------------- Description: SAM Tools provide various utilities for manipulating alignments in the SAM format, including sorting, merging, indexing and generating alignments in a per-position format. Versions: SAMtools/0.1.19 SAMtools/1.5 SAMtools/1.7 SAMtools/1.8 SAMtools/1.9 $ module spider PCRE -------------------------- PCRE: -------------------------- Description: The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. Versions: PCRE/8.38 PCRE/8.39 PCRE/8.40 PCRE/8.41 Other possible modules matches: PCRE2 -------------------------- To find other possible module matches execute: $ module -r spider '.*PCRE.*' --------------------------","title":"General search using module spider"},{"location":"How_to_find_and_load_software_modules/#example-of-loading-r","text":"In this example, we are looking for available versions of R. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ ssh dev-intel18 $ module spider R ----------- R: ----------- Description: R is a free software environment for statistical computing and graphics. Versions: R/3.3.1 ... ... R/4.0.2 R/4.0.3 R/4.1.0 R/4.1.2 Suppose we want to load R/4.1.2, and in order to know how, we run 1 2 3 4 5 6 7 8 9 $ module spider R/4.1.2 Description: R is a free software environment for statistical computing and graphics. You will need to load all module(s) on any one of the lines below before the \"R/4.1.2\" module is available to load. GCC/11.2.0 OpenMPI/4.1.1 Above output tells us to load two prerequisites (GCC/11.2.0 and OpenMPI/4.1.1) before loading R. Therefore, we'll run 1 2 3 module purge # a MUST-HAVE module load GCC/11.2.0 OpenMPI/4.1.1 module load R/4.1.2 You are all set. Note that \"module purge\" is always needed before you start loading a different software module.","title":"Example of loading R"},{"location":"How_to_find_and_load_software_modules/#advanced-skill-searching-for-modules-in-module-file-hierarchy-using-module-avail","text":"If you start with a minimum set of loaded modules (most commonly a compiler-MPI pair, or a compiler alone), and want to know what software packages are available to load in the current MODULEPATH (run echo $MODULEPATH to see the paths), you can run module avail . module avail is different from module spider above (which lists all possible modules, not just the modules that can be seen in the current MODULEPATH ). To check availability of a particular module, use module avail keyword. If the keyword (such as \"openmpi\") is long and distinct, the search result would normally be clean. However, for \"R\" for example, a single letter that can appear in almost any module names, we need to use regular expression to fully specify the pattern when running module avail. See below. 1 2 3 4 5 6 7 8 9 10 11 $ module purge $ module load GCC/8.3.0 OpenMPI/3.1.4 $ module avail # a large amount of output will be printed on your screen and they are omitted here. # Now we search for available R versions under the current GCC-OpenMPI pair. $ module -r avail '^R$' --------------------------------------- /opt/modules/MPI/GCC/8.3.0/OpenMPI/3.1.4 ---------------------------- R/3.6.2 R/3.6.3 R/4.0.2.bak R/4.0.2.test R/4.0.2 R/4.1.0 (D) Where: D: Default Module","title":"Advanced skill: Searching for modules in module file hierarchy using \"module avail\""},{"location":"How_to_find_and_load_software_modules/#troubleshooting","text":"Sometimes, \" module spider \" doesn't work because your personal module cache is out of date. To clear it, do rm -r ~/.lmod.d/.cache","title":"Troubleshooting"},{"location":"ICER_HPC_Classroom_Support/","text":"Classroom Support This document provides a defined pathway for instructors at MSU to utilize ICER/HPCC resources for classroom education. Services Available ICER will work with instructors to provide the following services during the class: Training workshops on using the HPCC, via our asynchronous Desire2Learn-based training modules. HPCC student accounts and research space for use in classes and for sharing data files Access to OnDemand , a web-based portal to use Python via Jupyter notebooks, RStudio, Matlab, Stata, and an interactive Linux desktop. OnDemand has access to all of the HPCC's file systems. Software installation for teaching purpose Reservations for computing resources in the cluster so that the students can instantly have access to the resources to finish the assignments Helpdesk tickets (submitting your questions via our online form ) Request Classroom Support Requests should be submitted two weeks in advance , to allow for time for account creation, specialized software installation, etc. Request individual student user accounts Instructors need to submit to New Account Request online form for all students enrolled in the course. Please note that instructors are also responsible for requesting the accounts be closed when students are no longer enrolled in the course or when the course is finished. The following information should also be included in the request. MSU NetIDs of students enrolled in the course Course name for the group name of the student accounts Software installation, CPU/GPU core reservation (as needed) Start and end dates of the course Request a research space for course use Instructors can also submit a Research request to create a research space for the course group and add all students enrolled in the request form. Note on storage quota Both the home directory of a user and the research directory of a group are initialized with 50 GB storage quota. Additional space up to 1 TB may be requested for home directory or research space . Beyond 1 TB, ICER will need to charge the additional storage. Class Account Agreement Instructors are expected to make good faith efforts towards implementing the following policies: Awareness of ICER and MSU policies -- Instructors using ICER resources are expected to have an understanding of ICER and MSU IT policies. Broadly, they should be cognizant of MSU's data sharing policies, wait times on queued jobs and the possibility of unscheduled outages. These factors should be taken into consideration while designing assignments and projects that utilize ICER resources. ICER/HPCC policies are emailed to users upon account creation. Contacting ICER -- Instructors should provide clear policies in their syllabus about when students should contact ICER staff. ICER will provide account, hardware and system software support. Very limited applications software support for the course TAs and instructors is available. However, so as not to overwhelm the ticketing system with course-specific questions, we ask that all student questions be routed through the TA or course instructor who will then determine whether to forward these to ICER's ticketing system. While students are encouraged to visit ICER research consultants during office hours, these hours are meant for research support and are not designed to be used as a means of TA support. In particular, students should be aware that submitting queries to ICER that seek answers to homework problems will be considered cheating and a violation of the Honor Code. Planning for outages -- ICER resources may become unavailable as a result of an unscheduled system outage. Instructors are advised not to depend on ICER resources for final exams and/or projects that require a short turnaround time. Storing data -- Instructors should advise students against storing data on the \"scratch\" space. Files are typically purged on scratch after 45 days if no modification has been made, and cannot be recovered . Instructors may request a research space for students to store their class related data for the duration of the semester. Students may also store their data in their home directory. Accounts for course work are typically limited to 50 GB. Terminating educational accounts -- Student HPCC accounts, linked home folders, and group folders created for courses will be removed 30 days after the end of the class. This applies only to education-sponsored accounts and NOT research-sponsored student accounts. However, students who wish to convert their educated-sponsored account to a research-sponsored account after the course is completed must have their research supervisor submit a request for membership change no later than 30 days after the semester in which the course was completed in order to retain the data saved in the education-sponsored student account. Acknowledging ICER -- Instructors and students are encouraged to acknowledge the use of ICER/HPCC upon publication of data related to the resources used during the course. Last updated: Aug 2022","title":"Classroom support"},{"location":"ICER_HPC_Classroom_Support/#classroom-support","text":"This document provides a defined pathway for instructors at MSU to utilize ICER/HPCC resources for classroom education.","title":"Classroom Support"},{"location":"ICER_HPC_Classroom_Support/#services-available","text":"ICER will work with instructors to provide the following services during the class: Training workshops on using the HPCC, via our asynchronous Desire2Learn-based training modules. HPCC student accounts and research space for use in classes and for sharing data files Access to OnDemand , a web-based portal to use Python via Jupyter notebooks, RStudio, Matlab, Stata, and an interactive Linux desktop. OnDemand has access to all of the HPCC's file systems. Software installation for teaching purpose Reservations for computing resources in the cluster so that the students can instantly have access to the resources to finish the assignments Helpdesk tickets (submitting your questions via our online form )","title":"Services Available"},{"location":"ICER_HPC_Classroom_Support/#request-classroom-support","text":"Requests should be submitted two weeks in advance , to allow for time for account creation, specialized software installation, etc.","title":"Request Classroom Support"},{"location":"ICER_HPC_Classroom_Support/#request-individual-student-user-accounts","text":"Instructors need to submit to New Account Request online form for all students enrolled in the course. Please note that instructors are also responsible for requesting the accounts be closed when students are no longer enrolled in the course or when the course is finished. The following information should also be included in the request. MSU NetIDs of students enrolled in the course Course name for the group name of the student accounts Software installation, CPU/GPU core reservation (as needed) Start and end dates of the course","title":"Request individual student user accounts"},{"location":"ICER_HPC_Classroom_Support/#request-a-research-space-for-course-use","text":"Instructors can also submit a Research request to create a research space for the course group and add all students enrolled in the request form.","title":"Request a research space for course use"},{"location":"ICER_HPC_Classroom_Support/#note-on-storage-quota","text":"Both the home directory of a user and the research directory of a group are initialized with 50 GB storage quota. Additional space up to 1 TB may be requested for home directory or research space . Beyond 1 TB, ICER will need to charge the additional storage.","title":"Note on storage quota"},{"location":"ICER_HPC_Classroom_Support/#class-account-agreement","text":"Instructors are expected to make good faith efforts towards implementing the following policies: Awareness of ICER and MSU policies -- Instructors using ICER resources are expected to have an understanding of ICER and MSU IT policies. Broadly, they should be cognizant of MSU's data sharing policies, wait times on queued jobs and the possibility of unscheduled outages. These factors should be taken into consideration while designing assignments and projects that utilize ICER resources. ICER/HPCC policies are emailed to users upon account creation. Contacting ICER -- Instructors should provide clear policies in their syllabus about when students should contact ICER staff. ICER will provide account, hardware and system software support. Very limited applications software support for the course TAs and instructors is available. However, so as not to overwhelm the ticketing system with course-specific questions, we ask that all student questions be routed through the TA or course instructor who will then determine whether to forward these to ICER's ticketing system. While students are encouraged to visit ICER research consultants during office hours, these hours are meant for research support and are not designed to be used as a means of TA support. In particular, students should be aware that submitting queries to ICER that seek answers to homework problems will be considered cheating and a violation of the Honor Code. Planning for outages -- ICER resources may become unavailable as a result of an unscheduled system outage. Instructors are advised not to depend on ICER resources for final exams and/or projects that require a short turnaround time. Storing data -- Instructors should advise students against storing data on the \"scratch\" space. Files are typically purged on scratch after 45 days if no modification has been made, and cannot be recovered . Instructors may request a research space for students to store their class related data for the duration of the semester. Students may also store their data in their home directory. Accounts for course work are typically limited to 50 GB. Terminating educational accounts -- Student HPCC accounts, linked home folders, and group folders created for courses will be removed 30 days after the end of the class. This applies only to education-sponsored accounts and NOT research-sponsored student accounts. However, students who wish to convert their educated-sponsored account to a research-sponsored account after the course is completed must have their research supervisor submit a request for membership change no later than 30 days after the semester in which the course was completed in order to retain the data saved in the education-sponsored student account. Acknowledging ICER -- Instructors and students are encouraged to acknowledge the use of ICER/HPCC upon publication of data related to the resources used during the course. Last updated: Aug 2022","title":"Class Account Agreement"},{"location":"Information_for_Central_Michigan_University_and_Western_Michigan_University_Users/","text":"Information for Central Michigan University and Western Michigan University Users Note This information is for users at Central Michigan University and Western Michigan University. Users with MSU NetIDs should disregard this information. Contacting the HPCC for Help CMU Users: Please email cmichhelp@hpcc.msu.edu for help with the HPCC systems. WMU Users: Please email wmichhelp@hpcc.msu.edu for help with the HPCC systems. Changing your password First, connect to the HPCC using your credentials (for more information, see Connect to the HPCC ): 1 ssh userid@hpcc.msu.edu After logging into HPCC gateway, ssh to one of the development nodes. Use the \"passwd\" command on a dev node: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 login as: cmichtestCMICH Using keyboard-interactive authentication. Password: /_// ___// ___// _ ; / /_/ // _ ;/ ___// ___/ / // /__ / _/_ / _,.' / __ // _,.'/ /__ / /__ /_//____//____//_//_! /_/ /_//_/ /____//____/ ________________________________________________________________________________ Welcome to Michigan State's High Performance Computing Center ** Unauthorized access is prohibited ** ________________________________________________________________________________ For GPU development please use green nodes. Development Nodes (usage) -------------------------------- dev-amd20-v100 (low) dev-amd20 (low) dev-intel14-k20 (low) dev-intel14 (high) dev-intel16-k80 (low) dev-intel16 (high) dev-intel18 (low) ________________________________________________________________________________ [cmichtestCMICH@gateway-01 ~]$ ssh dev-amd20 === Please note that processes on development nodes are limited to two hours of CPU time; for longer-running jobs, please submit to the queue. Development nodes are a shared system; for information about performance considerations please see: https://wiki.hpcc.msu.edu/x/N4JnAg === [cmichtestCMICH@dev-amd20 ~]$ passwd Changing password for cmichtestCMICH. Enter login(LDAP) password: # enter your old password New Password: # enter your new password Password limitations: External user passwords are limited to eight characters. The system will check the password for common problems and reject any insecure choices. 1 Reenter New Password: Reenter your password. 1 LDAP password information changed for cmichtestCMICH Note It may take up to a hour for the password change to be propagated to all LDAP servers. Using CMU nodes Your account automatically has access to CMU nodes. The actual nodes assigned may vary depending on hardware availability.","title":"Central Michigan U and Western Michigan U"},{"location":"Information_for_Central_Michigan_University_and_Western_Michigan_University_Users/#information-for-central-michigan-university-and-western-michigan-university-users","text":"Note This information is for users at Central Michigan University and Western Michigan University. Users with MSU NetIDs should disregard this information.","title":"Information for Central Michigan University and Western Michigan University Users"},{"location":"Information_for_Central_Michigan_University_and_Western_Michigan_University_Users/#contacting-the-hpcc-for-help","text":"CMU Users: Please email cmichhelp@hpcc.msu.edu for help with the HPCC systems. WMU Users: Please email wmichhelp@hpcc.msu.edu for help with the HPCC systems.","title":"Contacting the HPCC for Help"},{"location":"Information_for_Central_Michigan_University_and_Western_Michigan_University_Users/#changing-your-password","text":"First, connect to the HPCC using your credentials (for more information, see Connect to the HPCC ): 1 ssh userid@hpcc.msu.edu After logging into HPCC gateway, ssh to one of the development nodes. Use the \"passwd\" command on a dev node: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 login as: cmichtestCMICH Using keyboard-interactive authentication. Password: /_// ___// ___// _ ; / /_/ // _ ;/ ___// ___/ / // /__ / _/_ / _,.' / __ // _,.'/ /__ / /__ /_//____//____//_//_! /_/ /_//_/ /____//____/ ________________________________________________________________________________ Welcome to Michigan State's High Performance Computing Center ** Unauthorized access is prohibited ** ________________________________________________________________________________ For GPU development please use green nodes. Development Nodes (usage) -------------------------------- dev-amd20-v100 (low) dev-amd20 (low) dev-intel14-k20 (low) dev-intel14 (high) dev-intel16-k80 (low) dev-intel16 (high) dev-intel18 (low) ________________________________________________________________________________ [cmichtestCMICH@gateway-01 ~]$ ssh dev-amd20 === Please note that processes on development nodes are limited to two hours of CPU time; for longer-running jobs, please submit to the queue. Development nodes are a shared system; for information about performance considerations please see: https://wiki.hpcc.msu.edu/x/N4JnAg === [cmichtestCMICH@dev-amd20 ~]$ passwd Changing password for cmichtestCMICH. Enter login(LDAP) password: # enter your old password New Password: # enter your new password Password limitations: External user passwords are limited to eight characters. The system will check the password for common problems and reject any insecure choices. 1 Reenter New Password: Reenter your password. 1 LDAP password information changed for cmichtestCMICH Note It may take up to a hour for the password change to be propagated to all LDAP servers.","title":"Changing your password"},{"location":"Information_for_Central_Michigan_University_and_Western_Michigan_University_Users/#using-cmu-nodes","text":"Your account automatically has access to CMU nodes. The actual nodes assigned may vary depending on hardware availability.","title":"Using CMU nodes"},{"location":"Installing_Local_Perl_Modules_with_CPAN/","text":"Installing Local Perl Modules with CPAN CPAN is a convenient way to build and install perl modules, but many people have difficulty knowing how to do this if they lack \"root\" permissions. This tutorial will demonstrate how to install Perl modules to a local user space using CPAN. Procedure First start the CPAN shell: 1 2 3 4 5 6 7 8 9 $ cpan Terminal does not support AddHistory. cpan shell -- CPAN exploration and modules installation ( v1.9402 ) Enter 'h' for help. cpan [ 1 ] > Next determine where you want to install your local Perl modules. Let's assume we are going to place them in: /mnt/home/myUid/perlmods Now from within the CPAN shell, enter the following three (3) commands: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 cpan [ 1 ] > o conf mbuildpl_arg \"--install_base /mnt/home/myUid/perlmods\" mbuildpl_arg [ -- install_base /mnt/ home /myUid/ perlmods ] Please use 'o conf commit' to make the config permanent ! cpan [ 2 ] > o conf makepl_arg \"PREFIX=/mnt/home/myUid/perlmods\" makepl_arg [ PREFIX = /mnt/ home /myUid/ perlmods ] Please use 'o conf commit' to make the config permanent ! cpan [ 3 ] > o conf prefs_dir \"/mnt/home/myUid/.cpan/prefs\" prefs_dir [ /mnt/ home /myUid/ . cpan / prefs ] Please use 'o conf commit' to make the config permanent ! If you want to make the settings above permanent, enter \"o conf commit\". Otherwise, bear in-mind you will need to reset this value every time you restart CPAN. If you do make the setting permanent, you can always change it later and re-commit as shown above. Now to install a module (lets assume we want to build \"Math::GMP\") simply enter: 1 cpan [ 4 ] > install Math:: GMP Respond to any prompts for information that might be requested. When you are finished, enter: 1 cpan [ 5 ] > quit Setting the PERL5LIB Path Now that you've successfully installed a local Perl module, you will need to tell Perl where to find them. This can be easily accomplished by setting the environmental path variable \" PERL5LIB \". For example: 1 $ export PERL5LIB = /mnt/home/myUid/perlmods: $PERL5LIB You can add this export to your .bashrc file if you'd like to ensure it is always loaded upon login. In addition, for any scripts that you write that utilize these local Perl modules that you run on the HPCC cluster, you should add this export statement to your job script, or create a custom user module that does that for you, and which can be loaded from within your job script.","title":"Installing Local Perl Modules with CPAN"},{"location":"Installing_Local_Perl_Modules_with_CPAN/#installing-local-perl-modules-with-cpan","text":"CPAN is a convenient way to build and install perl modules, but many people have difficulty knowing how to do this if they lack \"root\" permissions. This tutorial will demonstrate how to install Perl modules to a local user space using CPAN.","title":"Installing Local Perl Modules with CPAN"},{"location":"Installing_Local_Perl_Modules_with_CPAN/#procedure","text":"First start the CPAN shell: 1 2 3 4 5 6 7 8 9 $ cpan Terminal does not support AddHistory. cpan shell -- CPAN exploration and modules installation ( v1.9402 ) Enter 'h' for help. cpan [ 1 ] > Next determine where you want to install your local Perl modules. Let's assume we are going to place them in: /mnt/home/myUid/perlmods Now from within the CPAN shell, enter the following three (3) commands: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 cpan [ 1 ] > o conf mbuildpl_arg \"--install_base /mnt/home/myUid/perlmods\" mbuildpl_arg [ -- install_base /mnt/ home /myUid/ perlmods ] Please use 'o conf commit' to make the config permanent ! cpan [ 2 ] > o conf makepl_arg \"PREFIX=/mnt/home/myUid/perlmods\" makepl_arg [ PREFIX = /mnt/ home /myUid/ perlmods ] Please use 'o conf commit' to make the config permanent ! cpan [ 3 ] > o conf prefs_dir \"/mnt/home/myUid/.cpan/prefs\" prefs_dir [ /mnt/ home /myUid/ . cpan / prefs ] Please use 'o conf commit' to make the config permanent ! If you want to make the settings above permanent, enter \"o conf commit\". Otherwise, bear in-mind you will need to reset this value every time you restart CPAN. If you do make the setting permanent, you can always change it later and re-commit as shown above. Now to install a module (lets assume we want to build \"Math::GMP\") simply enter: 1 cpan [ 4 ] > install Math:: GMP Respond to any prompts for information that might be requested. When you are finished, enter: 1 cpan [ 5 ] > quit Setting the PERL5LIB Path Now that you've successfully installed a local Perl module, you will need to tell Perl where to find them. This can be easily accomplished by setting the environmental path variable \" PERL5LIB \". For example: 1 $ export PERL5LIB = /mnt/home/myUid/perlmods: $PERL5LIB You can add this export to your .bashrc file if you'd like to ensure it is always loaded upon login. In addition, for any scripts that you write that utilize these local Perl modules that you run on the HPCC cluster, you should add this export statement to your job script, or create a custom user module that does that for you, and which can be loaded from within your job script.","title":"Procedure"},{"location":"Installing_TensorFlow_1.x_with_virtualenv/","text":"Installing TensorFlow 1.x with virtualenv Installation To install TensorFlow (the latest stable release) in a python virtual environment, follow the steps below. Install TF 1.13.1 1 2 3 4 5 6 7 8 module purge module load GCC/6.4.0-2.28 OpenMPI/2.1.2 module load CUDA/10.0.130 cuDNN/7.5.0.56-CUDA-10.0.130 module load Python/3.6.4 virtualenv -p python3 tf-1.13.1-env source ~/tf-1.13.1-env/bin/activate pip install tensorflow-gpu In the above pip install command, specifying tensorflow-gpu as the package name will install the latest stable release with GPU support. As of May 2019, the latest version is 1.13.1. Notes: Prior to version 1.13, installation of TF requires CUDA 9. For installation of version 2.0 alpha, refer to another wiki in the TensorFlow menu. In October 2019, TF 2.0 was released. If you still want to install an older version 1.x, please specify the version explicitly when running pip install, such as pip instal tensorflow-gpu==1.14. Directly writing \"tensorflow-gpu\" will install the latest TF 2.0 which has fundamental updates/differences as compared with 1.x, and can produce errors. Testing example 1 We will test TF 1.13.1 with the following commands (using this cifar10 estimator tutorial ). 1 2 3 4 5 6 7 8 9 10 11 12 module purge module load GCC/6.4.0-2.28 OpenMPI/2.1.2 module load CUDA/10.0.130 cuDNN/7.5.0.56-CUDA-10.0.130 module load Python/3.6.4 source ~/tf-1.13.1-env/bin/activate export TF_CPP_MIN_LOG_LEVEL = 2 # disables the warning, doesn't enable AVX/FMA. cd <your working dir> git clone https://github.com/tensorflow/models.git cd models/tutorials/image/cifar10_estimator python generate_cifar10_tfrecords.py --data-dir = ${ PWD } /cifar-10-data python cifar10_main.py --data-dir = ${ PWD } /cifar-10-data --job-dir = ${ PWD } /TMP_cifar10 --num-gpus = 4 --train-steps = 1000 If you want to submit above commands through a SLURM job script, you will need add a directive line of #SBATCH --gres=gpu:4 in your script to request 4 GPUs. Additionally, you may want to make \" echo $CUDA_VISIBLE_DEVICES \" your first command before running the main part. See tutorial page \"Submitting a TensorFlow job\" for a job script example. Testing example 2 (tf.keras) We will run the following the python code test_keras.py (adopted from https://www.tensorflow.org/guide/keras ): test_keras.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import tensorflow as tf from tensorflow.keras import layers import numpy as np print ( tf . VERSION ) print ( tf . keras . __version__ ) model = tf . keras . Sequential ([ # Adds a densely-connected layer with 64 units to the model: layers . Dense ( 64 , activation = 'relu' , input_shape = ( 32 ,)), # Add another: layers . Dense ( 64 , activation = 'relu' ), # Add a softmax layer with 10 output units: layers . Dense ( 10 , activation = 'softmax' )]) model . compile ( optimizer = tf . train . AdamOptimizer ( 0.001 ), loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ]) data = np . random . random (( 1000 , 32 )) labels = np . random . random (( 1000 , 10 )) val_data = np . random . random (( 100 , 32 )) val_labels = np . random . random (( 100 , 10 )) model . fit ( data , labels , epochs = 10 , batch_size = 32 , validation_data = ( val_data , val_labels )) To run this code: 1 2 3 4 5 6 module purge module load GCC/6.4.0-2.28 OpenMPI/2.1.2 module load CUDA/10.0.130 cuDNN/7.5.0.56-CUDA-10.0.130 module load Python/3.6.4 source ~/tf-1.13.1-env/bin/activate python test_keras.py","title":"Installing TF 1.x"},{"location":"Installing_TensorFlow_1.x_with_virtualenv/#installing-tensorflow-1x-with-virtualenv","text":"","title":"Installing TensorFlow 1.x with virtualenv"},{"location":"Installing_TensorFlow_1.x_with_virtualenv/#installation","text":"To install TensorFlow (the latest stable release) in a python virtual environment, follow the steps below. Install TF 1.13.1 1 2 3 4 5 6 7 8 module purge module load GCC/6.4.0-2.28 OpenMPI/2.1.2 module load CUDA/10.0.130 cuDNN/7.5.0.56-CUDA-10.0.130 module load Python/3.6.4 virtualenv -p python3 tf-1.13.1-env source ~/tf-1.13.1-env/bin/activate pip install tensorflow-gpu In the above pip install command, specifying tensorflow-gpu as the package name will install the latest stable release with GPU support. As of May 2019, the latest version is 1.13.1. Notes: Prior to version 1.13, installation of TF requires CUDA 9. For installation of version 2.0 alpha, refer to another wiki in the TensorFlow menu. In October 2019, TF 2.0 was released. If you still want to install an older version 1.x, please specify the version explicitly when running pip install, such as pip instal tensorflow-gpu==1.14. Directly writing \"tensorflow-gpu\" will install the latest TF 2.0 which has fundamental updates/differences as compared with 1.x, and can produce errors.","title":"Installation"},{"location":"Installing_TensorFlow_1.x_with_virtualenv/#testing-example-1","text":"We will test TF 1.13.1 with the following commands (using this cifar10 estimator tutorial ). 1 2 3 4 5 6 7 8 9 10 11 12 module purge module load GCC/6.4.0-2.28 OpenMPI/2.1.2 module load CUDA/10.0.130 cuDNN/7.5.0.56-CUDA-10.0.130 module load Python/3.6.4 source ~/tf-1.13.1-env/bin/activate export TF_CPP_MIN_LOG_LEVEL = 2 # disables the warning, doesn't enable AVX/FMA. cd <your working dir> git clone https://github.com/tensorflow/models.git cd models/tutorials/image/cifar10_estimator python generate_cifar10_tfrecords.py --data-dir = ${ PWD } /cifar-10-data python cifar10_main.py --data-dir = ${ PWD } /cifar-10-data --job-dir = ${ PWD } /TMP_cifar10 --num-gpus = 4 --train-steps = 1000 If you want to submit above commands through a SLURM job script, you will need add a directive line of #SBATCH --gres=gpu:4 in your script to request 4 GPUs. Additionally, you may want to make \" echo $CUDA_VISIBLE_DEVICES \" your first command before running the main part. See tutorial page \"Submitting a TensorFlow job\" for a job script example.","title":"Testing example 1"},{"location":"Installing_TensorFlow_1.x_with_virtualenv/#testing-example-2-tfkeras","text":"We will run the following the python code test_keras.py (adopted from https://www.tensorflow.org/guide/keras ): test_keras.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import tensorflow as tf from tensorflow.keras import layers import numpy as np print ( tf . VERSION ) print ( tf . keras . __version__ ) model = tf . keras . Sequential ([ # Adds a densely-connected layer with 64 units to the model: layers . Dense ( 64 , activation = 'relu' , input_shape = ( 32 ,)), # Add another: layers . Dense ( 64 , activation = 'relu' ), # Add a softmax layer with 10 output units: layers . Dense ( 10 , activation = 'softmax' )]) model . compile ( optimizer = tf . train . AdamOptimizer ( 0.001 ), loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ]) data = np . random . random (( 1000 , 32 )) labels = np . random . random (( 1000 , 10 )) val_data = np . random . random (( 100 , 32 )) val_labels = np . random . random (( 100 , 10 )) model . fit ( data , labels , epochs = 10 , batch_size = 32 , validation_data = ( val_data , val_labels )) To run this code: 1 2 3 4 5 6 module purge module load GCC/6.4.0-2.28 OpenMPI/2.1.2 module load CUDA/10.0.130 cuDNN/7.5.0.56-CUDA-10.0.130 module load Python/3.6.4 source ~/tf-1.13.1-env/bin/activate python test_keras.py","title":"Testing example 2 (tf.keras)"},{"location":"Installing_TensorFlow_using_anaconda/","text":"Installing TensorFlow using anaconda An alternative to using virtual environment for installing TF is to use conda. You will need to first log into our GPU-equipped dev-node dev-intel16-k80. Install TF CPU-only Installation 1 2 3 4 5 export PATH =[ your top conda dir ] /bin: $PATH conda create --name tf source activate tf conda install -c conda-forge tensorflow source deactivate Test 1 2 3 4 5 6 7 8 9 10 export PATH =[ your top conda dir ] /bin: $PATH source activate tf python >>> import tensorflow as tf >>> print ( tf.__version__ ) >>> tf_session = tf.Session () >>> x = tf.constant ( 1 ) >>> y = tf.constant ( 1 ) >>> print ( tf_session.run ( x + y )) source deactivate Install TF with GPU support Installation 1 2 3 4 5 export PATH =[ your top conda dir ] /bin: $PATH conda create --name tf_gpu source activate tf_gpu conda install tensorflow-gpu source deactivate Test 1 2 3 4 5 6 export PATH =[ your top conda dir ] /bin: $PATH source activate tf_gpu python >>> import tensorflow as tf >>> sess = tf.Session ( config = tf.ConfigProto ( log_device_placement = True )) source deactivate","title":"Installing TF using anaconda"},{"location":"Installing_TensorFlow_using_anaconda/#installing-tensorflow-using-anaconda","text":"An alternative to using virtual environment for installing TF is to use conda. You will need to first log into our GPU-equipped dev-node dev-intel16-k80.","title":"Installing TensorFlow using anaconda"},{"location":"Installing_TensorFlow_using_anaconda/#install-tf-cpu-only","text":"Installation 1 2 3 4 5 export PATH =[ your top conda dir ] /bin: $PATH conda create --name tf source activate tf conda install -c conda-forge tensorflow source deactivate Test 1 2 3 4 5 6 7 8 9 10 export PATH =[ your top conda dir ] /bin: $PATH source activate tf python >>> import tensorflow as tf >>> print ( tf.__version__ ) >>> tf_session = tf.Session () >>> x = tf.constant ( 1 ) >>> y = tf.constant ( 1 ) >>> print ( tf_session.run ( x + y )) source deactivate","title":"Install TF CPU-only"},{"location":"Installing_TensorFlow_using_anaconda/#install-tf-with-gpu-support","text":"Installation 1 2 3 4 5 export PATH =[ your top conda dir ] /bin: $PATH conda create --name tf_gpu source activate tf_gpu conda install tensorflow-gpu source deactivate Test 1 2 3 4 5 6 export PATH =[ your top conda dir ] /bin: $PATH source activate tf_gpu python >>> import tensorflow as tf >>> sess = tf.Session ( config = tf.ConfigProto ( log_device_placement = True )) source deactivate","title":"Install TF with GPU support"},{"location":"Interactive_Job/","text":"Interactive Job It is helpful to run your work and see the response of the commands right away to check if there is any error in your work flow. To use the interactive mode with resources more than the limit imposed on the dev nodes, HPCC users can submit an interactive job using salloc or srun command with options of resource requests. salloc command salloc and GPUs srun command Job with graphical application salloc command For salloc , the command line 1 salloc -N 1 -c 2 --time = 1 :00:00 will allocate a job with resources of 1 node, 2 cores and walltime 1 hour. The execution will first wait until the job controller can provide the resources. 1 2 3 [ username@dev-intel18 WorkDir ] $ salloc -N 1 -c 2 --time = 1 :00:00 salloc: Required node not available ( down, drained or reserved ) salloc: job 7625 queued and waiting for resources Once that happens, the terminal will be transported to a command prompt on a compute node assigned to the job. 1 2 3 4 5 [ username@dev-intel18 WorkDir ] $ salloc -N 1 -c 2 --time = 1 :00:00 salloc: Required node not available ( down, drained or reserved ) salloc: job 7625 queued and waiting for resources salloc: job 7625 has been allocated resources [ username@test-skl-000 WorkDir ] $ where \" test-skl-000 \" after the symbol @ is the name of the assigned compute node. salloc and GPUs GPUs requested for an interactive job can now be used without submitting an additional srun. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [ username@dev-intel18 ~ ] $ salloc --gres = gpu:1 --time = 1 :00:00 salloc: Pending job allocation 14884030 salloc: job 14884030 queued and waiting for resources salloc: job 14884030 has been allocated resources salloc: Granted job allocation 14884030 salloc: Waiting for resource configuration salloc: Nodes csn-003 are ready for job [ username@csn-003 ~ ] $ nvidia-smi Mon Apr 8 10 :26:41 2019 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 418 .39 Driver Version: 418 .39 CUDA Version: 10 .1 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | =============================== + ====================== + ====================== | | 0 Tesla K20m On | 00000000 :03:00.0 Off | 0 | | N/A 30C P8 26W / 225W | 0MiB / 4743MiB | 0 % Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | | ============================================================================= | | No running processes found | +-----------------------------------------------------------------------------+ srun command A similar way can also be used with srun command: 1 2 3 4 5 [ username@dev-intel18 WorkDir ] $ srun -N 4 --ntasks-per-node = 2 -t 00 :60:00 --mem-per-cpu = 1000M --pty /bin/bash srun: Required node not available ( down, drained or reserved ) srun: job 7636 queued and waiting for resources srun: Granted job allocation 7636 [ username@test-skl-000 WorkDir ] $ As we can see, the specification \" --pty /bin/bash \" is required for srun command to request an interactive mode. Any command executed in this kind of interactive jobs will be launched parallelly with the number of task requested. srun can also be used in a command line without the specification \" --pty /bin/bash \". You may refer to the srun web site for more details. Job with graphical application To scheduling an interactive job able to use graphical user interface (GUI) software, the specification --x11 for X11 forwarding needs to be specified at the command (salloc or srun) line. If you are using Mac Terminal, you must have Xquartz installed, and you must use the \"-X\" parameter to allow x11 forwarding when connecting to a dev node prior to running the salloc command. The other option is to first log into our web-based remote desktop, and run the terminal there. See Web Site Access to HPCC for GUI software. If you are on Windows and using Moba Xterm to log in, these instructions will work with our with the \"-X\" parameter. Putty does not support X11 and so this will not work with putty. 1 2 3 4 5 6 7 8 9 10 [ username@gateway-03 ~ ] $ ssh -X dev-intel18 [ username@dev-intel18 ~ ] $ cd WorkdDir # this is optional, but you may want to select your work directory, for example [ username@dev-intel18 WorkDir ] $ salloc --ntasks = 1 --cpus-per-task 2 --time 00 :30:00 --x11 salloc: Granted job allocation 7708 salloc: Waiting for resource configuration salloc: Nodes css-076 are ready for job [ username@css-076 WorkDir ] $ module load MATLAB [ username@css-076 WorkDir ] $ matlab MATLAB is selecting SOFTWARE OPENGL rendering. Opening log file: /mnt/home/username/java.log.7159","title":"Interactive Job"},{"location":"Interactive_Job/#interactive-job","text":"It is helpful to run your work and see the response of the commands right away to check if there is any error in your work flow. To use the interactive mode with resources more than the limit imposed on the dev nodes, HPCC users can submit an interactive job using salloc or srun command with options of resource requests. salloc command salloc and GPUs srun command Job with graphical application","title":"Interactive Job"},{"location":"Interactive_Job/#salloc-command","text":"For salloc , the command line 1 salloc -N 1 -c 2 --time = 1 :00:00 will allocate a job with resources of 1 node, 2 cores and walltime 1 hour. The execution will first wait until the job controller can provide the resources. 1 2 3 [ username@dev-intel18 WorkDir ] $ salloc -N 1 -c 2 --time = 1 :00:00 salloc: Required node not available ( down, drained or reserved ) salloc: job 7625 queued and waiting for resources Once that happens, the terminal will be transported to a command prompt on a compute node assigned to the job. 1 2 3 4 5 [ username@dev-intel18 WorkDir ] $ salloc -N 1 -c 2 --time = 1 :00:00 salloc: Required node not available ( down, drained or reserved ) salloc: job 7625 queued and waiting for resources salloc: job 7625 has been allocated resources [ username@test-skl-000 WorkDir ] $ where \" test-skl-000 \" after the symbol @ is the name of the assigned compute node.","title":"salloc command"},{"location":"Interactive_Job/#salloc-and-gpus","text":"GPUs requested for an interactive job can now be used without submitting an additional srun. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [ username@dev-intel18 ~ ] $ salloc --gres = gpu:1 --time = 1 :00:00 salloc: Pending job allocation 14884030 salloc: job 14884030 queued and waiting for resources salloc: job 14884030 has been allocated resources salloc: Granted job allocation 14884030 salloc: Waiting for resource configuration salloc: Nodes csn-003 are ready for job [ username@csn-003 ~ ] $ nvidia-smi Mon Apr 8 10 :26:41 2019 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 418 .39 Driver Version: 418 .39 CUDA Version: 10 .1 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | =============================== + ====================== + ====================== | | 0 Tesla K20m On | 00000000 :03:00.0 Off | 0 | | N/A 30C P8 26W / 225W | 0MiB / 4743MiB | 0 % Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | | ============================================================================= | | No running processes found | +-----------------------------------------------------------------------------+","title":"salloc and GPUs"},{"location":"Interactive_Job/#srun-command","text":"A similar way can also be used with srun command: 1 2 3 4 5 [ username@dev-intel18 WorkDir ] $ srun -N 4 --ntasks-per-node = 2 -t 00 :60:00 --mem-per-cpu = 1000M --pty /bin/bash srun: Required node not available ( down, drained or reserved ) srun: job 7636 queued and waiting for resources srun: Granted job allocation 7636 [ username@test-skl-000 WorkDir ] $ As we can see, the specification \" --pty /bin/bash \" is required for srun command to request an interactive mode. Any command executed in this kind of interactive jobs will be launched parallelly with the number of task requested. srun can also be used in a command line without the specification \" --pty /bin/bash \". You may refer to the srun web site for more details.","title":"srun command"},{"location":"Interactive_Job/#job-with-graphical-application","text":"To scheduling an interactive job able to use graphical user interface (GUI) software, the specification --x11 for X11 forwarding needs to be specified at the command (salloc or srun) line. If you are using Mac Terminal, you must have Xquartz installed, and you must use the \"-X\" parameter to allow x11 forwarding when connecting to a dev node prior to running the salloc command. The other option is to first log into our web-based remote desktop, and run the terminal there. See Web Site Access to HPCC for GUI software. If you are on Windows and using Moba Xterm to log in, these instructions will work with our with the \"-X\" parameter. Putty does not support X11 and so this will not work with putty. 1 2 3 4 5 6 7 8 9 10 [ username@gateway-03 ~ ] $ ssh -X dev-intel18 [ username@dev-intel18 ~ ] $ cd WorkdDir # this is optional, but you may want to select your work directory, for example [ username@dev-intel18 WorkDir ] $ salloc --ntasks = 1 --cpus-per-task 2 --time 00 :30:00 --x11 salloc: Granted job allocation 7708 salloc: Waiting for resource configuration salloc: Nodes css-076 are ready for job [ username@css-076 WorkDir ] $ module load MATLAB [ username@css-076 WorkDir ] $ matlab MATLAB is selecting SOFTWARE OPENGL rendering. Opening log file: /mnt/home/username/java.log.7159","title":"Job with graphical application"},{"location":"Job-Arrays---Run-multiple-similar-jobs-simultaneously_40337504.html/","text":"Teaching : Job Arrays - Run multiple similar jobs simultaneously Job arrays are an efficient way to submit and manage a collection of jobs that differ from each other by only a single index parameter. All sub-jobs should have the same initial options (ex. size, time, etc.). For example, in the task below we wish to run python application, python_script.py ten times, each with a different input parameter for name. Instead of creating 10 separate Slurm job scripts and submitting them separately, we can create an array job. Make a copy of the job script hello.sb and name it array_job.sb Answer 1 2 cp hello.sb array_job.sb gedit array_job.sb Edit array_job.sb to request an array job consisting of 10 sub-jobs, with index parameters 1-10. We can do this by adding the following command to the slurm job script. 1 #SBATCH --array=1:10 Specify the job\u2019s stdin 1 cd test_$SLURM_ARRAY_JOB_ID Edit array_job.sb to simultaneously run python_script.py 1 python3 python_script.py This example was modified from: https://crc.ku.edu/hpc/how-to/arrays","title":"Job Arrays   Run multiple similar jobs simultaneously 40337504.html"},{"location":"Job-Arrays---Run-multiple-similar-jobs-simultaneously_40337504.html/#teaching-job-arrays-run-multiple-similar-jobs-simultaneously","text":"Job arrays are an efficient way to submit and manage a collection of jobs that differ from each other by only a single index parameter. All sub-jobs should have the same initial options (ex. size, time, etc.). For example, in the task below we wish to run python application, python_script.py ten times, each with a different input parameter for name. Instead of creating 10 separate Slurm job scripts and submitting them separately, we can create an array job. Make a copy of the job script hello.sb and name it array_job.sb Answer 1 2 cp hello.sb array_job.sb gedit array_job.sb Edit array_job.sb to request an array job consisting of 10 sub-jobs, with index parameters 1-10. We can do this by adding the following command to the slurm job script. 1 #SBATCH --array=1:10 Specify the job\u2019s stdin 1 cd test_$SLURM_ARRAY_JOB_ID Edit array_job.sb to simultaneously run python_script.py 1 python3 python_script.py This example was modified from: https://crc.ku.edu/hpc/how-to/arrays","title":"Teaching : Job Arrays - Run multiple similar jobs simultaneously"},{"location":"Job_Constraints/","text":"Job Constraints Constraints are set to restrict which node features are required for a given job. Use -C or --constraint = <list> when submitting your job to specify a constraint. Operators The following operators can be used to combine multiple features when specifying constraints. Operator Function Description Example feature & feature AND Nodes allocated for the job must have both features lac&ib feature | feature OR Each node allocated for the job must have one feature or the other. lac|vim [ feature | feature ] XOR All nodes allocated for the job must have either one feature or the other. Useful for multi-node shared memory jobs. [intel16|intel18] Multi-Node Jobs Multi-node jobs that use constraints to control which cluster hardware they run on must use the XOR syntax to specify multiple clusters, e.g. '[intel18|intel16]', and not the OR syntax, e.g. 'intel18|intel16'. Multi-node jobs using OR instead of XOR may experience a considerable performance decrease or may be killed by HPCC staff without warning. In most cases where OR is used, the job would be better served by XOR. For this reason, constraints using OR are automatically re-written to use XOR. If an OR constraint is required, prepend the constraint request with 'NOAUTO:'. Automatic Job Constraints When no constraints are specified, a job will get a default constraint of [intel14|intel16|intel18]. This ensures the job runs on only one type of cluster hardware. User specified constraints that don't use the feature|feature or [feature|feature] syntax are automatically combined with [intel14|intel16|intel18] . If a more complex constraint is required, 'NOAUTO:' must be prepended to the constraint. User Specified Constraint Literal Constraint Effective Constraint Result None [intel14|intel16|intel18] [intel14|intel16|intel18] Run this job on nodes that are all the same cluster type --constraint=lac lac&[intel14|intel16|intel18] lac Run this job on only Laconia nodes --constraint= \"lac&ib\" lac&ib&[intel14|intel16|intel18] lac&ib Run this job on only nodes with both lac and ib features --constraint= \"intel16|intel18\" [intel16|intel18] [intel16|intel18] Run this job on nodes that are either all intel16 or all intel18 --constraint= \"NOAUTO:lac|vim\" lac|vim lac|vim Run this job on nodes the each have either the lac or vim feature --constraint= \"[intel16&ib|intel18&ib]\" [intel16&ib|intel18&ib] [intel16&ib|intel18&ib] Run this job on nodes that are either all intel16&ib or all intel18&ib","title":"Job constraints"},{"location":"Job_Constraints/#job-constraints","text":"Constraints are set to restrict which node features are required for a given job. Use -C or --constraint = <list> when submitting your job to specify a constraint.","title":"Job Constraints"},{"location":"Job_Constraints/#operators","text":"The following operators can be used to combine multiple features when specifying constraints. Operator Function Description Example feature & feature AND Nodes allocated for the job must have both features lac&ib feature | feature OR Each node allocated for the job must have one feature or the other. lac|vim [ feature | feature ] XOR All nodes allocated for the job must have either one feature or the other. Useful for multi-node shared memory jobs. [intel16|intel18] Multi-Node Jobs Multi-node jobs that use constraints to control which cluster hardware they run on must use the XOR syntax to specify multiple clusters, e.g. '[intel18|intel16]', and not the OR syntax, e.g. 'intel18|intel16'. Multi-node jobs using OR instead of XOR may experience a considerable performance decrease or may be killed by HPCC staff without warning. In most cases where OR is used, the job would be better served by XOR. For this reason, constraints using OR are automatically re-written to use XOR. If an OR constraint is required, prepend the constraint request with 'NOAUTO:'.","title":"Operators"},{"location":"Job_Constraints/#automatic-job-constraints","text":"When no constraints are specified, a job will get a default constraint of [intel14|intel16|intel18]. This ensures the job runs on only one type of cluster hardware. User specified constraints that don't use the feature|feature or [feature|feature] syntax are automatically combined with [intel14|intel16|intel18] . If a more complex constraint is required, 'NOAUTO:' must be prepended to the constraint. User Specified Constraint Literal Constraint Effective Constraint Result None [intel14|intel16|intel18] [intel14|intel16|intel18] Run this job on nodes that are all the same cluster type --constraint=lac lac&[intel14|intel16|intel18] lac Run this job on only Laconia nodes --constraint= \"lac&ib\" lac&ib&[intel14|intel16|intel18] lac&ib Run this job on only nodes with both lac and ib features --constraint= \"intel16|intel18\" [intel16|intel18] [intel16|intel18] Run this job on nodes that are either all intel16 or all intel18 --constraint= \"NOAUTO:lac|vim\" lac|vim lac|vim Run this job on nodes the each have either the lac or vim feature --constraint= \"[intel16&ib|intel18&ib]\" [intel16&ib|intel18&ib] [intel16&ib|intel18&ib] Run this job on nodes that are either all intel16&ib or all intel18&ib","title":"Automatic Job Constraints"},{"location":"Job_Script_and_Job_Submission/","text":"General rules for writing a job script The sbatch command is used for submitting batch jobs to the cluster (same as Torque's qsub ). sbatch accepts a number of options either from the command line, or (more typically) from a batch job script. In this section, we will show you a simple job script and how to submit it to SLURM. Note the sbatch command only runs on development and compute nodes - it will not work on any gateway node. Job script A job script contains two parts: #SBATCH lines for resource request and command lines for job running. The script should be in plain text format. Below is an example used to explain what each lines means. Note that the job script below is not intended to be used as your job template (more on that later) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #!/bin/bash --login ########## SBATCH Lines for Resource Request ########## #SBATCH --time=00:10:00 # limit of wall clock time - how long the job will run (same as -t) #SBATCH --nodes=1-5 # number of different nodes - could be an exact number or a range of nodes (same as -N) #SBATCH --ntasks=5 # number of tasks - how many tasks (nodes) that you require (same as -n) #SBATCH --cpus-per-task=2 # number of CPUs (or cores) per task (same as -c) #SBATCH --mem-per-cpu=2G # memory required per allocated CPU (or core) #SBATCH --job-name Name_of_Job # you can give your job a name for easier identification (same as -J) ########## Command Lines for Job Running ########## module load GCC/6.4.0-2.28 OpenMPI ### load necessary modules. cd <path_to_code> ### change to the directory where your code is located. srun -n 5 <executable> ### call your executable. (use srun instead of mpirun.) scontrol show job $SLURM_JOB_ID ### write job information to SLURM output file. js -j $SLURM_JOB_ID ### write resource usage to SLURM output file (powertools command). As was previously said, the above job script is not intended to be used as your job template. Your job is most likely going to use much simpler resource specifications than shown above. For purpose of illustration, this job requests 10 minutes walltime, at least 1 at most 5 different nodes, a total of 5 parallel tasks (processes) in distributed memory, 2 cores per task for parallel threads in shared memory and 2 GB memory per core (total 2 GB x 5 tasks x 2 cpus-per-task = 20 GB) with job name \"Name_of_Job\". After this job starts, it first loads two modules: GCC/6.4.0-2.28 and the default version of OpenMPI. Then, change the directory to the path of the code and run the specified executable in 5 parallel tasks. After running the program, it outputs the job information and quits. By default, SLURM will try to use the settings: --nodes=1, --tasks-per-node=1, --cpus-per-task=1, --time=00:01:00 and --mem-per-cpu=750 for each job if any of them can not be acquired from the job specifications. Also, the job script must begin with a specification of a shell type or an interpreter on the first line, such as #!/bin/bash or '#!/usr/bin/python' . All lines starting with #SBATCH need to be placed above the first command line in the script. If they are below the line, the job controller will not execute them and lead to unexpected results. Batch job submission Once the job script has been created, the job can be submitted using the sbatch command. If the command has been submitted successfully, the job controller will issue a job ID on the screen: 1 2 $ sbatch myjob.sb Submitted batch job 8929 Optionally, any job specification (by #SBATCH line) can also be requested by sbatch command line with an equivalent option. For instance, the #SBATCH --nodes=1-5 line could be removed from the job script, and instead be specified from the command line: 1 $ sbatch --nodes = 1 -5 myjob.sb Command line specifications take precedence over those in the job script. Advanced: job with multiple tasks In the SLURM job options, the number of tasks ( --ntasks ) is used for jobs running multiple tasks with distributed memory. It has the same meaning as the number of processes ( -np ) used in mpirun application. This is similar to number of nodes ( nodes= ) used in a Torque ( #PBS ) job script. Each task could also use more than 1 core in shared memory (controlled by --cpus-per-task similar to ppn= in #PBS job script) and each node could run more than 1 task (controlled by --tasks-per-node ). By default, SLURM will use 1 core per task if --cpus-per-task (or -c ) is not specified. It is better for multiple-task jobs to use srun command instead of mpirun to launch a software application. More details about srun, check out the SLURM web site .","title":"General rules for writing a job script"},{"location":"Job_Script_and_Job_Submission/#general-rules-for-writing-a-job-script","text":"The sbatch command is used for submitting batch jobs to the cluster (same as Torque's qsub ). sbatch accepts a number of options either from the command line, or (more typically) from a batch job script. In this section, we will show you a simple job script and how to submit it to SLURM. Note the sbatch command only runs on development and compute nodes - it will not work on any gateway node.","title":"General rules for writing a job script"},{"location":"Job_Script_and_Job_Submission/#job-script","text":"A job script contains two parts: #SBATCH lines for resource request and command lines for job running. The script should be in plain text format. Below is an example used to explain what each lines means. Note that the job script below is not intended to be used as your job template (more on that later) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #!/bin/bash --login ########## SBATCH Lines for Resource Request ########## #SBATCH --time=00:10:00 # limit of wall clock time - how long the job will run (same as -t) #SBATCH --nodes=1-5 # number of different nodes - could be an exact number or a range of nodes (same as -N) #SBATCH --ntasks=5 # number of tasks - how many tasks (nodes) that you require (same as -n) #SBATCH --cpus-per-task=2 # number of CPUs (or cores) per task (same as -c) #SBATCH --mem-per-cpu=2G # memory required per allocated CPU (or core) #SBATCH --job-name Name_of_Job # you can give your job a name for easier identification (same as -J) ########## Command Lines for Job Running ########## module load GCC/6.4.0-2.28 OpenMPI ### load necessary modules. cd <path_to_code> ### change to the directory where your code is located. srun -n 5 <executable> ### call your executable. (use srun instead of mpirun.) scontrol show job $SLURM_JOB_ID ### write job information to SLURM output file. js -j $SLURM_JOB_ID ### write resource usage to SLURM output file (powertools command). As was previously said, the above job script is not intended to be used as your job template. Your job is most likely going to use much simpler resource specifications than shown above. For purpose of illustration, this job requests 10 minutes walltime, at least 1 at most 5 different nodes, a total of 5 parallel tasks (processes) in distributed memory, 2 cores per task for parallel threads in shared memory and 2 GB memory per core (total 2 GB x 5 tasks x 2 cpus-per-task = 20 GB) with job name \"Name_of_Job\". After this job starts, it first loads two modules: GCC/6.4.0-2.28 and the default version of OpenMPI. Then, change the directory to the path of the code and run the specified executable in 5 parallel tasks. After running the program, it outputs the job information and quits. By default, SLURM will try to use the settings: --nodes=1, --tasks-per-node=1, --cpus-per-task=1, --time=00:01:00 and --mem-per-cpu=750 for each job if any of them can not be acquired from the job specifications. Also, the job script must begin with a specification of a shell type or an interpreter on the first line, such as #!/bin/bash or '#!/usr/bin/python' . All lines starting with #SBATCH need to be placed above the first command line in the script. If they are below the line, the job controller will not execute them and lead to unexpected results.","title":"Job script"},{"location":"Job_Script_and_Job_Submission/#batch-job-submission","text":"Once the job script has been created, the job can be submitted using the sbatch command. If the command has been submitted successfully, the job controller will issue a job ID on the screen: 1 2 $ sbatch myjob.sb Submitted batch job 8929 Optionally, any job specification (by #SBATCH line) can also be requested by sbatch command line with an equivalent option. For instance, the #SBATCH --nodes=1-5 line could be removed from the job script, and instead be specified from the command line: 1 $ sbatch --nodes = 1 -5 myjob.sb Command line specifications take precedence over those in the job script.","title":"Batch job submission"},{"location":"Job_Script_and_Job_Submission/#advanced-job-with-multiple-tasks","text":"In the SLURM job options, the number of tasks ( --ntasks ) is used for jobs running multiple tasks with distributed memory. It has the same meaning as the number of processes ( -np ) used in mpirun application. This is similar to number of nodes ( nodes= ) used in a Torque ( #PBS ) job script. Each task could also use more than 1 core in shared memory (controlled by --cpus-per-task similar to ppn= in #PBS job script) and each node could run more than 1 task (controlled by --tasks-per-node ). By default, SLURM will use 1 core per task if --cpus-per-task (or -c ) is not specified. It is better for multiple-task jobs to use srun command instead of mpirun to launch a software application. More details about srun, check out the SLURM web site .","title":"Advanced: job with multiple tasks"},{"location":"Job_with_Checkpointing_Run/","text":"Job with Checkpointing Run Checkpointing is a function to save a snapshot of an application's running state, so it can restart from the saved point in case job running fails or reaches the time limit. Some applications might already have this feature for long-term computation. If users develop their own program, it is encouraged to implement checkpointing as a part of their codes. They can develop a function to write result variables to file systems at regular intervals and a function to read those variables in when restart. However if the program you used does not and can not include the feature, you may consider using \"Distributed MultiThreaded CheckPointing\" (or DMTCP) installed on HPCC nodes. DMTCP is a tool for transparently checkpointing the state of a distributed program spread across many machines without modifying the user's program or the operating system kernel. For more details about DMTCP, please refer to their web site . Below we show examples of using DMTCP in HPCC system: Checkpoint with DMTCP It shows an example job script to do checkpointing by DMTCP commands. Powertools longjob by DMTCP It introduces how to use the longjob powertool for checkpointing on HPCC.","title":"Checkpointing"},{"location":"Job_with_Checkpointing_Run/#job-with-checkpointing-run","text":"Checkpointing is a function to save a snapshot of an application's running state, so it can restart from the saved point in case job running fails or reaches the time limit. Some applications might already have this feature for long-term computation. If users develop their own program, it is encouraged to implement checkpointing as a part of their codes. They can develop a function to write result variables to file systems at regular intervals and a function to read those variables in when restart. However if the program you used does not and can not include the feature, you may consider using \"Distributed MultiThreaded CheckPointing\" (or DMTCP) installed on HPCC nodes. DMTCP is a tool for transparently checkpointing the state of a distributed program spread across many machines without modifying the user's program or the operating system kernel. For more details about DMTCP, please refer to their web site . Below we show examples of using DMTCP in HPCC system:","title":"Job with Checkpointing Run"},{"location":"Job_with_Checkpointing_Run/#checkpoint-with-dmtcp","text":"It shows an example job script to do checkpointing by DMTCP commands.","title":"Checkpoint with DMTCP"},{"location":"Job_with_Checkpointing_Run/#powertools-longjob-by-dmtcp","text":"It introduces how to use the longjob powertool for checkpointing on HPCC.","title":"Powertools longjob by DMTCP"},{"location":"Kettering_Users/","text":"Kettering Users User-specific information This information is for users at Kettering University. Users with MSU NetIDs should disregard this information. 1) Request a Community ID You will need to request a Community ID at the following link: https://community.idm.msu.edu/selfservice/ 2) Click on link sent in Community ID verification email and verify the account 3) Go to the following link and specify who your PI is and you will receive a confirmation email with your username in the footer of the email https://contact.icer.msu.edu/community_id","title":"Kettering U"},{"location":"Kettering_Users/#kettering-users","text":"","title":"Kettering Users"},{"location":"Kettering_Users/#user-specific-information","text":"This information is for users at Kettering University. Users with MSU NetIDs should disregard this information. 1) Request a Community ID You will need to request a Community ID at the following link: https://community.idm.msu.edu/selfservice/ 2) Click on link sent in Community ID verification email and verify the account 3) Go to the following link and specify who your PI is and you will receive a confirmation email with your username in the footer of the email https://contact.icer.msu.edu/community_id","title":"User-specific information"},{"location":"LabNotebook_AntiSmash/","tags":["lab notebook","antismash"],"text":"CAUTION: The is as a Lab Notebook which describes how a specific problem at a specific time. Please keep this in mind as you read and use the content and pay attention to the date, version information and other details. Lab Notebook --- Installing AntiSMASH on HPCC using Conda AntiSMASH (The antibiotics & Secondary Metabolite Analysis Shell) is bioinformatics program for identifying genes belonging to secondary metabolite pathways, particularly in plant, fungi and bacteria species. Documentation for AntiSMASH can be found at https://docs.antismash.secondarymetabolites.org , but you will need to follow the instruction below to install AntiSMASH in your home directory on HPCC. Small analyses can be done through their webportal (see https://docs.antismash.secondarymetabolites.org/website_submission/ ), however if your analysis requires running AntiSMASH on the HPCC, please see the instruction below for how to install this program localy using Anaconda If you have not installed Anaconda in you home directory, see https://docs.icer.msu.edu/Using_conda/ Installing AnitSMASH 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # Clear modules and load Anaconda module purge module load Conda/3 # Add biconda to your Anaconda install conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge conda config --set channel_priority strict # Create a cona environemnt for antimash and its depdencies conda create -n antismash conda activate antismash # As of version 6.1, you need to install each of these # programs first before install antismash or it never resolves # Seems to be an issue with the conda recipie, not AntiSMASH itself conda install hmmer2 conda install hmmer conda install diamond conda install fasttree conda install prodigal conda install blast conda install muscle conda install glimmerhmm # Once the above installs have complete, this command will # install antismash and update the versions of the above programs # as needed. # # You may be asked to 'DOWNGRADE' diamond, hmmer, muslce, # and some perl libraries; this is normal conda install antismash # Test your install antismash --check-prereqs antismash --help # Download databases download-antismash-databases Once you have completed the above steps, to run AntiSMASH in the future, do: 1 2 3 4 5 6 7 module purge module load Conda/3 conda activate antismash # check antismash --help","title":"AntiSMASH"},{"location":"LabNotebook_AntiSmash/#lab-notebook-installing-antismash-on-hpcc-using-conda","text":"AntiSMASH (The antibiotics & Secondary Metabolite Analysis Shell) is bioinformatics program for identifying genes belonging to secondary metabolite pathways, particularly in plant, fungi and bacteria species. Documentation for AntiSMASH can be found at https://docs.antismash.secondarymetabolites.org , but you will need to follow the instruction below to install AntiSMASH in your home directory on HPCC. Small analyses can be done through their webportal (see https://docs.antismash.secondarymetabolites.org/website_submission/ ), however if your analysis requires running AntiSMASH on the HPCC, please see the instruction below for how to install this program localy using Anaconda If you have not installed Anaconda in you home directory, see https://docs.icer.msu.edu/Using_conda/ Installing AnitSMASH 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # Clear modules and load Anaconda module purge module load Conda/3 # Add biconda to your Anaconda install conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge conda config --set channel_priority strict # Create a cona environemnt for antimash and its depdencies conda create -n antismash conda activate antismash # As of version 6.1, you need to install each of these # programs first before install antismash or it never resolves # Seems to be an issue with the conda recipie, not AntiSMASH itself conda install hmmer2 conda install hmmer conda install diamond conda install fasttree conda install prodigal conda install blast conda install muscle conda install glimmerhmm # Once the above installs have complete, this command will # install antismash and update the versions of the above programs # as needed. # # You may be asked to 'DOWNGRADE' diamond, hmmer, muslce, # and some perl libraries; this is normal conda install antismash # Test your install antismash --check-prereqs antismash --help # Download databases download-antismash-databases Once you have completed the above steps, to run AntiSMASH in the future, do: 1 2 3 4 5 6 7 module purge module load Conda/3 conda activate antismash # check antismash --help","title":"Lab Notebook --- Installing AntiSMASH on HPCC using Conda"},{"location":"Learning_the_Shell/","text":"Learning the Shell Variables - Part I Variables - Part II Expansion Conditional statements Loops","title":"Learning the Shell"},{"location":"Learning_the_Shell/#learning-the-shell","text":"Variables - Part I Variables - Part II Expansion Conditional statements Loops","title":"Learning the Shell"},{"location":"Linux_Command_Line_Interface_for_Beginners_I/","text":"Linux Command Line Interface for Beginners I The OS of MSU HPC is CentOS which is a brach of Linux. So if you want to use our system, it is essential to equip some basic knowledge on Linux. Even though Linux supports GUI, most works are done on a dummy terminal via texts. The Linux command line is a text interface to Linux. We will walk through some practical exercises to become familiar with a few basic commands and concept. Navigation Relative and absolute Paths Creating and removing directories Creating and removing files Copying moving/renaming directories and files Listing directories and files Exercise Navigation Let's run the first command. Type pwd and pressing the Enter or Return key to run it (From now, I'll not mention pressing Enter/Return key part to run command). 1 2 $ pwd /mnt/home/iamsam/ You will see a path such as /mnt/home/user/your_id pwd is an abbreviation of 'print working directory'. It print out the shell's current working directory. You can change the working directory using the cd command, an abbereviation for 'change directory'. 1 2 3 $ cd / $ pwd / Now your working directory is '/' which is the root directory. There is nothing much you can do on the root directory, so let's go to your 'home' directory. 1 2 3 $ cd $ pwd /mnt/home/iamsam/ Regardless of your location, when you just type ' cd ', you will be home. You can also type ' cd \\~ ' instead of ' cd ' to be back your home. To go the previous directory, type ' cd - ' 1 2 3 $ cd - $ pwd / The root directory has many subdirectories including your home directory. Let's go to ' bin ' directory. 1 2 3 $ cd bin $ pwd /bin To go up to the parent directory (it is / for us now), use the special syntax of two dots with cd such as 1 2 3 $ cd .. $ pwd / To go up to the previous directory, use - with cd such as 1 2 3 $ cd - $ pwd /bin You can use .. more than once if you have to move up multiple levels of parent directories. 1 2 $ cd ../.. $ pwd Relative and absolute Paths A path is an address of a directory. Most of the examples we've looked at so far uses relative paths. So you final location is decides based on your current working directory. However, sometimes you want to use an absolute path than relative one. Your home's absolute path at HPC is /mnt/home/your_id. See the example to find how to use a relative and absolute paths. 1 2 3 4 5 6 7 8 9 10 $ cd / $ pwd / $ cd - $ pwd /mnt/home/iamsam $ cd / $ cd /mnt/home/iamsam $ pwd /mnt/home/iamsam Creating and removing directories To make a directory, use mkdir (short for 'make directory') such as 1 2 3 $ mkdir temp01 $ls temp01 ls is a command to list files and folder. We will learn it in a minute. You can create multiple directories as well. 1 2 3 $ mkdir temp02 temp03 temp04 $ ls temp01 temp02 temp03 temp04 To make a subdirectory, use with p option such as 1 2 3 $ mkdir -p temp04/temp05/temp06 $ ls temp04 temp05 To remove directory use rm command with r . Without r option, rm will not delete directories (but you can delete files). r means recursive. 1 2 3 $ rm temp04 rm: cannot remove 'temp04' : Is a directory $ rm -r temp04 Creating and removing files You can use editors to create files, but it is out of scope of this tutorial. Let's use ls and a pipe > (we will explain pipes later). 1 2 3 4 5 $ ls temp01 temp02 temp03 $ ls > list.txt $ ls list.txt temp01 temp02 temp03 Now we have three directories (temp01, temp02, temp03) and one file (list.txt). We already know how to delete directories. To delete files, we use command rm such as 1 2 3 $ rm list.txt $ ls temp01 temp02 temp03 Copying and renaming directories and files To copy files or directories, use cp such as 1 2 3 $ cp list.txt list2.txt $ ls list.txt list2.txt temp01 temp02 temp03 With r option, you can copy files and directories recursively. i.e., copy subdirectories and files. mv command mv files/directories or rename them. 1 2 3 4 5 6 7 8 9 10 11 $ mv list2.txt temp01 $ ls list.txt temp01 temp02 temp03 $ ls temp01 list2.txt $ cd temp01 $ mv list2.txt list3.txt $ ls list3.txt Listing directories and files We already used ls . To list directories and files, us ls (short for 'list'). 1 2 $ ls temp01 temp02 temp03 There are many options for ls . Most frequently used options are a: list all files and directories including hidden contents h: print sizes in human readable format (e.g.: 1K, 2.4M, 3.1G) l: list with a long listing format t: sort my modification time You can use options separately like ls -l -a -t or together like ls -lat. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ ls -l -a -t total 8 -rw-r--r-- 1 iamsam staff 30 Mar 23 15 :37 list.txt drwxr-xr-x 6 iamsam staff 192 Mar 23 15 :37 . drwxr-xr-x 2 iamsam staff 64 Mar 23 15 :37 temp03 drwxr-xr-x 2 iamsam staff 64 Mar 23 15 :37 temp02 drwxr-xr-x 2 iamsam staff 64 Mar 23 15 :37 temp01 drwxr-xr-x+ 113 iamsam staff 3616 Mar 23 15 :21 .. $ ls -lat total 8 -rw-r--r-- 1 iamsam staff 30 Mar 23 15 :37 list.txt drwxr-xr-x 6 iamsam staff 192 Mar 23 15 :37 . drwxr-xr-x 2 iamsam staff 64 Mar 23 15 :37 temp03 drwxr-xr-x 2 iamsam staff 64 Mar 23 15 :37 temp02 drwxr-xr-x 2 iamsam staff 64 Mar 23 15 :37 temp01 drwxr-xr-x+ 113 iamsam staff 3616 Mar 23 15 :21 .. Exercise Now let's do some exercise. Log in your MSU HPC account and go to any dev-node. Create linux_tutorial dir on your home. Copy a folder and contents for this tutorial from /mnt/research/common-data/workshops/intro2Linux_iamsam to linux_tutorial dir on your home Go to linux_tutorial Find a hidden directory and rename it to not_hidden Check the contents of not_hidden Create a new directory called new_dir Copy the file youfoundit.txt into new_dir remove garbage dir This is an answer (I do not include the login process). Expand source 1 2 3 4 5 6 7 8 9 $ mkdir linux_workshop $ cp -r /mnt/research/common-data/workshops/intro2Linux_iamsam linux_tutorial $ cp linux_tutorial $ ls -a $ mv .hidden not_hidden $ ls not_hidden $mkdir new_dir $ cp not_hidden/youfoundit.txt new_dir $ rm -r garbage","title":"Linux Command Line Interface for Beginners I"},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#linux-command-line-interface-for-beginners-i","text":"The OS of MSU HPC is CentOS which is a brach of Linux. So if you want to use our system, it is essential to equip some basic knowledge on Linux. Even though Linux supports GUI, most works are done on a dummy terminal via texts. The Linux command line is a text interface to Linux. We will walk through some practical exercises to become familiar with a few basic commands and concept. Navigation Relative and absolute Paths Creating and removing directories Creating and removing files Copying moving/renaming directories and files Listing directories and files Exercise","title":"Linux Command Line Interface for Beginners I"},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#navigation","text":"Let's run the first command. Type pwd and pressing the Enter or Return key to run it (From now, I'll not mention pressing Enter/Return key part to run command). 1 2 $ pwd /mnt/home/iamsam/ You will see a path such as /mnt/home/user/your_id pwd is an abbreviation of 'print working directory'. It print out the shell's current working directory. You can change the working directory using the cd command, an abbereviation for 'change directory'. 1 2 3 $ cd / $ pwd / Now your working directory is '/' which is the root directory. There is nothing much you can do on the root directory, so let's go to your 'home' directory. 1 2 3 $ cd $ pwd /mnt/home/iamsam/ Regardless of your location, when you just type ' cd ', you will be home. You can also type ' cd \\~ ' instead of ' cd ' to be back your home. To go the previous directory, type ' cd - ' 1 2 3 $ cd - $ pwd / The root directory has many subdirectories including your home directory. Let's go to ' bin ' directory. 1 2 3 $ cd bin $ pwd /bin To go up to the parent directory (it is / for us now), use the special syntax of two dots with cd such as 1 2 3 $ cd .. $ pwd / To go up to the previous directory, use - with cd such as 1 2 3 $ cd - $ pwd /bin You can use .. more than once if you have to move up multiple levels of parent directories. 1 2 $ cd ../.. $ pwd","title":"Navigation"},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#relative-and-absolute-paths","text":"A path is an address of a directory. Most of the examples we've looked at so far uses relative paths. So you final location is decides based on your current working directory. However, sometimes you want to use an absolute path than relative one. Your home's absolute path at HPC is /mnt/home/your_id. See the example to find how to use a relative and absolute paths. 1 2 3 4 5 6 7 8 9 10 $ cd / $ pwd / $ cd - $ pwd /mnt/home/iamsam $ cd / $ cd /mnt/home/iamsam $ pwd /mnt/home/iamsam","title":"Relative and absolute Paths"},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#creating-and-removing-directories","text":"To make a directory, use mkdir (short for 'make directory') such as 1 2 3 $ mkdir temp01 $ls temp01 ls is a command to list files and folder. We will learn it in a minute. You can create multiple directories as well. 1 2 3 $ mkdir temp02 temp03 temp04 $ ls temp01 temp02 temp03 temp04 To make a subdirectory, use with p option such as 1 2 3 $ mkdir -p temp04/temp05/temp06 $ ls temp04 temp05 To remove directory use rm command with r . Without r option, rm will not delete directories (but you can delete files). r means recursive. 1 2 3 $ rm temp04 rm: cannot remove 'temp04' : Is a directory $ rm -r temp04","title":"Creating and removing directories"},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#creating-and-removing-files","text":"You can use editors to create files, but it is out of scope of this tutorial. Let's use ls and a pipe > (we will explain pipes later). 1 2 3 4 5 $ ls temp01 temp02 temp03 $ ls > list.txt $ ls list.txt temp01 temp02 temp03 Now we have three directories (temp01, temp02, temp03) and one file (list.txt). We already know how to delete directories. To delete files, we use command rm such as 1 2 3 $ rm list.txt $ ls temp01 temp02 temp03","title":"Creating and removing files"},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#copying-and-renaming-directories-and-files","text":"To copy files or directories, use cp such as 1 2 3 $ cp list.txt list2.txt $ ls list.txt list2.txt temp01 temp02 temp03 With r option, you can copy files and directories recursively. i.e., copy subdirectories and files. mv command mv files/directories or rename them. 1 2 3 4 5 6 7 8 9 10 11 $ mv list2.txt temp01 $ ls list.txt temp01 temp02 temp03 $ ls temp01 list2.txt $ cd temp01 $ mv list2.txt list3.txt $ ls list3.txt","title":"Copying and renaming directories and files"},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#listing-directories-and-files","text":"We already used ls . To list directories and files, us ls (short for 'list'). 1 2 $ ls temp01 temp02 temp03 There are many options for ls . Most frequently used options are a: list all files and directories including hidden contents h: print sizes in human readable format (e.g.: 1K, 2.4M, 3.1G) l: list with a long listing format t: sort my modification time You can use options separately like ls -l -a -t or together like ls -lat. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ ls -l -a -t total 8 -rw-r--r-- 1 iamsam staff 30 Mar 23 15 :37 list.txt drwxr-xr-x 6 iamsam staff 192 Mar 23 15 :37 . drwxr-xr-x 2 iamsam staff 64 Mar 23 15 :37 temp03 drwxr-xr-x 2 iamsam staff 64 Mar 23 15 :37 temp02 drwxr-xr-x 2 iamsam staff 64 Mar 23 15 :37 temp01 drwxr-xr-x+ 113 iamsam staff 3616 Mar 23 15 :21 .. $ ls -lat total 8 -rw-r--r-- 1 iamsam staff 30 Mar 23 15 :37 list.txt drwxr-xr-x 6 iamsam staff 192 Mar 23 15 :37 . drwxr-xr-x 2 iamsam staff 64 Mar 23 15 :37 temp03 drwxr-xr-x 2 iamsam staff 64 Mar 23 15 :37 temp02 drwxr-xr-x 2 iamsam staff 64 Mar 23 15 :37 temp01 drwxr-xr-x+ 113 iamsam staff 3616 Mar 23 15 :21 ..","title":"Listing directories and files"},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#exercise","text":"Now let's do some exercise. Log in your MSU HPC account and go to any dev-node. Create linux_tutorial dir on your home. Copy a folder and contents for this tutorial from /mnt/research/common-data/workshops/intro2Linux_iamsam to linux_tutorial dir on your home Go to linux_tutorial Find a hidden directory and rename it to not_hidden Check the contents of not_hidden Create a new directory called new_dir Copy the file youfoundit.txt into new_dir remove garbage dir This is an answer (I do not include the login process). Expand source 1 2 3 4 5 6 7 8 9 $ mkdir linux_workshop $ cp -r /mnt/research/common-data/workshops/intro2Linux_iamsam linux_tutorial $ cp linux_tutorial $ ls -a $ mv .hidden not_hidden $ ls not_hidden $mkdir new_dir $ cp not_hidden/youfoundit.txt new_dir $ rm -r garbage","title":"Exercise"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/","text":"Linux Command Line Interface for Beginners II Files and folders File permissions Concatenate files Redirection Wildcards Manuals Commands of monitoring files Archiving and compression Environment variables Files and folders Linux has a single directory 'tree', separated by slash, the top is the 'root'. All additional disks are connected on /mnt ('mounted'). In Linux, there is no concept of driver letters. tree is a recursive directory listing program which prints a depth indented listing of files. With no arguments, tree lists the files in the current directory. You can change the depth with -L argument. The example shows the structure of the 'linux_tutorial' directory which we used for an exercise. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 $ tree . \u251c\u2500\u2500 000 .txt \u251c\u2500\u2500 001 .txt \u251c\u2500\u2500 002 .txt \u251c\u2500\u2500 009 .txt \u251c\u2500\u2500 010 .txt \u251c\u2500\u2500 a.txt \u251c\u2500\u2500 A.txt \u251c\u2500\u2500 b.txt \u251c\u2500\u2500 B.txt \u251c\u2500\u2500 c.txt \u251c\u2500\u2500 C.txt \u251c\u2500\u2500 garbage \u2502 \u251c\u2500\u2500 garbage_01 \u2502 \u251c\u2500\u2500 garbage_02 \u2502 \u2514\u2500\u2500 garbage_03 \u251c\u2500\u2500 my_data.dat \u251c\u2500\u2500 my-data.txt \u251c\u2500\u2500 z.txt \u2514\u2500\u2500 Z.txt $ tree -L 1 . \u251c\u2500\u2500 000 .txt \u251c\u2500\u2500 001 .txt \u251c\u2500\u2500 002 .txt \u251c\u2500\u2500 009 .txt \u251c\u2500\u2500 010 .txt \u251c\u2500\u2500 a.txt \u251c\u2500\u2500 A.txt \u251c\u2500\u2500 b.txt \u251c\u2500\u2500 B.txt \u251c\u2500\u2500 c.txt \u251c\u2500\u2500 C.txt \u251c\u2500\u2500 garbage \u251c\u2500\u2500 my_data.dat \u251c\u2500\u2500 my-data.txt \u251c\u2500\u2500 z.txt \u2514\u2500\u2500 Z.txt tree command does not show hidden files/directory by default. To see hidden files/directories, use -a argument. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ tree -a . \u251c\u2500\u2500 000 .txt \u251c\u2500\u2500 001 .txt \u251c\u2500\u2500 002 .txt \u251c\u2500\u2500 009 .txt \u251c\u2500\u2500 010 .txt \u251c\u2500\u2500 a.txt \u251c\u2500\u2500 A.txt \u251c\u2500\u2500 b.txt \u251c\u2500\u2500 B.txt \u251c\u2500\u2500 c.txt \u251c\u2500\u2500 C.txt \u251c\u2500\u2500 garbage \u2502 \u251c\u2500\u2500 garbage_01 \u2502 \u251c\u2500\u2500 garbage_02 \u2502 \u2514\u2500\u2500 garbage_03 \u251c\u2500\u2500 .hidden \u2502 \u2514\u2500\u2500 youfoundit.txt \u251c\u2500\u2500 my_data.dat \u251c\u2500\u2500 my-data.txt \u251c\u2500\u2500 z.txt \u2514\u2500\u2500 Z.txt 2 directories, 19 files File permissions Linux has a method to keep files private and safe. When you type ls -a you will get a lot of information of your directory including files such as The first letter shows a type of a file. -: normal file d: directory l: symbolic link Next nine characters shows permission; first three is for you, next three is for your group members, and last three is for a whole world. Let's see the first file in the picture which shows d rwxr-xr-x . The first character is 'd', and therefore, even the name is 'hello.txt', it is not a file, but a directory. Next nine charcters represent the settings for the three sets of permissions. The first three characters show the permissions for the user who owns the file (user permissions). The middle three characters show the permissions for members of the file\u2019s group (group permissions). The last three characters show the permissions for anyone not in the first two categories (other permissions). The letters represent: r: Read permissions. The file can be opened, and its content viewed. w: Write permissions. The file can be edited, modified, and deleted. x: Execute permissions. If the file is a script or a program, it can be run (executed). In our screenshot, the first three characters are 'rwx' which means you(owner) can read, write/modify or execute (it means you can go inside the directory here). Next three characters are 'r_x' which means your group members can read, or execute, but can not modify the contents. Last three characters are 'r_x' which means everyone else can read, and execute. Another examples: ---------: it means no permissions have been granted at all. rwxrwxrwx: it means full permissions have been granted to everyone. Permissions can be set with chmod command, and ownership set with chown . You can only change these for files you are the owner of. To use chmod , we need to tell it 'who' we are setting permissions for, 'what' change we are making, 'which' permissions we are setting. The \u201cwho\u201d values can be: u: User - the owner of the file. g: Group - members of the group the file belongs to. o: Others - people not governed by the u and g permissions. a: All - all of the above. If none of these are used, chmod behaves as if \u201ca\u201d had been used. The \u201cwhat\u201d values can be: \u2013: Minus sign. Removes the permission. +: Plus sign. Grants the permission. The permission is added to the existing permissions. If you want to have this permission and only this permission set, use the = option, described below. =: Equals sign. Set a permission and remove others. The \u201cwhich \u201d values can be: r: The read permission. w: The write permission. x: The execute permission. You can also use a three three-digit numbers (total nine) to provide the permission with chmod . The leftmost digit represents the permissions for the owner. The middle digit represents the permissions for the group members. The rightmost digit represents the permissions for the others. x is 1, w is 2, and r is 4. and the some of these three numbers are the permission. The digits you can use and what they represent are listed here: 0: (000) No permission. 1: (001) Execute permission. 2: (010) Write permission. 3: (011) Write and execute permissions. 4: (100) Read permission. 5: (101) Read and execute permissions. 6: (110) Read and write permissions. 7: (111) Read, write, and execute permissions. The tables shows a summary of the chmod command. User Type Permission u - user + add r - read (4) g - group - delete w - write (2) o - others = set exactly x - execute (1) a - all Examples: chmod u=rx, og=r my_file1.txt : set the user has read, and executable permissions; group/other have read permission only for my_file1.txt chmod 544 my_file1.txt : same as chmod u=rx, og=r my_file1.txt. chmod 750 my_file1.txt : set the user has read, write, and executable permissions; group has read/executable permission; others no permission. Concatenate files The cat (short for \u201c concatenate \u201c) command is one of the most frequently used command in Linux. cat command allows us to create single or multiple files, view contain of file, concatenate files and redirect output in terminal or files. To display contents of a file: cat filename To view contents of multiple files: cat file1 file2 To create a file: cat >file2.txt If file content is large, and won't fit in a terminal screen, you can use more and less with cat command such as (we are using pipes |, which will be covered later) cat file2.txt |more cat file2.txt |less more, less, and most (most has more features than more and less commands) are commands to open a given file for interactive reading. Redirection Redirection is a feature such that when you execute a command with it, you can change the standard input/output devices. The standard input (stdin) device is the keyboard, and the standard output (stdout) device is the screen. With redirection, stdin/stdout can be changed. > : Output redirection (overwriting) eg. ls -la > list.txt (ls -la results save as list.txt instead of standard output (screen)) >> : Output redirection (appending) eg. ls -l >> list.txt (ls -l result will be appended at the end of the list.txt. If there is no list.txt, it works as ls -la > list.txt) < : Input redirection >& : writing the output from one file to the input of another file. eg. matlab > outfile 2>&1 : send stdout and stderr to 'outfile'. Here 1 and 2 are file descriptors. File descriptor 1 is the standard output (stdout), and 2 is the standard error (stderr). > is redirection, and & indicates that what follows and precedes is a file descriptor and not a filename. Wildcards Wildcards are symbols or special characters that represent other characters. You can use them with any command such as ls/rm/cp etc. *: anything or nothing ?: single character [ ]: any character or range of characters [! ]: inverse the match of [ ] ls *.txt # list all txt files ls *-?.txt # list all files with \u2018-\u2018 and with one cha in front of \u2018.txt\u2019 ls [0-9]*.txt # list all files start with a number. ls [A-Z]*.txt # list all files start with a capital letter? [A-Z] can be different based on LC_COLLATE value. For further discussion, check here . In HPC at MSU, the default of [A-Z] is a, A, b, B, c, C, ....y, Y, z, Z, which is standard collations (en_US). Try ls [[:lower:]].txt ; ls [[:upper:]].txt ; ls [[:lower:][:upper:]].txt ls [!a-Z]*.tx t # list txt files that don\u2019t begin with any letter. For more informaiton, refer to Regular Expressions . Manuals Linux includes a built in manual for nearly all commands. Type \u2018man\u2019 followed by the commands. e.g. man man To navigate the man pages use the arrow keys to scroll up and down or use the enter key to advance al ine, space bar to advance a page, letter u to go back a page. Use the q key to quit out of the display. The manual pages often include these sections: Name: a one line desctiption of what it does Synopsis: basic syntax for the command line. Description: describes the program\u2019s functionalities. Options: lists commnand line options available for this program. Example: examples of some of the options available. See Also: list of related commands. Note that options can be with single dash \u2018-\u2018 or double dash \u2018\u2014\u2019 Commands of monitoring files Following commands are used for monitering files. wc : count words, lines or characters eg. who | wc -l # number of users logged in grep : find patterns in files or data, returns just matching lines eg. who |grep $USER sort : given a list of items, sort in various ways eg. who | sort head : list only top n lines of file who > who.txt; head who.txt tail : list only last n lines of file Archiving and compression Following commands are used for archiving and compression: zip, unzip, tar unzip: unzip tar: create (tar -c) or extract (tar -x) \u2018tape archive\u2019 file. Exercise : Using the man pages, find out what the default number of lines that head and tail will display, and how to limit those to just one line Can you tar all files and folders in workshop folder? Question? Use man. Environment variables Shell maintains and you can set \u2018variables\u2019 that the shell uses for configuration and in your script. Variables start with $, and can be seen with echo $VARNAME Explore common variables with the echo command and list what they are $HOME , $USER , $SHELL , $PATH . For more information, please refer to 1. Variables - Part I","title":"Linux Command Line Interface for Beginners II"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#linux-command-line-interface-for-beginners-ii","text":"Files and folders File permissions Concatenate files Redirection Wildcards Manuals Commands of monitoring files Archiving and compression Environment variables","title":"Linux Command Line Interface for Beginners II"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#files-and-folders","text":"Linux has a single directory 'tree', separated by slash, the top is the 'root'. All additional disks are connected on /mnt ('mounted'). In Linux, there is no concept of driver letters. tree is a recursive directory listing program which prints a depth indented listing of files. With no arguments, tree lists the files in the current directory. You can change the depth with -L argument. The example shows the structure of the 'linux_tutorial' directory which we used for an exercise. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 $ tree . \u251c\u2500\u2500 000 .txt \u251c\u2500\u2500 001 .txt \u251c\u2500\u2500 002 .txt \u251c\u2500\u2500 009 .txt \u251c\u2500\u2500 010 .txt \u251c\u2500\u2500 a.txt \u251c\u2500\u2500 A.txt \u251c\u2500\u2500 b.txt \u251c\u2500\u2500 B.txt \u251c\u2500\u2500 c.txt \u251c\u2500\u2500 C.txt \u251c\u2500\u2500 garbage \u2502 \u251c\u2500\u2500 garbage_01 \u2502 \u251c\u2500\u2500 garbage_02 \u2502 \u2514\u2500\u2500 garbage_03 \u251c\u2500\u2500 my_data.dat \u251c\u2500\u2500 my-data.txt \u251c\u2500\u2500 z.txt \u2514\u2500\u2500 Z.txt $ tree -L 1 . \u251c\u2500\u2500 000 .txt \u251c\u2500\u2500 001 .txt \u251c\u2500\u2500 002 .txt \u251c\u2500\u2500 009 .txt \u251c\u2500\u2500 010 .txt \u251c\u2500\u2500 a.txt \u251c\u2500\u2500 A.txt \u251c\u2500\u2500 b.txt \u251c\u2500\u2500 B.txt \u251c\u2500\u2500 c.txt \u251c\u2500\u2500 C.txt \u251c\u2500\u2500 garbage \u251c\u2500\u2500 my_data.dat \u251c\u2500\u2500 my-data.txt \u251c\u2500\u2500 z.txt \u2514\u2500\u2500 Z.txt tree command does not show hidden files/directory by default. To see hidden files/directories, use -a argument. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ tree -a . \u251c\u2500\u2500 000 .txt \u251c\u2500\u2500 001 .txt \u251c\u2500\u2500 002 .txt \u251c\u2500\u2500 009 .txt \u251c\u2500\u2500 010 .txt \u251c\u2500\u2500 a.txt \u251c\u2500\u2500 A.txt \u251c\u2500\u2500 b.txt \u251c\u2500\u2500 B.txt \u251c\u2500\u2500 c.txt \u251c\u2500\u2500 C.txt \u251c\u2500\u2500 garbage \u2502 \u251c\u2500\u2500 garbage_01 \u2502 \u251c\u2500\u2500 garbage_02 \u2502 \u2514\u2500\u2500 garbage_03 \u251c\u2500\u2500 .hidden \u2502 \u2514\u2500\u2500 youfoundit.txt \u251c\u2500\u2500 my_data.dat \u251c\u2500\u2500 my-data.txt \u251c\u2500\u2500 z.txt \u2514\u2500\u2500 Z.txt 2 directories, 19 files","title":"Files and folders"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#file-permissions","text":"Linux has a method to keep files private and safe. When you type ls -a you will get a lot of information of your directory including files such as The first letter shows a type of a file. -: normal file d: directory l: symbolic link Next nine characters shows permission; first three is for you, next three is for your group members, and last three is for a whole world. Let's see the first file in the picture which shows d rwxr-xr-x . The first character is 'd', and therefore, even the name is 'hello.txt', it is not a file, but a directory. Next nine charcters represent the settings for the three sets of permissions. The first three characters show the permissions for the user who owns the file (user permissions). The middle three characters show the permissions for members of the file\u2019s group (group permissions). The last three characters show the permissions for anyone not in the first two categories (other permissions). The letters represent: r: Read permissions. The file can be opened, and its content viewed. w: Write permissions. The file can be edited, modified, and deleted. x: Execute permissions. If the file is a script or a program, it can be run (executed). In our screenshot, the first three characters are 'rwx' which means you(owner) can read, write/modify or execute (it means you can go inside the directory here). Next three characters are 'r_x' which means your group members can read, or execute, but can not modify the contents. Last three characters are 'r_x' which means everyone else can read, and execute. Another examples: ---------: it means no permissions have been granted at all. rwxrwxrwx: it means full permissions have been granted to everyone. Permissions can be set with chmod command, and ownership set with chown . You can only change these for files you are the owner of. To use chmod , we need to tell it 'who' we are setting permissions for, 'what' change we are making, 'which' permissions we are setting. The \u201cwho\u201d values can be: u: User - the owner of the file. g: Group - members of the group the file belongs to. o: Others - people not governed by the u and g permissions. a: All - all of the above. If none of these are used, chmod behaves as if \u201ca\u201d had been used. The \u201cwhat\u201d values can be: \u2013: Minus sign. Removes the permission. +: Plus sign. Grants the permission. The permission is added to the existing permissions. If you want to have this permission and only this permission set, use the = option, described below. =: Equals sign. Set a permission and remove others. The \u201cwhich \u201d values can be: r: The read permission. w: The write permission. x: The execute permission. You can also use a three three-digit numbers (total nine) to provide the permission with chmod . The leftmost digit represents the permissions for the owner. The middle digit represents the permissions for the group members. The rightmost digit represents the permissions for the others. x is 1, w is 2, and r is 4. and the some of these three numbers are the permission. The digits you can use and what they represent are listed here: 0: (000) No permission. 1: (001) Execute permission. 2: (010) Write permission. 3: (011) Write and execute permissions. 4: (100) Read permission. 5: (101) Read and execute permissions. 6: (110) Read and write permissions. 7: (111) Read, write, and execute permissions. The tables shows a summary of the chmod command. User Type Permission u - user + add r - read (4) g - group - delete w - write (2) o - others = set exactly x - execute (1) a - all Examples: chmod u=rx, og=r my_file1.txt : set the user has read, and executable permissions; group/other have read permission only for my_file1.txt chmod 544 my_file1.txt : same as chmod u=rx, og=r my_file1.txt. chmod 750 my_file1.txt : set the user has read, write, and executable permissions; group has read/executable permission; others no permission.","title":"File permissions"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#concatenate-files","text":"The cat (short for \u201c concatenate \u201c) command is one of the most frequently used command in Linux. cat command allows us to create single or multiple files, view contain of file, concatenate files and redirect output in terminal or files. To display contents of a file: cat filename To view contents of multiple files: cat file1 file2 To create a file: cat >file2.txt If file content is large, and won't fit in a terminal screen, you can use more and less with cat command such as (we are using pipes |, which will be covered later) cat file2.txt |more cat file2.txt |less more, less, and most (most has more features than more and less commands) are commands to open a given file for interactive reading.","title":"Concatenate files"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#redirection","text":"Redirection is a feature such that when you execute a command with it, you can change the standard input/output devices. The standard input (stdin) device is the keyboard, and the standard output (stdout) device is the screen. With redirection, stdin/stdout can be changed. > : Output redirection (overwriting) eg. ls -la > list.txt (ls -la results save as list.txt instead of standard output (screen)) >> : Output redirection (appending) eg. ls -l >> list.txt (ls -l result will be appended at the end of the list.txt. If there is no list.txt, it works as ls -la > list.txt) < : Input redirection >& : writing the output from one file to the input of another file. eg. matlab > outfile 2>&1 : send stdout and stderr to 'outfile'. Here 1 and 2 are file descriptors. File descriptor 1 is the standard output (stdout), and 2 is the standard error (stderr). > is redirection, and & indicates that what follows and precedes is a file descriptor and not a filename.","title":"Redirection"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#wildcards","text":"Wildcards are symbols or special characters that represent other characters. You can use them with any command such as ls/rm/cp etc. *: anything or nothing ?: single character [ ]: any character or range of characters [! ]: inverse the match of [ ] ls *.txt # list all txt files ls *-?.txt # list all files with \u2018-\u2018 and with one cha in front of \u2018.txt\u2019 ls [0-9]*.txt # list all files start with a number. ls [A-Z]*.txt # list all files start with a capital letter? [A-Z] can be different based on LC_COLLATE value. For further discussion, check here . In HPC at MSU, the default of [A-Z] is a, A, b, B, c, C, ....y, Y, z, Z, which is standard collations (en_US). Try ls [[:lower:]].txt ; ls [[:upper:]].txt ; ls [[:lower:][:upper:]].txt ls [!a-Z]*.tx t # list txt files that don\u2019t begin with any letter. For more informaiton, refer to Regular Expressions .","title":"Wildcards"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#manuals","text":"Linux includes a built in manual for nearly all commands. Type \u2018man\u2019 followed by the commands. e.g. man man To navigate the man pages use the arrow keys to scroll up and down or use the enter key to advance al ine, space bar to advance a page, letter u to go back a page. Use the q key to quit out of the display. The manual pages often include these sections: Name: a one line desctiption of what it does Synopsis: basic syntax for the command line. Description: describes the program\u2019s functionalities. Options: lists commnand line options available for this program. Example: examples of some of the options available. See Also: list of related commands. Note that options can be with single dash \u2018-\u2018 or double dash \u2018\u2014\u2019","title":"Manuals"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#commands-of-monitoring-files","text":"Following commands are used for monitering files. wc : count words, lines or characters eg. who | wc -l # number of users logged in grep : find patterns in files or data, returns just matching lines eg. who |grep $USER sort : given a list of items, sort in various ways eg. who | sort head : list only top n lines of file who > who.txt; head who.txt tail : list only last n lines of file","title":"Commands of monitoring files"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#archiving-and-compression","text":"Following commands are used for archiving and compression: zip, unzip, tar unzip: unzip tar: create (tar -c) or extract (tar -x) \u2018tape archive\u2019 file. Exercise : Using the man pages, find out what the default number of lines that head and tail will display, and how to limit those to just one line Can you tar all files and folders in workshop folder? Question? Use man.","title":"Archiving and compression"},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#environment-variables","text":"Shell maintains and you can set \u2018variables\u2019 that the shell uses for configuration and in your script. Variables start with $, and can be seen with echo $VARNAME Explore common variables with the echo command and list what they are $HOME , $USER , $SHELL , $PATH . For more information, please refer to 1. Variables - Part I","title":"Environment variables"},{"location":"Linux_Shell/","text":"Linux shells A Unix/Linux shell is a command-line interpreter which provides a user interface for the Unix/Linux operating system. Users control the operation of a computer by submitting single commands or by submitting one or more commands via a shell script. Several common shell choices are available on HPCC: bash: a Bourne-shell (sh) compatible shell with many newer advanced features as well tcsh: an advanced variant on csh with all the features of modern shells zsh: an advanced shell which incorprates all the functionality of bash, tcsh, and ksh combined csh: the original C-style shell The default shell provided to HPCC users is the bash shell. To change your shell, please contact HPCC support . To find out your current shell run echo $SHELL . Note bash is the the only officially supported shell for the HPCC Environment variables Environment variables are a set of dynamically named values which can control the way running processes will behave on a computer. Many of the UNIX commands and tools require certain environment variables to be set. Many of these are set automatically for the users when they log in or load applications via the module command. To view your current set of environment variables run env . To assign a new value to an environment variable in either bash or zsh: export <name>=<value> To assign a new value to an environment variable in either tcsh or csh: setenv <name> <value> To print the value of a variable: echo $<name> Commonly used environment variables BASH variables are preceded with $ and optionally enclosed in brackets when used e.g. $USER or ${USER} . $HOSTNAME : Name of the computer currently running the script. This should be one of the nodes listed in the variable $SLURM_JOB_NODELIST $USER : User Name (NetID). Useful if you would like to dynamically generate a directory on some scratch space. $HOME : User's home directory. Can also use ~/ symbol. $SCRATCH : Your folder on the scratch disk. $TMPDIR : Temporary working folder of a running job at /mnt/local/$SLURM_JOBID . To see all variables in the context of your job, add this line to your job script, which will list all variables that contain the word 'SLURM' 1 $ env | grep SLURM Startup scripts A startup script is a shell script which the login process executes. It provides an opportunity to alter your environment. You are free to setup your own startup scripts but be careful to make sure they are set up correctly for both interactive and batch access or it may negatively affect your ability to log in or run batch jobs on the system: bash ~/.bashrc tcsh ~/.chsrc zsh ~/.zshrc csh ~/.cshrc","title":"Linux shells"},{"location":"Linux_Shell/#linux-shells","text":"A Unix/Linux shell is a command-line interpreter which provides a user interface for the Unix/Linux operating system. Users control the operation of a computer by submitting single commands or by submitting one or more commands via a shell script. Several common shell choices are available on HPCC: bash: a Bourne-shell (sh) compatible shell with many newer advanced features as well tcsh: an advanced variant on csh with all the features of modern shells zsh: an advanced shell which incorprates all the functionality of bash, tcsh, and ksh combined csh: the original C-style shell The default shell provided to HPCC users is the bash shell. To change your shell, please contact HPCC support . To find out your current shell run echo $SHELL . Note bash is the the only officially supported shell for the HPCC","title":"Linux shells"},{"location":"Linux_Shell/#environment-variables","text":"Environment variables are a set of dynamically named values which can control the way running processes will behave on a computer. Many of the UNIX commands and tools require certain environment variables to be set. Many of these are set automatically for the users when they log in or load applications via the module command. To view your current set of environment variables run env . To assign a new value to an environment variable in either bash or zsh: export <name>=<value> To assign a new value to an environment variable in either tcsh or csh: setenv <name> <value> To print the value of a variable: echo $<name>","title":"Environment variables"},{"location":"Linux_Shell/#commonly-used-environment-variables","text":"BASH variables are preceded with $ and optionally enclosed in brackets when used e.g. $USER or ${USER} . $HOSTNAME : Name of the computer currently running the script. This should be one of the nodes listed in the variable $SLURM_JOB_NODELIST $USER : User Name (NetID). Useful if you would like to dynamically generate a directory on some scratch space. $HOME : User's home directory. Can also use ~/ symbol. $SCRATCH : Your folder on the scratch disk. $TMPDIR : Temporary working folder of a running job at /mnt/local/$SLURM_JOBID . To see all variables in the context of your job, add this line to your job script, which will list all variables that contain the word 'SLURM' 1 $ env | grep SLURM","title":"Commonly used environment variables"},{"location":"Linux_Shell/#startup-scripts","text":"A startup script is a shell script which the login process executes. It provides an opportunity to alter your environment. You are free to setup your own startup scripts but be careful to make sure they are set up correctly for both interactive and batch access or it may negatively affect your ability to log in or run batch jobs on the system: bash ~/.bashrc tcsh ~/.chsrc zsh ~/.zshrc csh ~/.cshrc","title":"Startup scripts"},{"location":"List_Jobs_by_squeue_sview/","text":"List Jobs by squeue & sview squeue command After you submit jobs, you can check their information and status in the queue. The simplest way is to use squeue command to list all of your jobs: 1 2 3 4 5 6 $ squeue -l -u $USER Thu Aug 2 14 :45:57 2018 JOBID PARTITION NAME USER STATE TIME TIME_LIMI NODES NODELIST ( REASON ) 9405 general-s Job1 nobody RUNNING 0 :32 01 :00:00 1 lac-198 9406 general-s Rmpi_tes nobody RUNNING 0 :11 30 :00 2 lac- [ 386 -387 ] 9290 general-l LongJob nobody PENDING 0 :00 36 :00:00 40 ( Resources ) where the specification -l report more of the available information and -u specify which user's jobs to show. You may find a complete squeue specifications from the SLURM web site . HPCC staffs also write some powertools commands so users can see their jobs conveniently. Before using powertools commands, please make sure powertools module is loaded. (By default, the module should be loaded unless the user purge it.) 1 2 3 4 $ module list Currently Loaded Modules: 1 ) powertools/1.2 One of the commands sq works the same as the above squeue command: 1 2 3 4 5 6 $ sq # powertools command Thu Aug 2 14 :48:51 2018 JOBID PARTITION NAME USER STATE TIME TIME_LIMI NODES NODELIST ( REASON ) 9405 general-s Job1 username RUNNING 0 :35 01 :00:00 1 lac-198 9406 general-s Rmpi_tes username RUNNING 0 :14 30 :00 2 lac- [ 386 -387 ] 9290 general-l LongJob username PENDING 0 :00 36 :00:00 40 ( Resources ) where it shows the job IDs, job partition, job name, username, job state, elapsed time, walltime limit, total number of nodes and the node list or the waiting reason for the user's each jobs. Users can also use qs command to see more information ( similar to \"qstat -u $USER\" in Torque): 1 2 3 4 5 6 7 8 $ qs # powertools command Thu Aug 2 14 :49:27 2019 Start_Time/ JobID User Account Name Node CPUs TotMem Tres WallTime ST Elapsed_Time NodeList ( Reason ) ----------------------------------------------------------------------------------------------------------------------- 60889405 MyHPCCAcc general Job1 1 16 2G gpu:1 3 :55:00 R 1 :49:24 lac-198 60889496 MyHPCCAcc general Rmpi_test 2 8 1500M N/A 30 :00 R 29 :46 lac- [ 386 -387 ] 60889290 MyHPCCAcc classres LongJob 40 80 750M k80:1 3 -00:00:00 PD 08 -03T10:29:41 ( Resources ) where more items, such as, total number of CPUs, memory, gpu per node and job start time (for pending jobs) or elapsed time (for running jobs) are shown. For a complete usage of squeue command, please refer to the SLURM web site . sview command Besides the text listing of the jobs, SLURM also offer a command to show the squeue information with a graphical interface. Use the command sview : 1 $ sview You will see an image of a job list displaying all jobs in the queue: Click on each job, it will pop out another window and show the detailed information. For a complete usage of sview command, please refer to the SLURM web site .","title":"List Jobs by squeue & sview"},{"location":"List_Jobs_by_squeue_sview/#list-jobs-by-squeue-sview","text":"","title":"List Jobs by squeue &amp; sview"},{"location":"List_Jobs_by_squeue_sview/#squeue-command","text":"After you submit jobs, you can check their information and status in the queue. The simplest way is to use squeue command to list all of your jobs: 1 2 3 4 5 6 $ squeue -l -u $USER Thu Aug 2 14 :45:57 2018 JOBID PARTITION NAME USER STATE TIME TIME_LIMI NODES NODELIST ( REASON ) 9405 general-s Job1 nobody RUNNING 0 :32 01 :00:00 1 lac-198 9406 general-s Rmpi_tes nobody RUNNING 0 :11 30 :00 2 lac- [ 386 -387 ] 9290 general-l LongJob nobody PENDING 0 :00 36 :00:00 40 ( Resources ) where the specification -l report more of the available information and -u specify which user's jobs to show. You may find a complete squeue specifications from the SLURM web site . HPCC staffs also write some powertools commands so users can see their jobs conveniently. Before using powertools commands, please make sure powertools module is loaded. (By default, the module should be loaded unless the user purge it.) 1 2 3 4 $ module list Currently Loaded Modules: 1 ) powertools/1.2 One of the commands sq works the same as the above squeue command: 1 2 3 4 5 6 $ sq # powertools command Thu Aug 2 14 :48:51 2018 JOBID PARTITION NAME USER STATE TIME TIME_LIMI NODES NODELIST ( REASON ) 9405 general-s Job1 username RUNNING 0 :35 01 :00:00 1 lac-198 9406 general-s Rmpi_tes username RUNNING 0 :14 30 :00 2 lac- [ 386 -387 ] 9290 general-l LongJob username PENDING 0 :00 36 :00:00 40 ( Resources ) where it shows the job IDs, job partition, job name, username, job state, elapsed time, walltime limit, total number of nodes and the node list or the waiting reason for the user's each jobs. Users can also use qs command to see more information ( similar to \"qstat -u $USER\" in Torque): 1 2 3 4 5 6 7 8 $ qs # powertools command Thu Aug 2 14 :49:27 2019 Start_Time/ JobID User Account Name Node CPUs TotMem Tres WallTime ST Elapsed_Time NodeList ( Reason ) ----------------------------------------------------------------------------------------------------------------------- 60889405 MyHPCCAcc general Job1 1 16 2G gpu:1 3 :55:00 R 1 :49:24 lac-198 60889496 MyHPCCAcc general Rmpi_test 2 8 1500M N/A 30 :00 R 29 :46 lac- [ 386 -387 ] 60889290 MyHPCCAcc classres LongJob 40 80 750M k80:1 3 -00:00:00 PD 08 -03T10:29:41 ( Resources ) where more items, such as, total number of CPUs, memory, gpu per node and job start time (for pending jobs) or elapsed time (for running jobs) are shown. For a complete usage of squeue command, please refer to the SLURM web site .","title":"squeue command"},{"location":"List_Jobs_by_squeue_sview/#sview-command","text":"Besides the text listing of the jobs, SLURM also offer a command to show the squeue information with a graphical interface. Use the command sview : 1 $ sview You will see an image of a job list displaying all jobs in the queue: Click on each job, it will pop out another window and show the detailed information. For a complete usage of sview command, please refer to the SLURM web site .","title":"sview command"},{"location":"List_of_Job_Specifications/","text":"List of Job Specifications The following is a list of basic #SBATCH specifications. To see the complete options of SBATCH, please refer to the SLURM sbatch command page . #SBATCH Options Description Examples -a , --array=<indexes> Submit a job array, with multiple jobs to be executed. The indexes specification identifies what array ID values should be used. Each job has the same job ID ( $SLURM_JOB_ID ) but different array ID ( $SLURM_ARRAY_TASK_ID variable). #SBATCH -a 0-15 #SBATCH --array=0,6,16-32 Can use a step function with the : separator. #SBATCH --a 0-15:4 (same as #SBATCH \u2013a 0,4,8,12 ) A maximum number of simultaneously running jobs may be specified with the % separator. #SBATCH --array=0-15%4 (4 jobs running simultaneously) -A , --account=<account> This option tells SLURM to use the specified buy-in account. Unless you are an authorized user of the account, your job will not run. #SBATCH -A --begin=<time> Submit the batch script to the Slurm controller immediately, like normal, but tell the controller to defer the allocation of the job until the specified time. Time may be of the form HH:MM:SS to run a job at a specific time of day (seconds are optional). #SBATCH --begin=16:00 -C , --constraint=<list> Request node feature. May be specified with symbol & for and, \\| for or, etc. Constraints using \\| must be prepended with NOAUTO: . Click here for more information about constraints. #SBATCH -C NOAUTO:intel16\\|intel14 -c , --cpus-per-task=<ncpus> Require <ncpus> number of processors per task #SBATCH -c 3 (3 cores per node) -d , --dependency=<dependency_list> Defer the start of this job until the specified dependencies have been satisfied completed. <dependency_list> Is of the form: #SBATCH -d after:<JobID1>:<JobID2>,afterok:<JobID3> after:job_id[:jobid...] This job can begin execution after the specified jobs have begun execution. afterany:job_id[:jobid...] This job can begin execution after the specified jobs have terminated. afterburstbuffer:job_id[:jobid...] This job can begin execution after the specified jobs have terminated and any associated burst buffer stage out operations have completed. aftercorr:job_id[:jobid...] A task of this job array can begin execution after the corresponding task ID in the specified job has completed successfully (ran to completion with an exit code of zero). afternotok:job_id[:jobid...] This job can begin execution after the specified jobs have terminated in some failed state (non-zero exit code, node failure, timed out, etc). afterok:job_id[:jobid...] This job can begin execution after the specified jobs have successfully executed (ran to completion with an exit code of zero). expand:job_id Resources allocated to this job should be used to expand the specified job. The job to expand must share the same QOS (Quality of Service) and partition. Gang scheduling of resources in the partition is also not supported. singleton This job can begin execution after any previously launched jobs sharing the same job name and user have terminated. -D , --chdir=<directory> Set the working directory of the batch script to <directory> before it is executed. The path can be specified as full path or relative path to the directory where the command is executed. #SBATCH -D /mnt/scratch/username -e , --error=<filename> Instruct Slurm to connect the batch script's standard error directly to the file name specified. By default both standard output and standard error are directed to the same file. See -o , --output for the default file name. #SBATCH -e /home/username/myerrorfile --export=<environment variables [ALL] \\| NONE> Identify which environment variables are propagated to the launched application, by default all are propagated. Multiple environment variable names should be comma separated. #SBATCH --export=EDITOR=/bin/emacs,ALL --gres=<list> Specifies a comma delimited list of generic consumable resources. The format of each entry on the list is name[[:type]:count] , where name is that of the consumable resource. To request for GPU, --gres=gpu:k20:1 is an example to request one k20 GPU. Valid GPU types are k20, k80 and v100. Note that type is optional, but the number of GPUs is necessary. #SBATCH --gres=gpu:2 (request 2 GPUs per node) #SBATCH --gres=gpu:k80:2 (request 2 K80 GPUs per node) --gres-flags=enforce-binding This option ensures that CPUs available to the job will be those bound to allocated GPUs. The enforce-binding type may increase the performance of some GPU jobs. NOTE: The number of available CPUs bound with GPUs on a node is smaller than the total number of CPUs on the node. Configuration details can be seen in /etc/slurm/gres.conf . #SBATCH --gres-flags=enforce-binding -G , --gpus=[<type>:]<number> Specify the total number of GPUs required for the job. An optional GPU type specification can be supplied. Valid GPU types are k20, k80 and v100. Note that type is optional, but the number of GPUs is necessary. #SBATCH --gpus=k80:2 (request 2 k80 GPUs for entire job) #SBATCH --gpus=2 (request 2 GPUs for entire job) --gpus-per-node=[<type>:]<number> Specify the number of GPUs required for the job on each node included in the job's resource allocation. An optional GPU type specification can be supplied. Valid GPU types are k20, k80 and v100. Note that type is optional, but the number of GPUs is necessary. #SBATCH --gpus-per-node=v100:8 (request 8 v100 GPUs for each node requested by job) #SBATCH --gpus-per-node=8 (request 8 GPUs for each node requested by job) --gpus-per-task=[<type>:]<number> Specify the number of GPUs required for the job on each task to be spawned in the job's resource allocation. An optional GPU type specification can be supplied. Valid GPU types are k20, k80 and v100. Note that type is optional, but the number of GPUs is necessary. #SBATCH --gpus-per-task=k80:2 (request 2 k80 GPUs for each task requested by job) #SBATCH --gpus-per-task=2 (request 2 GPUs for each task requested by job) -H , --hold Specify the job is to be submitted in a held state (priority of zero). A held job can now be released using scontrol to reset its priority (e.g. scontrol release <job_id> ). -I , --immediate The batch script will only be submitted to the controller if the resources necessary to grant its job allocation are immediately available. If the job allocation will have to wait in a queue of pending jobs, the batch script will not be submitted. -i , --input=<filename pattern> Instruct Slurm to connect the batch script's standard input directly to the file name specified in the \"filename pattern\". -J , --job-name=<jobname> Specify a name for the job allocation. #SBATCH -J MySuperComputing --jobid=<jobid> Allocate resources as the specified job id. -L , --licenses=<license> Specification of licenses (or other resources available on all nodes of the cluster) which must be allocated to this job. #SBATCH -L comsol@1718@lm-01.i --mail-type=<type> Notify user by email when certain event types occur. Valid type values are NONE, BEGIN, END, FAIL, REQUEUE, ALL (equivalent to BEGIN, END, FAIL, REQUEUE, and STAGE_OUT), STAGE_OUT (burst buffer stage out and teardown completed), TIME_LIMIT, TIME_LIMIT_90 (reached 90 percent of time limit), TIME_LIMIT_80 (reached 80 percent of time limit), TIME_LIMIT_50 (reached 50 percent of time limit) and ARRAY_TASKS (send emails for each array task). #SBATCH --mail-type=BEGIN,END --mail-user=<user> User to receive email notification of state changes as defined by --mail-type . The default value is the submitting user. #SBATCH --mail-user=user@msu.edu --mem=<size[units]> Specify the real memory required per node. #SBATCH --mem=2G (M or G bytes) --mem-per-cpu=<size[units]> Minimum memory required per allocated CPU #SBATCH --mem-per-cpu=2G (M or G bytes) -N , --nodes=<minnodes[-maxnodes]> Request that a minimum of minnodes nodes be allocated to this job. A maximum node count may also be specified with maxnodes . If only one number is specified, this is used as both the minimum and maximum node count. #SBATCH --nodes=2-4 (Request 2 to 4 different nodes) --no-requeue Request that a job not be requeued under any circumstances. Jobs are requeued by default if a node they are running on fails. This options may be useful for jobs that will not run properly after having run partially and failing. #SBATCH --no-requeue -n , --ntasks=<number> Request total <number> of tasks. The default is one task per node, but note that the --cpus-per-task option will change this default. #SBATCH -n 4 (All tasks could be in 1 to 4 different nodes) --ntasks-per-node=<ntasks> Request that <ntasks> be invoked on each node. This is related to --cpus-per-task=ncpus , but does not require knowledge of the actual number of cpus on each node. --tasks-per-node=<ntasks> -o , --output=<filename pattern> Instruct Slurm to connect the batch script's standard output directly to the file name specified in the <filename pattern> . #SBATCH -o /home/username/output-file The default file name is slurm-%j.out , where the %j is replaced by the job ID. For job arrays, the default file name is slurm-%A_%a.out , %A is replaced by the job ID and %a with the array index. Need a file name or filename pattern not just a directory. -t , --time=<time> Set a limit on the total run time of the job allocation. The total run time in the form: HH:MM:SS or DD-HH:MM:SS #SBATCH -t 00:20:00 --tmp=<size[units]> Specify a minimum amount of temporary disk space per node. #SBATCH --tmp=2G -v , --verbose Increase the verbosity of sbatch's informational messages. Multiple v will further increase sbatch's verbosity. By default only errors will be displayed. -w , --nodelist=<node name list> Request a specific list of your buy-in nodes. The job will contain all of these hosts and possibly additional hosts as needed to satisfy resource requirements. #SBATCH --nodelist=host1,host2,host3,... The list may be specified as a comma-separated list of hosts, a range of hosts, or a filename. The host list will be assumed to be a filename if it contains a / character. #SBATCH -w host[1-5,7,...] #SBATCH -w /mnt/home/userid/nodelist -x , --exclude=<node name list> Explicitly exclude certain nodes from the resources granted to the job.","title":"List of Job Specifications"},{"location":"List_of_Job_Specifications/#list-of-job-specifications","text":"The following is a list of basic #SBATCH specifications. To see the complete options of SBATCH, please refer to the SLURM sbatch command page . #SBATCH Options Description Examples -a , --array=<indexes> Submit a job array, with multiple jobs to be executed. The indexes specification identifies what array ID values should be used. Each job has the same job ID ( $SLURM_JOB_ID ) but different array ID ( $SLURM_ARRAY_TASK_ID variable). #SBATCH -a 0-15 #SBATCH --array=0,6,16-32 Can use a step function with the : separator. #SBATCH --a 0-15:4 (same as #SBATCH \u2013a 0,4,8,12 ) A maximum number of simultaneously running jobs may be specified with the % separator. #SBATCH --array=0-15%4 (4 jobs running simultaneously) -A , --account=<account> This option tells SLURM to use the specified buy-in account. Unless you are an authorized user of the account, your job will not run. #SBATCH -A --begin=<time> Submit the batch script to the Slurm controller immediately, like normal, but tell the controller to defer the allocation of the job until the specified time. Time may be of the form HH:MM:SS to run a job at a specific time of day (seconds are optional). #SBATCH --begin=16:00 -C , --constraint=<list> Request node feature. May be specified with symbol & for and, \\| for or, etc. Constraints using \\| must be prepended with NOAUTO: . Click here for more information about constraints. #SBATCH -C NOAUTO:intel16\\|intel14 -c , --cpus-per-task=<ncpus> Require <ncpus> number of processors per task #SBATCH -c 3 (3 cores per node) -d , --dependency=<dependency_list> Defer the start of this job until the specified dependencies have been satisfied completed. <dependency_list> Is of the form: #SBATCH -d after:<JobID1>:<JobID2>,afterok:<JobID3> after:job_id[:jobid...] This job can begin execution after the specified jobs have begun execution. afterany:job_id[:jobid...] This job can begin execution after the specified jobs have terminated. afterburstbuffer:job_id[:jobid...] This job can begin execution after the specified jobs have terminated and any associated burst buffer stage out operations have completed. aftercorr:job_id[:jobid...] A task of this job array can begin execution after the corresponding task ID in the specified job has completed successfully (ran to completion with an exit code of zero). afternotok:job_id[:jobid...] This job can begin execution after the specified jobs have terminated in some failed state (non-zero exit code, node failure, timed out, etc). afterok:job_id[:jobid...] This job can begin execution after the specified jobs have successfully executed (ran to completion with an exit code of zero). expand:job_id Resources allocated to this job should be used to expand the specified job. The job to expand must share the same QOS (Quality of Service) and partition. Gang scheduling of resources in the partition is also not supported. singleton This job can begin execution after any previously launched jobs sharing the same job name and user have terminated. -D , --chdir=<directory> Set the working directory of the batch script to <directory> before it is executed. The path can be specified as full path or relative path to the directory where the command is executed. #SBATCH -D /mnt/scratch/username -e , --error=<filename> Instruct Slurm to connect the batch script's standard error directly to the file name specified. By default both standard output and standard error are directed to the same file. See -o , --output for the default file name. #SBATCH -e /home/username/myerrorfile --export=<environment variables [ALL] \\| NONE> Identify which environment variables are propagated to the launched application, by default all are propagated. Multiple environment variable names should be comma separated. #SBATCH --export=EDITOR=/bin/emacs,ALL --gres=<list> Specifies a comma delimited list of generic consumable resources. The format of each entry on the list is name[[:type]:count] , where name is that of the consumable resource. To request for GPU, --gres=gpu:k20:1 is an example to request one k20 GPU. Valid GPU types are k20, k80 and v100. Note that type is optional, but the number of GPUs is necessary. #SBATCH --gres=gpu:2 (request 2 GPUs per node) #SBATCH --gres=gpu:k80:2 (request 2 K80 GPUs per node) --gres-flags=enforce-binding This option ensures that CPUs available to the job will be those bound to allocated GPUs. The enforce-binding type may increase the performance of some GPU jobs. NOTE: The number of available CPUs bound with GPUs on a node is smaller than the total number of CPUs on the node. Configuration details can be seen in /etc/slurm/gres.conf . #SBATCH --gres-flags=enforce-binding -G , --gpus=[<type>:]<number> Specify the total number of GPUs required for the job. An optional GPU type specification can be supplied. Valid GPU types are k20, k80 and v100. Note that type is optional, but the number of GPUs is necessary. #SBATCH --gpus=k80:2 (request 2 k80 GPUs for entire job) #SBATCH --gpus=2 (request 2 GPUs for entire job) --gpus-per-node=[<type>:]<number> Specify the number of GPUs required for the job on each node included in the job's resource allocation. An optional GPU type specification can be supplied. Valid GPU types are k20, k80 and v100. Note that type is optional, but the number of GPUs is necessary. #SBATCH --gpus-per-node=v100:8 (request 8 v100 GPUs for each node requested by job) #SBATCH --gpus-per-node=8 (request 8 GPUs for each node requested by job) --gpus-per-task=[<type>:]<number> Specify the number of GPUs required for the job on each task to be spawned in the job's resource allocation. An optional GPU type specification can be supplied. Valid GPU types are k20, k80 and v100. Note that type is optional, but the number of GPUs is necessary. #SBATCH --gpus-per-task=k80:2 (request 2 k80 GPUs for each task requested by job) #SBATCH --gpus-per-task=2 (request 2 GPUs for each task requested by job) -H , --hold Specify the job is to be submitted in a held state (priority of zero). A held job can now be released using scontrol to reset its priority (e.g. scontrol release <job_id> ). -I , --immediate The batch script will only be submitted to the controller if the resources necessary to grant its job allocation are immediately available. If the job allocation will have to wait in a queue of pending jobs, the batch script will not be submitted. -i , --input=<filename pattern> Instruct Slurm to connect the batch script's standard input directly to the file name specified in the \"filename pattern\". -J , --job-name=<jobname> Specify a name for the job allocation. #SBATCH -J MySuperComputing --jobid=<jobid> Allocate resources as the specified job id. -L , --licenses=<license> Specification of licenses (or other resources available on all nodes of the cluster) which must be allocated to this job. #SBATCH -L comsol@1718@lm-01.i --mail-type=<type> Notify user by email when certain event types occur. Valid type values are NONE, BEGIN, END, FAIL, REQUEUE, ALL (equivalent to BEGIN, END, FAIL, REQUEUE, and STAGE_OUT), STAGE_OUT (burst buffer stage out and teardown completed), TIME_LIMIT, TIME_LIMIT_90 (reached 90 percent of time limit), TIME_LIMIT_80 (reached 80 percent of time limit), TIME_LIMIT_50 (reached 50 percent of time limit) and ARRAY_TASKS (send emails for each array task). #SBATCH --mail-type=BEGIN,END --mail-user=<user> User to receive email notification of state changes as defined by --mail-type . The default value is the submitting user. #SBATCH --mail-user=user@msu.edu --mem=<size[units]> Specify the real memory required per node. #SBATCH --mem=2G (M or G bytes) --mem-per-cpu=<size[units]> Minimum memory required per allocated CPU #SBATCH --mem-per-cpu=2G (M or G bytes) -N , --nodes=<minnodes[-maxnodes]> Request that a minimum of minnodes nodes be allocated to this job. A maximum node count may also be specified with maxnodes . If only one number is specified, this is used as both the minimum and maximum node count. #SBATCH --nodes=2-4 (Request 2 to 4 different nodes) --no-requeue Request that a job not be requeued under any circumstances. Jobs are requeued by default if a node they are running on fails. This options may be useful for jobs that will not run properly after having run partially and failing. #SBATCH --no-requeue -n , --ntasks=<number> Request total <number> of tasks. The default is one task per node, but note that the --cpus-per-task option will change this default. #SBATCH -n 4 (All tasks could be in 1 to 4 different nodes) --ntasks-per-node=<ntasks> Request that <ntasks> be invoked on each node. This is related to --cpus-per-task=ncpus , but does not require knowledge of the actual number of cpus on each node. --tasks-per-node=<ntasks> -o , --output=<filename pattern> Instruct Slurm to connect the batch script's standard output directly to the file name specified in the <filename pattern> . #SBATCH -o /home/username/output-file The default file name is slurm-%j.out , where the %j is replaced by the job ID. For job arrays, the default file name is slurm-%A_%a.out , %A is replaced by the job ID and %a with the array index. Need a file name or filename pattern not just a directory. -t , --time=<time> Set a limit on the total run time of the job allocation. The total run time in the form: HH:MM:SS or DD-HH:MM:SS #SBATCH -t 00:20:00 --tmp=<size[units]> Specify a minimum amount of temporary disk space per node. #SBATCH --tmp=2G -v , --verbose Increase the verbosity of sbatch's informational messages. Multiple v will further increase sbatch's verbosity. By default only errors will be displayed. -w , --nodelist=<node name list> Request a specific list of your buy-in nodes. The job will contain all of these hosts and possibly additional hosts as needed to satisfy resource requirements. #SBATCH --nodelist=host1,host2,host3,... The list may be specified as a comma-separated list of hosts, a range of hosts, or a filename. The host list will be assumed to be a filename if it contains a / character. #SBATCH -w host[1-5,7,...] #SBATCH -w /mnt/home/userid/nodelist -x , --exclude=<node name list> Explicitly exclude certain nodes from the resources granted to the job.","title":"List of Job Specifications"},{"location":"Load_the_software/","text":"Load the software When running module spider orthomcl on a dev-node, you will see the following: 1 2 3 4 5 6 7 8 9 10 ---------- OrthoMCL: ---------- Description: OrthoMCL is a genome-scale algorithm for grouping orthologous protein sequences. Versions: OrthoMCL/2.0.9-custom-Perl-5.24.0 OrthoMCL/2.0.9-Perl-5.24.0 They are the same version of OrthoMCL, except that the custom one has fixed a possible error due to sequence identifier. See the issue reported in GitHub. Since OrthoMCL uses BLAST, it needs to be loaded as well. An example of loading OrthoMCL would be 1 2 3 4 module purge module load icc/2016.3.210-GCC-5.4.0-2.26 impi/5.1.3.181 module load OrthoMCL/2.0.9-custom-Perl-5.24.0 module load BLAST/2.2.26-Linux_x86_64 The next thing you need to do is to request and set up your MySQL configuration file, see https://docs.icer.msu.edu/MySQL_configuration/ After you have your config file (say orthomcl.config ) ready, you need to run orthomclInstallSchema to install the required schema into the database: 1 orthomclInstallSchema orthomcl.config install_schema.log For the rest of your analysis steps, please read http://orthomcl.org/common/downloads/software/v2.0/UserGuide.txt","title":"OrthoMCL: load module"},{"location":"Load_the_software/#load-the-software","text":"When running module spider orthomcl on a dev-node, you will see the following: 1 2 3 4 5 6 7 8 9 10 ---------- OrthoMCL: ---------- Description: OrthoMCL is a genome-scale algorithm for grouping orthologous protein sequences. Versions: OrthoMCL/2.0.9-custom-Perl-5.24.0 OrthoMCL/2.0.9-Perl-5.24.0 They are the same version of OrthoMCL, except that the custom one has fixed a possible error due to sequence identifier. See the issue reported in GitHub. Since OrthoMCL uses BLAST, it needs to be loaded as well. An example of loading OrthoMCL would be 1 2 3 4 module purge module load icc/2016.3.210-GCC-5.4.0-2.26 impi/5.1.3.181 module load OrthoMCL/2.0.9-custom-Perl-5.24.0 module load BLAST/2.2.26-Linux_x86_64 The next thing you need to do is to request and set up your MySQL configuration file, see https://docs.icer.msu.edu/MySQL_configuration/ After you have your config file (say orthomcl.config ) ready, you need to run orthomclInstallSchema to install the required schema into the database: 1 orthomclInstallSchema orthomcl.config install_schema.log For the rest of your analysis steps, please read http://orthomcl.org/common/downloads/software/v2.0/UserGuide.txt","title":"Load the software"},{"location":"Local_File_Systems/","text":"Local File Systems The local file systems are available on each cluster compute node and dev-node. However, the files saved in each node are different from the others. Their introductions are below. /tmp or /mnt/local LOCAL space is a hard drive on a node. The files on this space can be accessed locally on each node without going through network. This space is a good choice for jobs using a single node or multiple nodes where I/O is processed only on each node's local file. When network traffic is high, using this space will likely allow your program to run faster than running on HOME, RESEARCH or SCRATCH space. Please note that LOCAL space is shared with all processes running on the same node and there is no direct I/O from other nodes. The space also has no auto backup. It should be used as temporary storage space. When the execution of programs in a job is completed, any useful files in this space should be saved back to HOME or RESEARCH space. $TMPDIR TMPDIR directory is automatically created as /tmp/local/$SLURM_JOBID when your job starts on the local node and is deleted after it finishes. /dev/shm RAMDISK space is a \u201clogical\u201d storage space; it sits inside a node\u2019s RAM, not disk. Linux supports a system tool that provides an interface for users to intercept the I/O requests to /dev/shm with memory operations. We may think of it as a virtual disk in memory. Due to this nature, access to this space is actually access to RAM. Since the bandwidth of the access is much higher, the I/O operations are considered faster than LOCAL space. However, since programs take up some of the node's memory, the usable RAM space for program execution becomes less. This space is good for programs that do not require large memory and perform very frequent I/O on small files. Warning Please limit the use of the local spaces. It is also used for MPI runtime to implement fast communication between MPI processes. Users are advised to clean up the space after use. The files that over 2 weeks old will be removed without notice. In the situation that the space is over 90% full, we may clean up those not currently used files without notice.","title":"Local file system"},{"location":"Local_File_Systems/#local-file-systems","text":"The local file systems are available on each cluster compute node and dev-node. However, the files saved in each node are different from the others. Their introductions are below.","title":"Local File Systems"},{"location":"Local_File_Systems/#tmp-or-mntlocal","text":"LOCAL space is a hard drive on a node. The files on this space can be accessed locally on each node without going through network. This space is a good choice for jobs using a single node or multiple nodes where I/O is processed only on each node's local file. When network traffic is high, using this space will likely allow your program to run faster than running on HOME, RESEARCH or SCRATCH space. Please note that LOCAL space is shared with all processes running on the same node and there is no direct I/O from other nodes. The space also has no auto backup. It should be used as temporary storage space. When the execution of programs in a job is completed, any useful files in this space should be saved back to HOME or RESEARCH space.","title":"/tmp or /mnt/local"},{"location":"Local_File_Systems/#tmpdir","text":"TMPDIR directory is automatically created as /tmp/local/$SLURM_JOBID when your job starts on the local node and is deleted after it finishes.","title":"$TMPDIR"},{"location":"Local_File_Systems/#devshm","text":"RAMDISK space is a \u201clogical\u201d storage space; it sits inside a node\u2019s RAM, not disk. Linux supports a system tool that provides an interface for users to intercept the I/O requests to /dev/shm with memory operations. We may think of it as a virtual disk in memory. Due to this nature, access to this space is actually access to RAM. Since the bandwidth of the access is much higher, the I/O operations are considered faster than LOCAL space. However, since programs take up some of the node's memory, the usable RAM space for program execution becomes less. This space is good for programs that do not require large memory and perform very frequent I/O on small files. Warning Please limit the use of the local spaces. It is also used for MPI runtime to implement fast communication between MPI processes. Users are advised to clean up the space after use. The files that over 2 weeks old will be removed without notice. In the situation that the space is over 90% full, we may clean up those not currently used files without notice.","title":"/dev/shm"},{"location":"Makefile/","text":"Makefile Makeifles are a simple way to organize code compilation. This tutorial offers a very basic idea of what is possible using make . For this tutorial, please download three files, a main program hello.c , a functional code hellofunc.c , and an include file hello.h under the 'hello' directory. To compile these codes, you would use the following command: 1 gcc -o hello hello.c hellofunc.c -I. This command compiles the two c files, and names the executable hello . With the '-I.' flag, gcc will look in the current directory for the include file 'hello.h'. With only two .c files, it is easy to compile with the above approach, but with more files, it is more likely having typos. In addition, if you are only making changes to one .c file, the above approach recompiles all of .c files every time which is time-consuming and inefficient. So it is time to learn how makefile will be helpful for such cases. First, create a file which has the following two lines. (The filename should be makefile or Makefile), and put it under the 'hello' directory. 1 2 hello: hello.c hellofunc.c gcc -o hello hello.c hellofunc.c -I. Now, type make on the terminal and check if the executable is created. The make command will execute the compile command as you have written it in the makefile. Note that make with no arguments executes the first rule in the file. Furthermore, by putting the list of files on which the command depends on the first line after the ':', make knows that the rule hello needs to be executed if any of those files change. One very important thing to note is that there should be a tab before the gcc command in the makefile (multiple spaces do not work!). There must be a tab at the beginning of any command, otherwise, you will get a lot of errors. Can we make it a little bit more efficient? Let's modify our makefile as following: 1 2 3 4 CC = gcc CFLAGS = -I. hello: hello.o hellofunc.o $( CC ) -o hello hello.o hellofunc.o In this makefile , we define CC and CFLAGS, which are special macros communicating to make how we want to compile the files hello.c and hellofunc.c. In particular, CC is for the C compiler, and CFLAGS is the list of flags to pass to C compiler. By putting the object files (hello.o and hellofunc.o) in the dependency list and in the rule, make knows it must first compile the .c files individually, and then build the executable hello . If your project is small, like consisting of a few separate codes, this form of makefile is enough to handle the set of codes. However, this makefile misses include files. For example, if you made a change to 'hello.h', make would not recompile the .c files, even though they needed to be. In order to fix this problem, we need to tell make that all .c files depend on certain .h files. It can be done by writing a simple rule and adding it to the makefile . 1 2 3 4 5 6 7 8 9 CC = gcc CFLAGS = -I. DEPS = hello.h %.o: %.c $( DEPS ) $( CC ) -c -o $@ $< $( CFLAGS ) hello: hello.o hellofunc.o $( CC ) -o hello hello.o hellofunc.o This addition first creates the macro DEPS (the macro name does not have to be DEPS. You can use any name.), which is the set of .h files on which the .c files depend. Then we define a rule for all .o files. The rule says that the .o file depends on the .c files, and the .h files which are included in the DEPS. Next, the rule says that to generate the .o file, make needs to compile the .c file using the compiler defined in the CC . The -c flag says to generate the object file, the -o $@ says to put the output of the compilation in the file named on the left side of the : , the $< is the first item in the dependencies list, and the CFLAGS macro is defined on the 2nd line. For the simplification, you can use special macros $@ and $^ , which are the left and right sides of the : , respectively, to make the overall compilation rule more general. In the example below, all of the include files should be listed as part of the macro DEPS, and all of the object files should be listed as part of the macro OBJ. 1 2 3 4 5 6 7 8 9 10 CC = gcc CFLAGS = -I. DEPS = hello.h OBJ = hello.o hellofunc.o %.o: %.c $( DEPS ) $( CC ) -c -o $@ $< $( CFLAGS ) hello: $( OBJ ) $( CC ) -o $@ $^ $( CFLAGS ) Now you have a good sense of makefile. For more information on makefiles and the make function, check out the GNU Make Manual , which will tell you everything on makefile. You can download some makefile examples using getexample in our HPCC.","title":"Makefile"},{"location":"Makefile/#makefile","text":"Makeifles are a simple way to organize code compilation. This tutorial offers a very basic idea of what is possible using make . For this tutorial, please download three files, a main program hello.c , a functional code hellofunc.c , and an include file hello.h under the 'hello' directory. To compile these codes, you would use the following command: 1 gcc -o hello hello.c hellofunc.c -I. This command compiles the two c files, and names the executable hello . With the '-I.' flag, gcc will look in the current directory for the include file 'hello.h'. With only two .c files, it is easy to compile with the above approach, but with more files, it is more likely having typos. In addition, if you are only making changes to one .c file, the above approach recompiles all of .c files every time which is time-consuming and inefficient. So it is time to learn how makefile will be helpful for such cases. First, create a file which has the following two lines. (The filename should be makefile or Makefile), and put it under the 'hello' directory. 1 2 hello: hello.c hellofunc.c gcc -o hello hello.c hellofunc.c -I. Now, type make on the terminal and check if the executable is created. The make command will execute the compile command as you have written it in the makefile. Note that make with no arguments executes the first rule in the file. Furthermore, by putting the list of files on which the command depends on the first line after the ':', make knows that the rule hello needs to be executed if any of those files change. One very important thing to note is that there should be a tab before the gcc command in the makefile (multiple spaces do not work!). There must be a tab at the beginning of any command, otherwise, you will get a lot of errors. Can we make it a little bit more efficient? Let's modify our makefile as following: 1 2 3 4 CC = gcc CFLAGS = -I. hello: hello.o hellofunc.o $( CC ) -o hello hello.o hellofunc.o In this makefile , we define CC and CFLAGS, which are special macros communicating to make how we want to compile the files hello.c and hellofunc.c. In particular, CC is for the C compiler, and CFLAGS is the list of flags to pass to C compiler. By putting the object files (hello.o and hellofunc.o) in the dependency list and in the rule, make knows it must first compile the .c files individually, and then build the executable hello . If your project is small, like consisting of a few separate codes, this form of makefile is enough to handle the set of codes. However, this makefile misses include files. For example, if you made a change to 'hello.h', make would not recompile the .c files, even though they needed to be. In order to fix this problem, we need to tell make that all .c files depend on certain .h files. It can be done by writing a simple rule and adding it to the makefile . 1 2 3 4 5 6 7 8 9 CC = gcc CFLAGS = -I. DEPS = hello.h %.o: %.c $( DEPS ) $( CC ) -c -o $@ $< $( CFLAGS ) hello: hello.o hellofunc.o $( CC ) -o hello hello.o hellofunc.o This addition first creates the macro DEPS (the macro name does not have to be DEPS. You can use any name.), which is the set of .h files on which the .c files depend. Then we define a rule for all .o files. The rule says that the .o file depends on the .c files, and the .h files which are included in the DEPS. Next, the rule says that to generate the .o file, make needs to compile the .c file using the compiler defined in the CC . The -c flag says to generate the object file, the -o $@ says to put the output of the compilation in the file named on the left side of the : , the $< is the first item in the dependencies list, and the CFLAGS macro is defined on the 2nd line. For the simplification, you can use special macros $@ and $^ , which are the left and right sides of the : , respectively, to make the overall compilation rule more general. In the example below, all of the include files should be listed as part of the macro DEPS, and all of the object files should be listed as part of the macro OBJ. 1 2 3 4 5 6 7 8 9 10 CC = gcc CFLAGS = -I. DEPS = hello.h OBJ = hello.o hellofunc.o %.o: %.c $( DEPS ) $( CC ) -c -o $@ $< $( CFLAGS ) hello: $( OBJ ) $( CC ) -o $@ $^ $( CFLAGS ) Now you have a good sense of makefile. For more information on makefiles and the make function, check out the GNU Make Manual , which will tell you everything on makefile. You can download some makefile examples using getexample in our HPCC.","title":"Makefile"},{"location":"Managing_Perl_modules/","text":"Managing Perl modules To manage your Perl environment, we recommend using perlbrew, http://metacpan.org/module/perlbrew . This allows you to create one or more self-contained Perl installations which live in your local $HOME directory, and allows you to easily and reliably install Perl modules. Instructions are as follows: Installation It is the simplest to use the perlbrew installer, just paste this statement to your terminal: 1 [ongbw@dev-intel10 ~]$ curl -kL http://install.perlbrew.pl | bash This installs perlbrew to $HOME/perl5/perlbrew Append \"source \\~/perl5/perlbrew/etc/bashrc\" to the end of your \\~/.bash_profile 1 [ongbw@dev-intel10 ~]$ echo \"source $HOME/perl5/perlbrew/etc/bashrc\" >> ~/.bash_profile Log out of that development node, and log back in. You should now have a fully functional perlbrew installation. You can check this by checking that the environment variables are setup properly 1 2 3 4 5 6 [ongbw@dev-intel10 ~]$ env |grep -i perl PERL5LIB=/usr/local/lib/perl5 PERLBREW_BASHRC_VERSION=0.63 PERLBREW_ROOT=/mnt/home/ongbw/perl5/perlbrew PATH=/mnt/home/ongbw/perl5/perlbrew/bin:/opt/software/Stata/12.0:/opt/software/R/2.15.1--GCC-4.4.5/bin:/opt/software/MATLAB/R2011b/bin:/opt/software/Python/2.7.2--GCC-4.4.5/bin:/opt/software/cmake/2.8.5--GCC-4.4.5/bin:/opt/software/OpenMPI/1.4.3--GCC-4.4.5/bin:/usr/lib64/qt-3.3/bin:/opt/software/lmod/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/usr/local/hpcc/bin:/mnt/home/ongbw/bin PERLBREW_HOME=/mnt/home/ongbw/.perlbrew Next, we will install cpanminus http://metacpan.org/module/App::cpanminus , a script to get, unpack, build and install modules from CPAN, 1 $ perlbrew install-cpanm Brief tutorial Now that we have a working perlbrew installation, there are various useful commands. perlbrew available shows the version of Perl available for install 1 2 3 4 5 6 7 8 9 10 11 12 [ongbw@dev-intel10 ~]$ perlbrew available perl-5.19.0 perl-5.18.0 perl-5.16.3 perl-5.14.4 perl-5.12.5 perl-5.10.1 perl-5.8.9 perl-5.6.2 perl5.005_04 perl5.004_05 perl5.003_07 perlbrew list shows the list of Perl installations managed by perlbrew. It is an empty list since I haven't installed anything yet: 1 2 [ongbw@dev-intel10 ~]$ perlbrew list [ongbw@dev-intel10 ~]$ To install a new version of perl using perlbrew, type perlbrew install \\<version>. It might take a while. 1 [ongbw@dev-intel10 ~]$ perlbrew install perl-5.18.0 we can now check that it is indeed installed: 1 2 [ongbw@dev-intel10 ~]$ perlbrew list perl-5.18.0 To use this version of perl, type perlbrew use 1 2 3 4 5 6 7 8 9 10 [ongbw@dev-intel10 ~]$ perlbrew use perl-5.18.0 [ongbw@dev-intel10 ~]$ perl --version This is perl 5, version 18, subversion 0 (v5.18.0) built for x86_64-linux Copyright 1987-2013, Larry Wall Perl may be copied only under the terms of either the Artistic License or the GNU General Public License, which may be found in the Perl 5 source kit. Complete documentation for Perl, including FAQ lists, should be found on this system using \"man perl\" or \"perldoc perl\". If you have access to the Internet, point your browser at http://www.perl.org/, the Perl Home Page. To install a module for loaded version of perl, use cpanm. It will pull all the dependencies nicely for you 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 [ongbw@dev-intel10 CPAN]$ cpanm Pod::WikiDoc --> Working on Pod::WikiDoc Fetching http://www.cpan.org/authors/id/D/DA/DAGOLDEN/Pod-WikiDoc-0.20.tar.gz ... OK Configuring Pod-WikiDoc-0.20 ... OK ==> Found dependencies: File::pushd, Probe::Perl, IPC::Run3, IO::String, Getopt::Lucid, Parse::RecDescent --> Working on File::pushd Fetching http://www.cpan.org/authors/id/D/DA/DAGOLDEN/File-pushd-1.005.tar.gz ... OK Configuring File-pushd-1.005 ... OK Building and testing File-pushd-1.005 ... OK Successfully installed File-pushd-1.005 --> Working on Probe::Perl Fetching http://www.cpan.org/authors/id/K/KW/KWILLIAMS/Probe-Perl-0.02.tar.gz ... OK Configuring Probe-Perl-0.02 ... OK Building and testing Probe-Perl-0.02 ... OK Successfully installed Probe-Perl-0.02 --> Working on IPC::Run3 Fetching http://www.cpan.org/authors/id/R/RJ/RJBS/IPC-Run3-0.045.tar.gz ... OK Configuring IPC-Run3-0.045 ... OK Building and testing IPC-Run3-0.045 ... OK Successfully installed IPC-Run3-0.045 --> Working on IO::String Fetching http://www.cpan.org/authors/id/G/GA/GAAS/IO-String-1.08.tar.gz ... OK Configuring IO-String-1.08 ... OK Building and testing IO-String-1.08 ... OK Successfully installed IO-String-1.08 --> Working on Getopt::Lucid Fetching http://www.cpan.org/authors/id/D/DA/DAGOLDEN/Getopt-Lucid-1.05.tar.gz ... OK Configuring Getopt-Lucid-1.05 ... OK ==> Found dependencies: Exception::Class::TryCatch, Exception::Class --> Working on Exception::Class::TryCatch Fetching http://www.cpan.org/authors/id/D/DA/DAGOLDEN/Exception-Class-TryCatch-1.12.tar.gz ... OK Configuring Exception-Class-TryCatch-1.12 ... OK ==> Found dependencies: Exception::Class --> Working on Exception::Class Fetching http://www.cpan.org/authors/id/D/DR/DROLSKY/Exception-Class-1.37.tar.gz ... OK Configuring Exception-Class-1.37 ... OK ==> Found dependencies: Class::Data::Inheritable, Devel::StackTrace --> Working on Class::Data::Inheritable Fetching http://www.cpan.org/authors/id/T/TM/TMTM/Class-Data-Inheritable-0.08.tar.gz ... OK Configuring Class-Data-Inheritable-0.08 ... OK Building and testing Class-Data-Inheritable-0.08 ... OK Successfully installed Class-Data-Inheritable-0.08 --> Working on Devel::StackTrace Fetching http://www.cpan.org/authors/id/D/DR/DROLSKY/Devel-StackTrace-1.30.tar.gz ... OK Configuring Devel-StackTrace-1.30 ... OK Building and testing Devel-StackTrace-1.30 ... OK Successfully installed Devel-StackTrace-1.30 Building and testing Exception-Class-1.37 ... OK Successfully installed Exception-Class-1.37 Building and testing Exception-Class-TryCatch-1.12 ... OK Successfully installed Exception-Class-TryCatch-1.12 Building and testing Getopt-Lucid-1.05 ... OK Successfully installed Getopt-Lucid-1.05 --> Working on Parse::RecDescent Fetching http://www.cpan.org/authors/id/J/JT/JTBRAUN/Parse-RecDescent-1.967009.tar.gz ... OK Configuring Parse-RecDescent-1.967009 ... OK Building and testing Parse-RecDescent-1.967009 ... OK Successfully installed Parse-RecDescent-1.967009 Building and testing Pod-WikiDoc-0.20 ... OK Successfully installed Pod-WikiDoc-0.20 11 distributions installed Local Libraries If you want to use a local lib/ for perl modules with PERL5LIB (recommended), you'll have to make a few changes. First remove ~/.cpan (or modify ~/.cpan/CPAN/MyConfig.pm appropriately. 1 $ rm -rf ~/.cpan Then, issue the command 1 $ cpanm --local-lib ~/lib App::local::lib::helper Additional Documentation perlbrew: http://metacpan.org/module/perlbrew cpanm: http://metacpan.org/module/App::cpanminus local-lib: http://metacpan.org/module/local::lib local::lib::helper: https://metacpan.org/module/App::local::lib::helper","title":"Managing Perl modules"},{"location":"Managing_Perl_modules/#managing-perl-modules","text":"To manage your Perl environment, we recommend using perlbrew, http://metacpan.org/module/perlbrew . This allows you to create one or more self-contained Perl installations which live in your local $HOME directory, and allows you to easily and reliably install Perl modules. Instructions are as follows:","title":"Managing Perl modules"},{"location":"Managing_Perl_modules/#installation","text":"It is the simplest to use the perlbrew installer, just paste this statement to your terminal: 1 [ongbw@dev-intel10 ~]$ curl -kL http://install.perlbrew.pl | bash This installs perlbrew to $HOME/perl5/perlbrew Append \"source \\~/perl5/perlbrew/etc/bashrc\" to the end of your \\~/.bash_profile 1 [ongbw@dev-intel10 ~]$ echo \"source $HOME/perl5/perlbrew/etc/bashrc\" >> ~/.bash_profile Log out of that development node, and log back in. You should now have a fully functional perlbrew installation. You can check this by checking that the environment variables are setup properly 1 2 3 4 5 6 [ongbw@dev-intel10 ~]$ env |grep -i perl PERL5LIB=/usr/local/lib/perl5 PERLBREW_BASHRC_VERSION=0.63 PERLBREW_ROOT=/mnt/home/ongbw/perl5/perlbrew PATH=/mnt/home/ongbw/perl5/perlbrew/bin:/opt/software/Stata/12.0:/opt/software/R/2.15.1--GCC-4.4.5/bin:/opt/software/MATLAB/R2011b/bin:/opt/software/Python/2.7.2--GCC-4.4.5/bin:/opt/software/cmake/2.8.5--GCC-4.4.5/bin:/opt/software/OpenMPI/1.4.3--GCC-4.4.5/bin:/usr/lib64/qt-3.3/bin:/opt/software/lmod/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/usr/local/hpcc/bin:/mnt/home/ongbw/bin PERLBREW_HOME=/mnt/home/ongbw/.perlbrew Next, we will install cpanminus http://metacpan.org/module/App::cpanminus , a script to get, unpack, build and install modules from CPAN, 1 $ perlbrew install-cpanm","title":"Installation"},{"location":"Managing_Perl_modules/#brief-tutorial","text":"Now that we have a working perlbrew installation, there are various useful commands. perlbrew available shows the version of Perl available for install 1 2 3 4 5 6 7 8 9 10 11 12 [ongbw@dev-intel10 ~]$ perlbrew available perl-5.19.0 perl-5.18.0 perl-5.16.3 perl-5.14.4 perl-5.12.5 perl-5.10.1 perl-5.8.9 perl-5.6.2 perl5.005_04 perl5.004_05 perl5.003_07 perlbrew list shows the list of Perl installations managed by perlbrew. It is an empty list since I haven't installed anything yet: 1 2 [ongbw@dev-intel10 ~]$ perlbrew list [ongbw@dev-intel10 ~]$ To install a new version of perl using perlbrew, type perlbrew install \\<version>. It might take a while. 1 [ongbw@dev-intel10 ~]$ perlbrew install perl-5.18.0 we can now check that it is indeed installed: 1 2 [ongbw@dev-intel10 ~]$ perlbrew list perl-5.18.0 To use this version of perl, type perlbrew use 1 2 3 4 5 6 7 8 9 10 [ongbw@dev-intel10 ~]$ perlbrew use perl-5.18.0 [ongbw@dev-intel10 ~]$ perl --version This is perl 5, version 18, subversion 0 (v5.18.0) built for x86_64-linux Copyright 1987-2013, Larry Wall Perl may be copied only under the terms of either the Artistic License or the GNU General Public License, which may be found in the Perl 5 source kit. Complete documentation for Perl, including FAQ lists, should be found on this system using \"man perl\" or \"perldoc perl\". If you have access to the Internet, point your browser at http://www.perl.org/, the Perl Home Page. To install a module for loaded version of perl, use cpanm. It will pull all the dependencies nicely for you 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 [ongbw@dev-intel10 CPAN]$ cpanm Pod::WikiDoc --> Working on Pod::WikiDoc Fetching http://www.cpan.org/authors/id/D/DA/DAGOLDEN/Pod-WikiDoc-0.20.tar.gz ... OK Configuring Pod-WikiDoc-0.20 ... OK ==> Found dependencies: File::pushd, Probe::Perl, IPC::Run3, IO::String, Getopt::Lucid, Parse::RecDescent --> Working on File::pushd Fetching http://www.cpan.org/authors/id/D/DA/DAGOLDEN/File-pushd-1.005.tar.gz ... OK Configuring File-pushd-1.005 ... OK Building and testing File-pushd-1.005 ... OK Successfully installed File-pushd-1.005 --> Working on Probe::Perl Fetching http://www.cpan.org/authors/id/K/KW/KWILLIAMS/Probe-Perl-0.02.tar.gz ... OK Configuring Probe-Perl-0.02 ... OK Building and testing Probe-Perl-0.02 ... OK Successfully installed Probe-Perl-0.02 --> Working on IPC::Run3 Fetching http://www.cpan.org/authors/id/R/RJ/RJBS/IPC-Run3-0.045.tar.gz ... OK Configuring IPC-Run3-0.045 ... OK Building and testing IPC-Run3-0.045 ... OK Successfully installed IPC-Run3-0.045 --> Working on IO::String Fetching http://www.cpan.org/authors/id/G/GA/GAAS/IO-String-1.08.tar.gz ... OK Configuring IO-String-1.08 ... OK Building and testing IO-String-1.08 ... OK Successfully installed IO-String-1.08 --> Working on Getopt::Lucid Fetching http://www.cpan.org/authors/id/D/DA/DAGOLDEN/Getopt-Lucid-1.05.tar.gz ... OK Configuring Getopt-Lucid-1.05 ... OK ==> Found dependencies: Exception::Class::TryCatch, Exception::Class --> Working on Exception::Class::TryCatch Fetching http://www.cpan.org/authors/id/D/DA/DAGOLDEN/Exception-Class-TryCatch-1.12.tar.gz ... OK Configuring Exception-Class-TryCatch-1.12 ... OK ==> Found dependencies: Exception::Class --> Working on Exception::Class Fetching http://www.cpan.org/authors/id/D/DR/DROLSKY/Exception-Class-1.37.tar.gz ... OK Configuring Exception-Class-1.37 ... OK ==> Found dependencies: Class::Data::Inheritable, Devel::StackTrace --> Working on Class::Data::Inheritable Fetching http://www.cpan.org/authors/id/T/TM/TMTM/Class-Data-Inheritable-0.08.tar.gz ... OK Configuring Class-Data-Inheritable-0.08 ... OK Building and testing Class-Data-Inheritable-0.08 ... OK Successfully installed Class-Data-Inheritable-0.08 --> Working on Devel::StackTrace Fetching http://www.cpan.org/authors/id/D/DR/DROLSKY/Devel-StackTrace-1.30.tar.gz ... OK Configuring Devel-StackTrace-1.30 ... OK Building and testing Devel-StackTrace-1.30 ... OK Successfully installed Devel-StackTrace-1.30 Building and testing Exception-Class-1.37 ... OK Successfully installed Exception-Class-1.37 Building and testing Exception-Class-TryCatch-1.12 ... OK Successfully installed Exception-Class-TryCatch-1.12 Building and testing Getopt-Lucid-1.05 ... OK Successfully installed Getopt-Lucid-1.05 --> Working on Parse::RecDescent Fetching http://www.cpan.org/authors/id/J/JT/JTBRAUN/Parse-RecDescent-1.967009.tar.gz ... OK Configuring Parse-RecDescent-1.967009 ... OK Building and testing Parse-RecDescent-1.967009 ... OK Successfully installed Parse-RecDescent-1.967009 Building and testing Pod-WikiDoc-0.20 ... OK Successfully installed Pod-WikiDoc-0.20 11 distributions installed","title":"Brief tutorial"},{"location":"Managing_Perl_modules/#local-libraries","text":"If you want to use a local lib/ for perl modules with PERL5LIB (recommended), you'll have to make a few changes. First remove ~/.cpan (or modify ~/.cpan/CPAN/MyConfig.pm appropriately. 1 $ rm -rf ~/.cpan Then, issue the command 1 $ cpanm --local-lib ~/lib App::local::lib::helper","title":"Local Libraries"},{"location":"Managing_Perl_modules/#additional-documentation","text":"perlbrew: http://metacpan.org/module/perlbrew cpanm: http://metacpan.org/module/App::cpanminus local-lib: http://metacpan.org/module/local::lib local::lib::helper: https://metacpan.org/module/App::local::lib::helper","title":"Additional Documentation"},{"location":"Mapping_HPC_drives_with_SSHFS/","text":"Mapping HPC drives with SSHFS Besides mapping HPCC with SMB , SSHFS can also enable mounting of HPCC file systems on a local computer. Different from SMB mapping, which can only map to home or research space via MSU campus network, this method can also work on your scratch space and uses any internet network. If users would like to use SSHFS without entering password, they can generate an authentication key by following the direction of SSH Key-Based Authentication . After connect to HPCC with a terminal and execute the command ssh-keygen, two files, id_rsa and id_rsa.pub , are generated in the hidden directory ~/.ssh. Besides copying the file id_rsa.pub to the file authorized_keys , you also need to copy (transfer) the file id_rsa to your local computer. Make sure you know the path of the file in the local computer. You will need the file location when you set up SSHFS. On Mac OSX On Linux Systems On Windows OS Other Mapping Software On Mac OSX Download and install (or upgrade) the most recent versions of the following packages: FUSE for macOS and SSHFS from https://osxfuse.github.io . Reboot (not required) Using the Terminal, create a directory (as <local_mount_point> in step 4) for each filesystem you wish to mount. If you are creating the folder outside of your home directory, you may need to use 'sudo' before each command (sudo = superuser do ). 1 2 3 4 5 6 7 [MacBook-Pro:~ icer2]$ mkdir <local_mount_point> /* begin example home directory */ [MacBook-Pro:~ icer2]$ mkdir /Users/icer2/hpcc_home /* end example home directory */ /* begin example scratch directory in the Mac /Volumes folder where drives are mounted */ [MacBook-Pro:~ icer2]$ mkdir /Volumes/scratch /* end example scratch directory */ Mount the directory using sshfs command. We suggest you add these additional flags to the command to get it be more \"Mac-like\" : -ocache=no , -onolocalcaches and -o volname=hpcc_home . For the last option, ' -o volname ' this is the name that displays in the Finder title bar, so change it for difference file folders (e.g. use -o volname=hpcc_scratch for your scratch folder). After running the command, enter the password for logging into HPCC and the FUSE drive icon will show on the desktop of your local Mac computer. 1 2 3 4 5 6 7 8 9 10 11 [MacBook-Pro:~ icer2]$ sshfs <user_id>@rsync.hpcc.msu.edu:<remote_directory_to_mount> <local_mount_point> -ovolname=hpcc -o allow_other,defer_permissions,follow_symlinks,reconnect -ocache=no -onolocalcaches <net_id>@rsync.hpcc.msu.edu's password: /* begin example hpc's home directory, using /mnt/home/hpc/ */ [MacBook-Pro:~ icer2]$ sshfs hpc@rsync.hpcc.msu.edu:/mnt/home/hpc/ /Users/icer2/hpcc_home -o volname=hpcc_home -o allow_other,defer_permissions,follow_symlinks,reconnect -ocache=no -onolocalcaches hpc@rsync.hpcc.msu.edu's password: /* end example home directory */ /* begin example hpc's scratch directory with authorized key file ~/.ssh/id_rsa, using /mnt/gs18/scratch/users/hpc */ [MacBook-Pro:~ icer2]$ mkdir /Volumes/scratch [MacBook-Pro:~ icer2]$ sshfs hpc@rsync.hpcc.msu.edu:/mnt/gs18/scratch/users/hpc /Volumes/scratch -o volname=hpcc_scratch -o allow_other,defer_permissions,follow_symlinks,reconnect,IdentityFile=~/.ssh/id_rsa -ocache=no -onolocalcaches (No password input) /* end example scratch directory */ If <remote_directory_to_mount> is a static link, please make sure to put \" / \" at the end of the directory path: For home space, please use \" /mnt/home/<user_id>/ \" instead of \" /mnt/home/<user_id> \". For research space, please use \" /mnt/research/<group_name>/ \" instead of \" /mnt/research/<group_name> \". As the above example (starting from line 5), home space \" /mnt/home/hpc/ \" is used on line 7 instead of \" /mnt/home/hpc \". To unmount a filesystem, use the \"umount\" command (note: its just the letter u before mount, not 'unmount'). 1 2 3 4 umount <local_mount_point> /* begin example */ umount /Users/icer2/hpcc_home /* end example */ You'll see these folders in the finder if you use the \"Go\" menu, but you won't see them listed in the left side with the other mounted drives. You must use the terminal and 'umount' command to disconnect. References: https://www.digitalocean.com/community/tutorials/how-to-use-sshfs-to-mount-remote-file-systems-over-ssh On Linux Systems Please refer to the web site: https://tecadmin.net/install-sshfs-on-linux-and-mount-remote-filesystem/ for how to mount remote filesystem over SSH on Linux. On Windows OS Install latest stable release of winfsp (version 1.11.22176 as of 8/11/22) WinFsp installer Full upstream documentation: WinFsp Documentation NOTE: Administrative access is required Install latest Stable release of SSHFS-Win (version 3.5.20357 as of 8/11/22) SSHFS-Win installer Full upstream documentation: SSHFS-Win Documentation NOTE: Administrative access is required Mounting of Home and Scratch can be done through the command prompt or with the windows file explorer once WinFsp and SSHFS-Win. Mounting Home directory with the command prompt DriveLetter (H: or Z: etc.) is optional, but username is required Command structure C:>net use [DriveLetter>:] \\sshfs\\ @rsync.hpcc.msu.edu example C:> net use F: \\sshfs\\ryanjos2@rsync.hpcc.msu.edu Mounting Scratch directory with the command prompt requires a slightly different command. Note the addition of '.r' to sshfs (\\\\sshfs.r\\) that designates to start from the root directory. DriveLetter (H: or Z: etc.) is optional, but username is required Command structure C:>net use [DriveLetter:] \\sshfs.r\\ @rsync.hpcc.msu.edu\\mnt\\scratch\\ example C:>net use R: \\sshfs.r\\ryanjos2@rsync.hpcc.msu.edu\\mnt\\scratch\\ryanjos2 Mounting Home or Scratch directory with Microsoft file browser: Both Home and Scratch use the same steps, just with different network paths Home: \\sshfs\\username@rsync.hpcc.msu.edu Scratch: \\sshfs.r\\username@rsync.hpcc.msu.edu\\mnt\\scratch\\username In file browser on the 'This PC' page, select 'Map Network Drive' Fill in the appropriate network path (\\sshfs\\username@rsync.hpcc.msu.edu), select a drive letter and any desired options, and click Finish. Mounting a research space requires an additional tool, the SSHFS-Win Manager GUI. This Tool can also be used to mount Home and scratch space as well using the correct directory. These filesystems do not need any advanced settings. For home directory, leave the path blank. For Scratch space, use \\mnt\\scratch\\hpcc_username\\ for the path with the trailing slash. Install latest stable release of sshfs-win-manager (1.3.1 as of 8/11/22) SSHFS-Win Manager installer SSHFS-Win Manager documentation Note: administrative privileges are not required open the SSHFS-Win Manager application If the SSHFS-Win Manager application is minimized, it can be opened using the task bar Create a new mount for the research space Basics tab Name: HPCC-groupname (descriptive name for mounted drive) Host: rsync.hpcc.msu.edu Auth Method: password (ask on connect) Path: /mnt/research/research_groupname/ (Note: ending slash) Drive letter can be left as auto or you can select a specific Drive letter Select the Advanced tab and add the following options (connect on startup and reconnect on connection loss are optional) Custom Command Line Params: On idmap -> user create_file_umask -> 000 umask -> 000 sftp_server -> sg research_groupname -c /usr/libexec/openssh/sftp-server After Saving the new drive mapping, click the connect icon After successful authentication, the drive should show connected (green) and the folder is now available in the file browser Other Mapping Software DirectNet Drive: http://www.directnet-drive.net/ for Windows only","title":"Mapping drives using SSHFS"},{"location":"Mapping_HPC_drives_with_SSHFS/#mapping-hpc-drives-with-sshfs","text":"Besides mapping HPCC with SMB , SSHFS can also enable mounting of HPCC file systems on a local computer. Different from SMB mapping, which can only map to home or research space via MSU campus network, this method can also work on your scratch space and uses any internet network. If users would like to use SSHFS without entering password, they can generate an authentication key by following the direction of SSH Key-Based Authentication . After connect to HPCC with a terminal and execute the command ssh-keygen, two files, id_rsa and id_rsa.pub , are generated in the hidden directory ~/.ssh. Besides copying the file id_rsa.pub to the file authorized_keys , you also need to copy (transfer) the file id_rsa to your local computer. Make sure you know the path of the file in the local computer. You will need the file location when you set up SSHFS. On Mac OSX On Linux Systems On Windows OS Other Mapping Software","title":"Mapping HPC drives with SSHFS"},{"location":"Mapping_HPC_drives_with_SSHFS/#on-mac-osx","text":"Download and install (or upgrade) the most recent versions of the following packages: FUSE for macOS and SSHFS from https://osxfuse.github.io . Reboot (not required) Using the Terminal, create a directory (as <local_mount_point> in step 4) for each filesystem you wish to mount. If you are creating the folder outside of your home directory, you may need to use 'sudo' before each command (sudo = superuser do ). 1 2 3 4 5 6 7 [MacBook-Pro:~ icer2]$ mkdir <local_mount_point> /* begin example home directory */ [MacBook-Pro:~ icer2]$ mkdir /Users/icer2/hpcc_home /* end example home directory */ /* begin example scratch directory in the Mac /Volumes folder where drives are mounted */ [MacBook-Pro:~ icer2]$ mkdir /Volumes/scratch /* end example scratch directory */ Mount the directory using sshfs command. We suggest you add these additional flags to the command to get it be more \"Mac-like\" : -ocache=no , -onolocalcaches and -o volname=hpcc_home . For the last option, ' -o volname ' this is the name that displays in the Finder title bar, so change it for difference file folders (e.g. use -o volname=hpcc_scratch for your scratch folder). After running the command, enter the password for logging into HPCC and the FUSE drive icon will show on the desktop of your local Mac computer. 1 2 3 4 5 6 7 8 9 10 11 [MacBook-Pro:~ icer2]$ sshfs <user_id>@rsync.hpcc.msu.edu:<remote_directory_to_mount> <local_mount_point> -ovolname=hpcc -o allow_other,defer_permissions,follow_symlinks,reconnect -ocache=no -onolocalcaches <net_id>@rsync.hpcc.msu.edu's password: /* begin example hpc's home directory, using /mnt/home/hpc/ */ [MacBook-Pro:~ icer2]$ sshfs hpc@rsync.hpcc.msu.edu:/mnt/home/hpc/ /Users/icer2/hpcc_home -o volname=hpcc_home -o allow_other,defer_permissions,follow_symlinks,reconnect -ocache=no -onolocalcaches hpc@rsync.hpcc.msu.edu's password: /* end example home directory */ /* begin example hpc's scratch directory with authorized key file ~/.ssh/id_rsa, using /mnt/gs18/scratch/users/hpc */ [MacBook-Pro:~ icer2]$ mkdir /Volumes/scratch [MacBook-Pro:~ icer2]$ sshfs hpc@rsync.hpcc.msu.edu:/mnt/gs18/scratch/users/hpc /Volumes/scratch -o volname=hpcc_scratch -o allow_other,defer_permissions,follow_symlinks,reconnect,IdentityFile=~/.ssh/id_rsa -ocache=no -onolocalcaches (No password input) /* end example scratch directory */ If <remote_directory_to_mount> is a static link, please make sure to put \" / \" at the end of the directory path: For home space, please use \" /mnt/home/<user_id>/ \" instead of \" /mnt/home/<user_id> \". For research space, please use \" /mnt/research/<group_name>/ \" instead of \" /mnt/research/<group_name> \". As the above example (starting from line 5), home space \" /mnt/home/hpc/ \" is used on line 7 instead of \" /mnt/home/hpc \". To unmount a filesystem, use the \"umount\" command (note: its just the letter u before mount, not 'unmount'). 1 2 3 4 umount <local_mount_point> /* begin example */ umount /Users/icer2/hpcc_home /* end example */ You'll see these folders in the finder if you use the \"Go\" menu, but you won't see them listed in the left side with the other mounted drives. You must use the terminal and 'umount' command to disconnect. References: https://www.digitalocean.com/community/tutorials/how-to-use-sshfs-to-mount-remote-file-systems-over-ssh","title":"On Mac OSX"},{"location":"Mapping_HPC_drives_with_SSHFS/#on-linux-systems","text":"Please refer to the web site: https://tecadmin.net/install-sshfs-on-linux-and-mount-remote-filesystem/ for how to mount remote filesystem over SSH on Linux.","title":"On Linux Systems"},{"location":"Mapping_HPC_drives_with_SSHFS/#on-windows-os","text":"Install latest stable release of winfsp (version 1.11.22176 as of 8/11/22) WinFsp installer Full upstream documentation: WinFsp Documentation NOTE: Administrative access is required Install latest Stable release of SSHFS-Win (version 3.5.20357 as of 8/11/22) SSHFS-Win installer Full upstream documentation: SSHFS-Win Documentation NOTE: Administrative access is required Mounting of Home and Scratch can be done through the command prompt or with the windows file explorer once WinFsp and SSHFS-Win. Mounting Home directory with the command prompt DriveLetter (H: or Z: etc.) is optional, but username is required Command structure C:>net use [DriveLetter>:] \\sshfs\\ @rsync.hpcc.msu.edu example C:> net use F: \\sshfs\\ryanjos2@rsync.hpcc.msu.edu Mounting Scratch directory with the command prompt requires a slightly different command. Note the addition of '.r' to sshfs (\\\\sshfs.r\\) that designates to start from the root directory. DriveLetter (H: or Z: etc.) is optional, but username is required Command structure C:>net use [DriveLetter:] \\sshfs.r\\ @rsync.hpcc.msu.edu\\mnt\\scratch\\ example C:>net use R: \\sshfs.r\\ryanjos2@rsync.hpcc.msu.edu\\mnt\\scratch\\ryanjos2 Mounting Home or Scratch directory with Microsoft file browser: Both Home and Scratch use the same steps, just with different network paths Home: \\sshfs\\username@rsync.hpcc.msu.edu Scratch: \\sshfs.r\\username@rsync.hpcc.msu.edu\\mnt\\scratch\\username In file browser on the 'This PC' page, select 'Map Network Drive' Fill in the appropriate network path (\\sshfs\\username@rsync.hpcc.msu.edu), select a drive letter and any desired options, and click Finish. Mounting a research space requires an additional tool, the SSHFS-Win Manager GUI. This Tool can also be used to mount Home and scratch space as well using the correct directory. These filesystems do not need any advanced settings. For home directory, leave the path blank. For Scratch space, use \\mnt\\scratch\\hpcc_username\\ for the path with the trailing slash. Install latest stable release of sshfs-win-manager (1.3.1 as of 8/11/22) SSHFS-Win Manager installer SSHFS-Win Manager documentation Note: administrative privileges are not required open the SSHFS-Win Manager application If the SSHFS-Win Manager application is minimized, it can be opened using the task bar Create a new mount for the research space Basics tab Name: HPCC-groupname (descriptive name for mounted drive) Host: rsync.hpcc.msu.edu Auth Method: password (ask on connect) Path: /mnt/research/research_groupname/ (Note: ending slash) Drive letter can be left as auto or you can select a specific Drive letter Select the Advanced tab and add the following options (connect on startup and reconnect on connection loss are optional) Custom Command Line Params: On idmap -> user create_file_umask -> 000 umask -> 000 sftp_server -> sg research_groupname -c /usr/libexec/openssh/sftp-server After Saving the new drive mapping, click the connect icon After successful authentication, the drive should show connected (green) and the folder is now available in the file browser","title":"On Windows OS"},{"location":"Mapping_HPC_drives_with_SSHFS/#other-mapping-software","text":"DirectNet Drive: http://www.directnet-drive.net/ for Windows only","title":"Other Mapping Software"},{"location":"Mapping_HPC_drives_with_Samba/","text":"Mapping HPC drives with Samba Warning Samba now uses campus AD for user authentication, if you are unable to login and have not updated your netid password recently please try updating your netid password before opening a ticket This will only work if your computer has a university IP address. If you are off campus, you can use MSU VPN to obtain an MSU IP, which is available to all graduate students, staff and faculty. If file transfer speed is a concern please use the sftp protocol for transferring large data sets, or use our Globus endpoint The following tutorial will show you how to map your HPC home or research directory using SMB or CIFS File Sharing . Determining your Network Path Windows 10 Command-line Windows NetBIOS Commands MacOS Example Linux Ubuntu Mount Example Ubuntu Example (Older Versions) More Information Determining your Network Path We have the powertools command show-samba-paths to show all paths of your home and research space: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ ml powertools # if powertools is not loaded $ show-samba-paths HOME | Samba Path =================================================================== username | \\\\ ufs.hpcc.msu.edu \\h ome-021 \\u sername ( Windows ) | smb://ufs.hpcc.msu.edu/home-021/username ( Mac ) RESEARCH | Samba Path =============================================================================== helpdesk | \\\\ ufs.hpcc.msu.edu \\r s-011 \\h elpdesk ( Windows ) | smb://ufs.hpcc.msu.edu/rs-011/helpdesk ( Mac ) ------------------+------------------------------------------------------------ common-data | \\\\ ufs-12-b.hpcc.msu.edu \\c ommon-data ( Windows ) | smb://ufs-12-b.hpcc.msu.edu/common-data ( Mac ) ------------------+------------------------------------------------------------ BiCEP | \\\\ ufs.hpcc.msu.edu \\r s-001 \\B iCEP ( Windows ) | smb://ufs.hpcc.msu.edu/rs-001/BiCEP ( Mac ) ------------------+------------------------------------------------------------ education-data | \\\\ ufs-12-b.hpcc.msu.edu \\e ducation-data ( Windows ) | smb://ufs-12-b.hpcc.msu.edu/education-data ( Mac ) where the paths are the same for Mac and Window computers but with different formats. Windows 10 Step 1. Enable NetBIOS over TCP/IP on Windows: Click on Desktop icon on your Windows 8 screen Right click on Network icon on start bar at right hand side and click on open network and sharing center Click on Change adapter settings Right click on your Network interface and click on Properties Follow the steps from 1d. in the Windows 7 instruction above to enable NetBIOS over TCP/IP. Step 2. Disable SMB1 Disable Samba V1 protocol with PowerShell This step must be completed or your client will not be able to map the drive. If you have other mounts on on the HPC cluster and they are using samba V1 they will stop working. Press the windows start button In the search box type \"power shell\" Right click on the \"Windows PowerShell\" icon and select \"Run as Administrator\" Select Ok when security warning appears Disable Samba V1 by entering the following command into the windows power shell. 1 Set-ItemProperty -Path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\LanmanServer\\Parameters\" SMB1 -Type DWORD -Value 0 -Force Ensure SMB V2 and SMB V3 are enabled by entering the follwoing command. In the past, on some versions of windows and for some file systems we recommended the opposite of this setting. Running this ensures it's enabled again. 1 Set-ItemProperty -Path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\LanmanServer\\Parameters\" SMB2 -Type DWORD -Value 1 -Force Navigate to \"Computer\" and click on the text labeled \"Map Network Drive\" at the top of the screen. From this menu you need to type your Network Path. Please see #Determining your Network Path for help Once you have typed in your Network Path you need to click on the box \"Connect using different credentials.\" This will open a window where you type in your MSU netid and password: Warning If you aren't able to sign in, You will need to add \"CAMPUSAD\\\" to the beginning of your username. An indicator of this issue is if Windows displays the error \"The specified network password is not correct\" in the username dialog window. For example: substitute \"CAMPUSAD\\sparty\" for username \"sparty\" in the username field. The slash character is a backslash. A forward slash character will not work. Finally, select \"Finish\" and you will see your system trying to connect Command-line Windows NetBIOS Commands If you're working in Windows, you can use command line tools to manage your drive mapping. These commands also work in .bat files, if you're so inclined to connect/disconnect drives in that manner. Note you may also have to Disable SMBV1 and Enable SMB2 per instructions above. From the Start Menu -> Run -> type 'cmd' in the box and hit enter, the command shell should open. You can then use the following commands to diagnose, disconnect and connect drives. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 # to show all mapped connections/drives # use 'net use' C:\\Documents and Settings\\Administrator>net use New connections will not be remembered. Status Local Remote Network ------------------------------------------------------------------------------- Unavailable J: \\\\MachineA\\consult$ Microsoft Windows Network Unavailable P: \\\\MachineB\\everyone$ Microsoft Windows Network Z: \\\\vmware-host\\Shared Folders VMware Shared Folders The command completed successfully. # to map a drive use: # net use <drive_letter>: \\\\<hostname>\\<mount> /user:hpcc\\<net_id> # where <hostname> and <mount> were determined from above 'Determining Your Network Path' and <net_id> is your MSU Net ID C:\\Documents and Settings\\Administrator>net use m: \\\\ufs-10-a.hpcc.msu.edu\\jal /user:hpcc\\jal The command completed successfully. # You can use 'net use' again to show the drive mapping status. C:\\Documents and Settings\\Administrator>net use New connections will not be remembered. Status Local Remote Network ------------------------------------------------------------------------------- Unavailable J: \\\\MachineA\\consult$ Microsoft Windows Network OK M: \\\\ufs-10-a.hpcc.msu.edu\\jal Microsoft Windows Network Unavailable P: \\\\MachineB\\everyone$ Microsoft Windows Network Z: \\\\vmware-host\\Shared Folders VMware Shared Folders The command completed successfully. # To disconnect a drive use: # net use <drive_letter>: /delete ## NOTE: this will only drop the connection, not delete the share or any data on the share ## C:\\Documents and Settings\\Administrator>net use m: /delete m: was deleted successfully. # you can now use 'net use' again to show your mapping status after the delete to ensure it's gone. C:\\Documents and Settings\\Administrator>net use New connections will not be remembered. Status Local Remote Network ------------------------------------------------------------------------------- Unavailable J: \\\\MachineA\\consult$ Microsoft Windows Network Unavailable P: \\\\MachineB\\everyone$ Microsoft Windows Network Z: \\\\vmware-host\\Shared Folders VMware Shared Folders The command completed successfully. C:\\Documents and Settings\\Administrator> MacOS Example Video Tutorial - Map Home directory using MacOS Open the Finder. Under \"GO\" click on \"Connect to Server\" From this menu you need to type your Network Path. Please see #Determining your Network Path for help. Enter your MSU NeID and password for authentication and click \"Connect\". Linux Install smb-client Ubuntu / Debian 1 apt install smbclient Red Hat / Fedora 1 yum install samba-client Edit /etc/samba/smb.conf 1 sudo vi /etc/samba/smb.conf Add the following lines to disable samba V1 This step must be completed or your client will not be able to map the drive. If you have other mounts on on the HPC cluster and they are using samba V1 they will stop working. In this case please use SSHFS . 1 2 client min protocol = SMB2 client max protocol = SMB3 Ubuntu Mount Example Open a File Browser window. In the \"File\" menu, select \"Connect to Server...\" Type your network path in the server address box. (Format is the same as the Mac format) 3. Enter your userid and password and click connect. 4. If connected properly the drive should appear in the file manager screen. Ubuntu Example (Older Versions) Open a File Browser window. In the \"File\" menu, select \"Connect to Server...\" In the window that appears, select \"SSH\" from the drop-down menu next to \"Service type,\" enter \"hpcc.msu.edu\" for \"Server,\" enter \"/mnt/home/username\" (where username is your NetID) for \"Folder,\" and type your username next to \"User Name.\" For quick access to the drive in future sessions, check the \"Add bookmark\" box and enter a descriptive label for \"Bookmark name.\" Once all of this information has been entered, click \"Connect.\" After a brief delay, a new window will appear, asking for your password. Enter it, choose whether or not your password should be saved using the radio buttons, and click \"Connect.\" The password window will then close, giving way to a File Browser window displaying the contents of your home directory. During future sessions, access the bookmark you added to reconnect. This can be done from the \"Bookmarks\" menu in a File Browser window. More Information: http://us3.samba.org/samba/","title":"Mapping drives using Samba"},{"location":"Mapping_HPC_drives_with_Samba/#mapping-hpc-drives-with-samba","text":"Warning Samba now uses campus AD for user authentication, if you are unable to login and have not updated your netid password recently please try updating your netid password before opening a ticket This will only work if your computer has a university IP address. If you are off campus, you can use MSU VPN to obtain an MSU IP, which is available to all graduate students, staff and faculty. If file transfer speed is a concern please use the sftp protocol for transferring large data sets, or use our Globus endpoint The following tutorial will show you how to map your HPC home or research directory using SMB or CIFS File Sharing . Determining your Network Path Windows 10 Command-line Windows NetBIOS Commands MacOS Example Linux Ubuntu Mount Example Ubuntu Example (Older Versions) More Information","title":"Mapping HPC drives with Samba"},{"location":"Mapping_HPC_drives_with_Samba/#determining-your-network-path","text":"We have the powertools command show-samba-paths to show all paths of your home and research space: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ ml powertools # if powertools is not loaded $ show-samba-paths HOME | Samba Path =================================================================== username | \\\\ ufs.hpcc.msu.edu \\h ome-021 \\u sername ( Windows ) | smb://ufs.hpcc.msu.edu/home-021/username ( Mac ) RESEARCH | Samba Path =============================================================================== helpdesk | \\\\ ufs.hpcc.msu.edu \\r s-011 \\h elpdesk ( Windows ) | smb://ufs.hpcc.msu.edu/rs-011/helpdesk ( Mac ) ------------------+------------------------------------------------------------ common-data | \\\\ ufs-12-b.hpcc.msu.edu \\c ommon-data ( Windows ) | smb://ufs-12-b.hpcc.msu.edu/common-data ( Mac ) ------------------+------------------------------------------------------------ BiCEP | \\\\ ufs.hpcc.msu.edu \\r s-001 \\B iCEP ( Windows ) | smb://ufs.hpcc.msu.edu/rs-001/BiCEP ( Mac ) ------------------+------------------------------------------------------------ education-data | \\\\ ufs-12-b.hpcc.msu.edu \\e ducation-data ( Windows ) | smb://ufs-12-b.hpcc.msu.edu/education-data ( Mac ) where the paths are the same for Mac and Window computers but with different formats.","title":"Determining your Network Path"},{"location":"Mapping_HPC_drives_with_Samba/#windows-10","text":"","title":"Windows 10"},{"location":"Mapping_HPC_drives_with_Samba/#step-1-enable-netbios-over-tcpip-on-windows","text":"Click on Desktop icon on your Windows 8 screen Right click on Network icon on start bar at right hand side and click on open network and sharing center Click on Change adapter settings Right click on your Network interface and click on Properties Follow the steps from 1d. in the Windows 7 instruction above to enable NetBIOS over TCP/IP.","title":"Step 1. Enable NetBIOS over TCP/IP on Windows:"},{"location":"Mapping_HPC_drives_with_Samba/#step-2-disable-smb1","text":"","title":"Step 2. Disable SMB1"},{"location":"Mapping_HPC_drives_with_Samba/#disable-samba-v1-protocol-with-powershell","text":"This step must be completed or your client will not be able to map the drive. If you have other mounts on on the HPC cluster and they are using samba V1 they will stop working. Press the windows start button In the search box type \"power shell\" Right click on the \"Windows PowerShell\" icon and select \"Run as Administrator\" Select Ok when security warning appears Disable Samba V1 by entering the following command into the windows power shell. 1 Set-ItemProperty -Path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\LanmanServer\\Parameters\" SMB1 -Type DWORD -Value 0 -Force Ensure SMB V2 and SMB V3 are enabled by entering the follwoing command. In the past, on some versions of windows and for some file systems we recommended the opposite of this setting. Running this ensures it's enabled again. 1 Set-ItemProperty -Path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\LanmanServer\\Parameters\" SMB2 -Type DWORD -Value 1 -Force Navigate to \"Computer\" and click on the text labeled \"Map Network Drive\" at the top of the screen. From this menu you need to type your Network Path. Please see #Determining your Network Path for help Once you have typed in your Network Path you need to click on the box \"Connect using different credentials.\" This will open a window where you type in your MSU netid and password: Warning If you aren't able to sign in, You will need to add \"CAMPUSAD\\\" to the beginning of your username. An indicator of this issue is if Windows displays the error \"The specified network password is not correct\" in the username dialog window. For example: substitute \"CAMPUSAD\\sparty\" for username \"sparty\" in the username field. The slash character is a backslash. A forward slash character will not work. Finally, select \"Finish\" and you will see your system trying to connect","title":"Disable Samba V1 protocol with PowerShell"},{"location":"Mapping_HPC_drives_with_Samba/#command-line-windows-netbios-commands","text":"If you're working in Windows, you can use command line tools to manage your drive mapping. These commands also work in .bat files, if you're so inclined to connect/disconnect drives in that manner. Note you may also have to Disable SMBV1 and Enable SMB2 per instructions above. From the Start Menu -> Run -> type 'cmd' in the box and hit enter, the command shell should open. You can then use the following commands to diagnose, disconnect and connect drives. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 # to show all mapped connections/drives # use 'net use' C:\\Documents and Settings\\Administrator>net use New connections will not be remembered. Status Local Remote Network ------------------------------------------------------------------------------- Unavailable J: \\\\MachineA\\consult$ Microsoft Windows Network Unavailable P: \\\\MachineB\\everyone$ Microsoft Windows Network Z: \\\\vmware-host\\Shared Folders VMware Shared Folders The command completed successfully. # to map a drive use: # net use <drive_letter>: \\\\<hostname>\\<mount> /user:hpcc\\<net_id> # where <hostname> and <mount> were determined from above 'Determining Your Network Path' and <net_id> is your MSU Net ID C:\\Documents and Settings\\Administrator>net use m: \\\\ufs-10-a.hpcc.msu.edu\\jal /user:hpcc\\jal The command completed successfully. # You can use 'net use' again to show the drive mapping status. C:\\Documents and Settings\\Administrator>net use New connections will not be remembered. Status Local Remote Network ------------------------------------------------------------------------------- Unavailable J: \\\\MachineA\\consult$ Microsoft Windows Network OK M: \\\\ufs-10-a.hpcc.msu.edu\\jal Microsoft Windows Network Unavailable P: \\\\MachineB\\everyone$ Microsoft Windows Network Z: \\\\vmware-host\\Shared Folders VMware Shared Folders The command completed successfully. # To disconnect a drive use: # net use <drive_letter>: /delete ## NOTE: this will only drop the connection, not delete the share or any data on the share ## C:\\Documents and Settings\\Administrator>net use m: /delete m: was deleted successfully. # you can now use 'net use' again to show your mapping status after the delete to ensure it's gone. C:\\Documents and Settings\\Administrator>net use New connections will not be remembered. Status Local Remote Network ------------------------------------------------------------------------------- Unavailable J: \\\\MachineA\\consult$ Microsoft Windows Network Unavailable P: \\\\MachineB\\everyone$ Microsoft Windows Network Z: \\\\vmware-host\\Shared Folders VMware Shared Folders The command completed successfully. C:\\Documents and Settings\\Administrator>","title":"Command-line Windows NetBIOS Commands"},{"location":"Mapping_HPC_drives_with_Samba/#macos-example","text":"Video Tutorial - Map Home directory using MacOS Open the Finder. Under \"GO\" click on \"Connect to Server\" From this menu you need to type your Network Path. Please see #Determining your Network Path for help. Enter your MSU NeID and password for authentication and click \"Connect\".","title":"MacOS Example"},{"location":"Mapping_HPC_drives_with_Samba/#linux","text":"Install smb-client Ubuntu / Debian 1 apt install smbclient Red Hat / Fedora 1 yum install samba-client Edit /etc/samba/smb.conf 1 sudo vi /etc/samba/smb.conf Add the following lines to disable samba V1 This step must be completed or your client will not be able to map the drive. If you have other mounts on on the HPC cluster and they are using samba V1 they will stop working. In this case please use SSHFS . 1 2 client min protocol = SMB2 client max protocol = SMB3","title":"Linux"},{"location":"Mapping_HPC_drives_with_Samba/#ubuntu-mount-example","text":"Open a File Browser window. In the \"File\" menu, select \"Connect to Server...\" Type your network path in the server address box. (Format is the same as the Mac format) 3. Enter your userid and password and click connect. 4. If connected properly the drive should appear in the file manager screen.","title":"Ubuntu Mount Example"},{"location":"Mapping_HPC_drives_with_Samba/#ubuntu-example-older-versions","text":"Open a File Browser window. In the \"File\" menu, select \"Connect to Server...\" In the window that appears, select \"SSH\" from the drop-down menu next to \"Service type,\" enter \"hpcc.msu.edu\" for \"Server,\" enter \"/mnt/home/username\" (where username is your NetID) for \"Folder,\" and type your username next to \"User Name.\" For quick access to the drive in future sessions, check the \"Add bookmark\" box and enter a descriptive label for \"Bookmark name.\" Once all of this information has been entered, click \"Connect.\" After a brief delay, a new window will appear, asking for your password. Enter it, choose whether or not your password should be saved using the radio buttons, and click \"Connect.\" The password window will then close, giving way to a File Browser window displaying the contents of your home directory. During future sessions, access the bookmark you added to reconnect. This can be done from the \"Bookmarks\" menu in a File Browser window.","title":"Ubuntu Example (Older Versions)"},{"location":"Mapping_HPC_drives_with_Samba/#more-information","text":"http://us3.samba.org/samba/","title":"More Information:"},{"location":"Matlab/","text":"About MATLAB There is a Matlab portal for Michigan State University users . Users are encouraged to visit it for information and support provided by Mathworks. Several versions of MATLAB are installed on the cluster. By default, MATLAB/2018a is loaded. Other available version of MATLAB can be discovered by typing 1 hpc@dev-intel18:~> module spider MATLAB and then switching to a different version, for example, to switch from 2018a to 2017a, one can directly load the the version as 1 hpc@dev-intel18:~> module load MATLAB/2019a MATLAB's many built-in functions have multi-threaded capability. On your personal computer, when a MATLAB function with multi-threads is called, MATLAB will automatically spawn as many threads as the number of cores on the machine. To avoid over utilizing compute nodes on HPCC, user should set the max number of threads by using \u201cmaxNumCompThreads(N)\u201d in matlab where N is the maximum number of threads matlab would use in the session. User could also use option \"-SingleCompThread\" to launch matlab session that would only use single thread. Without the option, matlab session will potentially spawn as many as 28 (on intel16 nodes) or 20 (on intel14 nodes) threads when a built-in multi-threaded function is called. To allow the multi-thread functions in matlab, users need to do the following: Specify the maximum number of compute threads to be used with \" maxNumCompThreads(N) \" at the beginning of the matlab program where N is the maximum number of threads in the program. For example, maxNumCompThreads(4) will set the maximum number of threads used in the program to four. If submit to run as batch job, specify \" --cpus-per-task=N \" in your job script where N should match the maximum number of compute threads set in \" maxNumCompThreads(N)\". Warning Starting Nov. 1, 2018 on new HPCC system, the matlab default setting of using single compute thread is changed. \u201cmatlab-mt\u201d and \u201cmatlab\u201d commands would be the same for current versions and \"matlab-mt\" will no longer exist from later versions starting 2019. HPCC has many toolboxes installed. To see a list of installed toolboxes and licenses, type 1 2 3 4 5 6 7 8 9 10 11 hpc@dev-intel18:~> module load powertools hpc@dev-intel18:~> licensecheck matlab Checking Licenses for matlab lmstat -a -c 27000@lm-02.i Users of MATLAB: (Total of 10000 licenses issued; Total of 44 licenses in use) Users of SIMULINK: (Total of 10000 licenses issued; Total of 1 license in use) Users of Bioinformatics_Toolbox: (Total of 10000 licenses issued; Total of 0 licenses in use) Users of Communication_Toolbox: (Total of 10000 licenses issued; Total of 0 licenses in use) ...... Running MATLAB on Scratch Space It is strongly recommended that iCER users run programs on scratch space. This may improve the speed of job execution as well as the whole system's performance. MATLAB users should carefully check whether any temporary files involved in the program execution need to be stored in scratch space. For example, if you use the Matlab compiler to make a Matlab program into a standalone program, you need to set the environment variable MCR_CACHE_ROOT in the scratch directory with: export MCR_CACHE_ROOT=$SCRATCH before starting the execution. This line should be added to the your job script before the line where you specify the task you want to run. This setting will override the default setting by MATLAB Compiler Runtime. By default, a directory for temporary data cache used by the MATLAB Compiler Runtime is created at user's home directory $HOME. Without this setting, users may run into the situation that the program running on scratch space frequently access to its cache space in the home space which will greatly slow down the execution of the code, and as well, may potentially slow down the whole system or even cause system instability. Running MATLAB Interactively In this document, we refer to an interactive session as one that involves a user typing commands into the MATLAB command windows. Short Sessions (< two hours) ssh to one of the dev nodes and run Matlab: 1 2 3 4 5 6 7 8 hpc@gateway:~> ssh dev-intel16 Last login: Mon Dec 4 12:54:44 2017 from gateway === This front-end node is not meant for running big or long-running jobs. Jobs that need to run longer than a few minutes should be submitted to the queue. Long-running jobs on front-end nodes will be killed without warning. === hpc@dev-intel16:~> matlab -nodisplay More information about running jobs interactively on compute nodes can be reviewed at Running Programs Interactively If you require graphics, please ensure that you have an Xserver running. For Linux and MAC users, 1 ssh -X username@hpcc.msu.edu If you want graphics, but don't want the desktop, type 1 hpc@dev-intel16:~> matlab -nodesktop Interactive MATLAB jobs running on development nodes are limited to a two hour wall time limit, and will be killed automatically afte two hours. Long Sessions (> two hours) Longer interactive sessions are possible, but are not recommended. Modify the following commands to suit your requirements. If graphics are not required, 1 2 3 4 5 6 7 8 9 10 hpc@dev-intel18:-> salloc --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2gb --time=04:00:00 salloc: Granted job allocation 310982 salloc: Waiting for resource configuration salloc: Nodes lac-376 are ready for job hpc@dev-intel18:-> matlab -nodisplay If graphics are required, add the option \"\u2013x11\" to the salloc command: 1 hpc@dev-intel18:-> salloc --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2gb --time=04:00:00 --x11 Warning The above commands submit a job to the cluster. If the resources are not immediately available, you will have to wait till the requested resources are available. Requesting a job for four hours or less will typically be scheduled relatively quickly. User may need to adjust the resources accordingly with the usage of the MATLAB program. Running MATLAB Non-Interactively Short Jobs (< two hours) A short job could be run on develop node without open matlab command window. From a development node, type 1 hpc@dev-intel16:~> matlab -nodisplay -r \"myMatlab\" & \"myMatlab.m\" will start running in background. Long Jobs (> two hours) Single MATLAB Job To submit jobs to the cluster, a job script needs to be written. And submitted to the queue. The following sample job script file can be modified to suit your needs: myJob.sbatch 1 2 3 4 5 6 7 #!/bin/bash -login #SBATCH --job-name=myJobName # specify a job name #SBATCH --nodes=1 --ntasks=1 --cpus-per-task=1 --time=02:05:00 --mem=550mb # specify the resources needed #SBATCH --license=matlab@27000@lm-02.i # specify the license request cd $SLURM_SUBMIT_DIR # go to the directory where this job is submitted matlab -nodisplay -r \"myMatlab\" myJobName is a string to make your job easier to identify it when managing or monitoring your jobs. --nodes=1 because you are running one matlab client in the job. If you want put multiple matlab run in a single job script, you may request more nodes for the job. --cpus-per-task=1 here because the matlab script \"MyMatlab.m\" does not use multi cores. If you are using multicore parallel, you may need to request more cpus. 02:05:00 is the number of hours:minutes:seconds your job needs to run. If it runs longer than this, it will be killed. If you request more time than you need, your job may be delayed while the scheduler finds a time to run it. If you don't know how long your job needs, you will have to make a guess and use the real running time to improve this number on future runs. The maximum walltime that can be requested is 168:00:00. --mem=550mb reserves 550 Maga bytes of total memory for the job. We recommend user to serve at least 550mb for each session plus the total size of data variables used in the computation. User could click here to see the recommendation of the matlab requirement by Mathworks. myMatlab is the name of your matlab script without the .m extension Submit your job with: 1 hpc@dev-intel16:~> sbatch myJob.sbatch \"myJob.sbatch\" is the name of your job script as shown above. Using the MATLAB Parallel Computing Toolbox The MATLAB Parallel Computing Toolbox (PCT) provides users several parallel computing features. Parallel for-loops (parfor) . (User could run \"module load powertools; getexample MATLAB_parfor\" to down load a directory \"MATLAB_parfor\" which contains an example of using parpool and parfor with \"local\" profile) Support GPU computing Offload computing from your laptop to HPCC cluster (with MATLAB Distributed Computing Server) Distributed arrays and spmd (single-program-multiple-data) for large dataset handling and data-parallel algorithms (1) One GPU card will be used for each worker. In order to use multiple GPUs, user need to use spmd capability that each instance of the program will use one card and multiple instances of the program take multiple cards. (2) If you use GPU capability, you need to have matlab run on a node with GPU. dev-intel14-k20, dev-intel16-k80 are the develop nodes with GPU. To request for GPU, use \"\u2013gres=gpu:\\<type>:\\<number>\" to request certain number and type of GPU. \"#SBATCH --gres=gpu:k20:1\" is an example to request one k20 GPU. Valid GPU type are k20, k80 and v100. Note that type is optional. But the number of GPU is necessary. Using the MATLAB Parallel Server The MATLAB Parallel Server lets users solve computationally and data-intensive problems by executing MATLAB and Simulink based applications on the HPCC cluster and clouds. (see the document for more information). HPCC cluster has this produce installed. We recommend that users prototype their applications using the Parallel Computing Toolbox, and then scale up to a cluster using MATLAB Parallel Server. To scale up to cluster, user does not need to recode the program. User only need to change the profile of the cluster. Setup and validate your cluster profile In this step you define a cluster profile to be used in subsequent steps. Start the Cluster Profile Manager from the MATLAB desktop by selecting on the Home tab in the Environment area Parallel > Create and Manage Clusters. Create a new profile in the Cluster Profile Manager by selecting Add Cluster Profile > Slurm. With the new profile selected in the list, click Rename to edit the profile name, Press Enter. Select a profile in the list, click Edit to edit the profile accordingly. After finishing editing, click Done to save the profile. Click validation to validate the profile. The profile could be used when it pass all the validation tests. (1) Since we switch to SLURM scheduler, the command \"configCluster\" used to configure the cluster on HPCC no longer works. The class \"ClusterInfo\" does not work either. (2) Starting from version 2018a, MATLAB supports SLURM scheduler. Please refer to Mathworks' document for how to configure the cluster with SLURM scheduler here . Using the MATLAB thread-based worker pool Started from Matlab version R2020a, thread-based worker pool is introduced. Please refer to the document for more details. On HPCC, we provide our users an example showing how to use thread-base pool with parfor, as well as the comparison between process-based and thread-based pools. To obtain the example, module load \"powertools\" and \"MATLAB/2020a\", then run \"getexample MATLAB_threadPool\". Running the MATLAB/2020a on AMD nodes There is a bug in MATLAB/2020a that would lead to \"segmentation fault\" on AMD node associated with java virtual machine. The patch may be introduce in next release. If you find that the code works on other version but crashed in 2020a version, you may try the workaround that launch the matlab session without java virtual machine as the following: 1 hpc@eval-epyc19 ~]$ matlab -nodisplay -nojvm -r \"myExample\" Running the MATLAB on intel14 nodes There is an existing problem of running some functions on Intel14 nodes. If you run into 'Illegal instruction detected' error, please use a node of other type. Please add constrain to exclude intel14 type of compute nodes when submit to SLURM. For example: 1 #SBATCH --constraint=[intel16|intel18|amd20] Using the MATLAB with Python Here is the link to the cheat sheets provided by the Mathworks for users' reference.","title":"Matlab"},{"location":"Matlab/#about-matlab","text":"There is a Matlab portal for Michigan State University users . Users are encouraged to visit it for information and support provided by Mathworks. Several versions of MATLAB are installed on the cluster. By default, MATLAB/2018a is loaded. Other available version of MATLAB can be discovered by typing 1 hpc@dev-intel18:~> module spider MATLAB and then switching to a different version, for example, to switch from 2018a to 2017a, one can directly load the the version as 1 hpc@dev-intel18:~> module load MATLAB/2019a MATLAB's many built-in functions have multi-threaded capability. On your personal computer, when a MATLAB function with multi-threads is called, MATLAB will automatically spawn as many threads as the number of cores on the machine. To avoid over utilizing compute nodes on HPCC, user should set the max number of threads by using \u201cmaxNumCompThreads(N)\u201d in matlab where N is the maximum number of threads matlab would use in the session. User could also use option \"-SingleCompThread\" to launch matlab session that would only use single thread. Without the option, matlab session will potentially spawn as many as 28 (on intel16 nodes) or 20 (on intel14 nodes) threads when a built-in multi-threaded function is called. To allow the multi-thread functions in matlab, users need to do the following: Specify the maximum number of compute threads to be used with \" maxNumCompThreads(N) \" at the beginning of the matlab program where N is the maximum number of threads in the program. For example, maxNumCompThreads(4) will set the maximum number of threads used in the program to four. If submit to run as batch job, specify \" --cpus-per-task=N \" in your job script where N should match the maximum number of compute threads set in \" maxNumCompThreads(N)\". Warning Starting Nov. 1, 2018 on new HPCC system, the matlab default setting of using single compute thread is changed. \u201cmatlab-mt\u201d and \u201cmatlab\u201d commands would be the same for current versions and \"matlab-mt\" will no longer exist from later versions starting 2019. HPCC has many toolboxes installed. To see a list of installed toolboxes and licenses, type 1 2 3 4 5 6 7 8 9 10 11 hpc@dev-intel18:~> module load powertools hpc@dev-intel18:~> licensecheck matlab Checking Licenses for matlab lmstat -a -c 27000@lm-02.i Users of MATLAB: (Total of 10000 licenses issued; Total of 44 licenses in use) Users of SIMULINK: (Total of 10000 licenses issued; Total of 1 license in use) Users of Bioinformatics_Toolbox: (Total of 10000 licenses issued; Total of 0 licenses in use) Users of Communication_Toolbox: (Total of 10000 licenses issued; Total of 0 licenses in use) ......","title":"About MATLAB"},{"location":"Matlab/#running-matlab-on-scratch-space","text":"It is strongly recommended that iCER users run programs on scratch space. This may improve the speed of job execution as well as the whole system's performance. MATLAB users should carefully check whether any temporary files involved in the program execution need to be stored in scratch space. For example, if you use the Matlab compiler to make a Matlab program into a standalone program, you need to set the environment variable MCR_CACHE_ROOT in the scratch directory with: export MCR_CACHE_ROOT=$SCRATCH before starting the execution. This line should be added to the your job script before the line where you specify the task you want to run. This setting will override the default setting by MATLAB Compiler Runtime. By default, a directory for temporary data cache used by the MATLAB Compiler Runtime is created at user's home directory $HOME. Without this setting, users may run into the situation that the program running on scratch space frequently access to its cache space in the home space which will greatly slow down the execution of the code, and as well, may potentially slow down the whole system or even cause system instability.","title":"Running MATLAB on Scratch Space"},{"location":"Matlab/#running-matlab-interactively","text":"In this document, we refer to an interactive session as one that involves a user typing commands into the MATLAB command windows.","title":"Running MATLAB Interactively"},{"location":"Matlab/#short-sessions-two-hours","text":"ssh to one of the dev nodes and run Matlab: 1 2 3 4 5 6 7 8 hpc@gateway:~> ssh dev-intel16 Last login: Mon Dec 4 12:54:44 2017 from gateway === This front-end node is not meant for running big or long-running jobs. Jobs that need to run longer than a few minutes should be submitted to the queue. Long-running jobs on front-end nodes will be killed without warning. === hpc@dev-intel16:~> matlab -nodisplay More information about running jobs interactively on compute nodes can be reviewed at Running Programs Interactively If you require graphics, please ensure that you have an Xserver running. For Linux and MAC users, 1 ssh -X username@hpcc.msu.edu If you want graphics, but don't want the desktop, type 1 hpc@dev-intel16:~> matlab -nodesktop Interactive MATLAB jobs running on development nodes are limited to a two hour wall time limit, and will be killed automatically afte two hours.","title":"Short Sessions (&lt; two hours)"},{"location":"Matlab/#long-sessions-two-hours","text":"Longer interactive sessions are possible, but are not recommended. Modify the following commands to suit your requirements. If graphics are not required, 1 2 3 4 5 6 7 8 9 10 hpc@dev-intel18:-> salloc --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2gb --time=04:00:00 salloc: Granted job allocation 310982 salloc: Waiting for resource configuration salloc: Nodes lac-376 are ready for job hpc@dev-intel18:-> matlab -nodisplay If graphics are required, add the option \"\u2013x11\" to the salloc command: 1 hpc@dev-intel18:-> salloc --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2gb --time=04:00:00 --x11 Warning The above commands submit a job to the cluster. If the resources are not immediately available, you will have to wait till the requested resources are available. Requesting a job for four hours or less will typically be scheduled relatively quickly. User may need to adjust the resources accordingly with the usage of the MATLAB program.","title":"Long Sessions (&gt; two hours)"},{"location":"Matlab/#running-matlab-non-interactively","text":"","title":"Running MATLAB Non-Interactively"},{"location":"Matlab/#short-jobs-two-hours","text":"A short job could be run on develop node without open matlab command window. From a development node, type 1 hpc@dev-intel16:~> matlab -nodisplay -r \"myMatlab\" & \"myMatlab.m\" will start running in background.","title":"Short Jobs (&lt; two hours)"},{"location":"Matlab/#long-jobs-two-hours","text":"","title":"Long Jobs (> two hours)"},{"location":"Matlab/#single-matlab-job","text":"To submit jobs to the cluster, a job script needs to be written. And submitted to the queue. The following sample job script file can be modified to suit your needs: myJob.sbatch 1 2 3 4 5 6 7 #!/bin/bash -login #SBATCH --job-name=myJobName # specify a job name #SBATCH --nodes=1 --ntasks=1 --cpus-per-task=1 --time=02:05:00 --mem=550mb # specify the resources needed #SBATCH --license=matlab@27000@lm-02.i # specify the license request cd $SLURM_SUBMIT_DIR # go to the directory where this job is submitted matlab -nodisplay -r \"myMatlab\" myJobName is a string to make your job easier to identify it when managing or monitoring your jobs. --nodes=1 because you are running one matlab client in the job. If you want put multiple matlab run in a single job script, you may request more nodes for the job. --cpus-per-task=1 here because the matlab script \"MyMatlab.m\" does not use multi cores. If you are using multicore parallel, you may need to request more cpus. 02:05:00 is the number of hours:minutes:seconds your job needs to run. If it runs longer than this, it will be killed. If you request more time than you need, your job may be delayed while the scheduler finds a time to run it. If you don't know how long your job needs, you will have to make a guess and use the real running time to improve this number on future runs. The maximum walltime that can be requested is 168:00:00. --mem=550mb reserves 550 Maga bytes of total memory for the job. We recommend user to serve at least 550mb for each session plus the total size of data variables used in the computation. User could click here to see the recommendation of the matlab requirement by Mathworks. myMatlab is the name of your matlab script without the .m extension Submit your job with: 1 hpc@dev-intel16:~> sbatch myJob.sbatch \"myJob.sbatch\" is the name of your job script as shown above.","title":"Single MATLAB Job"},{"location":"Matlab/#using-the-matlab-parallel-computing-toolbox","text":"The MATLAB Parallel Computing Toolbox (PCT) provides users several parallel computing features. Parallel for-loops (parfor) . (User could run \"module load powertools; getexample MATLAB_parfor\" to down load a directory \"MATLAB_parfor\" which contains an example of using parpool and parfor with \"local\" profile) Support GPU computing Offload computing from your laptop to HPCC cluster (with MATLAB Distributed Computing Server) Distributed arrays and spmd (single-program-multiple-data) for large dataset handling and data-parallel algorithms (1) One GPU card will be used for each worker. In order to use multiple GPUs, user need to use spmd capability that each instance of the program will use one card and multiple instances of the program take multiple cards. (2) If you use GPU capability, you need to have matlab run on a node with GPU. dev-intel14-k20, dev-intel16-k80 are the develop nodes with GPU. To request for GPU, use \"\u2013gres=gpu:\\<type>:\\<number>\" to request certain number and type of GPU. \"#SBATCH --gres=gpu:k20:1\" is an example to request one k20 GPU. Valid GPU type are k20, k80 and v100. Note that type is optional. But the number of GPU is necessary.","title":"Using the MATLAB Parallel Computing Toolbox"},{"location":"Matlab/#using-the-matlab-parallel-server","text":"The MATLAB Parallel Server lets users solve computationally and data-intensive problems by executing MATLAB and Simulink based applications on the HPCC cluster and clouds. (see the document for more information). HPCC cluster has this produce installed. We recommend that users prototype their applications using the Parallel Computing Toolbox, and then scale up to a cluster using MATLAB Parallel Server. To scale up to cluster, user does not need to recode the program. User only need to change the profile of the cluster.","title":"Using the MATLAB Parallel Server"},{"location":"Matlab/#setup-and-validate-your-cluster-profile","text":"In this step you define a cluster profile to be used in subsequent steps. Start the Cluster Profile Manager from the MATLAB desktop by selecting on the Home tab in the Environment area Parallel > Create and Manage Clusters. Create a new profile in the Cluster Profile Manager by selecting Add Cluster Profile > Slurm. With the new profile selected in the list, click Rename to edit the profile name, Press Enter. Select a profile in the list, click Edit to edit the profile accordingly. After finishing editing, click Done to save the profile. Click validation to validate the profile. The profile could be used when it pass all the validation tests. (1) Since we switch to SLURM scheduler, the command \"configCluster\" used to configure the cluster on HPCC no longer works. The class \"ClusterInfo\" does not work either. (2) Starting from version 2018a, MATLAB supports SLURM scheduler. Please refer to Mathworks' document for how to configure the cluster with SLURM scheduler here .","title":"Setup and validate your cluster profile"},{"location":"Matlab/#using-the-matlab-thread-based-worker-pool","text":"Started from Matlab version R2020a, thread-based worker pool is introduced. Please refer to the document for more details. On HPCC, we provide our users an example showing how to use thread-base pool with parfor, as well as the comparison between process-based and thread-based pools. To obtain the example, module load \"powertools\" and \"MATLAB/2020a\", then run \"getexample MATLAB_threadPool\".","title":"Using the MATLAB thread-based worker pool"},{"location":"Matlab/#running-the-matlab2020a-on-amd-nodes","text":"There is a bug in MATLAB/2020a that would lead to \"segmentation fault\" on AMD node associated with java virtual machine. The patch may be introduce in next release. If you find that the code works on other version but crashed in 2020a version, you may try the workaround that launch the matlab session without java virtual machine as the following: 1 hpc@eval-epyc19 ~]$ matlab -nodisplay -nojvm -r \"myExample\"","title":"Running the MATLAB/2020a on AMD nodes"},{"location":"Matlab/#running-the-matlab-on-intel14-nodes","text":"There is an existing problem of running some functions on Intel14 nodes. If you run into 'Illegal instruction detected' error, please use a node of other type. Please add constrain to exclude intel14 type of compute nodes when submit to SLURM. For example: 1 #SBATCH --constraint=[intel16|intel18|amd20]","title":"Running the MATLAB on intel14 nodes"},{"location":"Matlab/#using-the-matlab-with-python","text":"Here is the link to the cheat sheets provided by the Mathworks for users' reference.","title":"Using the MATLAB with Python"},{"location":"Module_System_and_Software_Installation/","text":"Module System and Software Installation There are many software installed on HPCC system. It is very difficult to find a particular one and use it with correct environment setup without a module system. In order to manage the software and their dependencies systematically, we use the Lua-based Lmod module system which provides a convenient way to dynamically change the users' environment through module files. In the following section, we introduce the module commands for how to find and load (or unload) a software. Although many software are available in our system, users might still want to install more and build their own software and module system. Here, we recommend using EasyBuild. The program is helpful to build software and module system under user's home or research space. The system built by users can also work together with HPCC software and module system. Common Module Commands User Created Modules Software Installation by EasyBuild Compilers and Libraries Using Version Control Systems","title":"Module System and Software Installation"},{"location":"Module_System_and_Software_Installation/#module-system-and-software-installation","text":"There are many software installed on HPCC system. It is very difficult to find a particular one and use it with correct environment setup without a module system. In order to manage the software and their dependencies systematically, we use the Lua-based Lmod module system which provides a convenient way to dynamically change the users' environment through module files. In the following section, we introduce the module commands for how to find and load (or unload) a software. Although many software are available in our system, users might still want to install more and build their own software and module system. Here, we recommend using EasyBuild. The program is helpful to build software and module system under user's home or research space. The system built by users can also work together with HPCC software and module system.","title":"Module System and Software Installation"},{"location":"Module_System_and_Software_Installation/#common-module-commands","text":"","title":"Common Module Commands"},{"location":"Module_System_and_Software_Installation/#user-created-modules","text":"","title":"User Created Modules"},{"location":"Module_System_and_Software_Installation/#software-installation-by-easybuild","text":"","title":"Software Installation by EasyBuild"},{"location":"Module_System_and_Software_Installation/#compilers-and-libraries","text":"","title":"Compilers and Libraries"},{"location":"Module_System_and_Software_Installation/#using-version-control-systems","text":"","title":"Using Version Control Systems"},{"location":"Mothur/","text":"Mothur Loading Mothur Mothur version 1.40.3 on the HPCC has two running modes, with and without MPI functionality. You can load either one by: 1 2 3 module purge module load icc/2017.1.132-GCC-6.3.0-2.27 impi/2017.1.132 Mothur/1.40.3-nonMPI-Python-2.7.13 # Mothur non-MPI version module load icc/2017.1.132-GCC-6.3.0-2.27 impi/2017.1.132 Mothur/1.40.3-Python-2.7.13 # Mothur MPI version As of Feb 7, 2019, the highest version is Mothur/1.41.3 (in MPI mode only). As reported by some Mothur users, when using Mothur and vsearch together, the only compatible version of vsearch is 1.8. So after loading Mothur, you would add a line of \" module load vsearch/1.8.0 \". Running Mothur Take a look at this example code batch.m : /mnt/research/common-data/Examples/mothur/batch.m : 1 2 set.current(fasta=ex.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.subsample.fasta, count=ex.trim.contigs.good.unique.good.filter.unique.precluster.uchime.pick.pick.subsample.count_table, processors=1) dist.seqs(fasta=current, cutoff=0.2, processors=8) where we specified processors=8 in line 2. To be able to actually utilize 8 processors, you need to launch Mothur using either of the following commands, depending on whether MPI is enabled or not. 1 2 MPI: mpirun -np 8 mothur batch.m non-MPI: mothur batch.m Differences betweem MPI and non-MPI runs MPI jobs can run across multiple nodes at the cost of overhead. This can lead to increased memory usage and decreased performance. The additional processor advantages offered by MPI may be cancelled out by I/O waits to disk. If you request many more processes than can be provided by a single node, use MPI mode. If you choose the MPI type, specify number of processes in the SLURM script by --ntask=8 for the example above. SLURM will determine how many nodes and tasks per node are needed. Also, memory request in this case should be made on a per CPU basis (by defining --mem-per-cpu ). Non-MPI jobs run on a single node with multiple threads/processes. For above Mothur command, you should set up something like #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=8 in your job submission script. If most of the nodes in the cluster are highly occupied, the job scheduler may have a hard time finding the nodes with availability of your desired number of threads.","title":"Mothur"},{"location":"Mothur/#mothur","text":"","title":"Mothur"},{"location":"Mothur/#loading-mothur","text":"Mothur version 1.40.3 on the HPCC has two running modes, with and without MPI functionality. You can load either one by: 1 2 3 module purge module load icc/2017.1.132-GCC-6.3.0-2.27 impi/2017.1.132 Mothur/1.40.3-nonMPI-Python-2.7.13 # Mothur non-MPI version module load icc/2017.1.132-GCC-6.3.0-2.27 impi/2017.1.132 Mothur/1.40.3-Python-2.7.13 # Mothur MPI version As of Feb 7, 2019, the highest version is Mothur/1.41.3 (in MPI mode only). As reported by some Mothur users, when using Mothur and vsearch together, the only compatible version of vsearch is 1.8. So after loading Mothur, you would add a line of \" module load vsearch/1.8.0 \".","title":"Loading Mothur"},{"location":"Mothur/#running-mothur","text":"Take a look at this example code batch.m : /mnt/research/common-data/Examples/mothur/batch.m : 1 2 set.current(fasta=ex.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.subsample.fasta, count=ex.trim.contigs.good.unique.good.filter.unique.precluster.uchime.pick.pick.subsample.count_table, processors=1) dist.seqs(fasta=current, cutoff=0.2, processors=8) where we specified processors=8 in line 2. To be able to actually utilize 8 processors, you need to launch Mothur using either of the following commands, depending on whether MPI is enabled or not. 1 2 MPI: mpirun -np 8 mothur batch.m non-MPI: mothur batch.m","title":"Running Mothur"},{"location":"Mothur/#differences-betweem-mpi-and-non-mpi-runs","text":"MPI jobs can run across multiple nodes at the cost of overhead. This can lead to increased memory usage and decreased performance. The additional processor advantages offered by MPI may be cancelled out by I/O waits to disk. If you request many more processes than can be provided by a single node, use MPI mode. If you choose the MPI type, specify number of processes in the SLURM script by --ntask=8 for the example above. SLURM will determine how many nodes and tasks per node are needed. Also, memory request in this case should be made on a per CPU basis (by defining --mem-per-cpu ). Non-MPI jobs run on a single node with multiple threads/processes. For above Mothur command, you should set up something like #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=8 in your job submission script. If most of the nodes in the cluster are highly occupied, the job scheduler may have a hard time finding the nodes with availability of your desired number of threads.","title":"Differences betweem MPI and non-MPI runs"},{"location":"MySQL_configuration/","text":"MySQL configuration for OrthoMCL Overview OrthoMCL is a program that aids in the identification of orthologs. The OrthoMCL tool uses NCBI BLAST and the MCL application in conjunction with a relational database (MySQL). OrthoMCL version 2.0.2 is available on the HPCC and can be loaded as a module. However, because of the relational database requirement, the HPCC must be contacted in advance to setup a database for your runs. This tutorial briefly describes how to obtain access to the program, and how to use configuration files provided by the HPCC. Database Access Before beginning your HPCC runs, you will need to complete an help request ticket request to request a database and MySQL account for your use of OrthoMCL. This information will be provided to you in the form of a configuration file, and you save this to your directory. You can use the filename as a command-line argument to relevant scripts comprising the OrthoMCL application, which tell OrthMCL how to connect to your database. Configuration File The following is an example of the configuration file you will receive: 1 2 3 4 5 6 7 8 9 10 11 12 dbVendor=mysql dbConnectString=dbi:mysql:someUserdb:db-01:3306 dbLogin=someUser dbPassword=somePassword # DO NOT CHANGE ANYTHING ABOVE THIS LINE UNLESS YOU KNOW WHAT YOU'RE DOING similarSequencesTable=SimilarSequences orthologTable=Ortholog inParalogTable=InParalog coOrthologTable=CoOrtholog interTaxonMatchView=InterTaxonMatch percentMatchCutoff=50 evalueExponentCutoff=-5 Command Example A typical run that would require the database configuration file might look something like the following: 1 orthomclPairs orthomcl.config log_file cleanup=[yes|no|only|all] <startAfter=TAG> In the example above, the file \" orthomcl.config \" is the name of the configuration/connection file (provided by the HPCC) that you want to use for your run. Purging the Database Between Runs To facilitate multiple concurrent, or faster consecutive runs, many users ask for more than one database at setup time. HPCC will typically be able to provide you with up to four (4) such databases. Please specify this in your request. Once your run is completed, you will need to purge the database of its contents prior to beginning new runs using the same database. To have this performed, please contact ICER via https://contact.icer.msu.edu , select \"other\" in the form and let us know you need your database purge and the name of the database if you have multiple databases (or just provide the connection string like dbConnectString=dbi:mysql:someUserdb:db-01:3306 ). When your work with OrthoMCL is complete, please notify the staff via the ICER contact form so we can purge your databases from the system. Modifying the Configuration File Most users will not need to modify the configuration file provided by the HPCC. The most common modification needed will be to change the name of the database to be accessed in those cases where users are provided with access to more than one database. The relevant line to be modified is shown below: 1 dbConnectString=dbi:mysql:someUserdb:db-01:3306 In the example above, the database name is \"someUserdb\". Let's assume (for example), a user had been issued 4 databases named: someUserdb, someUserdb2, someUserdb3, someUserdb4. To perform a run using one of these other databases, we would need to make a copy of the configuration file and change the name in that file, for example: 1 dbConnectString=dbi:mysql:someUserdb2:db-01:3306 You may then structure the command for each OrthoMCL run to use the configuration file (and related database) desired. More Information Refer to the OrthoMCL User Manual .","title":"OrthoMCL: MySQL configuration"},{"location":"MySQL_configuration/#mysql-configuration-for-orthomcl","text":"","title":"MySQL configuration for OrthoMCL"},{"location":"MySQL_configuration/#overview","text":"OrthoMCL is a program that aids in the identification of orthologs. The OrthoMCL tool uses NCBI BLAST and the MCL application in conjunction with a relational database (MySQL). OrthoMCL version 2.0.2 is available on the HPCC and can be loaded as a module. However, because of the relational database requirement, the HPCC must be contacted in advance to setup a database for your runs. This tutorial briefly describes how to obtain access to the program, and how to use configuration files provided by the HPCC.","title":"Overview"},{"location":"MySQL_configuration/#database-access","text":"Before beginning your HPCC runs, you will need to complete an help request ticket request to request a database and MySQL account for your use of OrthoMCL. This information will be provided to you in the form of a configuration file, and you save this to your directory. You can use the filename as a command-line argument to relevant scripts comprising the OrthoMCL application, which tell OrthMCL how to connect to your database.","title":"Database Access"},{"location":"MySQL_configuration/#configuration-file","text":"The following is an example of the configuration file you will receive: 1 2 3 4 5 6 7 8 9 10 11 12 dbVendor=mysql dbConnectString=dbi:mysql:someUserdb:db-01:3306 dbLogin=someUser dbPassword=somePassword # DO NOT CHANGE ANYTHING ABOVE THIS LINE UNLESS YOU KNOW WHAT YOU'RE DOING similarSequencesTable=SimilarSequences orthologTable=Ortholog inParalogTable=InParalog coOrthologTable=CoOrtholog interTaxonMatchView=InterTaxonMatch percentMatchCutoff=50 evalueExponentCutoff=-5","title":"Configuration File"},{"location":"MySQL_configuration/#command-example","text":"A typical run that would require the database configuration file might look something like the following: 1 orthomclPairs orthomcl.config log_file cleanup=[yes|no|only|all] <startAfter=TAG> In the example above, the file \" orthomcl.config \" is the name of the configuration/connection file (provided by the HPCC) that you want to use for your run.","title":"Command Example"},{"location":"MySQL_configuration/#purging-the-database-between-runs","text":"To facilitate multiple concurrent, or faster consecutive runs, many users ask for more than one database at setup time. HPCC will typically be able to provide you with up to four (4) such databases. Please specify this in your request. Once your run is completed, you will need to purge the database of its contents prior to beginning new runs using the same database. To have this performed, please contact ICER via https://contact.icer.msu.edu , select \"other\" in the form and let us know you need your database purge and the name of the database if you have multiple databases (or just provide the connection string like dbConnectString=dbi:mysql:someUserdb:db-01:3306 ). When your work with OrthoMCL is complete, please notify the staff via the ICER contact form so we can purge your databases from the system.","title":"Purging the Database Between Runs"},{"location":"MySQL_configuration/#modifying-the-configuration-file","text":"Most users will not need to modify the configuration file provided by the HPCC. The most common modification needed will be to change the name of the database to be accessed in those cases where users are provided with access to more than one database. The relevant line to be modified is shown below: 1 dbConnectString=dbi:mysql:someUserdb:db-01:3306 In the example above, the database name is \"someUserdb\". Let's assume (for example), a user had been issued 4 databases named: someUserdb, someUserdb2, someUserdb3, someUserdb4. To perform a run using one of these other databases, we would need to make a copy of the configuration file and change the name in that file, for example: 1 dbConnectString=dbi:mysql:someUserdb2:db-01:3306 You may then structure the command for each OrthoMCL run to use the configuration file (and related database) desired.","title":"Modifying the Configuration File"},{"location":"MySQL_configuration/#more-information","text":"Refer to the OrthoMCL User Manual .","title":"More Information"},{"location":"NCBI_Entrez_Direct_tools%202/","text":"NCBI Entrez Direct tools Entrez Direct (eDirect) provides access to the NCBI's suite of interconnected databases (publication, sequence, structure, gene, variation, expression, etc.) using Linux command lines. People who need to retrieve sequence and other type of data from NCBI regularly should use the eDirect toolset. To use edirect commands on HPCC, run: 1 2 3 ssh dev-intel18 module load Perl/5.28.1 module load edirect A full documentation of eDirect is available from https://www.ncbi.nlm.nih.gov/books/NBK179288/ A few useful examples can be found in this blog http://bioinformatics.cvr.ac.uk/blog/ncbi-entrez-direct-unix-e-utilities/ eDirect cookbook: https://github.com/NCBI-Hackathons/EDirectCookbook/blob/master/README.md","title":"NCBI Entrez Direct tools"},{"location":"NCBI_Entrez_Direct_tools%202/#ncbi-entrez-direct-tools","text":"Entrez Direct (eDirect) provides access to the NCBI's suite of interconnected databases (publication, sequence, structure, gene, variation, expression, etc.) using Linux command lines. People who need to retrieve sequence and other type of data from NCBI regularly should use the eDirect toolset. To use edirect commands on HPCC, run: 1 2 3 ssh dev-intel18 module load Perl/5.28.1 module load edirect A full documentation of eDirect is available from https://www.ncbi.nlm.nih.gov/books/NBK179288/ A few useful examples can be found in this blog http://bioinformatics.cvr.ac.uk/blog/ncbi-entrez-direct-unix-e-utilities/ eDirect cookbook: https://github.com/NCBI-Hackathons/EDirectCookbook/blob/master/README.md","title":"NCBI Entrez Direct tools"},{"location":"NCBI_Entrez_Direct_tools/","text":"NCBI Entrez Direct tools Entrez Direct (eDirect) provides access to the NCBI's suite of interconnected databases (publication, sequence, structure, gene, variation, expression, etc.) using Linux command lines. People who need to retrieve sequence and other type of data from NCBI regularly should use the eDirect toolset. To use edirect commands on HPCC, run: 1 2 3 ssh dev-intel18 module load Perl/5.28.1 module load edirect A full documentation of eDirect is available from https://www.ncbi.nlm.nih.gov/books/NBK179288/ A few useful examples can be found in this blog http://bioinformatics.cvr.ac.uk/blog/ncbi-entrez-direct-unix-e-utilities/ eDirect cookbook: https://github.com/NCBI-Hackathons/EDirectCookbook/blob/master/README.md","title":"NCBI eDirect"},{"location":"NCBI_Entrez_Direct_tools/#ncbi-entrez-direct-tools","text":"Entrez Direct (eDirect) provides access to the NCBI's suite of interconnected databases (publication, sequence, structure, gene, variation, expression, etc.) using Linux command lines. People who need to retrieve sequence and other type of data from NCBI regularly should use the eDirect toolset. To use edirect commands on HPCC, run: 1 2 3 ssh dev-intel18 module load Perl/5.28.1 module load edirect A full documentation of eDirect is available from https://www.ncbi.nlm.nih.gov/books/NBK179288/ A few useful examples can be found in this blog http://bioinformatics.cvr.ac.uk/blog/ncbi-entrez-direct-unix-e-utilities/ eDirect cookbook: https://github.com/NCBI-Hackathons/EDirectCookbook/blob/master/README.md","title":"NCBI Entrez Direct tools"},{"location":"Oakland_University_Users/","text":"Oakland University Users User-specific information This information is for users at Oakland University. Users with MSU NetIDs should disregard this information. Oakland University users should refer to the documentation hosted on the Oakland University knowledge base.","title":"Oakland U"},{"location":"Oakland_University_Users/#oakland-university-users","text":"","title":"Oakland University Users"},{"location":"Oakland_University_Users/#user-specific-information","text":"This information is for users at Oakland University. Users with MSU NetIDs should disregard this information. Oakland University users should refer to the documentation hosted on the Oakland University knowledge base.","title":"User-specific information"},{"location":"Open_OnDemand/","text":"Open OnDemand Open OnDemand helps researchers efficiently utilize the HPCC by providing easy web access with graphical user interfaces from any device. The features include, though are not limited to: Plugin-Free web experience Easy file management Easy job submission, management, and monitoring Graphical desktop environments and desktop applications; Jupyter Notebook, RStudio, ... One-click app icons on a desktop to launch your favorite GUI applications Command-line shell access Connect to HPCC OnDemand To connect to HPCC OnDemand, visit https://ondemand.hpcc.msu.edu/ . It will first redirect to the CILogon website for authentication. From here, select \" Michigan State University \" as the identity provider and click \" Log On \". This will redirect you to a page where you can log in with your MSU credentials. Note A valid MSU email address with NetID and password are required to access HPCC OnDemand. This may prohibits the access of external collaborators. Please take this into consideration when you choose tools provided on HPCC OnDemand for your group projects involving external collaborators. After sign in, you will reach the HPCC OnDemand web portal with the following menu options: Files All user's files in the home, research, scratch, and software file spaces can be accessed. Select a directory then use the File Explorer to download, upload, view, edit, and move files. Jobs Active Jobs: List jobs in the queue; monitor or manage those jobs Job Composer: Submit a job script with resource requests and command lines; create new scripts from a job template, copy a previous job submission, or select an existing job script from a specified directory Interactive Apps Desktops: Request an Interactive Desktop ; once it starts, a graphical user interface (GUI) is provided for running GUI based applications GUIs: Launch an HPCC provided, GUI based application directly MATLAB : 2018a-2021a versions are available Stata : 15SE, 15MP, 17SE, 17MP are available Jupyter notebook : By default, Python 3.7.2 will be used; choose your own Python by selecting 'Launch Jupyter Notebook using the Anaconda installation in my home directory' RStudio : Various versions are available (3.5.0\\~ 4.1.2) Note Please make sure you request enough memory. Otherwise, your interactive session won't start or your running processes will be terminated prior to completion. Development Nodes Start a bash session a specified development node and input commands using the terminal's command line interface. Note Currently, there is no remote graphical display capability while in the terminal i.e., no X11 forwarding. For GUI based applications, please use the Interactive Apps feature. My Interactive Sessions All of a user's interactive jobs are displayed, along with a description of the resources currently allocated to each job. User's may then manage aspects of those jobs using the asscociated action buttons. We are currently working to add more functionality to the OnDemand interface. Please feel free to contact us if you have any questions or suggestions.","title":"Open OnDemand"},{"location":"Open_OnDemand/#open-ondemand","text":"Open OnDemand helps researchers efficiently utilize the HPCC by providing easy web access with graphical user interfaces from any device. The features include, though are not limited to: Plugin-Free web experience Easy file management Easy job submission, management, and monitoring Graphical desktop environments and desktop applications; Jupyter Notebook, RStudio, ... One-click app icons on a desktop to launch your favorite GUI applications Command-line shell access","title":"Open OnDemand"},{"location":"Open_OnDemand/#connect-to-hpcc-ondemand","text":"To connect to HPCC OnDemand, visit https://ondemand.hpcc.msu.edu/ . It will first redirect to the CILogon website for authentication. From here, select \" Michigan State University \" as the identity provider and click \" Log On \". This will redirect you to a page where you can log in with your MSU credentials. Note A valid MSU email address with NetID and password are required to access HPCC OnDemand. This may prohibits the access of external collaborators. Please take this into consideration when you choose tools provided on HPCC OnDemand for your group projects involving external collaborators. After sign in, you will reach the HPCC OnDemand web portal with the following menu options:","title":"Connect to HPCC OnDemand"},{"location":"Open_OnDemand/#files","text":"All user's files in the home, research, scratch, and software file spaces can be accessed. Select a directory then use the File Explorer to download, upload, view, edit, and move files.","title":"Files"},{"location":"Open_OnDemand/#jobs","text":"Active Jobs: List jobs in the queue; monitor or manage those jobs Job Composer: Submit a job script with resource requests and command lines; create new scripts from a job template, copy a previous job submission, or select an existing job script from a specified directory","title":"Jobs"},{"location":"Open_OnDemand/#interactive-apps","text":"Desktops: Request an Interactive Desktop ; once it starts, a graphical user interface (GUI) is provided for running GUI based applications GUIs: Launch an HPCC provided, GUI based application directly MATLAB : 2018a-2021a versions are available Stata : 15SE, 15MP, 17SE, 17MP are available Jupyter notebook : By default, Python 3.7.2 will be used; choose your own Python by selecting 'Launch Jupyter Notebook using the Anaconda installation in my home directory' RStudio : Various versions are available (3.5.0\\~ 4.1.2) Note Please make sure you request enough memory. Otherwise, your interactive session won't start or your running processes will be terminated prior to completion.","title":"Interactive Apps"},{"location":"Open_OnDemand/#development-nodes","text":"Start a bash session a specified development node and input commands using the terminal's command line interface. Note Currently, there is no remote graphical display capability while in the terminal i.e., no X11 forwarding. For GUI based applications, please use the Interactive Apps feature.","title":"Development Nodes"},{"location":"Open_OnDemand/#my-interactive-sessions","text":"All of a user's interactive jobs are displayed, along with a description of the resources currently allocated to each job. User's may then manage aspects of those jobs using the asscociated action buttons. We are currently working to add more functionality to the OnDemand interface. Please feel free to contact us if you have any questions or suggestions.","title":"My Interactive Sessions"},{"location":"Other_Universities/","text":"Other Universities Information for Central Michigan University and Western Michigan University Users Kettering Users Oakland University Users","title":"Other Universities"},{"location":"Other_Universities/#other-universities","text":"","title":"Other Universities"},{"location":"Other_Universities/#information-for-central-michigan-university-and-western-michigan-university-users","text":"","title":"Information for Central Michigan University and Western Michigan University Users"},{"location":"Other_Universities/#kettering-users","text":"","title":"Kettering Users"},{"location":"Other_Universities/#oakland-university-users","text":"","title":"Oakland University Users"},{"location":"PC_to_HPC/","text":"PC to HPC Lecture notes from Dr. Xiaoge Wang \u2013 \"PC to HPC\":","title":"PC to HPC"},{"location":"PC_to_HPC/#pc-to-hpc","text":"Lecture notes from Dr. Xiaoge Wang \u2013 \"PC to HPC\":","title":"PC to HPC"},{"location":"Packages/","text":"Packages R workshop tutorial rstan rjags R2OpenBUGS R interface to TensorFlow/Keras","title":"Packages"},{"location":"Packages/#packages","text":"R workshop tutorial rstan rjags R2OpenBUGS R interface to TensorFlow/Keras","title":"Packages"},{"location":"Powertools/","text":"Powertools By default, powertools is loaded when users log into a Centos 7 dev node. Users may also run the module list command to check if it is loaded. If it is not, please use the command: 1 'bash$ module load powertools' to load the module before running powertools commands. qs Display job list. node_status Display a list of compute nodes and their properties. bi Information of Buy-In Account js Display job steps and their resource usages. getexample Download user examples.","title":"Overview"},{"location":"Powertools/#powertools","text":"By default, powertools is loaded when users log into a Centos 7 dev node. Users may also run the module list command to check if it is loaded. If it is not, please use the command: 1 'bash$ module load powertools' to load the module before running powertools commands.","title":"Powertools"},{"location":"Powertools/#qs","text":"Display job list.","title":"qs"},{"location":"Powertools/#node_status","text":"Display a list of compute nodes and their properties.","title":"node_status"},{"location":"Powertools/#bi","text":"Information of Buy-In Account","title":"bi"},{"location":"Powertools/#js","text":"Display job steps and their resource usages.","title":"js"},{"location":"Powertools/#getexample","text":"Download user examples.","title":"getexample"},{"location":"Powertools_longjob_by_DMTCP/","text":"Powertools longjob by DMTCP The following are instructions for trying out longjob powertool on HPCC system. First, you start with a basic submission script. For example, consider the following simple submission script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/bash -login #SBATCH --nodes=1 --ntasks=1 --cpus-per-task=1 #SBATCH --time=168:00:00 #SBATCH --mem=2gb #SBATCH --constraint=intel16 #SBATCH --job-name=MyJob0 #SBATCH --mail-type=FAIL,END module purge module load GCC/7.3.0-2.30 OpenMPI/3.1.1 srcdir = ${ SLURM_SUBMIT_DIR } /bin/ WORK = /mnt/scratch/ ${ USER } /KineticSN/ ${ SLURM_JOBID } mkdir -p ${ WORK } # Copy files to work directory cp -r $srcdir /* $WORK / #Move to the working directory cd $WORK #Run my program ./SimulationTest -scattering_flag 0 -weak_reaction_flag 0 -outputVisData 100 ret = $? scontrol show job ${ SLURM_JOBID } exit $ret To get longjob to work, the following modificaitons might need to be made: Change walltime to be less than 4 hours if you would like to have more available nodes to your job. Wrap all setup-code that only needs to be run once in an if statement that checks for the file \"Files_Copied\". This will ensure that the setup-code only runs the first time the script is run because in the first time there should be no file with the name \"Files_Copied\". Add the longjob command before the command in the submission script that you want to checkpoint. Load the powertools module and turn on aliases. i.e. add the following lines of code to the script: 1 2 shopt -s expand_aliases module load powertools Set the following enviornment variables as appropriate for your job: JobScript \u2013 Name of the job script file which will get resubmitted. The default is the first submitted job script name. DMTCP_Checkpoint_Time \u2013 Time (in seconds) which DMTCP needs to work on checkpointing. The default is 5 minutes. DMTCP_CHECKPOINT_INTERVAL \u2013 Time (in seconds) between automatic checkpoints. The default is 4-8 hours. For walltime less than 4 hours, the default will do checkpointing once at ${ DMTCP_Checkpoint_Time } + 1 minute before the end of walltime. DMTCP_CHECKPOINT_DIR \u2013 Name of the directory to save checkpoint image and log flies. The default is ckpt_${SLURM_JOB_NAME}. For job array, the default is ckpt_${SLURM_JOB_NAME}_${SLURM_ARRAY_TASK_ID}. If two different jobs use the the same directory to run with the longjob command, please make sure the environment variables (or SLURM_JOB_NAME ) are set different so their image files are not saved in the same directory. The following is a modified example script with the changes: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #!/bin/bash -login #SBATCH --nodes=1 --ntasks=1 --cpus-per-task=1 #SBATCH --time=04:00:00 #SBATCH --mem=2gb #SBATCH --constraint=intel16 #SBATCH --job-name=MyJob0 #SBATCH --mail-type=FAIL,END module purge module load GCC/7.3.0-2.30 OpenMPI/3.1.1 module load powertools # Change checkpointing environment variables if necessary: # export DMTCP_Checkpoint_Time=60 -- change checkpointing time # export DMTCP_CHECKPOINT_INTERVAL=7200 -- change time interval between checkpoints # export DMTCP_CHECKPOINT_DIR=ckpt_${SLURM_JOB_NAME} -- change where to save checkpointing files # Change to a directory other than ${SLURM_SUBMIT_DIR} if necessary: # cd /mnt/scratch/${USER}/WorkPlace if [ ! -f Files_Copied ] then srcdir = ${ SLURM_SUBMIT_DIR } /bin/ WORK = /mnt/scratch/ ${ USER } /KineticSN/ ${ SLURM_JOBID } mkdir -p ${ WORK } # Copy files to work directory cp -r $srcdir /* $WORK / #Run main simulation program cd $WORK touch Files_Copied fi longjob ./SimulationTest -scattering_flag 0 -weak_reaction_flag 0 -outputVisData 100 ret = $? exit $ret If everything works as expected, you should be able to submit the above job script and it will resubmit itself until the job completes. Note, this is rough code, not completely tested and does work in all cases. For example, one case that could propose a problem is if the main program gets caught in a loop and never exits. In this case, the code will keep submitting itself indefinitely. Note that in addition you can't include output redirection as you'd expect, that is a command like \"myprogram.py > myoutput.txt\" and \"longjob myprogram.py > myoutput.py\" is not the same (the redirection here applies to longjob, not your program). If you have difficulty, please contact us .","title":"Powertools longjob by DMTCP"},{"location":"Powertools_longjob_by_DMTCP/#powertools-longjob-by-dmtcp","text":"The following are instructions for trying out longjob powertool on HPCC system. First, you start with a basic submission script. For example, consider the following simple submission script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/bash -login #SBATCH --nodes=1 --ntasks=1 --cpus-per-task=1 #SBATCH --time=168:00:00 #SBATCH --mem=2gb #SBATCH --constraint=intel16 #SBATCH --job-name=MyJob0 #SBATCH --mail-type=FAIL,END module purge module load GCC/7.3.0-2.30 OpenMPI/3.1.1 srcdir = ${ SLURM_SUBMIT_DIR } /bin/ WORK = /mnt/scratch/ ${ USER } /KineticSN/ ${ SLURM_JOBID } mkdir -p ${ WORK } # Copy files to work directory cp -r $srcdir /* $WORK / #Move to the working directory cd $WORK #Run my program ./SimulationTest -scattering_flag 0 -weak_reaction_flag 0 -outputVisData 100 ret = $? scontrol show job ${ SLURM_JOBID } exit $ret To get longjob to work, the following modificaitons might need to be made: Change walltime to be less than 4 hours if you would like to have more available nodes to your job. Wrap all setup-code that only needs to be run once in an if statement that checks for the file \"Files_Copied\". This will ensure that the setup-code only runs the first time the script is run because in the first time there should be no file with the name \"Files_Copied\". Add the longjob command before the command in the submission script that you want to checkpoint. Load the powertools module and turn on aliases. i.e. add the following lines of code to the script: 1 2 shopt -s expand_aliases module load powertools Set the following enviornment variables as appropriate for your job: JobScript \u2013 Name of the job script file which will get resubmitted. The default is the first submitted job script name. DMTCP_Checkpoint_Time \u2013 Time (in seconds) which DMTCP needs to work on checkpointing. The default is 5 minutes. DMTCP_CHECKPOINT_INTERVAL \u2013 Time (in seconds) between automatic checkpoints. The default is 4-8 hours. For walltime less than 4 hours, the default will do checkpointing once at ${ DMTCP_Checkpoint_Time } + 1 minute before the end of walltime. DMTCP_CHECKPOINT_DIR \u2013 Name of the directory to save checkpoint image and log flies. The default is ckpt_${SLURM_JOB_NAME}. For job array, the default is ckpt_${SLURM_JOB_NAME}_${SLURM_ARRAY_TASK_ID}. If two different jobs use the the same directory to run with the longjob command, please make sure the environment variables (or SLURM_JOB_NAME ) are set different so their image files are not saved in the same directory. The following is a modified example script with the changes: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #!/bin/bash -login #SBATCH --nodes=1 --ntasks=1 --cpus-per-task=1 #SBATCH --time=04:00:00 #SBATCH --mem=2gb #SBATCH --constraint=intel16 #SBATCH --job-name=MyJob0 #SBATCH --mail-type=FAIL,END module purge module load GCC/7.3.0-2.30 OpenMPI/3.1.1 module load powertools # Change checkpointing environment variables if necessary: # export DMTCP_Checkpoint_Time=60 -- change checkpointing time # export DMTCP_CHECKPOINT_INTERVAL=7200 -- change time interval between checkpoints # export DMTCP_CHECKPOINT_DIR=ckpt_${SLURM_JOB_NAME} -- change where to save checkpointing files # Change to a directory other than ${SLURM_SUBMIT_DIR} if necessary: # cd /mnt/scratch/${USER}/WorkPlace if [ ! -f Files_Copied ] then srcdir = ${ SLURM_SUBMIT_DIR } /bin/ WORK = /mnt/scratch/ ${ USER } /KineticSN/ ${ SLURM_JOBID } mkdir -p ${ WORK } # Copy files to work directory cp -r $srcdir /* $WORK / #Run main simulation program cd $WORK touch Files_Copied fi longjob ./SimulationTest -scattering_flag 0 -weak_reaction_flag 0 -outputVisData 100 ret = $? exit $ret If everything works as expected, you should be able to submit the above job script and it will resubmit itself until the job completes. Note, this is rough code, not completely tested and does work in all cases. For example, one case that could propose a problem is if the main program gets caught in a loop and never exits. In this case, the code will keep submitting itself indefinitely. Note that in addition you can't include output redirection as you'd expect, that is a command like \"myprogram.py > myoutput.txt\" and \"longjob myprogram.py > myoutput.py\" is not the same (the redirection here applies to longjob, not your program). If you have difficulty, please contact us .","title":"Powertools longjob by DMTCP"},{"location":"Python/","text":"Python packages It's difficult for the HPCC to host a vast volume of Python packages, and to resolve conflicts between Python versions. Users are encouraged to install and manage packages on their own, through conda or virtual environments. Using conda Users install their own version of Python through Anaconda in the home or research space. This gives them full control on their preferred versions of Python and packages. Isolated virtual environments can be created and controlled by conda environments . Using virtualenv By default, Python version 3.6.4 (compiled by GCC/6.4.0) is loaded and ready to use after logging into HPCC dev-nodes. Other versions can be found by running module spider Python . Usually, pre-installed packages are available for each Python version. If users need to install new ones, they can install them in the home or research space through virtual environments .","title":"Package management"},{"location":"Python/#python-packages","text":"It's difficult for the HPCC to host a vast volume of Python packages, and to resolve conflicts between Python versions. Users are encouraged to install and manage packages on their own, through conda or virtual environments.","title":"Python packages"},{"location":"Python/#using-conda","text":"Users install their own version of Python through Anaconda in the home or research space. This gives them full control on their preferred versions of Python and packages. Isolated virtual environments can be created and controlled by conda environments .","title":"Using conda"},{"location":"Python/#using-virtualenv","text":"By default, Python version 3.6.4 (compiled by GCC/6.4.0) is loaded and ready to use after logging into HPCC dev-nodes. Other versions can be found by running module spider Python . Usually, pre-installed packages are available for each Python version. If users need to install new ones, they can install them in the home or research space through virtual environments .","title":"Using virtualenv"},{"location":"Python_on_HPC/","text":"Python on HPC Python is popular because it makes a great first impression; i) clean, clear syntax, ii) multi-paradigm, iii) interpreted, iv) duck typing, garbage collection, and most of all, v) instant productivity. It keeps up with users' needs. it has i) flexible, full-featured data structures ii) extensive standard libraries iii) reusable open-source package iv) package management tools v) good unit testing frameworks. In exchange of these user-friendliness and ease of use, Python becomes one of the slowest computer languages, primarily because it is an interpreted language, and allows a single thread to run in the interpreter's memory space at once. It is known that Python is typically 30 to 300 times slower than C or Fortran. However, Python has the most powerful and enthusiastic open-source community which continuously improves the capability of Python. In this tutorial, we want to explain what MSU HPC is doing to support Python users. provide guidance to help users improve Python performance at the HPC. point out tools that support developers of Python in HPC. We assume that you know and use Python, or you know and use HPC and are curious about using Python in your own HPC work. How to use Python on HPC? Python with virtual environments Python with Conda Jupyter Notebook Can my Python code be faster? Vectorization Numba Use Threaded Libraries MPI Profiling Other Python Resources How to use Python on HPC? Python applications usually use packages and modules that require specific version of libraries. This means one installed application may conflict with another application due to using the same library but with different versions. It is difficult to meet the requirements of every application by one global Python installation. To resolve this issue, users can create an isolated virtual environment with a particular version of Python on our system in a self-contained directory of their home or research space. users can Install their own version of Python through Anaconda in home or research space. This gives users full control on their preferred versions of python and packages. Python with virtual environments More details of how to use virtual environment can be found at this page Python with Conda To use Python on HPC, you have to load a Python module. A few helpful module commands would be module avail Python , module spider Python , and module load Python . For more information for our module system can be found here Jupyter Notebook For Jupyter notebook users, we have the Open Ondemand platform To connect to HPCC OnDemand, visit https://ondemand.hpcc.msu.edu . After logging in, choose interactive apps, select Jupyter Notebook, request resources as you need. Your Jupyter Notebook will start when the requested resources are ready. Can my Python code be faster? Now, you are ready to use Python on HPC. Now, let's learn a few tips to make your Python codes faster. Vectorization Vectorization speeds up the Python code without using loop. Instead of loop, NumPy can help in minimizing the running time of code efficiently. NumPy offers various operations are being performed over vector such as dot product, cross product, matrix multiplication. Numba Numba compiles Python codes just in time with a few decorators, without much modification of codes. Performance tests can be found here and here. In addition, Numba offers automatic parallelization which is very easy to use. You just need to add on line decorator, @njit(parallel=True). For more information can be found here. Numba also supports NVIDIA CUDA. It is easy to use (at least much easier to use than other programming languages). A well-written Jupyter Notebook for performance tests among CPU vs GPU with NumPy, Numba and CUDA can be found here. Use Threaded Libraries Packages like NumPy, SciPy are already built with MPI and tread support via BLAS/LAPACK, and MKL. In general, it is a plausible guess that most solvers have been already implemented in pure Python. In addition, many of famous threaded libraries and packages already have binds such as PyTrilinos, Petsc4py, Elemental, and SLEPc. So, don't try to reinvent wheels. If it is not new, probably it is already implemented in a very nice way. MPI Python has a package for MPI, mpi4py. It is a pythonic wrapping of the system's native MPI. providing almost all MPI-1, 2 and common MPI-3 features. very well maintained. distributed with major Python distributions. portalbe and scalable. requires only NumPy, Cython (build only), and MPI library. More information can be found here . Profiling Will be added soon. Other Python Resources The following are a few Python resource links. https://www.python.org/about/gettingstarted/ https://wiki.python.org/moin/BeginnersGuide/ https://www.codecademy.com/learn/python/ https://www.coursera.org/specializations/python/ https://software-carpentry.org/lessons/ https://pymotw.com/ HPCC wiki Python page Python video on youtube https://www.py4e.com/","title":"Python on HPCC"},{"location":"Python_on_HPC/#python-on-hpc","text":"Python is popular because it makes a great first impression; i) clean, clear syntax, ii) multi-paradigm, iii) interpreted, iv) duck typing, garbage collection, and most of all, v) instant productivity. It keeps up with users' needs. it has i) flexible, full-featured data structures ii) extensive standard libraries iii) reusable open-source package iv) package management tools v) good unit testing frameworks. In exchange of these user-friendliness and ease of use, Python becomes one of the slowest computer languages, primarily because it is an interpreted language, and allows a single thread to run in the interpreter's memory space at once. It is known that Python is typically 30 to 300 times slower than C or Fortran. However, Python has the most powerful and enthusiastic open-source community which continuously improves the capability of Python. In this tutorial, we want to explain what MSU HPC is doing to support Python users. provide guidance to help users improve Python performance at the HPC. point out tools that support developers of Python in HPC. We assume that you know and use Python, or you know and use HPC and are curious about using Python in your own HPC work. How to use Python on HPC? Python with virtual environments Python with Conda Jupyter Notebook Can my Python code be faster? Vectorization Numba Use Threaded Libraries MPI Profiling Other Python Resources","title":"Python on HPC"},{"location":"Python_on_HPC/#how-to-use-python-on-hpc","text":"Python applications usually use packages and modules that require specific version of libraries. This means one installed application may conflict with another application due to using the same library but with different versions. It is difficult to meet the requirements of every application by one global Python installation. To resolve this issue, users can create an isolated virtual environment with a particular version of Python on our system in a self-contained directory of their home or research space. users can Install their own version of Python through Anaconda in home or research space. This gives users full control on their preferred versions of python and packages.","title":"How to use Python on HPC?"},{"location":"Python_on_HPC/#python-with-virtual-environments","text":"More details of how to use virtual environment can be found at this page","title":"Python with virtual environments"},{"location":"Python_on_HPC/#python-with-conda","text":"To use Python on HPC, you have to load a Python module. A few helpful module commands would be module avail Python , module spider Python , and module load Python . For more information for our module system can be found here","title":"Python with Conda"},{"location":"Python_on_HPC/#jupyter-notebook","text":"For Jupyter notebook users, we have the Open Ondemand platform To connect to HPCC OnDemand, visit https://ondemand.hpcc.msu.edu . After logging in, choose interactive apps, select Jupyter Notebook, request resources as you need. Your Jupyter Notebook will start when the requested resources are ready.","title":"Jupyter Notebook"},{"location":"Python_on_HPC/#can-my-python-code-be-faster","text":"Now, you are ready to use Python on HPC. Now, let's learn a few tips to make your Python codes faster.","title":"Can my Python code be faster?"},{"location":"Python_on_HPC/#vectorization","text":"Vectorization speeds up the Python code without using loop. Instead of loop, NumPy can help in minimizing the running time of code efficiently. NumPy offers various operations are being performed over vector such as dot product, cross product, matrix multiplication.","title":"Vectorization"},{"location":"Python_on_HPC/#numba","text":"Numba compiles Python codes just in time with a few decorators, without much modification of codes. Performance tests can be found here and here. In addition, Numba offers automatic parallelization which is very easy to use. You just need to add on line decorator, @njit(parallel=True). For more information can be found here. Numba also supports NVIDIA CUDA. It is easy to use (at least much easier to use than other programming languages). A well-written Jupyter Notebook for performance tests among CPU vs GPU with NumPy, Numba and CUDA can be found here.","title":"Numba"},{"location":"Python_on_HPC/#use-threaded-libraries","text":"Packages like NumPy, SciPy are already built with MPI and tread support via BLAS/LAPACK, and MKL. In general, it is a plausible guess that most solvers have been already implemented in pure Python. In addition, many of famous threaded libraries and packages already have binds such as PyTrilinos, Petsc4py, Elemental, and SLEPc. So, don't try to reinvent wheels. If it is not new, probably it is already implemented in a very nice way.","title":"Use Threaded Libraries"},{"location":"Python_on_HPC/#mpi","text":"Python has a package for MPI, mpi4py. It is a pythonic wrapping of the system's native MPI. providing almost all MPI-1, 2 and common MPI-3 features. very well maintained. distributed with major Python distributions. portalbe and scalable. requires only NumPy, Cython (build only), and MPI library. More information can be found here .","title":"MPI"},{"location":"Python_on_HPC/#profiling","text":"Will be added soon.","title":"Profiling"},{"location":"Python_on_HPC/#other-python-resources","text":"The following are a few Python resource links. https://www.python.org/about/gettingstarted/ https://wiki.python.org/moin/BeginnersGuide/ https://www.codecademy.com/learn/python/ https://www.coursera.org/specializations/python/ https://software-carpentry.org/lessons/ https://pymotw.com/ HPCC wiki Python page Python video on youtube https://www.py4e.com/","title":"Other Python Resources"},{"location":"QIIME_2/","text":"QIIME 2 QIIME (Quantitative Insights Into Microbial Ecology) is an open-source bioinformatics pipeline for performing microbiome analysis from raw DNA sequencing data. Since QIIME 1 is no longer supported officially (see announcement at http://qiime.org , it's not installed on the CentOS 7 system of HPCC. The way QIIME 2 is installed and run on the HPCC CentOS 7 is through conda https://docs.qiime2.org/2018.2/install/native/ . You may follow https://docs.anaconda.com/anaconda/install/linux to install anaconda in your home directory. Below is how to install QIIME 2 (version 2018.2) in your home directory via conda: Install QIIME 2 1 2 3 4 5 6 7 8 9 export PATH = $PATH : $HOME /anaconda3/bin wget https://data.qiime2.org/distro/core/qiime2-2018.2-py35-linux-conda.yml conda env create -n qiime2-2018.2 --file qiime2-2018.2-py35-linux-conda.yml rm qiime2-2018.2-py35-linux-conda.yml source activate qiime2-2018.2 qiime --help # test if installation is successful # all your QIIME commands go here source deactivate The example below is from a previous version of the current tutorial Example of analysis 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 mkdir qiime2-moving-pictures-tutorial cd qiime2-moving-pictures-tutorial wget -O \"sample-metadata.tsv\" \"https://data.qiime2.org/2018.2/tutorials/moving-pictures/sample_metadata.tsv\" mkdir emp-single-end-sequences wget -O \"emp-single-end-sequences/barcodes.fastq.gz\" \"https://data.qiime2.org/2018.2/tutorials/moving-pictures/emp-single-end-sequences/barcodes.fastq.gz\" wget -O \"emp-single-end-sequences/sequences.fastq.gz\" \"https://data.qiime2.org/2018.2/tutorials/moving-pictures/emp-single-end-sequences/sequences.fastq.gz\" qiime tools import \\ --type EMPSingleEndSequences \\ --input-path emp-single-end-sequences \\ --output-path emp-single-end-sequences.qza qiime demux emp-single \\ --i-seqs emp-single-end-sequences.qza \\ --m-barcodes-file sample-metadata.tsv \\ --m-barcodes-column BarcodeSequence \\ --o-per-sample-sequences demux.qza qiime demux summarize \\ --i-data demux.qza \\ --o-visualization demux.qzv qiime tools view demux.qzv","title":"QIIME 2"},{"location":"QIIME_2/#qiime-2","text":"QIIME (Quantitative Insights Into Microbial Ecology) is an open-source bioinformatics pipeline for performing microbiome analysis from raw DNA sequencing data. Since QIIME 1 is no longer supported officially (see announcement at http://qiime.org , it's not installed on the CentOS 7 system of HPCC. The way QIIME 2 is installed and run on the HPCC CentOS 7 is through conda https://docs.qiime2.org/2018.2/install/native/ . You may follow https://docs.anaconda.com/anaconda/install/linux to install anaconda in your home directory. Below is how to install QIIME 2 (version 2018.2) in your home directory via conda: Install QIIME 2 1 2 3 4 5 6 7 8 9 export PATH = $PATH : $HOME /anaconda3/bin wget https://data.qiime2.org/distro/core/qiime2-2018.2-py35-linux-conda.yml conda env create -n qiime2-2018.2 --file qiime2-2018.2-py35-linux-conda.yml rm qiime2-2018.2-py35-linux-conda.yml source activate qiime2-2018.2 qiime --help # test if installation is successful # all your QIIME commands go here source deactivate The example below is from a previous version of the current tutorial Example of analysis 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 mkdir qiime2-moving-pictures-tutorial cd qiime2-moving-pictures-tutorial wget -O \"sample-metadata.tsv\" \"https://data.qiime2.org/2018.2/tutorials/moving-pictures/sample_metadata.tsv\" mkdir emp-single-end-sequences wget -O \"emp-single-end-sequences/barcodes.fastq.gz\" \"https://data.qiime2.org/2018.2/tutorials/moving-pictures/emp-single-end-sequences/barcodes.fastq.gz\" wget -O \"emp-single-end-sequences/sequences.fastq.gz\" \"https://data.qiime2.org/2018.2/tutorials/moving-pictures/emp-single-end-sequences/sequences.fastq.gz\" qiime tools import \\ --type EMPSingleEndSequences \\ --input-path emp-single-end-sequences \\ --output-path emp-single-end-sequences.qza qiime demux emp-single \\ --i-seqs emp-single-end-sequences.qza \\ --m-barcodes-file sample-metadata.tsv \\ --m-barcodes-column BarcodeSequence \\ --o-per-sample-sequences demux.qza qiime demux summarize \\ --i-data demux.qza \\ --o-visualization demux.qzv qiime tools view demux.qzv","title":"QIIME 2"},{"location":"R2OpenBUGS/","text":"R2OpenBUGS R package R2OpenBUGS is available on the HPCC. OpenBUGS is a software package that performs Bayesian inference Using Gibbs Sampling. The latest version of OpenBUGS on the HPCC is 3.2.3. R2OpenBUGS is an R package that allows users to run OpenBUGS from R. To load R and OpenBUGS, run: module purge module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 module load OpenBUGS Then, in your R session, use \"library(R2OpenBUGS)\" to load this package. You can execute the following testing R code to see if it works for you. **Testing R2OpenBUGS** 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 library ( R2OpenBUGS ) # Define the model BUGSmodel <- function () { for ( j in 1 :N ) { Y [ j ] ~ dnorm ( mu [ j ] , tau ) mu [ j ] <- alpha + beta * ( x [ j ] - xbar ) } # Priors alpha ~ dnorm ( 0 , 0 . 001 ) beta ~ dnorm ( 0 , 0 . 001 ) tau ~ dgamma ( 0 . 001 , 0 . 001 ) sigma <- 1 / sqrt ( tau ) } # Data Data <- list ( Y = c ( 1 , 3 , 3 , 3 , 5 ), x = c ( 1 , 2 , 3 , 4 , 5 ), N = 5 , xbar = 3 ) # Initial values for the MCMC Inits <- function () { list ( alpha = 1 , beta = 1 , tau = 1 ) } # Run BUGS out <- bugs ( data = Data , inits = Inits , parameters . to . save = c ( \"alpha\" , \"beta\" , \"sigma\" ), model . file = BUGSmodel , n . chains = 1 , n . iter = 10000 ) # Examine the BUGS output out # Inference for Bugs model at \"/tmp/RtmphywZxh/model77555ea534aa.txt\", # Current: 1 chains, each with 10000 iterations (first 5000 discarded) # Cumulative: n.sims = 5000 iterations saved # mean sd 2.5% 25% 50% 75% 97.5% # alpha 3.0 0.6 1.9 2.7 3.0 3.3 4.1 # beta 0.8 0.4 0.1 0.6 0.8 1.0 1.6 # sigma 1.0 0.7 0.4 0.6 0.8 1.2 2.8 # deviance 13.0 3.7 8.8 10.2 12.0 14.6 22.9 # # DIC info (using the rule, pD = Dbar-Dhat) # pD = 3.9 and DIC = 16.9 # DIC is an estimate of expected predictive error (lower deviance is better).","title":"R2OpenBUGS"},{"location":"R2OpenBUGS/#r2openbugs","text":"R package R2OpenBUGS is available on the HPCC. OpenBUGS is a software package that performs Bayesian inference Using Gibbs Sampling. The latest version of OpenBUGS on the HPCC is 3.2.3. R2OpenBUGS is an R package that allows users to run OpenBUGS from R. To load R and OpenBUGS, run: module purge module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 module load OpenBUGS Then, in your R session, use \"library(R2OpenBUGS)\" to load this package. You can execute the following testing R code to see if it works for you. **Testing R2OpenBUGS** 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 library ( R2OpenBUGS ) # Define the model BUGSmodel <- function () { for ( j in 1 :N ) { Y [ j ] ~ dnorm ( mu [ j ] , tau ) mu [ j ] <- alpha + beta * ( x [ j ] - xbar ) } # Priors alpha ~ dnorm ( 0 , 0 . 001 ) beta ~ dnorm ( 0 , 0 . 001 ) tau ~ dgamma ( 0 . 001 , 0 . 001 ) sigma <- 1 / sqrt ( tau ) } # Data Data <- list ( Y = c ( 1 , 3 , 3 , 3 , 5 ), x = c ( 1 , 2 , 3 , 4 , 5 ), N = 5 , xbar = 3 ) # Initial values for the MCMC Inits <- function () { list ( alpha = 1 , beta = 1 , tau = 1 ) } # Run BUGS out <- bugs ( data = Data , inits = Inits , parameters . to . save = c ( \"alpha\" , \"beta\" , \"sigma\" ), model . file = BUGSmodel , n . chains = 1 , n . iter = 10000 ) # Examine the BUGS output out # Inference for Bugs model at \"/tmp/RtmphywZxh/model77555ea534aa.txt\", # Current: 1 chains, each with 10000 iterations (first 5000 discarded) # Cumulative: n.sims = 5000 iterations saved # mean sd 2.5% 25% 50% 75% 97.5% # alpha 3.0 0.6 1.9 2.7 3.0 3.3 4.1 # beta 0.8 0.4 0.1 0.6 0.8 1.0 1.6 # sigma 1.0 0.7 0.4 0.6 0.8 1.2 2.8 # deviance 13.0 3.7 8.8 10.2 12.0 14.6 22.9 # # DIC info (using the rule, pD = Dbar-Dhat) # pD = 3.9 and DIC = 16.9 # DIC is an estimate of expected predictive error (lower deviance is better).","title":"R2OpenBUGS"},{"location":"RCentOS7generalinfo/","text":"General information A quick start R 4.0.2 has the largest number of libraries installed and is the recommended one to use. To load it, you can run: module purge module load GCC/8.3.0 OpenMPI/3.1.4 module load R/4.0.2 Some users may have a local R package directory specified in ~/.Renviron ; this may create a problem if you load your local packages which were built with an older version of R. Unless you have updated them all, we recommend that you launch R by R --no-environ which suppresses the search of local packages. Be sure to specify version number While it is valid to load R simply by module load R , it may point to a different R version than your desired one. Please be specific about the version, as with any software module. Rstudio The best way to launch an Rstudio session is to log into our OnDemand server, which is dedicated to running GUI applications. Here is a short video showing how to request an Rstudio \"job\" from the HPCC cluster after you've logged into https://ondemand.hpcc.msu.edu/.","title":"General information"},{"location":"RCentOS7generalinfo/#general-information","text":"","title":"General information"},{"location":"RCentOS7generalinfo/#a-quick-start","text":"R 4.0.2 has the largest number of libraries installed and is the recommended one to use. To load it, you can run: module purge module load GCC/8.3.0 OpenMPI/3.1.4 module load R/4.0.2 Some users may have a local R package directory specified in ~/.Renviron ; this may create a problem if you load your local packages which were built with an older version of R. Unless you have updated them all, we recommend that you launch R by R --no-environ which suppresses the search of local packages.","title":"A quick start"},{"location":"RCentOS7generalinfo/#be-sure-to-specify-version-number","text":"While it is valid to load R simply by module load R , it may point to a different R version than your desired one. Please be specific about the version, as with any software module.","title":"Be sure to specify version number"},{"location":"RCentOS7generalinfo/#rstudio","text":"The best way to launch an Rstudio session is to log into our OnDemand server, which is dedicated to running GUI applications. Here is a short video showing how to request an Rstudio \"job\" from the HPCC cluster after you've logged into https://ondemand.hpcc.msu.edu/.","title":"Rstudio"},{"location":"RNA_seq_example/","text":"RNA-seq example While RNA-seq analysis pipeline is changing, here is an example for demonstration purpose. The pipeline involves tophat , cufflinks and so on. We will provide updates to this page as needed.","title":"RNA-seq"},{"location":"RNA_seq_example/#rna-seq-example","text":"While RNA-seq analysis pipeline is changing, here is an example for demonstration purpose. The pipeline involves tophat , cufflinks and so on. We will provide updates to this page as needed.","title":"RNA-seq example"},{"location":"R_interface_to_TensorFlow_Keras/","text":"R interface to TensorFlow/Keras - [Prerequisites](../ITH/R_interface_to_TensorFlow_Keras) - [Installing R libraries](../ITH/R_interface_to_TensorFlow_Keras) - [A deep learning example](../ITH/R_interface_to_TensorFlow_Keras) Prerequisites In order to run TF/keras from within R, we need to 0. Login to a GPU node: ssh dev-intel16-k80 1. Install Tensorflow as the backend of running keras \u2013 local install by user 2. Install R library {tensorflow} and {keras} \u2013 local install by user We show here how to install R TF libraries in one's home directory, after loading the system-wide R (version 3.5.1 at the time of writing). We don't show here how to install TF ; it's included in another wiki at https://wiki.hpcc.msu.edu/x/04ZaAQ . All the paths presented below should be replaced by your own paths. Installing R libraries As mentioned above, we will install the two TF libraries in your home directory. Let's say you have an existing directory named ~/Rlibs (create one if you don't), the installation would go as follows: **Install R libraries in your HOME dir** 1 2 3 4 5 6 7 8 9 # Load R module purge module load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604 # Install R libraries tensorflow and keras in an existing directory under your home: ~/Rlibs R > library ( devtools ) > with_libpaths ( new = \"~/Rlibs\" , install_github ( 'rstudio/tensorflow' )) > with_libpaths ( new = \"~/Rlibs\" , install_github ( 'rstudio/keras' )) A deep learning example Now let's try out a deep learning analysis (classifying MNIST handwritten digits using Multi-Layer Perceptrons) with code available from here . For convenience, the code is pasted below; you can run it either by opening an R console or saving the code to an R script so that you can run the script on the command line. Here are some important things to note: (1) Before executing the R code, you need to set up a couple of conda related environment variables. That is, 1 2 3 export PATH = /mnt/home/longnany/anaconda3-march2019/bin: $PATH export LD_LIBRARY_PATH = /mnt/home/longnany/anaconda3-march2019/envs/tf_gpu/lib: $LD_LIBRARY_PATH export TF_CPP_MIN_LOG_LEVEL = 2 # to filter out warning messages (2) As you may have noted in the R code below, running TF within R requires some configurations (line 3 and line 4) so that R knows where to find your TF conda environment. (3) Tip: while the code is running, you can start a new HPCC login session and, after running ssh dev-intel16-k80 , type the command gpustat -cpuP . You should be able to see your GPU processes (R processes in this case). **R code for deep learning** 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 . libPaths ( \"~/Rlibs\" ) # add local lib to R search path; o.w. tensorflow/keras can't be loaded library ( keras ) use_python ( \"/mnt/home/longnany/anaconda3-march2019/envs/tf_gpu/bin/python\" ) use_condaenv ( \"/mnt/home/longnany/anaconda3-march2019/envs/tf_gpu\" ) #loading the keras inbuilt mnist dataset data <- dataset_mnist () #separating train and test file train_x <- data $train$x train_y <- data $train$y test_x <- data $test$x test_y <- data $test$y rm ( data ) # converting a 2D array into a 1D array for feeding into the MLP and normalising the matrix train_x <- array ( train_x , dim = c ( dim ( train_x ) [ 1 ] , prod ( dim ( train_x ) [- 1 ] ))) / 255 test_x <- array ( test_x , dim = c ( dim ( test_x ) [ 1 ] , prod ( dim ( test_x ) [- 1 ] ))) / 255 #converting the target variable to once hot encoded vectors using keras inbuilt function train_y <- to_categorical ( train_y , 10 ) test_y <- to_categorical ( test_y , 10 ) #defining a keras sequential model model <- keras_model_sequential () #defining the model with 1 input layer[784 neurons], 1 hidden layer[784 neurons] with dropout rate 0.4 and 1 output layer[10 neurons] #i.e number of digits from 0 to 9 model %>% layer_dense(units = 784, input_shape = 784) %>% layer_dropout(rate=0.4)%>% layer_activation(activation = 'relu' ) %>% layer_dense(units = 10) %>% layer_activation(activation = 'softmax' ) #compiling the defined model with metric = accuracy and optimiser as adam. model %>% compile( loss = 'categorical_crossentropy', optimizer = 'adam', metrics = c('accuracy') ) #fitting the model on the training dataset model %>% fit(train_x, train_y , epochs = 100 , batch_size = 128 ) #Evaluating model on the cross validation dataset loss_and_metrics <- model %> % evaluate(test_x, test_y , batch_size = 128 )","title":"R interface to TensorFlow/Keras"},{"location":"R_interface_to_TensorFlow_Keras/#r-interface-to-tensorflowkeras","text":"- [Prerequisites](../ITH/R_interface_to_TensorFlow_Keras) - [Installing R libraries](../ITH/R_interface_to_TensorFlow_Keras) - [A deep learning example](../ITH/R_interface_to_TensorFlow_Keras)","title":"R interface to TensorFlow/Keras"},{"location":"R_interface_to_TensorFlow_Keras/#prerequisites","text":"In order to run TF/keras from within R, we need to 0. Login to a GPU node: ssh dev-intel16-k80 1. Install Tensorflow as the backend of running keras \u2013 local install by user 2. Install R library {tensorflow} and {keras} \u2013 local install by user We show here how to install R TF libraries in one's home directory, after loading the system-wide R (version 3.5.1 at the time of writing). We don't show here how to install TF ; it's included in another wiki at https://wiki.hpcc.msu.edu/x/04ZaAQ . All the paths presented below should be replaced by your own paths.","title":"Prerequisites"},{"location":"R_interface_to_TensorFlow_Keras/#installing-r-libraries","text":"As mentioned above, we will install the two TF libraries in your home directory. Let's say you have an existing directory named ~/Rlibs (create one if you don't), the installation would go as follows: **Install R libraries in your HOME dir** 1 2 3 4 5 6 7 8 9 # Load R module purge module load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604 # Install R libraries tensorflow and keras in an existing directory under your home: ~/Rlibs R > library ( devtools ) > with_libpaths ( new = \"~/Rlibs\" , install_github ( 'rstudio/tensorflow' )) > with_libpaths ( new = \"~/Rlibs\" , install_github ( 'rstudio/keras' ))","title":"Installing R libraries"},{"location":"R_interface_to_TensorFlow_Keras/#a-deep-learning-example","text":"Now let's try out a deep learning analysis (classifying MNIST handwritten digits using Multi-Layer Perceptrons) with code available from here . For convenience, the code is pasted below; you can run it either by opening an R console or saving the code to an R script so that you can run the script on the command line. Here are some important things to note: (1) Before executing the R code, you need to set up a couple of conda related environment variables. That is, 1 2 3 export PATH = /mnt/home/longnany/anaconda3-march2019/bin: $PATH export LD_LIBRARY_PATH = /mnt/home/longnany/anaconda3-march2019/envs/tf_gpu/lib: $LD_LIBRARY_PATH export TF_CPP_MIN_LOG_LEVEL = 2 # to filter out warning messages (2) As you may have noted in the R code below, running TF within R requires some configurations (line 3 and line 4) so that R knows where to find your TF conda environment. (3) Tip: while the code is running, you can start a new HPCC login session and, after running ssh dev-intel16-k80 , type the command gpustat -cpuP . You should be able to see your GPU processes (R processes in this case). **R code for deep learning** 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 . libPaths ( \"~/Rlibs\" ) # add local lib to R search path; o.w. tensorflow/keras can't be loaded library ( keras ) use_python ( \"/mnt/home/longnany/anaconda3-march2019/envs/tf_gpu/bin/python\" ) use_condaenv ( \"/mnt/home/longnany/anaconda3-march2019/envs/tf_gpu\" ) #loading the keras inbuilt mnist dataset data <- dataset_mnist () #separating train and test file train_x <- data $train$x train_y <- data $train$y test_x <- data $test$x test_y <- data $test$y rm ( data ) # converting a 2D array into a 1D array for feeding into the MLP and normalising the matrix train_x <- array ( train_x , dim = c ( dim ( train_x ) [ 1 ] , prod ( dim ( train_x ) [- 1 ] ))) / 255 test_x <- array ( test_x , dim = c ( dim ( test_x ) [ 1 ] , prod ( dim ( test_x ) [- 1 ] ))) / 255 #converting the target variable to once hot encoded vectors using keras inbuilt function train_y <- to_categorical ( train_y , 10 ) test_y <- to_categorical ( test_y , 10 ) #defining a keras sequential model model <- keras_model_sequential () #defining the model with 1 input layer[784 neurons], 1 hidden layer[784 neurons] with dropout rate 0.4 and 1 output layer[10 neurons] #i.e number of digits from 0 to 9 model %>% layer_dense(units = 784, input_shape = 784) %>% layer_dropout(rate=0.4)%>% layer_activation(activation = 'relu' ) %>% layer_dense(units = 10) %>% layer_activation(activation = 'softmax' ) #compiling the defined model with metric = accuracy and optimiser as adam. model %>% compile( loss = 'categorical_crossentropy', optimizer = 'adam', metrics = c('accuracy') ) #fitting the model on the training dataset model %>% fit(train_x, train_y , epochs = 100 , batch_size = 128 ) #Evaluating model on the cross validation dataset loss_and_metrics <- model %> % evaluate(test_x, test_y , batch_size = 128 )","title":"A deep learning example"},{"location":"R_others/","text":"Some packages and other information rstan The example here follows that in RStan Getting Started . To test rstan on the HPCC, first load R 3.6.2: 1 2 module purge module load GCC/8.3.0 OpenMPI/3.1.4 R/3.6.2-X11-20180604 As of February 2020, the rstan version is 2.19.2. The stan model file \"8schools.stan\" contains: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 data { int< lower = 0 > J ; // number of schools real y [ J ] ; // estimated treatment effects real< lower = 0 > sigma [ J ] ; // standard error of effect estimates } parameters { real mu ; // population treatment effect real< lower = 0 > tau ; // standard deviation in treatment effects vector [ J ] eta ; // unscaled deviation from mu by school } transformed parameters { vector [ J ] theta = mu + tau * eta ; // school treatment effects } model { target += normal_lpdf ( eta | 0 , 1 ) ; // prior log-density target += normal_lpdf ( y | theta, sigma ) ; // log-likelihood } The R code (\"run.R\") to run stan model contains: 1 2 3 4 5 6 7 8 9 10 11 12 library ( \"rstan\" ) options ( mc . cores = parallel :: detectCores ()) rstan_options ( auto_write = TRUE ) schools_dat <- list ( J = 8 , y = c ( 28 , 8 , - 3 , 7 , - 1 , 1 , 18 , 12 ), sigma = c ( 15 , 10 , 16 , 11 , 9 , 11 , 10 , 18 )) fit <- stan ( file = '8schools.stan' , data = schools_dat ) print ( fit ) pairs ( fit , pars = c ( \"mu\" , \"tau\" , \"lp__\" )) la <- extract ( fit , permuted = TRUE ) # return a list of arrays mu <- la $mu To run the model from the command line: Rscript run.R In addition to the results printed to the stdout, there is an R object file named 8schools.rds generated. This is due to that we've set auto_write to TRUE in run.R. More about the auto_write option: Logical, defaulting to the value of rstan_options(\"auto_write\"), indicating whether to write the object to the hard disk using saveRDS. Although this argument is FALSE by default, we recommend calling rstan_options(\"auto_write\" = TRUE) in order to avoid unnecessary recompilations. If file is supplied and its dirname is writable, then the object will be written to that same directory, substituting a .rds extension for the .stan extension. Otherwise, the object will be written to the tempdir. rjags To use {rjags}, first load R/3.5.1 and JAGS from a dev-node (dev-intel16 or dev-intel18) as follows: Loading R/3.5.1 and JAGS 1 2 3 module purge module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 module load JAGS/4.3.0 Next, we will run a short example of data analysis using rjags. This example comes from this tutorial which presents many Bayesian models using this package. To invoke R from the command line: R --vanilla Then, in the R console, you can run the following codes (for detailed explanation refer to the tutorial mentioned above): Sample R code using {rjags} commands 1 2 3 4 5 6 7 8 9 10 11 library ( rjags ) data <- read . csv ( \"data1.csv\" ) N <- length ( data $y ) dat <- list ( \"N\" = N , \"y\" = data $y , \"V\" = data $V ) inits <- list ( d = 0 . 0 ) jags . m <- jags . model ( file = \"aspirinFE.txt\" , data = dat , inits = inits , n . chains = 1 , n . adapt = 500 ) params <- c ( \"d\" , \"OR\" ) samps <- coda . samples ( jags . m , params , n . iter = 10000 ) summary ( window ( samps , start = 5001 )) plot ( samps ) where the two input files, data1.csv and aspirinFE.txt, need to be located in the working directory. The content of the two files is below. data1.csv 1 2 3 4 5 6 7 8 N,y,V 1,0.3289011,0.0388957 2,0.3845458,0.0411673 3,0.2195622,0.0204915 4,0.2222206,0.0647646 5,0.2254672,0.0351996 6,-0.1246363,0.0096167 7,0.1109658,0.0015062 aspirinFE.txt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 model { for ( i in 1:N ) { P[i] <- 1/V[i] y[i] ~ dnorm(d, P[i]) } ### Define the priors d ~ dnorm(0, 0.00001) ### Transform the ln(OR) to OR OR <- exp(d) } A screen shot of the entire run including the output figures is attached here. R2OpenBUGS R package R2OpenBUGS is available on the HPCC. OpenBUGS is a software package that performs Bayesian inference Using Gibbs Sampling. The latest version of OpenBUGS on the HPCC is 3.2.3. R2OpenBUGS is an R package that allows users to run OpenBUGS from R. To load R and OpenBUGS, run: 1 2 3 module purge module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 module load OpenBUGS Then, in your R session, use \"library(R2OpenBUGS)\" to load this package. You can execute the following testing R code to see if it works for you. Testing R2OpenBUGS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 library ( R2OpenBUGS ) # Define the model BUGSmodel <- function () { for ( j in 1 :N ) { Y [ j ] ~ dnorm ( mu [ j ] , tau ) mu [ j ] <- alpha + beta * ( x [ j ] - xbar ) } # Priors alpha ~ dnorm ( 0 , 0 . 001 ) beta ~ dnorm ( 0 , 0 . 001 ) tau ~ dgamma ( 0 . 001 , 0 . 001 ) sigma <- 1 / sqrt ( tau ) } # Data Data <- list ( Y = c ( 1 , 3 , 3 , 3 , 5 ), x = c ( 1 , 2 , 3 , 4 , 5 ), N = 5 , xbar = 3 ) # Initial values for the MCMC Inits <- function () { list ( alpha = 1 , beta = 1 , tau = 1 ) } # Run BUGS out <- bugs ( data = Data , inits = Inits , parameters . to . save = c ( \"alpha\" , \"beta\" , \"sigma\" ), model . file = BUGSmodel , n . chains = 1 , n . iter = 10000 ) # Examine the BUGS output out # Inference for Bugs model at \"/tmp/RtmphywZxh/model77555ea534aa.txt\", # Current: 1 chains, each with 10000 iterations (first 5000 discarded) # Cumulative: n.sims = 5000 iterations saved # mean sd 2.5% 25% 50% 75% 97.5% # alpha 3.0 0.6 1.9 2.7 3.0 3.3 4.1 # beta 0.8 0.4 0.1 0.6 0.8 1.0 1.6 # sigma 1.0 0.7 0.4 0.6 0.8 1.2 2.8 # deviance 13.0 3.7 8.8 10.2 12.0 14.6 22.9 # # DIC info (using the rule, pD = Dbar-Dhat) # pD = 3.9 and DIC = 16.9 # DIC is an estimate of expected predictive error (lower deviance is better). R interface to TensorFlow/Keras Prerequisites In order to run TF/keras from within R, we need to 0. Login to a GPU node: ssh dev-intel16-k80 1. Install Tensorflow as the backend of running keras \u2013 local install by user 2. Install R library {tensorflow} and {keras} \u2013 local install by user We show here how to install R TF libraries in one's home directory, after loading the system-wide R (version 3.5.1 at the time of writing). We don't show here how to install TF ; it's included in another wiki All the paths presented below should be replaced by your own paths. Installing R libraries As mentioned above, we will install the two TF libraries in your home directory. Let's say you have an existing directory named ~/Rlibs (create one if you don't), the installation would go as follows: Install R libraries in your home dir 1 2 3 4 5 6 7 8 9 # Load R module purge module load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604 # Install R libraries tensorflow and keras in an existing directory under your home: ~/Rlibs R > library ( devtools ) > with_libpaths ( new = \"~/Rlibs\" , install_github ( 'rstudio/tensorflow' )) > with_libpaths ( new = \"~/Rlibs\" , install_github ( 'rstudio/keras' )) A deep learning example Now let's try out a deep learning analysis (classifying MNIST handwritten digits using Multi-Layer Perceptrons) with code available from here . For convenience, the code is pasted below; you can run it either by opening an R console or saving the code to an R script so that you can run the script on the command line. Here are some important things to note: (1) Before executing the R code, you need to set up a couple of conda related environment variables. That is, 1 2 3 export PATH = /mnt/home/longnany/anaconda3-march2019/bin: $PATH export LD_LIBRARY_PATH = /mnt/home/longnany/anaconda3-march2019/envs/tf_gpu/lib: $LD_LIBRARY_PATH export TF_CPP_MIN_LOG_LEVEL = 2 # to filter out warning messages (2) As you may have noted in the R code below, running TF within R requires some configurations (line 3 and line 4) so that R knows where to find your TF conda environment. (3) Tip: while the code is running, you can start a new HPCC login session and, after running ssh dev-intel16-k80 , type the command gpustat -cpuP . You should be able to see your GPU processes (R processes in this case). R code for deep learning 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 . libPaths ( \"~/Rlibs\" ) # add local lib to R search path; o.w. tensorflow/keras can't be loaded library ( keras ) use_python ( \"/mnt/home/longnany/anaconda3-march2019/envs/tf_gpu/bin/python\" ) use_condaenv ( \"/mnt/home/longnany/anaconda3-march2019/envs/tf_gpu\" ) #loading the keras inbuilt mnist dataset data <- dataset_mnist () #separating train and test file train_x <- data $train$x train_y <- data $train$y test_x <- data $test$x test_y <- data $test$y rm ( data ) # converting a 2D array into a 1D array for feeding into the MLP and normalising the matrix train_x <- array ( train_x , dim = c ( dim ( train_x ) [ 1 ] , prod ( dim ( train_x ) [- 1 ] ))) / 255 test_x <- array ( test_x , dim = c ( dim ( test_x ) [ 1 ] , prod ( dim ( test_x ) [- 1 ] ))) / 255 #converting the target variable to once hot encoded vectors using keras inbuilt function train_y <- to_categorical ( train_y , 10 ) test_y <- to_categorical ( test_y , 10 ) #defining a keras sequential model model <- keras_model_sequential () #defining the model with 1 input layer[784 neurons], 1 hidden layer[784 neurons] with dropout rate 0.4 and 1 output layer[10 neurons] #i.e number of digits from 0 to 9 model %>% layer_dense(units = 784, input_shape = 784) %>% layer_dropout(rate=0.4)%>% layer_activation(activation = 'relu' ) %>% layer_dense(units = 10) %>% layer_activation(activation = 'softmax' ) #compiling the defined model with metric = accuracy and optimiser as adam. model %>% compile( loss = 'categorical_crossentropy', optimizer = 'adam', metrics = c('accuracy') ) #fitting the model on the training dataset model %>% fit(train_x, train_y , epochs = 100 , batch_size = 128 ) #Evaluating model on the cross validation dataset loss_and_metrics <- model %> % evaluate(test_x, test_y , batch_size = 128 ) R 3.5.1 with Intel MKL Intel MKL can accelerate R's speed in linear algebra calculations (such as cross-product, matrix decomposition, inverse computation, linear regression and etc.) by providing BLAS with higher performance. On the HPCC, only 3.5.1 has been built by linking to Intel MKL. Loading R Loading R 3.5.1 built w/ OpenBLAS 1 2 module purge module load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604 Loading R 3.5.1 built w/ MKL 1 2 module purge module load R-Core/3.5.1-intel-mkl You could double check the BLAS/LAPACK libraries linked by running sessionInfo() in R. Benchmarking We have a simple R code, crossprod.R , for testing the computation time. The code is below, where the function crossprod can run in a multi-threaded mode implemented by OpenMP. crossprod.R 1 2 3 4 5 set . seed ( 1 ) m <- 5000 n <- 20000 A <- matrix ( runif ( m * n ), m , n ) system . time ( B <- crossprod ( A )) Now, open an interactive SLURM job session by requesting 4 cores: Benchmarking OpenBLAS vs. MKL (multi-threads) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 salloc --time = 2 :00:00 --cpus-per-task = 4 --mem = 50G --constraint = intel18 # After you are allocated a compute node, do the following. # 1) Run R/OpenBLAS module purge module load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604 Rscript --vanilla crossprod.R & # Time output: # user system elapsed # 50.036 1.574 14.156 # 2) Run R/MKL module purge module load R-Core/3.5.1-intel-mkl Rscript --vanilla crossprod.R & # Time output: # user system elapsed # 28.484 1.664 8.737 Above, the boost in speed is clear from using MKL as compared with OpenBLAS. Even if we use a single thread (by requesting one core), MKL still shows some advantage. Benchmarking OpenBLAS vs. MKL (single-thread) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 salloc --time = 2 :00:00 --cpus-per-task = 1 --mem = 50G --constraint = intel18 # After you are allocated a compute node, do the following. # 1) Run R/OpenBLAS module purge module load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604 Rscript --vanilla crossprod.R & # Time output: # user system elapsed # 47.763 0.598 49.287 # 2) Run R/MKL module purge module load R-Core/3.5.1-intel-mkl Rscript --vanilla crossprod.R & # Time output: # user system elapsed # 25.846 0.641 27.006 Notes When loading R, the OpenMP environment variable OMP_NUM_THREADS is left unset. This means that when running R code directly on a dev-node, all CPUs on that node will be used by the internal multithreading library compiled into R. This is discouraged since the node will be overloaded and your job may even fail. Therefore, please set OMP_NUM_THREADS to a proper value before running the R code. For example, $ OMP_NUM_THREADS=4 $ Rscript --vanilla crossprod.R On the other hand, when the code is run on a compute node allocated by SLURM, you don\u2019t need to set OMP_NUM_THREADS as R would automatically detect CPUs available for use (which should have been requested in your salloc command or sbatch script).","title":"Some packages and other information"},{"location":"R_others/#some-packages-and-other-information","text":"","title":"Some packages and other information"},{"location":"R_others/#rstan","text":"The example here follows that in RStan Getting Started . To test rstan on the HPCC, first load R 3.6.2: 1 2 module purge module load GCC/8.3.0 OpenMPI/3.1.4 R/3.6.2-X11-20180604 As of February 2020, the rstan version is 2.19.2. The stan model file \"8schools.stan\" contains: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 data { int< lower = 0 > J ; // number of schools real y [ J ] ; // estimated treatment effects real< lower = 0 > sigma [ J ] ; // standard error of effect estimates } parameters { real mu ; // population treatment effect real< lower = 0 > tau ; // standard deviation in treatment effects vector [ J ] eta ; // unscaled deviation from mu by school } transformed parameters { vector [ J ] theta = mu + tau * eta ; // school treatment effects } model { target += normal_lpdf ( eta | 0 , 1 ) ; // prior log-density target += normal_lpdf ( y | theta, sigma ) ; // log-likelihood } The R code (\"run.R\") to run stan model contains: 1 2 3 4 5 6 7 8 9 10 11 12 library ( \"rstan\" ) options ( mc . cores = parallel :: detectCores ()) rstan_options ( auto_write = TRUE ) schools_dat <- list ( J = 8 , y = c ( 28 , 8 , - 3 , 7 , - 1 , 1 , 18 , 12 ), sigma = c ( 15 , 10 , 16 , 11 , 9 , 11 , 10 , 18 )) fit <- stan ( file = '8schools.stan' , data = schools_dat ) print ( fit ) pairs ( fit , pars = c ( \"mu\" , \"tau\" , \"lp__\" )) la <- extract ( fit , permuted = TRUE ) # return a list of arrays mu <- la $mu To run the model from the command line: Rscript run.R In addition to the results printed to the stdout, there is an R object file named 8schools.rds generated. This is due to that we've set auto_write to TRUE in run.R. More about the auto_write option: Logical, defaulting to the value of rstan_options(\"auto_write\"), indicating whether to write the object to the hard disk using saveRDS. Although this argument is FALSE by default, we recommend calling rstan_options(\"auto_write\" = TRUE) in order to avoid unnecessary recompilations. If file is supplied and its dirname is writable, then the object will be written to that same directory, substituting a .rds extension for the .stan extension. Otherwise, the object will be written to the tempdir.","title":"rstan"},{"location":"R_others/#rjags","text":"To use {rjags}, first load R/3.5.1 and JAGS from a dev-node (dev-intel16 or dev-intel18) as follows: Loading R/3.5.1 and JAGS 1 2 3 module purge module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 module load JAGS/4.3.0 Next, we will run a short example of data analysis using rjags. This example comes from this tutorial which presents many Bayesian models using this package. To invoke R from the command line: R --vanilla Then, in the R console, you can run the following codes (for detailed explanation refer to the tutorial mentioned above): Sample R code using {rjags} commands 1 2 3 4 5 6 7 8 9 10 11 library ( rjags ) data <- read . csv ( \"data1.csv\" ) N <- length ( data $y ) dat <- list ( \"N\" = N , \"y\" = data $y , \"V\" = data $V ) inits <- list ( d = 0 . 0 ) jags . m <- jags . model ( file = \"aspirinFE.txt\" , data = dat , inits = inits , n . chains = 1 , n . adapt = 500 ) params <- c ( \"d\" , \"OR\" ) samps <- coda . samples ( jags . m , params , n . iter = 10000 ) summary ( window ( samps , start = 5001 )) plot ( samps ) where the two input files, data1.csv and aspirinFE.txt, need to be located in the working directory. The content of the two files is below. data1.csv 1 2 3 4 5 6 7 8 N,y,V 1,0.3289011,0.0388957 2,0.3845458,0.0411673 3,0.2195622,0.0204915 4,0.2222206,0.0647646 5,0.2254672,0.0351996 6,-0.1246363,0.0096167 7,0.1109658,0.0015062 aspirinFE.txt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 model { for ( i in 1:N ) { P[i] <- 1/V[i] y[i] ~ dnorm(d, P[i]) } ### Define the priors d ~ dnorm(0, 0.00001) ### Transform the ln(OR) to OR OR <- exp(d) } A screen shot of the entire run including the output figures is attached here.","title":"rjags"},{"location":"R_others/#r2openbugs","text":"R package R2OpenBUGS is available on the HPCC. OpenBUGS is a software package that performs Bayesian inference Using Gibbs Sampling. The latest version of OpenBUGS on the HPCC is 3.2.3. R2OpenBUGS is an R package that allows users to run OpenBUGS from R. To load R and OpenBUGS, run: 1 2 3 module purge module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 module load OpenBUGS Then, in your R session, use \"library(R2OpenBUGS)\" to load this package. You can execute the following testing R code to see if it works for you. Testing R2OpenBUGS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 library ( R2OpenBUGS ) # Define the model BUGSmodel <- function () { for ( j in 1 :N ) { Y [ j ] ~ dnorm ( mu [ j ] , tau ) mu [ j ] <- alpha + beta * ( x [ j ] - xbar ) } # Priors alpha ~ dnorm ( 0 , 0 . 001 ) beta ~ dnorm ( 0 , 0 . 001 ) tau ~ dgamma ( 0 . 001 , 0 . 001 ) sigma <- 1 / sqrt ( tau ) } # Data Data <- list ( Y = c ( 1 , 3 , 3 , 3 , 5 ), x = c ( 1 , 2 , 3 , 4 , 5 ), N = 5 , xbar = 3 ) # Initial values for the MCMC Inits <- function () { list ( alpha = 1 , beta = 1 , tau = 1 ) } # Run BUGS out <- bugs ( data = Data , inits = Inits , parameters . to . save = c ( \"alpha\" , \"beta\" , \"sigma\" ), model . file = BUGSmodel , n . chains = 1 , n . iter = 10000 ) # Examine the BUGS output out # Inference for Bugs model at \"/tmp/RtmphywZxh/model77555ea534aa.txt\", # Current: 1 chains, each with 10000 iterations (first 5000 discarded) # Cumulative: n.sims = 5000 iterations saved # mean sd 2.5% 25% 50% 75% 97.5% # alpha 3.0 0.6 1.9 2.7 3.0 3.3 4.1 # beta 0.8 0.4 0.1 0.6 0.8 1.0 1.6 # sigma 1.0 0.7 0.4 0.6 0.8 1.2 2.8 # deviance 13.0 3.7 8.8 10.2 12.0 14.6 22.9 # # DIC info (using the rule, pD = Dbar-Dhat) # pD = 3.9 and DIC = 16.9 # DIC is an estimate of expected predictive error (lower deviance is better).","title":"R2OpenBUGS"},{"location":"R_others/#r-interface-to-tensorflowkeras","text":"","title":"R interface to TensorFlow/Keras"},{"location":"R_others/#prerequisites","text":"In order to run TF/keras from within R, we need to 0. Login to a GPU node: ssh dev-intel16-k80 1. Install Tensorflow as the backend of running keras \u2013 local install by user 2. Install R library {tensorflow} and {keras} \u2013 local install by user We show here how to install R TF libraries in one's home directory, after loading the system-wide R (version 3.5.1 at the time of writing). We don't show here how to install TF ; it's included in another wiki All the paths presented below should be replaced by your own paths.","title":"Prerequisites"},{"location":"R_others/#installing-r-libraries","text":"As mentioned above, we will install the two TF libraries in your home directory. Let's say you have an existing directory named ~/Rlibs (create one if you don't), the installation would go as follows: Install R libraries in your home dir 1 2 3 4 5 6 7 8 9 # Load R module purge module load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604 # Install R libraries tensorflow and keras in an existing directory under your home: ~/Rlibs R > library ( devtools ) > with_libpaths ( new = \"~/Rlibs\" , install_github ( 'rstudio/tensorflow' )) > with_libpaths ( new = \"~/Rlibs\" , install_github ( 'rstudio/keras' ))","title":"Installing R libraries"},{"location":"R_others/#a-deep-learning-example","text":"Now let's try out a deep learning analysis (classifying MNIST handwritten digits using Multi-Layer Perceptrons) with code available from here . For convenience, the code is pasted below; you can run it either by opening an R console or saving the code to an R script so that you can run the script on the command line. Here are some important things to note: (1) Before executing the R code, you need to set up a couple of conda related environment variables. That is, 1 2 3 export PATH = /mnt/home/longnany/anaconda3-march2019/bin: $PATH export LD_LIBRARY_PATH = /mnt/home/longnany/anaconda3-march2019/envs/tf_gpu/lib: $LD_LIBRARY_PATH export TF_CPP_MIN_LOG_LEVEL = 2 # to filter out warning messages (2) As you may have noted in the R code below, running TF within R requires some configurations (line 3 and line 4) so that R knows where to find your TF conda environment. (3) Tip: while the code is running, you can start a new HPCC login session and, after running ssh dev-intel16-k80 , type the command gpustat -cpuP . You should be able to see your GPU processes (R processes in this case). R code for deep learning 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 . libPaths ( \"~/Rlibs\" ) # add local lib to R search path; o.w. tensorflow/keras can't be loaded library ( keras ) use_python ( \"/mnt/home/longnany/anaconda3-march2019/envs/tf_gpu/bin/python\" ) use_condaenv ( \"/mnt/home/longnany/anaconda3-march2019/envs/tf_gpu\" ) #loading the keras inbuilt mnist dataset data <- dataset_mnist () #separating train and test file train_x <- data $train$x train_y <- data $train$y test_x <- data $test$x test_y <- data $test$y rm ( data ) # converting a 2D array into a 1D array for feeding into the MLP and normalising the matrix train_x <- array ( train_x , dim = c ( dim ( train_x ) [ 1 ] , prod ( dim ( train_x ) [- 1 ] ))) / 255 test_x <- array ( test_x , dim = c ( dim ( test_x ) [ 1 ] , prod ( dim ( test_x ) [- 1 ] ))) / 255 #converting the target variable to once hot encoded vectors using keras inbuilt function train_y <- to_categorical ( train_y , 10 ) test_y <- to_categorical ( test_y , 10 ) #defining a keras sequential model model <- keras_model_sequential () #defining the model with 1 input layer[784 neurons], 1 hidden layer[784 neurons] with dropout rate 0.4 and 1 output layer[10 neurons] #i.e number of digits from 0 to 9 model %>% layer_dense(units = 784, input_shape = 784) %>% layer_dropout(rate=0.4)%>% layer_activation(activation = 'relu' ) %>% layer_dense(units = 10) %>% layer_activation(activation = 'softmax' ) #compiling the defined model with metric = accuracy and optimiser as adam. model %>% compile( loss = 'categorical_crossentropy', optimizer = 'adam', metrics = c('accuracy') ) #fitting the model on the training dataset model %>% fit(train_x, train_y , epochs = 100 , batch_size = 128 ) #Evaluating model on the cross validation dataset loss_and_metrics <- model %> % evaluate(test_x, test_y , batch_size = 128 )","title":"A deep learning example"},{"location":"R_others/#r-351-with-intel-mkl","text":"Intel MKL can accelerate R's speed in linear algebra calculations (such as cross-product, matrix decomposition, inverse computation, linear regression and etc.) by providing BLAS with higher performance. On the HPCC, only 3.5.1 has been built by linking to Intel MKL.","title":"R 3.5.1 with Intel MKL"},{"location":"R_others/#loading-r","text":"Loading R 3.5.1 built w/ OpenBLAS 1 2 module purge module load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604 Loading R 3.5.1 built w/ MKL 1 2 module purge module load R-Core/3.5.1-intel-mkl You could double check the BLAS/LAPACK libraries linked by running sessionInfo() in R.","title":"Loading R"},{"location":"R_others/#benchmarking","text":"We have a simple R code, crossprod.R , for testing the computation time. The code is below, where the function crossprod can run in a multi-threaded mode implemented by OpenMP. crossprod.R 1 2 3 4 5 set . seed ( 1 ) m <- 5000 n <- 20000 A <- matrix ( runif ( m * n ), m , n ) system . time ( B <- crossprod ( A )) Now, open an interactive SLURM job session by requesting 4 cores: Benchmarking OpenBLAS vs. MKL (multi-threads) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 salloc --time = 2 :00:00 --cpus-per-task = 4 --mem = 50G --constraint = intel18 # After you are allocated a compute node, do the following. # 1) Run R/OpenBLAS module purge module load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604 Rscript --vanilla crossprod.R & # Time output: # user system elapsed # 50.036 1.574 14.156 # 2) Run R/MKL module purge module load R-Core/3.5.1-intel-mkl Rscript --vanilla crossprod.R & # Time output: # user system elapsed # 28.484 1.664 8.737 Above, the boost in speed is clear from using MKL as compared with OpenBLAS. Even if we use a single thread (by requesting one core), MKL still shows some advantage. Benchmarking OpenBLAS vs. MKL (single-thread) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 salloc --time = 2 :00:00 --cpus-per-task = 1 --mem = 50G --constraint = intel18 # After you are allocated a compute node, do the following. # 1) Run R/OpenBLAS module purge module load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604 Rscript --vanilla crossprod.R & # Time output: # user system elapsed # 47.763 0.598 49.287 # 2) Run R/MKL module purge module load R-Core/3.5.1-intel-mkl Rscript --vanilla crossprod.R & # Time output: # user system elapsed # 25.846 0.641 27.006","title":"Benchmarking"},{"location":"R_others/#notes","text":"When loading R, the OpenMP environment variable OMP_NUM_THREADS is left unset. This means that when running R code directly on a dev-node, all CPUs on that node will be used by the internal multithreading library compiled into R. This is discouraged since the node will be overloaded and your job may even fail. Therefore, please set OMP_NUM_THREADS to a proper value before running the R code. For example, $ OMP_NUM_THREADS=4 $ Rscript --vanilla crossprod.R On the other hand, when the code is run on a compute node allocated by SLURM, you don\u2019t need to set OMP_NUM_THREADS as R would automatically detect CPUs available for use (which should have been requested in your salloc command or sbatch script).","title":"Notes"},{"location":"R_workshop_tutorial/","text":"R workshop tutorial Preparation Basic knowledge of R language, Linux, and the HPCC environment. Login ssh -XY YourAccount@hpcc.msu.edu ssh dev-amd20 We will be using R 4.0.2. To load it: module purge; module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 Copy files that we are using in this workshop to your home directory: cp -r /mnt/research/common-data/workshops/R-workshop . R startup When an R session starts, it looks for and loads two hidden configuration files, .Renviron and .Rprofile . .Renviron : contains environment variables to be set in R sessions .Rprofile : contains R commands to be run in R sessions The following search order is applied: your current working directory, your home directory, and the system-wide R_HOME/etc/ (you can use R command R.home() to check path of R_HOME ). Below are examples of the two files which have been placed in our R workshop directory. You need to use ls -a to list them since they are \"hidden\" files. .Rprofile (an example) 1 2 3 4 5 6 7 8 9 10 11 cat ( \"Sourcing .Rprofile from the R-workshop directory.\\n\" ) # To avoid setting the CRAN mirror each time you run install.packages local ({ options ( repos = \"https://repo.miserver.it.umich.edu/cran/\" ) }) options ( width = 100 ) # max number of columns when printing vectors/matrices/arrays .First <- function () cat ( \"##### R session begins for\" , paste ( Sys.getenv ( \"USER\" ) , \".\" , sep = \"\" ) , \"You are currently in\" , getwd () , \"#####\\n\\n\" ) .Last <- function () cat ( \"\\n##### R session ends for\" , paste ( Sys.getenv ( \"USER\" ) , \".\" , sep = \"\" ) , \"You are currently in\" , getwd () , \"#####\\n\\n\" ) .Renviron (an example) 1 2 R_DEFAULT_PACKAGES = \"datasets,utils,grDevices,graphics,stats,methods\" R_LIBS_USER = /mnt/home/longnany/Rlibs Let's run a short Rscript command to see what we get: 1 $ Rscript -e 'date()' Notes: Personalizing these two files can reduce code portability. Rscript by default doesn't load package {methods} in order to speed up initialization. To customize your own loading packages, add them in .Renviron (see above). If you don't want R or Rscript to read any Renviron or Rprofile files when starting an R session, use option --vanilla . A caveat : if you explicitly export an R environment variable, such as export R_LIBS_USER=~/Rlibs , then adding --vanilla will not ignore its value. See below the result. 1 2 3 4 5 6 $ Rscript --vanilla -e '.libPaths()' # .libPaths() is used to see the directories where R searches for libraries [1] \"/opt/software/R/4.0.2-foss-2019b/lib64/R/library\" $ export R_LIBS_USER=~/Rlibs $ Rscript --vanilla -e '.libPaths()' [1] \"/mnt/home/longnany/Rlibs\" [2] \"/opt/software/R/4.0.2-foss-2019b/lib64/R/library\" Rscript one-liner The general format of Rscript: 1 Rscript [ options ] [ -e expression1 -e expression2 ... | source file ] [ args ] options : can be multiple; all beginning with -- (e.g., --vanilla as mentioned above). To learn about all the options, run Rscript --help on the command line. expression1, expression2 ... : can be one or multiple. They are R commands. source file : R source code. args : arguments to be passed. You may have both expressions and source file present in your Rscript line. Here are a few one-liner examples: Rscript one-liner examples 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Ex1: simple loop $ Rscript - e 'for (i in 1:5) print(paste(\"g\", i))' # Ex2: print time $ Rscript - e 'date()' # Ex3: quick math (calculating quotient and remainder) $ Rscript - e '128 %/% 11' - e '128 %% 11' # Ex4: get help for command \"paste\" $ Rscript - e 'help(paste)' # Ex5: used in conjunction with pipe. # Generate three sets of random Normal variables with different means (sd all being one); means are given in file test.dat. $ cat > test . dat # ctrl+D to go back when done typing 1 10 20 $ cat test . dat | Rscript - e 'input_con <- file(\"stdin\"); open(input_con); while (length(oneLine <- readLines(con = input_con, n = 1, warn = FALSE)) > 0) {print(rnorm(5,mean=as.numeric(oneLine)))};close(input_con)' Using Rscript with source code a. simple usage: Instead of using '-e your_commands ', we now put R commands in a source file and run it with Rscript. Below is an R script file and we can run Rscript multiplots.R to get a PDF figure multiplots.pdf . To view it: evince multiplots.pdf multiplots.R : a very simple example of making 4 plots on the same page 1 2 3 4 pdf ( \"multiplots.pdf\" ) par ( mfrow = c ( 2 , 2 )) for ( i in 1 : 4 ) plot ( 1 : 10 , 1 : 10 , type = \"b\" , xlab = bquote ( X [. ( i ) ] ), ylab = bquote ( Y [. ( i ) ] ), main = bquote ( \"Multiple plots: #\" ~ . ( i ))) dev . off () b. passing command line arguments: We can also pass arguments to our R script, as shown in the example below. args-1.R 1 2 3 4 5 6 7 8 9 10 args <- commandArgs ( trailingOnly = TRUE ) nargs <- length ( args ) for ( i in 1 :nargs ) { cat ( \"Arg\" , i , \":\" , args [ i ] , \" \\n \" ) } cat ( \"Generating\" , as . numeric ( args [ nargs - 1 ] ), \"normal variables with mean =\" , as . numeric ( args [ nargs ] ), \"and sd = 1 \\n \" ) rnorm ( as . numeric ( args [ nargs - 1 ] ), mean = as . numeric ( args [ nargs ] )) Running script args-1.R : 1 2 3 4 5 6 7 8 9 10 $ Rscript args-1.R 5 3 Sourcing .Rprofile from the 11092017 -R-Workshop directory. ##### R session begins for longnany. You are currently in /mnt/home/longnany/Documents/11092017-R-Workshop ##### Arg 1 : 5 Arg 2 : 3 Generating 5 normal variables with mean = 3 and sd = 1 [ 1 ] 2 .707162 3 .677923 3 .192272 2 .531973 3 .699060 ##### R session ends for longnany. You are currently in /mnt/home/longnany/Documents/11092017-R-Workshop ##### c. processing command line arguments with { getopt }: args-2.R 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 require ( \"getopt\" , quietly = TRUE ) spec = matrix ( c ( \"Number\" , \"n\" , 1 , \"integer\" , \"Mean\" , \"m\" , 1 , \"double\" ), byrow = TRUE , ncol = 4 ) # cf. https://cran.r-project.org/web/packages/getopt/getopt.pdf opt = getopt ( spec ); if ( is . null ( opt $Number )) { n <- 5 } else { n <- opt $Number } if ( is . null ( opt $Mean )) { m <- 3 } else { m <- opt $Mean } cat ( \"Generating\" , n , \"normal variables with mean =\" , m , \"and sd = 1 \\n \" ) rnorm ( n = n , mean = m ) Running the script args-2.R : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Use long flag names $ Rscript --vanilla args-2.R --Number 10 --Mean -2 Generating 10 normal variables with mean = -2 and sd = 1 [ 1 ] -0.4776278 -1.7759145 -0.9977682 -2.6452126 -3.4050587 -2.2358362 [ 7 ] -1.2696362 -1.6213633 -2.7013074 -1.9271954 # Use short flag names $ Rscript --vanilla args-2.R -n 10 -m -2 Generating 10 normal variables with mean = -2 and sd = 1 [ 1 ] -2.2241837 -1.6704711 0 .1481244 0 .2072124 -1.0385386 -1.5194874 [ 7 ] -2.6744478 -2.4683039 -0.7962113 -1.1901021 # No arguments provided so defaults are used $ Rscript --vanilla args-2.R Generating 5 normal variables with mean = 3 and sd = 1 [ 1 ] 3 .951492 4 .255879 4 .485044 2 .727223 3 .039532 Submitting parallel jobs to the cluster using {doParallel}: single node, multiple cores To submit a single-node job, we recommend {doParallel}'s multicore functionality. R_doParallel_singlenode.R : run 10,000 bootstrap iterations of fitting a logistic regression model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 library ( doParallel ) # Request a single node (this uses the \"multicore\" functionality) registerDoParallel ( cores = as . numeric ( Sys . getenv ( \"SLURM_CPUS_ON_NODE\" ) [ 1 ] )) x <- iris [ which ( iris [ , 5 ] != \"setosa\" ), c ( 1 , 5 ) ] trials <- 10000 ptime <- system . time ({ r <- foreach ( icount ( trials ), . combine = cbind ) % dopar % { ind <- sample ( 100 , 100 , replace = TRUE ) result1 <- glm ( x [ ind , 2 ]~ x [ ind , 1 ] , family = binomial ( logit )) coefficients ( result1 ) } }) [ 3 ] cat ( \"Time elapsed:\" , ptime , \" \\n \" ) cat ( \"Currently registered backend:\" , getDoParName (), \" \\n \" ) cat ( \"Number of workers used:\" , getDoParWorkers (), \" \\n \" ) print ( str ( r )) # column-binded result Now, submit the job to the HPCC through the following SLURM script: submit-doParallel.sbatch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #!/bin/bash # Job name: #SBATCH --job-name=doParallel_test # # Number of nodes needed: #SBATCH --nodes=1 # # Tasks per node: #SBATCH --ntasks-per-node=1 # # Processors per task: #SBATCH --cpus-per-task=4 # # Memory per node: #SBATCH --mem=500M # # Wall time (e.g. \"minutes\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\"): #SBATCH --time=3:00:00 # # Standard out and error: #SBATCH --output=%x-%j.SLURMout module purge module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 Rscript --vanilla R_doParallel_singlenode.R > R_doParallel_singlenode.Rout Submission command: sbatch submit-doParallel.sbatch The output file R_doParallel_singlenode.Rout should look like: 1 2 3 4 5 6 7 8 Time elapsed: 8.946 Currently registered backend: doParallelMC Number of workers used: 4 num [1:2, 1:10000] -14.6 2.26 -11.86 1.91 -7.75 ... - attr(*, \"dimnames\")=List of 2 ..$ : chr [1:2] \"(Intercept)\" \"x[ind, 1]\" ..$ : chr [1:10000] \"result.1\" \"result.2\" \"result.3\" \"result.4\" ... NULL Submitting parallel jobs to the cluster using {Rmpi}: multiple nodes MPI stands for M essage P assing I nterface, and the R package Rmpi is an implementation of it for R. Function usage in {Rmpi} is complicated. For ease of implementation, we recommend the use of another simplified R MPI library, {pbdMPI}, as described next. Submitting parallel jobs to the cluster using {pbdMPI}: multiple nodes pbdMPI is a more recent R MPI package, which simplifies MPI interaction and thus eases programming. It works in Single Program/Multiple Data (SPMD) mode. As an illustration, we consider the problem of computing the log likelihood of data following a 2-dimensional Multi-Variate Normal (MVN) distribution. The example below applies Cholesky decomposition on the 2-by-2 covariance matrix, solves a system of linear equations (this is where parallelism plays a vital role), and performs some matrix/vector operations. Here is a graphic illustration of solving a system of linear equations by part . MVN.R : MPI in SPMD mode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # Load pbdMPI and initialize the communicator library ( pbdMPI , quiet = TRUE ) init () # Check processes comm . cat ( \"All processes start... \\n\\n \" ) msg <- sprintf ( \"I am rank %d on host %s of %d processes \\n \" , comm . rank (), Sys . info () [ \"nodename\" ] , comm . size ()) comm . cat ( msg , all . rank = TRUE , quiet = TRUE ) # quiet=T tells each rank not to \"announce\" itself when it's printing set . seed ( 1234 ) N <- 100 p <- 2 X <- matrix ( rnorm ( N * p ), ncol = p ) mu <- c ( 0 . 1 , 0 . 2 ) Sigma <- matrix ( c ( 0 . 9 , 0 . 1 , 0 . 1 , 0 . 9 ), ncol = p ) # Load data partially by processors id . get <- get . jid ( N ) X . spmd <- matrix ( X [ id . get , ] , ncol = p ) comm . cat ( \" \\n Print out the matrix on each process/rank: \\n\\n \" , quiet = TRUE ) comm . print ( X . spmd , all . rank = TRUE , quiet = TRUE ) # Cholesky decomposition U <- chol ( Sigma ) # U'U = Sigma logdet <- sum ( log ( abs ( diag ( U )))) # Call R's backsolve function for each chunk of the data matrix X (i.e. B.spmd) B . spmd <- t ( X . spmd ) - mu A . spmd <- backsolve ( U , B . spmd , upper . tri = TRUE , transpose = TRUE ) # U'A = B distval . spmd <- colSums ( A . spmd * A . spmd ) # Use sum as the reduction operation sum . distval <- allreduce ( sum ( distval . spmd ), op = \"sum\" ) total . logL <- - 0 . 5 * ( N * ( p * log ( 2 * pi ) + logdet * 2 ) + sum . distval ) # Output comm . cat ( \" \\n Final log-likelihood: \\n\\n \" , quiet = TRUE ) comm . print ( total . logL , quiet = TRUE ) finalize () SLURM submission script: submit-pbdMPI.sbatch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #!/bin/bash # Job name: #SBATCH --job-name=pbdMPI_test # # Number of MPI tasks: #SBATCH --ntasks=20 # # Processors per task: #SBATCH --cpus-per-task=1 # # Memory: #SBATCH --mem-per-cpu=800M # # Wall clock limit (e.g. \"minutes\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\"): #SBATCH --time=30 # # Standard out and error: #SBATCH --output=%x-%j.SLURMout echo \"SLURM_NTASKS: $SLURM_NTASKS \" module purge module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 # Suppress warnings about forks and missing CUDA libraries export OMPI_MCA_mpi_warn_on_fork = 0 export OMPI_MCA_mpi_cuda_support = 0 mpirun -n $SLURM_NTASKS Rscript --vanilla MVN.R > MVN.Rout Now, submit the job by sbatch submit-pbdMPI.sbatch When finished, MVN.Rout should contain the following information: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 COMM.RANK = 0 All processes start... I am rank 0 on host lac-421 of 20 processes I am rank 1 on host lac-421 of 20 processes ... ... I am rank 18 on host lac-421 of 20 processes I am rank 19 on host lac-421 of 20 processes Print out the matrix on each process/rank: [,1] [,2] [1,] -0.1850210 -0.2771503 [2,] -0.8596753 -0.1081122 [3,] -1.0927853 0.8690750 [4,] -0.5831948 0.8032846 [5,] 1.0796870 0.2354514 [,1] [,2] [1,] 0.05274001 -0.8410549 [2,] -0.92568393 0.9461704 [3,] 1.01632804 0.6875080 [4,] 0.34271061 0.4905065 [5,] 0.84192956 -1.6685933 ... ... [,1] [,2] [1,] -0.55673659 -0.1229023 [2,] -0.03156318 -0.8501178 [3,] -1.28832627 -1.3115801 [4,] -0.47546826 -0.4856559 [5,] 0.81134183 0.7499220 [,1] [,2] [1,] -0.95620195 0.6560605 [2,] 0.04671396 0.6093924 [3,] 0.18986742 0.2077641 [4,] 0.73281327 -1.0452661 [5,] 2.27968859 1.0611428 Final log-likelihood: [1] -283.2159","title":"R workshop"},{"location":"R_workshop_tutorial/#r-workshop-tutorial","text":"","title":"R workshop tutorial"},{"location":"R_workshop_tutorial/#preparation","text":"Basic knowledge of R language, Linux, and the HPCC environment. Login ssh -XY YourAccount@hpcc.msu.edu ssh dev-amd20 We will be using R 4.0.2. To load it: module purge; module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 Copy files that we are using in this workshop to your home directory: cp -r /mnt/research/common-data/workshops/R-workshop .","title":"Preparation"},{"location":"R_workshop_tutorial/#r-startup","text":"When an R session starts, it looks for and loads two hidden configuration files, .Renviron and .Rprofile . .Renviron : contains environment variables to be set in R sessions .Rprofile : contains R commands to be run in R sessions The following search order is applied: your current working directory, your home directory, and the system-wide R_HOME/etc/ (you can use R command R.home() to check path of R_HOME ). Below are examples of the two files which have been placed in our R workshop directory. You need to use ls -a to list them since they are \"hidden\" files. .Rprofile (an example) 1 2 3 4 5 6 7 8 9 10 11 cat ( \"Sourcing .Rprofile from the R-workshop directory.\\n\" ) # To avoid setting the CRAN mirror each time you run install.packages local ({ options ( repos = \"https://repo.miserver.it.umich.edu/cran/\" ) }) options ( width = 100 ) # max number of columns when printing vectors/matrices/arrays .First <- function () cat ( \"##### R session begins for\" , paste ( Sys.getenv ( \"USER\" ) , \".\" , sep = \"\" ) , \"You are currently in\" , getwd () , \"#####\\n\\n\" ) .Last <- function () cat ( \"\\n##### R session ends for\" , paste ( Sys.getenv ( \"USER\" ) , \".\" , sep = \"\" ) , \"You are currently in\" , getwd () , \"#####\\n\\n\" ) .Renviron (an example) 1 2 R_DEFAULT_PACKAGES = \"datasets,utils,grDevices,graphics,stats,methods\" R_LIBS_USER = /mnt/home/longnany/Rlibs Let's run a short Rscript command to see what we get: 1 $ Rscript -e 'date()' Notes: Personalizing these two files can reduce code portability. Rscript by default doesn't load package {methods} in order to speed up initialization. To customize your own loading packages, add them in .Renviron (see above). If you don't want R or Rscript to read any Renviron or Rprofile files when starting an R session, use option --vanilla . A caveat : if you explicitly export an R environment variable, such as export R_LIBS_USER=~/Rlibs , then adding --vanilla will not ignore its value. See below the result. 1 2 3 4 5 6 $ Rscript --vanilla -e '.libPaths()' # .libPaths() is used to see the directories where R searches for libraries [1] \"/opt/software/R/4.0.2-foss-2019b/lib64/R/library\" $ export R_LIBS_USER=~/Rlibs $ Rscript --vanilla -e '.libPaths()' [1] \"/mnt/home/longnany/Rlibs\" [2] \"/opt/software/R/4.0.2-foss-2019b/lib64/R/library\"","title":"R startup"},{"location":"R_workshop_tutorial/#rscript-one-liner","text":"The general format of Rscript: 1 Rscript [ options ] [ -e expression1 -e expression2 ... | source file ] [ args ] options : can be multiple; all beginning with -- (e.g., --vanilla as mentioned above). To learn about all the options, run Rscript --help on the command line. expression1, expression2 ... : can be one or multiple. They are R commands. source file : R source code. args : arguments to be passed. You may have both expressions and source file present in your Rscript line. Here are a few one-liner examples: Rscript one-liner examples 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Ex1: simple loop $ Rscript - e 'for (i in 1:5) print(paste(\"g\", i))' # Ex2: print time $ Rscript - e 'date()' # Ex3: quick math (calculating quotient and remainder) $ Rscript - e '128 %/% 11' - e '128 %% 11' # Ex4: get help for command \"paste\" $ Rscript - e 'help(paste)' # Ex5: used in conjunction with pipe. # Generate three sets of random Normal variables with different means (sd all being one); means are given in file test.dat. $ cat > test . dat # ctrl+D to go back when done typing 1 10 20 $ cat test . dat | Rscript - e 'input_con <- file(\"stdin\"); open(input_con); while (length(oneLine <- readLines(con = input_con, n = 1, warn = FALSE)) > 0) {print(rnorm(5,mean=as.numeric(oneLine)))};close(input_con)'","title":"Rscript one-liner"},{"location":"R_workshop_tutorial/#using-rscript-with-source-code","text":"a. simple usage: Instead of using '-e your_commands ', we now put R commands in a source file and run it with Rscript. Below is an R script file and we can run Rscript multiplots.R to get a PDF figure multiplots.pdf . To view it: evince multiplots.pdf multiplots.R : a very simple example of making 4 plots on the same page 1 2 3 4 pdf ( \"multiplots.pdf\" ) par ( mfrow = c ( 2 , 2 )) for ( i in 1 : 4 ) plot ( 1 : 10 , 1 : 10 , type = \"b\" , xlab = bquote ( X [. ( i ) ] ), ylab = bquote ( Y [. ( i ) ] ), main = bquote ( \"Multiple plots: #\" ~ . ( i ))) dev . off () b. passing command line arguments: We can also pass arguments to our R script, as shown in the example below. args-1.R 1 2 3 4 5 6 7 8 9 10 args <- commandArgs ( trailingOnly = TRUE ) nargs <- length ( args ) for ( i in 1 :nargs ) { cat ( \"Arg\" , i , \":\" , args [ i ] , \" \\n \" ) } cat ( \"Generating\" , as . numeric ( args [ nargs - 1 ] ), \"normal variables with mean =\" , as . numeric ( args [ nargs ] ), \"and sd = 1 \\n \" ) rnorm ( as . numeric ( args [ nargs - 1 ] ), mean = as . numeric ( args [ nargs ] )) Running script args-1.R : 1 2 3 4 5 6 7 8 9 10 $ Rscript args-1.R 5 3 Sourcing .Rprofile from the 11092017 -R-Workshop directory. ##### R session begins for longnany. You are currently in /mnt/home/longnany/Documents/11092017-R-Workshop ##### Arg 1 : 5 Arg 2 : 3 Generating 5 normal variables with mean = 3 and sd = 1 [ 1 ] 2 .707162 3 .677923 3 .192272 2 .531973 3 .699060 ##### R session ends for longnany. You are currently in /mnt/home/longnany/Documents/11092017-R-Workshop ##### c. processing command line arguments with { getopt }: args-2.R 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 require ( \"getopt\" , quietly = TRUE ) spec = matrix ( c ( \"Number\" , \"n\" , 1 , \"integer\" , \"Mean\" , \"m\" , 1 , \"double\" ), byrow = TRUE , ncol = 4 ) # cf. https://cran.r-project.org/web/packages/getopt/getopt.pdf opt = getopt ( spec ); if ( is . null ( opt $Number )) { n <- 5 } else { n <- opt $Number } if ( is . null ( opt $Mean )) { m <- 3 } else { m <- opt $Mean } cat ( \"Generating\" , n , \"normal variables with mean =\" , m , \"and sd = 1 \\n \" ) rnorm ( n = n , mean = m ) Running the script args-2.R : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Use long flag names $ Rscript --vanilla args-2.R --Number 10 --Mean -2 Generating 10 normal variables with mean = -2 and sd = 1 [ 1 ] -0.4776278 -1.7759145 -0.9977682 -2.6452126 -3.4050587 -2.2358362 [ 7 ] -1.2696362 -1.6213633 -2.7013074 -1.9271954 # Use short flag names $ Rscript --vanilla args-2.R -n 10 -m -2 Generating 10 normal variables with mean = -2 and sd = 1 [ 1 ] -2.2241837 -1.6704711 0 .1481244 0 .2072124 -1.0385386 -1.5194874 [ 7 ] -2.6744478 -2.4683039 -0.7962113 -1.1901021 # No arguments provided so defaults are used $ Rscript --vanilla args-2.R Generating 5 normal variables with mean = 3 and sd = 1 [ 1 ] 3 .951492 4 .255879 4 .485044 2 .727223 3 .039532","title":"Using Rscript with source code"},{"location":"R_workshop_tutorial/#submitting-parallel-jobs-to-the-cluster-using-doparallel-single-node-multiple-cores","text":"To submit a single-node job, we recommend {doParallel}'s multicore functionality. R_doParallel_singlenode.R : run 10,000 bootstrap iterations of fitting a logistic regression model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 library ( doParallel ) # Request a single node (this uses the \"multicore\" functionality) registerDoParallel ( cores = as . numeric ( Sys . getenv ( \"SLURM_CPUS_ON_NODE\" ) [ 1 ] )) x <- iris [ which ( iris [ , 5 ] != \"setosa\" ), c ( 1 , 5 ) ] trials <- 10000 ptime <- system . time ({ r <- foreach ( icount ( trials ), . combine = cbind ) % dopar % { ind <- sample ( 100 , 100 , replace = TRUE ) result1 <- glm ( x [ ind , 2 ]~ x [ ind , 1 ] , family = binomial ( logit )) coefficients ( result1 ) } }) [ 3 ] cat ( \"Time elapsed:\" , ptime , \" \\n \" ) cat ( \"Currently registered backend:\" , getDoParName (), \" \\n \" ) cat ( \"Number of workers used:\" , getDoParWorkers (), \" \\n \" ) print ( str ( r )) # column-binded result Now, submit the job to the HPCC through the following SLURM script: submit-doParallel.sbatch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #!/bin/bash # Job name: #SBATCH --job-name=doParallel_test # # Number of nodes needed: #SBATCH --nodes=1 # # Tasks per node: #SBATCH --ntasks-per-node=1 # # Processors per task: #SBATCH --cpus-per-task=4 # # Memory per node: #SBATCH --mem=500M # # Wall time (e.g. \"minutes\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\"): #SBATCH --time=3:00:00 # # Standard out and error: #SBATCH --output=%x-%j.SLURMout module purge module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 Rscript --vanilla R_doParallel_singlenode.R > R_doParallel_singlenode.Rout Submission command: sbatch submit-doParallel.sbatch The output file R_doParallel_singlenode.Rout should look like: 1 2 3 4 5 6 7 8 Time elapsed: 8.946 Currently registered backend: doParallelMC Number of workers used: 4 num [1:2, 1:10000] -14.6 2.26 -11.86 1.91 -7.75 ... - attr(*, \"dimnames\")=List of 2 ..$ : chr [1:2] \"(Intercept)\" \"x[ind, 1]\" ..$ : chr [1:10000] \"result.1\" \"result.2\" \"result.3\" \"result.4\" ... NULL","title":"Submitting parallel jobs to the cluster using {doParallel}: single node, multiple cores"},{"location":"R_workshop_tutorial/#submitting-parallel-jobs-to-the-cluster-using-rmpi-multiple-nodes","text":"MPI stands for M essage P assing I nterface, and the R package Rmpi is an implementation of it for R. Function usage in {Rmpi} is complicated. For ease of implementation, we recommend the use of another simplified R MPI library, {pbdMPI}, as described next.","title":"Submitting parallel jobs to the cluster using\u00a0{Rmpi}: multiple nodes"},{"location":"R_workshop_tutorial/#submitting-parallel-jobs-to-the-cluster-using-pbdmpi-multiple-nodes","text":"pbdMPI is a more recent R MPI package, which simplifies MPI interaction and thus eases programming. It works in Single Program/Multiple Data (SPMD) mode. As an illustration, we consider the problem of computing the log likelihood of data following a 2-dimensional Multi-Variate Normal (MVN) distribution. The example below applies Cholesky decomposition on the 2-by-2 covariance matrix, solves a system of linear equations (this is where parallelism plays a vital role), and performs some matrix/vector operations. Here is a graphic illustration of solving a system of linear equations by part . MVN.R : MPI in SPMD mode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # Load pbdMPI and initialize the communicator library ( pbdMPI , quiet = TRUE ) init () # Check processes comm . cat ( \"All processes start... \\n\\n \" ) msg <- sprintf ( \"I am rank %d on host %s of %d processes \\n \" , comm . rank (), Sys . info () [ \"nodename\" ] , comm . size ()) comm . cat ( msg , all . rank = TRUE , quiet = TRUE ) # quiet=T tells each rank not to \"announce\" itself when it's printing set . seed ( 1234 ) N <- 100 p <- 2 X <- matrix ( rnorm ( N * p ), ncol = p ) mu <- c ( 0 . 1 , 0 . 2 ) Sigma <- matrix ( c ( 0 . 9 , 0 . 1 , 0 . 1 , 0 . 9 ), ncol = p ) # Load data partially by processors id . get <- get . jid ( N ) X . spmd <- matrix ( X [ id . get , ] , ncol = p ) comm . cat ( \" \\n Print out the matrix on each process/rank: \\n\\n \" , quiet = TRUE ) comm . print ( X . spmd , all . rank = TRUE , quiet = TRUE ) # Cholesky decomposition U <- chol ( Sigma ) # U'U = Sigma logdet <- sum ( log ( abs ( diag ( U )))) # Call R's backsolve function for each chunk of the data matrix X (i.e. B.spmd) B . spmd <- t ( X . spmd ) - mu A . spmd <- backsolve ( U , B . spmd , upper . tri = TRUE , transpose = TRUE ) # U'A = B distval . spmd <- colSums ( A . spmd * A . spmd ) # Use sum as the reduction operation sum . distval <- allreduce ( sum ( distval . spmd ), op = \"sum\" ) total . logL <- - 0 . 5 * ( N * ( p * log ( 2 * pi ) + logdet * 2 ) + sum . distval ) # Output comm . cat ( \" \\n Final log-likelihood: \\n\\n \" , quiet = TRUE ) comm . print ( total . logL , quiet = TRUE ) finalize () SLURM submission script: submit-pbdMPI.sbatch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #!/bin/bash # Job name: #SBATCH --job-name=pbdMPI_test # # Number of MPI tasks: #SBATCH --ntasks=20 # # Processors per task: #SBATCH --cpus-per-task=1 # # Memory: #SBATCH --mem-per-cpu=800M # # Wall clock limit (e.g. \"minutes\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\"): #SBATCH --time=30 # # Standard out and error: #SBATCH --output=%x-%j.SLURMout echo \"SLURM_NTASKS: $SLURM_NTASKS \" module purge module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 # Suppress warnings about forks and missing CUDA libraries export OMPI_MCA_mpi_warn_on_fork = 0 export OMPI_MCA_mpi_cuda_support = 0 mpirun -n $SLURM_NTASKS Rscript --vanilla MVN.R > MVN.Rout Now, submit the job by sbatch submit-pbdMPI.sbatch When finished, MVN.Rout should contain the following information: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 COMM.RANK = 0 All processes start... I am rank 0 on host lac-421 of 20 processes I am rank 1 on host lac-421 of 20 processes ... ... I am rank 18 on host lac-421 of 20 processes I am rank 19 on host lac-421 of 20 processes Print out the matrix on each process/rank: [,1] [,2] [1,] -0.1850210 -0.2771503 [2,] -0.8596753 -0.1081122 [3,] -1.0927853 0.8690750 [4,] -0.5831948 0.8032846 [5,] 1.0796870 0.2354514 [,1] [,2] [1,] 0.05274001 -0.8410549 [2,] -0.92568393 0.9461704 [3,] 1.01632804 0.6875080 [4,] 0.34271061 0.4905065 [5,] 0.84192956 -1.6685933 ... ... [,1] [,2] [1,] -0.55673659 -0.1229023 [2,] -0.03156318 -0.8501178 [3,] -1.28832627 -1.3115801 [4,] -0.47546826 -0.4856559 [5,] 0.81134183 0.7499220 [,1] [,2] [1,] -0.95620195 0.6560605 [2,] 0.04671396 0.6093924 [3,] 0.18986742 0.2077641 [4,] 0.73281327 -1.0452661 [5,] 2.27968859 1.0611428 Final log-likelihood: [1] -283.2159","title":"Submitting parallel jobs to the cluster using\u00a0{pbdMPI}: multiple nodes"},{"location":"Rclone_-_rsync_for_cloud_storage/","text":"Rclone - rsync for cloud storage Users could use this software to copy files from/to their Microsoft OneDrive or Google Drive cloud storage to/from HPCC disk space. This tool could also mount their cloud storage to HPCC disk so that the storage on cloud could be used as extended disk space. Rclone is installed on HPCC system wide. To use it, users should first load the software module into their environment using command \"module load Rclone\". For more details of using rclone, users can visit Rclone web site at https://rclone.org/ . To start using it, user should run command \"rclone config\" to configure it. The instructions of this command could be found at https://rclone.org/commands/rclone_config/ . Specifically, to configure for Google Drive, see https://rclone.org/drive/ , and to configure for Microsoft Onedrive, see https://rclone.org/onedrive/ for instructions. The specific details of how to start using this software on HPCC could be found in the document Rclone.pdf After successfully configured, users should be able to use \"rclone\" command to copy or mount the cloud storage to HPCC. There many rclone commands could be used to handle the file transfer and manage files on HPCC and cloud storage. To get help, use \"rclone --help\" as show in the following 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 [ hpc@dev-intel16-k80 ~ ] $ module load Rclone [ hpc@dev-intel16-k80 ~ ] $ rclone --help Rclone syncs files to and from cloud storage providers as well as mounting them, listing them in lots of different ways. See the home page ( https://rclone.org/ ) for installation, usage, documentation, changelog and configuration walkthroughs. Usage: rclone [ flags ] rclone [ command ] Available Commands: about Get quota information from the remote. authorize Remote authorization. cachestats Print cache stats for a remote cat Concatenates any files and sends them to stdout. check Checks the files in the source and destination match. cleanup Clean up the remote if possible config Enter an interactive configuration session. copy Copy files from source to dest, skipping already copied copyto Copy files from source to dest, skipping already copied copyurl Copy url content to dest. cryptcheck Cryptcheck checks the integrity of a crypted remote. cryptdecode Cryptdecode returns unencrypted file names. dbhashsum Produces a Dropbox hash file for all the objects in the path. dedupe Interactively find duplicate files and delete/rename them. delete Remove the contents of path. deletefile Remove a single file from remote. genautocomplete Output completion script for a given shell. gendocs Output markdown docs for rclone to the directory supplied. hashsum Produces an hashsum file for all the objects in the path. help Show help for rclone commands, flags and backends. link Generate public link to file/folder. listremotes List all the remotes in the config file. ls List the objects in the path with size and path. lsd List all directories/containers/buckets in the path. lsf List directories and objects in remote:path formatted for parsing lsjson List directories and objects in the path in JSON format. lsl List the objects in path with modification time, size and path. md5sum Produces an md5sum file for all the objects in the path. mkdir Make the path if it does not already exist. mount Mount the remote as file system on a mountpoint. move Move files from source to dest. moveto Move file or directory from source to dest. ncdu Explore a remote with a text based user interface. obscure Obscure password for use in the rclone.conf purge Remove the path and all of its contents. rc Run a command against a running rclone. rcat Copies standard input to file on remote. rcd Run rclone listening to remote control commands only. rmdir Remove the path if empty. rmdirs Remove empty directories under the path. serve Serve a remote over a protocol. settier Changes storage class/tier of objects in remote. sha1sum Produces an sha1sum file for all the objects in the path. size Prints the total size and number of objects in remote:path. sync Make source and dest identical, modifying destination only. touch Create new file or change file modification time. tree List the contents of the remote in a tree like fashion. version Show the version number. Use \"rclone [command] --help\" for more information about a command. Use \"rclone help flags\" for to see the global flags. Use \"rclone help backends\" for a list of supported services. [ hpc@dev-intel16-k80 ~ ] $ A tool \"cloudSync\" is developed for user to synchronize the files between their cloud storages. It is accessible through \"powertools\". To use it, users need to have the module \"powertools\" loaded. Users are welcome to try it and report any problems to us via contact form here . Following are a few examples of running rclone commands after successfully configured the cloud storage. Assume that the cloud storage is configured as the name \"MyOneDrive\". (1) see current remote storage: As show, we can see that there are currently two remote cloud storage, \"MyOneDrive\" and \"googledoc\" are configured. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [ user@dev-intel18 ~ ] $ rclone config Current remotes: Name Type ==== ==== MyOneDrive onedrive googledoc drive e ) Edit existing remote n ) New remote d ) Delete remote r ) Rename remote c ) Copy remote s ) Set configuration password q ) Quit config e/n/d/r/c/s/q> q [ user@dev-intel18 ~ ] $ (2) Check the remote storage information: We could check the remote storage usage and quota using \"rclone about\" command. 1 2 3 4 5 [ user@dev-intel16-k80 ~ ] $ rclone about MyOneDrive: Total: 5T Used: 450 .999M Free: 4 .998T Trashed: 404 .576k (3) List the contents of the cloud storage 1 2 3 4 5 6 [ user@dev-intel18 ~ ] $ rclone lsd MyOneDrive: -1 2018 -02-02 08 :57:54 0 Attachments -1 2019 -08-27 15 :43:33 1 IMAGES -1 2019 -08-22 15 :50:10 42 Matlab -1 2019 -02-26 17 :12:01 16 Microsoft Teams Chat Files -1 2018 -08-24 08 :56:32 1 Notebooks (4) Copy files on HPCC to remote cloud: 1 2 3 4 5 6 7 8 9 10 11 12 [ user@dev-intel18 ~ ] $ rclone copy Project MyOneDrive:Project # copy the content of directory \"Project\" to remote cloud storage [ user@dev-intel18 ~ ] $ rclone lsd MyOneDrive: # view the contents of cloud storage to confirm the copy -1 2018 -02-02 08 :57:54 0 Attachments -1 2019 -08-27 15 :43:33 1 IMPACT -1 2019 -08-22 15 :50:10 42 Matlab -1 2019 -02-26 17 :12:01 16 Microsoft Teams Chat Files -1 2018 -08-24 08 :56:32 1 Notebooks -1 2020 -04-27 15 :43:25 2 Project [ user@dev-intel18 ~ ] $ rclone lsd MyOneDrive:Project -1 2020 -04-27 15 :44:39 1 GPAW -1 2020 -04-27 15 :43:26 3 MATLAB (5) Copy files on cloud storage to HPCC: 1 2 3 4 5 [ user@dev-intel18 Project ] $ ls # current content of Project directory before copy GPAW MATLAB [ user@dev-intel18 Project ] $ rclone copy MyOneDrive:IMPACT ./ # copy the content of IMPACT in cloud to current directory [ user@dev-intel18 Project ] $ ls # confirm that the copy is done GPAW impact_run MATLAB Note Although \"rclone copy\" is similar as unix commands rsync and cp, when using it, users should be aware of the differences and know the details of its behavior. (1) \"rclone copy\" does not transfer unchanged files, testing by size and modification time or MD5SUM. In this sense, it is similar as linux command rsync; (2) When running \" rclone copy source:sourcepath dest:destpath\", if source:sourcepath is a directory, dest:destpath should also be a directory. It does not copy the directory source:sourcepath , instead, it will copy the content of the directory source:sourcepath to the destination dest:destpath . If dest:destpath does not exist, it will be created and the content of source:sourcepath will be stored in it. (3) \"rclone copyto\" is a very similar rclone command to \"rclone copy\". The only difference is that it can be used to upload single files to other than their current name. When running \" rclone copyto source:sourcepath dest:destpath\", if source:sourcepath is a file, dest:destpath c ould be a new file name. If source:sourcepath is a directory, it would be the same as using \" rclone copy\". (6) Checks the files in the source and destination match. 1 2 3 [ user@dev-intel18 Project ] $ rclone check impact_run MyOneDrive:IMPACT/impact_run # check if it is matched both sides 2020 /04/27 16 :19:01 NOTICE: One drive root 'IMPACT/impact_run' : 0 differences found 2020 /04/27 16 :19:01 NOTICE: One drive root 'IMPACT/impact_run' : 21 matching files Note For archiving your files to your cloud storage, if the connection between HPCC and your cloud storage is not stable, we would NOT recommend using \"rclone move\" because it may loss the data during the transfer. Instead, we recommend using \"rclone copy\" to successfully copy the files over and run \"rclone check\" to check if files are identical. After that, it is safe to delete local copy of the files. Note When use \"rclone mount\" command to mount your cloud storage to HPCC, there are two things users should be careful: (1) When running rclone mount, the process runs NOT as the user, instead, it runs as a \"root\" of the cloud storage. Therefore, user may see the error message like \"mount helper error: fusermount: failed to open mountpoint for reading: Permission denied\". User could use /tmp space for mount point because that space is accessible for all users. Users should be very careful to open the permission to others for the purpose of using rclone mount. (2) The \"rclone mount\" users should unmount it after use using \"fusermount -u \\<endpoint_dir>\". Note that sometimes the endpoint is not unmount from some nodes due to timeout or some reason, you may see the message like \"Transport endpoint is not connected\" when accessing the endpoint directory on the node. Just manually unmount it again should resolve the issue. Note When use \"rclone config\" command to configure your cloud storage on HPCC, the command will guide you through an interactive setup process. At the step of auto config, after you chose \"y\", it will start authentication. You will see something like: 1 2 3 4 If your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth Log in and authorize rclone for access Waiting for code... At this time, a firefox browser would be open. If you did not get the browser window, check if you used -X option to allow X11 forwarding when you run ssh. You may follow the instruction at [Connect to HPCC System to get the display right. It will take a few minutes to get the browser open and connected. Please be patient. If the browser window is open but does not open the authentication page, you could manually input the link provided by the \"rclone config\" command to the firefox browser's url address box to connect to the site. DO NOT use the link on your personal computer's browser. The authentication have to use the browser on HPCC development node.","title":"Using Rclone"},{"location":"Rclone_-_rsync_for_cloud_storage/#rclone-rsync-for-cloud-storage","text":"Users could use this software to copy files from/to their Microsoft OneDrive or Google Drive cloud storage to/from HPCC disk space. This tool could also mount their cloud storage to HPCC disk so that the storage on cloud could be used as extended disk space. Rclone is installed on HPCC system wide. To use it, users should first load the software module into their environment using command \"module load Rclone\". For more details of using rclone, users can visit Rclone web site at https://rclone.org/ . To start using it, user should run command \"rclone config\" to configure it. The instructions of this command could be found at https://rclone.org/commands/rclone_config/ . Specifically, to configure for Google Drive, see https://rclone.org/drive/ , and to configure for Microsoft Onedrive, see https://rclone.org/onedrive/ for instructions. The specific details of how to start using this software on HPCC could be found in the document Rclone.pdf After successfully configured, users should be able to use \"rclone\" command to copy or mount the cloud storage to HPCC. There many rclone commands could be used to handle the file transfer and manage files on HPCC and cloud storage. To get help, use \"rclone --help\" as show in the following 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 [ hpc@dev-intel16-k80 ~ ] $ module load Rclone [ hpc@dev-intel16-k80 ~ ] $ rclone --help Rclone syncs files to and from cloud storage providers as well as mounting them, listing them in lots of different ways. See the home page ( https://rclone.org/ ) for installation, usage, documentation, changelog and configuration walkthroughs. Usage: rclone [ flags ] rclone [ command ] Available Commands: about Get quota information from the remote. authorize Remote authorization. cachestats Print cache stats for a remote cat Concatenates any files and sends them to stdout. check Checks the files in the source and destination match. cleanup Clean up the remote if possible config Enter an interactive configuration session. copy Copy files from source to dest, skipping already copied copyto Copy files from source to dest, skipping already copied copyurl Copy url content to dest. cryptcheck Cryptcheck checks the integrity of a crypted remote. cryptdecode Cryptdecode returns unencrypted file names. dbhashsum Produces a Dropbox hash file for all the objects in the path. dedupe Interactively find duplicate files and delete/rename them. delete Remove the contents of path. deletefile Remove a single file from remote. genautocomplete Output completion script for a given shell. gendocs Output markdown docs for rclone to the directory supplied. hashsum Produces an hashsum file for all the objects in the path. help Show help for rclone commands, flags and backends. link Generate public link to file/folder. listremotes List all the remotes in the config file. ls List the objects in the path with size and path. lsd List all directories/containers/buckets in the path. lsf List directories and objects in remote:path formatted for parsing lsjson List directories and objects in the path in JSON format. lsl List the objects in path with modification time, size and path. md5sum Produces an md5sum file for all the objects in the path. mkdir Make the path if it does not already exist. mount Mount the remote as file system on a mountpoint. move Move files from source to dest. moveto Move file or directory from source to dest. ncdu Explore a remote with a text based user interface. obscure Obscure password for use in the rclone.conf purge Remove the path and all of its contents. rc Run a command against a running rclone. rcat Copies standard input to file on remote. rcd Run rclone listening to remote control commands only. rmdir Remove the path if empty. rmdirs Remove empty directories under the path. serve Serve a remote over a protocol. settier Changes storage class/tier of objects in remote. sha1sum Produces an sha1sum file for all the objects in the path. size Prints the total size and number of objects in remote:path. sync Make source and dest identical, modifying destination only. touch Create new file or change file modification time. tree List the contents of the remote in a tree like fashion. version Show the version number. Use \"rclone [command] --help\" for more information about a command. Use \"rclone help flags\" for to see the global flags. Use \"rclone help backends\" for a list of supported services. [ hpc@dev-intel16-k80 ~ ] $ A tool \"cloudSync\" is developed for user to synchronize the files between their cloud storages. It is accessible through \"powertools\". To use it, users need to have the module \"powertools\" loaded. Users are welcome to try it and report any problems to us via contact form here . Following are a few examples of running rclone commands after successfully configured the cloud storage. Assume that the cloud storage is configured as the name \"MyOneDrive\". (1) see current remote storage: As show, we can see that there are currently two remote cloud storage, \"MyOneDrive\" and \"googledoc\" are configured. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [ user@dev-intel18 ~ ] $ rclone config Current remotes: Name Type ==== ==== MyOneDrive onedrive googledoc drive e ) Edit existing remote n ) New remote d ) Delete remote r ) Rename remote c ) Copy remote s ) Set configuration password q ) Quit config e/n/d/r/c/s/q> q [ user@dev-intel18 ~ ] $ (2) Check the remote storage information: We could check the remote storage usage and quota using \"rclone about\" command. 1 2 3 4 5 [ user@dev-intel16-k80 ~ ] $ rclone about MyOneDrive: Total: 5T Used: 450 .999M Free: 4 .998T Trashed: 404 .576k (3) List the contents of the cloud storage 1 2 3 4 5 6 [ user@dev-intel18 ~ ] $ rclone lsd MyOneDrive: -1 2018 -02-02 08 :57:54 0 Attachments -1 2019 -08-27 15 :43:33 1 IMAGES -1 2019 -08-22 15 :50:10 42 Matlab -1 2019 -02-26 17 :12:01 16 Microsoft Teams Chat Files -1 2018 -08-24 08 :56:32 1 Notebooks (4) Copy files on HPCC to remote cloud: 1 2 3 4 5 6 7 8 9 10 11 12 [ user@dev-intel18 ~ ] $ rclone copy Project MyOneDrive:Project # copy the content of directory \"Project\" to remote cloud storage [ user@dev-intel18 ~ ] $ rclone lsd MyOneDrive: # view the contents of cloud storage to confirm the copy -1 2018 -02-02 08 :57:54 0 Attachments -1 2019 -08-27 15 :43:33 1 IMPACT -1 2019 -08-22 15 :50:10 42 Matlab -1 2019 -02-26 17 :12:01 16 Microsoft Teams Chat Files -1 2018 -08-24 08 :56:32 1 Notebooks -1 2020 -04-27 15 :43:25 2 Project [ user@dev-intel18 ~ ] $ rclone lsd MyOneDrive:Project -1 2020 -04-27 15 :44:39 1 GPAW -1 2020 -04-27 15 :43:26 3 MATLAB (5) Copy files on cloud storage to HPCC: 1 2 3 4 5 [ user@dev-intel18 Project ] $ ls # current content of Project directory before copy GPAW MATLAB [ user@dev-intel18 Project ] $ rclone copy MyOneDrive:IMPACT ./ # copy the content of IMPACT in cloud to current directory [ user@dev-intel18 Project ] $ ls # confirm that the copy is done GPAW impact_run MATLAB Note Although \"rclone copy\" is similar as unix commands rsync and cp, when using it, users should be aware of the differences and know the details of its behavior. (1) \"rclone copy\" does not transfer unchanged files, testing by size and modification time or MD5SUM. In this sense, it is similar as linux command rsync; (2) When running \" rclone copy source:sourcepath dest:destpath\", if source:sourcepath is a directory, dest:destpath should also be a directory. It does not copy the directory source:sourcepath , instead, it will copy the content of the directory source:sourcepath to the destination dest:destpath . If dest:destpath does not exist, it will be created and the content of source:sourcepath will be stored in it. (3) \"rclone copyto\" is a very similar rclone command to \"rclone copy\". The only difference is that it can be used to upload single files to other than their current name. When running \" rclone copyto source:sourcepath dest:destpath\", if source:sourcepath is a file, dest:destpath c ould be a new file name. If source:sourcepath is a directory, it would be the same as using \" rclone copy\". (6) Checks the files in the source and destination match. 1 2 3 [ user@dev-intel18 Project ] $ rclone check impact_run MyOneDrive:IMPACT/impact_run # check if it is matched both sides 2020 /04/27 16 :19:01 NOTICE: One drive root 'IMPACT/impact_run' : 0 differences found 2020 /04/27 16 :19:01 NOTICE: One drive root 'IMPACT/impact_run' : 21 matching files Note For archiving your files to your cloud storage, if the connection between HPCC and your cloud storage is not stable, we would NOT recommend using \"rclone move\" because it may loss the data during the transfer. Instead, we recommend using \"rclone copy\" to successfully copy the files over and run \"rclone check\" to check if files are identical. After that, it is safe to delete local copy of the files. Note When use \"rclone mount\" command to mount your cloud storage to HPCC, there are two things users should be careful: (1) When running rclone mount, the process runs NOT as the user, instead, it runs as a \"root\" of the cloud storage. Therefore, user may see the error message like \"mount helper error: fusermount: failed to open mountpoint for reading: Permission denied\". User could use /tmp space for mount point because that space is accessible for all users. Users should be very careful to open the permission to others for the purpose of using rclone mount. (2) The \"rclone mount\" users should unmount it after use using \"fusermount -u \\<endpoint_dir>\". Note that sometimes the endpoint is not unmount from some nodes due to timeout or some reason, you may see the message like \"Transport endpoint is not connected\" when accessing the endpoint directory on the node. Just manually unmount it again should resolve the issue. Note When use \"rclone config\" command to configure your cloud storage on HPCC, the command will guide you through an interactive setup process. At the step of auto config, after you chose \"y\", it will start authentication. You will see something like: 1 2 3 4 If your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth Log in and authorize rclone for access Waiting for code... At this time, a firefox browser would be open. If you did not get the browser window, check if you used -X option to allow X11 forwarding when you run ssh. You may follow the instruction at [Connect to HPCC System to get the display right. It will take a few minutes to get the browser open and connected. Please be patient. If the browser window is open but does not open the authentication page, you could manually input the link provided by the \"rclone config\" command to the firefox browser's url address box to connect to the site. DO NOT use the link on your personal computer's browser. The authentication have to use the browser on HPCC development node.","title":"Rclone - rsync for cloud storage"},{"location":"Regular_Expressions/","text":"Regular Expressions A regular expression is a powerful tool to match patterns. With this tool, you can validate text input, search/replace text within a file, batch rename files, test for patterns within strings etc. There are two types of regular expressions: the basic regular expressions (BRE), and the extended regular expressions (ERE). Most utilities (including vi, sed, and grep) use the basic regular expression. awk and egrep use the extended expression. There are three parts to a regular expression: anchors, character sets, and modifiers. Anchors are used to specify the position of the pattern in relation to a line of text. Character sets match one or more characters in a single position. Modifiers specify how many times the previous character set is repeated. The anchor characters: the starting anchor ^ and the end anchor $ \"^\" is the starting anchor, and the character \"$\" is the end anchor. The regular expression \"^A\" will match all lines that start with a capital A. The expression \"A$\" will match all lines that end with the capital A. The anchor characters works only if there are located in a proper location. Otherwise, they no longer act as anchors. For example, the \"^\" is only an anchor if it is the first character in a regular expression. The \"$\" is only an anchor if it is the last character. The expression \"$1\" and \"1^\"do not have an anchor. If you want to match a \"^\" at the beginning of the line, or a \"$\" at the end of a line, you must escape the special characters with a backslash. pattern Matches ^A A at the beginning of a line A$ A at the end of a line A^ A^ anywhere on a line $A $A anywhere on a line ^^ ^ at the begging of a line $$ $ at the end of a line Matching a character with character sets The regular expression \"the\" has three characters: \"t,\" \"h\" and \"e\". It will match any line with the string \"the\" inside it. However, it will also match the word \"there\" or \"them\". To prevent this, put spaces before and after the pattern as \" the \". You can combine the string with an anchor such as \"^HPCC\". Specifying a range of characters: [ ] If you want to match specific characters, you can use the square brackets to identify the exact characters you are searching for. The pattern that will match any line of text that contains exactly one number is \"^[0123456789]$\" You can use the hyphen between two characters to specify a range \"^[0-9]$\" You can intermix explicit characters with character ranges. This pattern will match a single character that is a letter, number, or underscore \"[A-Za-z0-9_]\". Exceptions in character sets: [ ] [^] matches a single character that is not contained within the brackets. For example, [^abc]matches any character other than \"a\", \"b\", or \"c\". [^a-z] matches any single character that is not a lowercase letter from \"a\" to \"z\". Likewise, literal characters and ranges can be mixed. To match all characters except vowels use \"[^aeiou]\". Regular Expression Matches [] The characters \"[]\" [0] The character \"0\" [0-9] Any number [^0-9] Any character other than a number [-0-9] Any number or a \"-\" [0-9-] Any number or a \"-\" [^-0-9] Any character except a number or a \"-\" []0-9] Any number or a \"]\" [0-9]] Any number followed by a \"]\" [0-9-z] Any number, or any character between \"9\" and \"z\". [0-9\\-a\\]] Any number, or a \"-\", a \"a\", or a \"]\" Wildcard: . A dot \".\" is a special meta-characters. It will match any character, except the end-of-line character. For example, the pattern that will match a line with a single characters is \" ^.$ \", and two characters is \"^..$\". You can use \" ...\\. \" to match the first three (wildcard) characters, and escape the final wildcard meta-character to match the period instead. Repeating character sets: * \"*\" matches the preceding element zero or more times. For example, ab*c matches \"ac\", \"abc\", \"abbbc\", etc. [xyz]* matches \"\", \"x\", \"y\", \"z\", \"zx\", \"zyx\", \"xyzzy\", and so on. (ab)*matches \"\", \"ab\", \"abab\", \"ababab\", and so on. Matching a specific number of sets: \\{ and \\} You cannot specify a maximum number of sets with the \"*\" modifier. There is a special pattern you can use to specify the minimum and maximum number of repeats. This is done by putting those two numbers between \"\\{\" and \"\\}\". \"\\{m, n\\}\" matches the preceding element at least m and not more than n times. For example, a\\{3,5\\}matches only \"aaa\", \"aaaa\", and \"aaaaa\" (the extended regular expression is {m, n}). Another example is [a-z]\\{4,8\\} which matches 4, 5, 6, 7 or 8 lower case letters. more examples Regular expression matches .og any three-character string ending with \"og\", including \"dog\", \"fog\", and \"hog\". [df]og \"dog\" and \"fog\". [^d]og all strings matched by .at except \"dog\". [^df]og all strings matched by .at other than \"dog\" and \"fog\". ^[df]og \"dog\" and \"fog\", but only at the beginning of the string or line. [df]og$ \"dog\" and \"fog\", but only at the end of the string or line. \\[.\\] any single character surrounded by \"[\" and \"]\" since the brackets are escaped, for example: \"[a]\" and \"[b]\". b.* b followed by zero or more characters, for example: \"b\" and \"boy\" and \"bowl\". Extended regular expressions egrep and awk use the extended regular expressions. In extended extensions, special characters preceded by a backslash or period no longer have the special meaning as well as '\\digit'. Therefore, \\{\u2026\\} becomes {\u2026} and \\(\u2026\\) becomes (\u2026). Examples: \"[hc]+at\" matches with \"hat\", \"cat\", \"hhat\", \"chat\", \"hcat\", \"ccchat\" etc \"[hc]?at\" matches \"hat\", \"cat\" and \"at\" \"([cC]at)|([dD]og)\" matches \"cat\", \"Cat\", \"dog\" and \"Dog\" The characters (,),[,],.,*,?,+,|,^ and $ are special symbols and have to be escaped with a backslash symbol in order to be treated as literal characters. For example: \"a\\.(\\(|\\))\" matches with the string \"a.)\" or \"a.(\" Modern regular expression tools allow a quantifier to be specified as non-greedy, by putting a question mark after the quantifier: (\\[\\[.*?\\]\\]). BRE ERE matches \\( \\) ( ) a marked subexpression. The string matched within the parentheses can be recalled later. \\+ + the preceding element one or more times. \\? ? the preceding element one or zero times. \\| | the preceding element or the following element. \\{m, n\\} {m, n} the preceding element at least m and not more than n times. \\{m\\} {m} the preceding element exactly m times. \\{m,\\} {m,} the preceding element at least m times. \\{,n\\} {,n} the preceding element not more than n times. examples BRE ERE matched results \\(ab\\)* (ab)* \"\", \"ab\", \"abab\", \"ababab\" etc. ab\\+c ab+c \"abc\", \"abbbc\", etc, but not \"ac\". [xyz]|+ xyz+ \"x\", \"y\", \"z\", \"zx\", \"zyx\", \"xyzzy\", etc. \\(ab\\) (ab)+ \"ab\", \"abab\", \"ababab\" etc. ab\\?c ab?c \"ac\" or \"abc\". \\(ab\\)\\? (ab)? \"\" or \"ab\". abc\\|def abc|def \"abc\" or \"def\". a\\{3,5\\} a{3,5} \"aaa\", \"aaaa\", and \"aaaaa\". ba\\{,2\\}b ba{,2}b \"bb\", \"bab\", \"baab\". POXIS character sets POXIS has added newer and more convenient ways to search for character sets. For example, you can use [:upper:] instead of [A-Z]. In fact, [A-Z] can be different based on LC_COLLATE value. For further discussion, check here . In HPC at MSU, the default of [A-Z] is a, A, b, B, c, C, ....y, Y, z, Z, which is standard collations (en_US). You can use [[:upper:]] instead of [:upper:]. You can mix the old style and POSIX styles, such as [1-9[:upper:]]. Expression matches [:alnum:] Alphanumeric [:alpha:] Alphabetic [:blank:] Whitespace, tabs, etc [:cntrl:] Control character [:digit:] digit [:graph:] Printable and visible characters [:lower:] Lower case character [:print:] Printable character [:punct:] Punctuation [:space:] Whitespace [:upper:] Upper case character [:xdigit:] Extended digit","title":"Regular Expressions"},{"location":"Regular_Expressions/#regular-expressions","text":"A regular expression is a powerful tool to match patterns. With this tool, you can validate text input, search/replace text within a file, batch rename files, test for patterns within strings etc. There are two types of regular expressions: the basic regular expressions (BRE), and the extended regular expressions (ERE). Most utilities (including vi, sed, and grep) use the basic regular expression. awk and egrep use the extended expression. There are three parts to a regular expression: anchors, character sets, and modifiers. Anchors are used to specify the position of the pattern in relation to a line of text. Character sets match one or more characters in a single position. Modifiers specify how many times the previous character set is repeated.","title":"Regular Expressions"},{"location":"Regular_Expressions/#the-anchor-characters-the-starting-anchor-and-the-end-anchor","text":"\"^\" is the starting anchor, and the character \"$\" is the end anchor. The regular expression \"^A\" will match all lines that start with a capital A. The expression \"A$\" will match all lines that end with the capital A. The anchor characters works only if there are located in a proper location. Otherwise, they no longer act as anchors. For example, the \"^\" is only an anchor if it is the first character in a regular expression. The \"$\" is only an anchor if it is the last character. The expression \"$1\" and \"1^\"do not have an anchor. If you want to match a \"^\" at the beginning of the line, or a \"$\" at the end of a line, you must escape the special characters with a backslash. pattern Matches ^A A at the beginning of a line A$ A at the end of a line A^ A^ anywhere on a line $A $A anywhere on a line ^^ ^ at the begging of a line $$ $ at the end of a line","title":"The anchor characters: the starting anchor ^ and the end anchor $"},{"location":"Regular_Expressions/#matching-a-character-with-character-sets","text":"The regular expression \"the\" has three characters: \"t,\" \"h\" and \"e\". It will match any line with the string \"the\" inside it. However, it will also match the word \"there\" or \"them\". To prevent this, put spaces before and after the pattern as \" the \". You can combine the string with an anchor such as \"^HPCC\".","title":"Matching a character with character sets"},{"location":"Regular_Expressions/#specifying-a-range-of-characters","text":"If you want to match specific characters, you can use the square brackets to identify the exact characters you are searching for. The pattern that will match any line of text that contains exactly one number is \"^[0123456789]$\" You can use the hyphen between two characters to specify a range \"^[0-9]$\" You can intermix explicit characters with character ranges. This pattern will match a single character that is a letter, number, or underscore \"[A-Za-z0-9_]\".","title":"Specifying a range of characters: [ ]"},{"location":"Regular_Expressions/#exceptions-in-character-sets","text":"[^] matches a single character that is not contained within the brackets. For example, [^abc]matches any character other than \"a\", \"b\", or \"c\". [^a-z] matches any single character that is not a lowercase letter from \"a\" to \"z\". Likewise, literal characters and ranges can be mixed. To match all characters except vowels use \"[^aeiou]\". Regular Expression Matches [] The characters \"[]\" [0] The character \"0\" [0-9] Any number [^0-9] Any character other than a number [-0-9] Any number or a \"-\" [0-9-] Any number or a \"-\" [^-0-9] Any character except a number or a \"-\" []0-9] Any number or a \"]\" [0-9]] Any number followed by a \"]\" [0-9-z] Any number, or any character between \"9\" and \"z\". [0-9\\-a\\]] Any number, or a \"-\", a \"a\", or a \"]\"","title":"Exceptions in character sets: [ ]"},{"location":"Regular_Expressions/#wildcard","text":"A dot \".\" is a special meta-characters. It will match any character, except the end-of-line character. For example, the pattern that will match a line with a single characters is \" ^.$ \", and two characters is \"^..$\". You can use \" ...\\. \" to match the first three (wildcard) characters, and escape the final wildcard meta-character to match the period instead.","title":"Wildcard: ."},{"location":"Regular_Expressions/#repeating-character-sets","text":"\"*\" matches the preceding element zero or more times. For example, ab*c matches \"ac\", \"abc\", \"abbbc\", etc. [xyz]* matches \"\", \"x\", \"y\", \"z\", \"zx\", \"zyx\", \"xyzzy\", and so on. (ab)*matches \"\", \"ab\", \"abab\", \"ababab\", and so on.","title":"Repeating character sets: *"},{"location":"Regular_Expressions/#matching-a-specific-number-of-sets-and","text":"You cannot specify a maximum number of sets with the \"*\" modifier. There is a special pattern you can use to specify the minimum and maximum number of repeats. This is done by putting those two numbers between \"\\{\" and \"\\}\". \"\\{m, n\\}\" matches the preceding element at least m and not more than n times. For example, a\\{3,5\\}matches only \"aaa\", \"aaaa\", and \"aaaaa\" (the extended regular expression is {m, n}). Another example is [a-z]\\{4,8\\} which matches 4, 5, 6, 7 or 8 lower case letters.","title":"Matching a specific number of sets: \\{ and \\}"},{"location":"Regular_Expressions/#more-examples","text":"Regular expression matches .og any three-character string ending with \"og\", including \"dog\", \"fog\", and \"hog\". [df]og \"dog\" and \"fog\". [^d]og all strings matched by .at except \"dog\". [^df]og all strings matched by .at other than \"dog\" and \"fog\". ^[df]og \"dog\" and \"fog\", but only at the beginning of the string or line. [df]og$ \"dog\" and \"fog\", but only at the end of the string or line. \\[.\\] any single character surrounded by \"[\" and \"]\" since the brackets are escaped, for example: \"[a]\" and \"[b]\". b.* b followed by zero or more characters, for example: \"b\" and \"boy\" and \"bowl\".","title":"more examples"},{"location":"Regular_Expressions/#extended-regular-expressions","text":"egrep and awk use the extended regular expressions. In extended extensions, special characters preceded by a backslash or period no longer have the special meaning as well as '\\digit'. Therefore, \\{\u2026\\} becomes {\u2026} and \\(\u2026\\) becomes (\u2026). Examples: \"[hc]+at\" matches with \"hat\", \"cat\", \"hhat\", \"chat\", \"hcat\", \"ccchat\" etc \"[hc]?at\" matches \"hat\", \"cat\" and \"at\" \"([cC]at)|([dD]og)\" matches \"cat\", \"Cat\", \"dog\" and \"Dog\" The characters (,),[,],.,*,?,+,|,^ and $ are special symbols and have to be escaped with a backslash symbol in order to be treated as literal characters. For example: \"a\\.(\\(|\\))\" matches with the string \"a.)\" or \"a.(\" Modern regular expression tools allow a quantifier to be specified as non-greedy, by putting a question mark after the quantifier: (\\[\\[.*?\\]\\]). BRE ERE matches \\( \\) ( ) a marked subexpression. The string matched within the parentheses can be recalled later. \\+ + the preceding element one or more times. \\? ? the preceding element one or zero times. \\| | the preceding element or the following element. \\{m, n\\} {m, n} the preceding element at least m and not more than n times. \\{m\\} {m} the preceding element exactly m times. \\{m,\\} {m,} the preceding element at least m times. \\{,n\\} {,n} the preceding element not more than n times.","title":"Extended regular expressions"},{"location":"Regular_Expressions/#examples","text":"BRE ERE matched results \\(ab\\)* (ab)* \"\", \"ab\", \"abab\", \"ababab\" etc. ab\\+c ab+c \"abc\", \"abbbc\", etc, but not \"ac\". [xyz]|+ xyz+ \"x\", \"y\", \"z\", \"zx\", \"zyx\", \"xyzzy\", etc. \\(ab\\) (ab)+ \"ab\", \"abab\", \"ababab\" etc. ab\\?c ab?c \"ac\" or \"abc\". \\(ab\\)\\? (ab)? \"\" or \"ab\". abc\\|def abc|def \"abc\" or \"def\". a\\{3,5\\} a{3,5} \"aaa\", \"aaaa\", and \"aaaaa\". ba\\{,2\\}b ba{,2}b \"bb\", \"bab\", \"baab\".","title":"examples"},{"location":"Regular_Expressions/#poxis-character-sets","text":"POXIS has added newer and more convenient ways to search for character sets. For example, you can use [:upper:] instead of [A-Z]. In fact, [A-Z] can be different based on LC_COLLATE value. For further discussion, check here . In HPC at MSU, the default of [A-Z] is a, A, b, B, c, C, ....y, Y, z, Z, which is standard collations (en_US). You can use [[:upper:]] instead of [:upper:]. You can mix the old style and POSIX styles, such as [1-9[:upper:]]. Expression matches [:alnum:] Alphanumeric [:alpha:] Alphabetic [:blank:] Whitespace, tabs, etc [:cntrl:] Control character [:digit:] digit [:graph:] Printable and visible characters [:lower:] Lower case character [:print:] Printable character [:punct:] Punctuation [:space:] Whitespace [:upper:] Upper case character [:xdigit:] Extended digit","title":"POXIS character sets"},{"location":"Research_Space/","text":"Research Space A research group's central directory, or research space , is a shared directory established by a MSU principal investigator (PI) for use by the members of the PI's research group. To create a research space the PI must submit a Research Request form. The initial limit on stroage is 50GB and the initial limit on the number of files conatined in a research space is 1,000,000 files . A PI may request an increase in storage of up to 1TB of space at no cost by submitting a Quota Increase Request form or an increase beyond 1TB for an annual fee by submitting a Large Quota Increase Request form. Use the quota command to check a research group's current space and file quotas. 1 2 3 4 5 $ quota Research Groups: Space Space Space Space Files Files Files Files Quota Used Available % Used Quota Used Available % Used ----------------------------------------------------------------------------------------------- <groupname> 4096G 3733G 363G 91 % 2097152 432525 1664627 21 % The group's research space is associated with an assigned group name and is located at /mnt/research/<groupname> by default. It is accessible to all users who have been added to the group by the PI. Memebers of the group must set their user file mode creation mask and file permissions correctly such that the other group members have the appropriate access to the shared files in the research space. See the section Using a research space for more details. All research space files are periodically, automatically backed up (except those files that a group has opted to store in a specially requested nodr space). To access file backups, please submit a help ticket containing the file paths and the period i.e. , the time frame, from which the files should be restored. Using a research space To configure a research space such that all group members have the appropriate access to the files and directories contained within, it is important to read and follow the instructions below. 1. Ensure that all directories created in a research space have the group ownership set to the <groupname> and the set-group-ID (setgid) bit set to s in the place of the executable bit on the group sector. By default, the group's research space /mnt/research/<groupname> is set with the correct group ownership and setgid bit. To check the group ownership and setgid bit of a directory within the research space use the ls -ld <path/to/directory> command. 1 2 $ ls -ld /mnt/research/<groupname>/<subdirectory> drwxrws--- 9 <username> <groupname> 8192 Jul 22 08 :38 /mnt/research/<groupname>/<subdirectory> The letter s in the permissions drwxrws--- of the directory is the setgid bit which makes newly created sub-directories and files within inherit the group ownership of the parent directory rather than the primary group of the individual user. See the page File Permissions on HPCC for more details. If the settings of a sub-directory are not set correctly, the group may encounter a Disk quota exceeded error when creating or copying files. See the section Quotas on a research space for more details. Use the folliwng commands to ensure all directories and files in the group's research space have the proper settings: 1 2 $ find /mnt/research/<groupname>/ -not -group <groupname> -print0 | xargs -0 chgrp <groupname> $ find /mnt/research/<groupname>/ -type d -print0 | xargs -0 chmod g+s 2. Do not use the mv command or the -p option when using the cp command to transfer files into the group's research space directories. Both mv <filename> and cp -p <filename> may preserve an undesired group ownership attribute even when transfered into a research space directory with ownership and permissions configured correctly. 3. Use the rsync --chmod=Dg+s command to transfer files from a local machine to the HPCC research space. For example, use the command 1 $ rsync -avz testdir --chmod = Dg+s <username>@rsync.hpcc.msu.edu:/mnt/research/<groupname>/ to transfer a directory testdir from your local machine to the HPCC research space /mnt/research/<groupname> . This will automatically configure the tranfered directory with the setgid bit. 4. During a bash session set the user file mode creation mask to 0007 or any lower value . Use the umask command to set the file mode creation mask e.g. , 1 $ umask 0007 For the duration of the seesion, files and directories created by the user are now readable, writable and executable for all members of the research group. Alternatively, a user may run the following Powertools command one time to add the line umask 0002 to the user's .bashrc file: 1 2 $ module load powertools $ umask_in_bash 5. Ensure that each group member's currently active group ID is set to the research group of interest. Each time a group member logs in to the HPCC that user's group ID is set to the primary group ID assigned by default when a user's account was created. However, a user may be added to other research groups at the request of that group's PI. After a login, the user may then toggle between group memberships using the newgrp <groupname> command when needed. For more information, refer to the Change Primary Group page. A user may request that the primary group ID be changed from the default setting by submitting a request via a help ticket . Quotas on a research space The space and file quotas on an research space are claculated by matching the group ownership settings of files stored on the HPCC to that of the research space group name. Hence, a user may not create files larger than 8 MB in a specified research space with a group ownership atribute different from that of the research space. Attempting to do so will likely result in a Disk quota exceeded error even though use of the quota command indicates that the reseach space quotas have not been exceeded. To resolve the Disk quota exceeded error in the absence of actual quota violations, users should follow the instructions in the section Using a research space to ensure that: The directory into which the files will be transfered has the same group ownership as the research space and the set-group-ID bit If the file already exists, its group ownership has been changed to that of the research space If the file is to be created, the primary group of the user creating the file is set to that of the research space","title":"Research space"},{"location":"Research_Space/#research-space","text":"A research group's central directory, or research space , is a shared directory established by a MSU principal investigator (PI) for use by the members of the PI's research group. To create a research space the PI must submit a Research Request form. The initial limit on stroage is 50GB and the initial limit on the number of files conatined in a research space is 1,000,000 files . A PI may request an increase in storage of up to 1TB of space at no cost by submitting a Quota Increase Request form or an increase beyond 1TB for an annual fee by submitting a Large Quota Increase Request form. Use the quota command to check a research group's current space and file quotas. 1 2 3 4 5 $ quota Research Groups: Space Space Space Space Files Files Files Files Quota Used Available % Used Quota Used Available % Used ----------------------------------------------------------------------------------------------- <groupname> 4096G 3733G 363G 91 % 2097152 432525 1664627 21 % The group's research space is associated with an assigned group name and is located at /mnt/research/<groupname> by default. It is accessible to all users who have been added to the group by the PI. Memebers of the group must set their user file mode creation mask and file permissions correctly such that the other group members have the appropriate access to the shared files in the research space. See the section Using a research space for more details. All research space files are periodically, automatically backed up (except those files that a group has opted to store in a specially requested nodr space). To access file backups, please submit a help ticket containing the file paths and the period i.e. , the time frame, from which the files should be restored.","title":"Research Space"},{"location":"Research_Space/#using-a-research-space","text":"To configure a research space such that all group members have the appropriate access to the files and directories contained within, it is important to read and follow the instructions below. 1. Ensure that all directories created in a research space have the group ownership set to the <groupname> and the set-group-ID (setgid) bit set to s in the place of the executable bit on the group sector. By default, the group's research space /mnt/research/<groupname> is set with the correct group ownership and setgid bit. To check the group ownership and setgid bit of a directory within the research space use the ls -ld <path/to/directory> command. 1 2 $ ls -ld /mnt/research/<groupname>/<subdirectory> drwxrws--- 9 <username> <groupname> 8192 Jul 22 08 :38 /mnt/research/<groupname>/<subdirectory> The letter s in the permissions drwxrws--- of the directory is the setgid bit which makes newly created sub-directories and files within inherit the group ownership of the parent directory rather than the primary group of the individual user. See the page File Permissions on HPCC for more details. If the settings of a sub-directory are not set correctly, the group may encounter a Disk quota exceeded error when creating or copying files. See the section Quotas on a research space for more details. Use the folliwng commands to ensure all directories and files in the group's research space have the proper settings: 1 2 $ find /mnt/research/<groupname>/ -not -group <groupname> -print0 | xargs -0 chgrp <groupname> $ find /mnt/research/<groupname>/ -type d -print0 | xargs -0 chmod g+s 2. Do not use the mv command or the -p option when using the cp command to transfer files into the group's research space directories. Both mv <filename> and cp -p <filename> may preserve an undesired group ownership attribute even when transfered into a research space directory with ownership and permissions configured correctly. 3. Use the rsync --chmod=Dg+s command to transfer files from a local machine to the HPCC research space. For example, use the command 1 $ rsync -avz testdir --chmod = Dg+s <username>@rsync.hpcc.msu.edu:/mnt/research/<groupname>/ to transfer a directory testdir from your local machine to the HPCC research space /mnt/research/<groupname> . This will automatically configure the tranfered directory with the setgid bit. 4. During a bash session set the user file mode creation mask to 0007 or any lower value . Use the umask command to set the file mode creation mask e.g. , 1 $ umask 0007 For the duration of the seesion, files and directories created by the user are now readable, writable and executable for all members of the research group. Alternatively, a user may run the following Powertools command one time to add the line umask 0002 to the user's .bashrc file: 1 2 $ module load powertools $ umask_in_bash 5. Ensure that each group member's currently active group ID is set to the research group of interest. Each time a group member logs in to the HPCC that user's group ID is set to the primary group ID assigned by default when a user's account was created. However, a user may be added to other research groups at the request of that group's PI. After a login, the user may then toggle between group memberships using the newgrp <groupname> command when needed. For more information, refer to the Change Primary Group page. A user may request that the primary group ID be changed from the default setting by submitting a request via a help ticket .","title":"Using a research space"},{"location":"Research_Space/#quotas-on-a-research-space","text":"The space and file quotas on an research space are claculated by matching the group ownership settings of files stored on the HPCC to that of the research space group name. Hence, a user may not create files larger than 8 MB in a specified research space with a group ownership atribute different from that of the research space. Attempting to do so will likely result in a Disk quota exceeded error even though use of the quota command indicates that the reseach space quotas have not been exceeded. To resolve the Disk quota exceeded error in the absence of actual quota violations, users should follow the instructions in the section Using a research space to ensure that: The directory into which the files will be transfered has the same group ownership as the research space and the set-group-ID bit If the file already exists, its group ownership has been changed to that of the research space If the file is to be created, the primary group of the user creating the file is set to that of the research space","title":"Quotas on a research space"},{"location":"Restore_Files_from_Back-Up/","text":"Restore Files from Back-Up If you need files restored, please submit a ticket. Please mention the paths of the folders and file names with the time frame you would like to be restored.","title":"Restore Files from Back-Up"},{"location":"Restore_Files_from_Back-Up/#restore-files-from-back-up","text":"If you need files restored, please submit a ticket. Please mention the paths of the folders and file names with the time frame you would like to be restored.","title":"Restore Files from Back-Up"},{"location":"Running-multiple-jobs-sequentially_34963786.html/","text":"Teaching : Running multiple jobs sequentially Create a python script (python_script.py)with the following code. python_script.py 1 print(\"Hello, World!\") Create an R script (r_script.R)with the following code. r_script.R 1 2 3 4 5 z=rnorm(10000,mean=10,sd=2) mean(z) sd(z) pdf(file=\"r_histogram.pdf\") hist(z,freq=FALSE,nclass=100) Make a copy of hello.sb and name it multi_seq.sb Edit multi_seq.sb to run the tasks, python_script.py and r_script.r : Request resources Load the required modules, sequentially run the two scripts Submit job to compute node Answer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #Make a copy of hello.sb and name it multi_seq.sb cp hello.sb multi_seq.sb #Using your favorite editor(nano, vi, emacs, gedit, etc.), #make the following edits to the multi_seq.sb script to run the tasks, python_script.py and r_script.r simultaneously: gedit multi_seq.sb #Request resources: Typically you'll need to request the largest number of nodes needed for each task. Since each task above only uses one node and one core there is no change to the number of nodes or cores. #Load the required modules module load R/3.5.0-X11-20180131 module load python #sequentially run the two scripts python3 python_script.py Rscript r_script.R #Submit job to compute node: Execute the following at the command line sbatch multi_seq.sb","title":"Running multiple jobs sequentially 34963786.html"},{"location":"Running-multiple-jobs-sequentially_34963786.html/#teaching-running-multiple-jobs-sequentially","text":"Create a python script (python_script.py)with the following code. python_script.py 1 print(\"Hello, World!\") Create an R script (r_script.R)with the following code. r_script.R 1 2 3 4 5 z=rnorm(10000,mean=10,sd=2) mean(z) sd(z) pdf(file=\"r_histogram.pdf\") hist(z,freq=FALSE,nclass=100) Make a copy of hello.sb and name it multi_seq.sb Edit multi_seq.sb to run the tasks, python_script.py and r_script.r : Request resources Load the required modules, sequentially run the two scripts Submit job to compute node Answer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #Make a copy of hello.sb and name it multi_seq.sb cp hello.sb multi_seq.sb #Using your favorite editor(nano, vi, emacs, gedit, etc.), #make the following edits to the multi_seq.sb script to run the tasks, python_script.py and r_script.r simultaneously: gedit multi_seq.sb #Request resources: Typically you'll need to request the largest number of nodes needed for each task. Since each task above only uses one node and one core there is no change to the number of nodes or cores. #Load the required modules module load R/3.5.0-X11-20180131 module load python #sequentially run the two scripts python3 python_script.py Rscript r_script.R #Submit job to compute node: Execute the following at the command line sbatch multi_seq.sb","title":"Teaching : Running multiple jobs sequentially"},{"location":"Running_Gaussian_by_Command_Lines/","text":"Running Gaussian by Command Lines Here we are going to do an geometry optimization calculation on a simple molecule Formamide (HCONH 2 ). Users can use the Gaussian input file g16.com : g16.com 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 %NProcShared = 4 %Mem = 5GB %NoSave %chk = g16.chk # opt freq b3lyp/cc-pvdz Title Card: Single Molecule Formamide 0 1 C 3 .89594917 -4.10509404 -0.06119675 O 5 .13960552 -4.14687675 0 .12626969 H 3 .41099598 -3.16562892 -0.22589552 N 3 .10941607 -5.34695260 -0.05391726 H 2 .79423275 -5.53644967 0 .87600227 H 2 .31994463 -5.24505745 -0.65918762 In the file, the % lines (Link 0 section) specify the system resources. %NprocShared gives how many CPUs to use in a node and %Mem indicates how much memory to use. If any file specified before the %NoSave line, it will not be saved once Gaussian finishes the calculation normally. %chk specify a check point file name to save and # line (Route section) specify the methods of Gaussian calculations. You can give this Gaussian input a title name in Title Card section. After that, use Molecule Specification section to assign the coordinates of the atoms with the charge and spin multiplicity of the system in the first line. Please make sure there is at least one empty line in the end of file . By logging into HPCC and ssh to a dev node, users can find Gaussian program installed in HPCC with the module commands: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ module spider Gaussian ---------------------------- Gaussian: ---------------------------- Versions: Gaussian/g16_AVX Gaussian/g16-AVX2 Gaussian/g16 ---------------------------- For detailed information about a specific \"Gaussian\" module (including how to load the modules) use the module's full name. For example: $ module spider Gaussian/g16 ---------------------------- Please load Gaussian module with g16 version: 1 2 3 $ module load Gaussian/g16 $ which g16 /opt/software/Gaussian/g16-AVX/g16/g16 and the Gaussian command \"g16\" can be used. Simply run the command by giving the input and output file names: 1 $ g16 < g16.com > g16.log and it starts to calculate the system on the node. It will take about 2 minutes to complete the calculation. After it is finished, you can check the output file g16.log . All dev nodes have 2-hour CPU limit. Please restrict your resource usage on them. Users can also use GaussView to create their molecular system and do calculations. To use GaussView, just run the command gview after loading the Gaussian module.","title":"Running Gaussian from command line"},{"location":"Running_Gaussian_by_Command_Lines/#running-gaussian-by-command-lines","text":"Here we are going to do an geometry optimization calculation on a simple molecule Formamide (HCONH 2 ). Users can use the Gaussian input file g16.com : g16.com 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 %NProcShared = 4 %Mem = 5GB %NoSave %chk = g16.chk # opt freq b3lyp/cc-pvdz Title Card: Single Molecule Formamide 0 1 C 3 .89594917 -4.10509404 -0.06119675 O 5 .13960552 -4.14687675 0 .12626969 H 3 .41099598 -3.16562892 -0.22589552 N 3 .10941607 -5.34695260 -0.05391726 H 2 .79423275 -5.53644967 0 .87600227 H 2 .31994463 -5.24505745 -0.65918762 In the file, the % lines (Link 0 section) specify the system resources. %NprocShared gives how many CPUs to use in a node and %Mem indicates how much memory to use. If any file specified before the %NoSave line, it will not be saved once Gaussian finishes the calculation normally. %chk specify a check point file name to save and # line (Route section) specify the methods of Gaussian calculations. You can give this Gaussian input a title name in Title Card section. After that, use Molecule Specification section to assign the coordinates of the atoms with the charge and spin multiplicity of the system in the first line. Please make sure there is at least one empty line in the end of file . By logging into HPCC and ssh to a dev node, users can find Gaussian program installed in HPCC with the module commands: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ module spider Gaussian ---------------------------- Gaussian: ---------------------------- Versions: Gaussian/g16_AVX Gaussian/g16-AVX2 Gaussian/g16 ---------------------------- For detailed information about a specific \"Gaussian\" module (including how to load the modules) use the module's full name. For example: $ module spider Gaussian/g16 ---------------------------- Please load Gaussian module with g16 version: 1 2 3 $ module load Gaussian/g16 $ which g16 /opt/software/Gaussian/g16-AVX/g16/g16 and the Gaussian command \"g16\" can be used. Simply run the command by giving the input and output file names: 1 $ g16 < g16.com > g16.log and it starts to calculate the system on the node. It will take about 2 minutes to complete the calculation. After it is finished, you can check the output file g16.log . All dev nodes have 2-hour CPU limit. Please restrict your resource usage on them. Users can also use GaussView to create their molecular system and do calculations. To use GaussView, just run the command gview after loading the Gaussian module.","title":"Running Gaussian by Command Lines"},{"location":"Running_Jobs_on_amd20_Test_Nodes/","text":"Running Jobs on amd20 Test Nodes The amd20 cluster is added to HPCC compute nodes for job runing. The feature of the nodes can be seen in the following table: Cluster Type Node Names Node Num. Processors Cores/Node Memory/Node Disk size/Node GPUs/Node amd20 amr-[000-101], amr-[137-209], amr-127, test-amr-[000-001] 178 AMD EPYC 7H12 Processor @2.595 GHz 128 493 GB 412 GB amr-[104-26], amr-[128-136] 32 AMD EPYC 7H12 Processor @2.595 GHz 128 996 GB 412 GB amr-[102-103] 2 AMD EPYC 7H12 Processor @2.595 GHz 128 2005 GB 412 GB nvf-[000-008] 9 Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz 48 178 GB 412 GB 4 Tesla V100S Users can use the following powertools command to see a list of the amd20 nodes: 1 $ node_status -f amd20 To launch jobs on amr cluster nodes, please add the following SBATCH line in your job script or use the job submission commands (such as sbatch , salloc or srun ) with the specification \" -C amr \": 1 #SBATCH -C amr For requesting nvf cluster nodes, please add the following line to your job script or specify \" -C nvf \" in your job submission on the command line: 1 #SBATCH -C nvf The amd20 cluster does not support AVX-512. If you\u2019ve found that your executable requires AVX-512 or want to continue to run on the existing clusters, add the following line to your submit scripts: 1 #SBATCH -C '[intel14|intel16|intel18]' If you have any question about running jobs on the nodes, please contact us .","title":"Running Jobs on amd20 Test Nodes"},{"location":"Running_Jobs_on_amd20_Test_Nodes/#running-jobs-on-amd20-test-nodes","text":"The amd20 cluster is added to HPCC compute nodes for job runing. The feature of the nodes can be seen in the following table: Cluster Type Node Names Node Num. Processors Cores/Node Memory/Node Disk size/Node GPUs/Node amd20 amr-[000-101], amr-[137-209], amr-127, test-amr-[000-001] 178 AMD EPYC 7H12 Processor @2.595 GHz 128 493 GB 412 GB amr-[104-26], amr-[128-136] 32 AMD EPYC 7H12 Processor @2.595 GHz 128 996 GB 412 GB amr-[102-103] 2 AMD EPYC 7H12 Processor @2.595 GHz 128 2005 GB 412 GB nvf-[000-008] 9 Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz 48 178 GB 412 GB 4 Tesla V100S Users can use the following powertools command to see a list of the amd20 nodes: 1 $ node_status -f amd20 To launch jobs on amr cluster nodes, please add the following SBATCH line in your job script or use the job submission commands (such as sbatch , salloc or srun ) with the specification \" -C amr \": 1 #SBATCH -C amr For requesting nvf cluster nodes, please add the following line to your job script or specify \" -C nvf \" in your job submission on the command line: 1 #SBATCH -C nvf The amd20 cluster does not support AVX-512. If you\u2019ve found that your executable requires AVX-512 or want to continue to run on the existing clusters, add the following line to your submit scripts: 1 #SBATCH -C '[intel14|intel16|intel18]' If you have any question about running jobs on the nodes, please contact us .","title":"Running Jobs on amd20 Test Nodes"},{"location":"SFTP_Mapping_on_HPCC_file_systems/","text":"SFTP Mapping on HPCC file systems SFTP Net Drive is the software which can map remote HPCC file systems on your local Windows computers via SFTP. (Mac computer is not available.) Once connected, you can browse and work with files on HPCC as if they were on a hard drive of your local machine. In order words, they do not need to be downloaded and uploaded when users read or modify them by their local computers. HPCC users can use the download site to get a free version of SFTP Net Drive. Once it is downloaded and executed, users can do its setup. In the main menu (below), you can choose your own Profile name and Drive Letter . Make sure Sever is set up as http://rsync.hpcc.msu.edu Username and Password are the same as your HPCC login. For more advanced setting, please click on the Advanced... button. Three setting menus: Connection , Protocol and Drive can be modified. For Connection , the value on Port has to be 22. A longer initiation time can be adjusted on Timeout . You can also set up reconnect times in case the connection is dropped and Send keep-alive to prevent disconnection. For Protocol , you may just use the default setting: For Drive , if you would like, you may set up a different Root folder to start with other than your home folder. You might want to click on Handle case-sensitive filenames since HPCC file system is case-sensitive. If you would like to show hidden files, you can click on Show files started with dot . Once all of them are set, you can click on Connect button in the main menu. If it is successfully connected, the Connect button will become Disconnect : Now, open the file explorer and click on This PC . You should find the HPCC file system shown in the Network locations area with the Profile name and the Driver Letter of your input: Mapping HPCC Drive with More Spaces If you would like to do SFTP mapping with more than one space in HPCC, the better way is to set static links in the Root folder of your setting. For example, you want to map your home space and research space on your local Windows computer. You can set the Root folder to be your home folder as the setup above. Use a ssh client to connect to HPCC or Web Site Access to HPCC to run a command line on a dev node: 1 [ username@dev-intel14-phi ~ ] $ ln -s /mnt/research/<Research Name> <Research Name> where a static link to your research space is set in your home folder. Once it is done, run ls command and the static link <Research Name> should be shown with the light blue color. To access the research space through your local computer, just click on the HPCC drive of your setting in the Network locations area as mentioned in the upper section. Look for the space link <Research Name> and click on it. You can now see the files in the research space. If you would like to map more spaces, such as your scratch spaces or other research spaces, just create more static links in your home folder by the same way.","title":"Mapping drives using SFTP"},{"location":"SFTP_Mapping_on_HPCC_file_systems/#sftp-mapping-on-hpcc-file-systems","text":"SFTP Net Drive is the software which can map remote HPCC file systems on your local Windows computers via SFTP. (Mac computer is not available.) Once connected, you can browse and work with files on HPCC as if they were on a hard drive of your local machine. In order words, they do not need to be downloaded and uploaded when users read or modify them by their local computers. HPCC users can use the download site to get a free version of SFTP Net Drive. Once it is downloaded and executed, users can do its setup. In the main menu (below), you can choose your own Profile name and Drive Letter . Make sure Sever is set up as http://rsync.hpcc.msu.edu Username and Password are the same as your HPCC login. For more advanced setting, please click on the Advanced... button. Three setting menus: Connection , Protocol and Drive can be modified. For Connection , the value on Port has to be 22. A longer initiation time can be adjusted on Timeout . You can also set up reconnect times in case the connection is dropped and Send keep-alive to prevent disconnection. For Protocol , you may just use the default setting: For Drive , if you would like, you may set up a different Root folder to start with other than your home folder. You might want to click on Handle case-sensitive filenames since HPCC file system is case-sensitive. If you would like to show hidden files, you can click on Show files started with dot . Once all of them are set, you can click on Connect button in the main menu. If it is successfully connected, the Connect button will become Disconnect : Now, open the file explorer and click on This PC . You should find the HPCC file system shown in the Network locations area with the Profile name and the Driver Letter of your input:","title":"SFTP Mapping on HPCC file systems"},{"location":"SFTP_Mapping_on_HPCC_file_systems/#mapping-hpcc-drive-with-more-spaces","text":"If you would like to do SFTP mapping with more than one space in HPCC, the better way is to set static links in the Root folder of your setting. For example, you want to map your home space and research space on your local Windows computer. You can set the Root folder to be your home folder as the setup above. Use a ssh client to connect to HPCC or Web Site Access to HPCC to run a command line on a dev node: 1 [ username@dev-intel14-phi ~ ] $ ln -s /mnt/research/<Research Name> <Research Name> where a static link to your research space is set in your home folder. Once it is done, run ls command and the static link <Research Name> should be shown with the light blue color. To access the research space through your local computer, just click on the HPCC drive of your setting in the Network locations area as mentioned in the upper section. Look for the space link <Research Name> and click on it. You can now see the files in the research space. If you would like to map more spaces, such as your scratch spaces or other research spaces, just create more static links in your home folder by the same way.","title":"Mapping HPCC Drive with More Spaces"},{"location":"SLURM_Check_Modify_and_Cancel_a_Job_by_scontrol_scancel_Command/","text":"SLURM Check, Modify and Cancel a Job by scontrol & scancel Command scontrol command Besides the brief listing of every jobs by squeue command, user can also see the detailed information of each job. Run SLURM command scontrol show with a job ID: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 $ scontrol show job 8929 JobId = 8929 JobName = test UserId = nobody ( 804293 ) GroupId = helpdesk ( 2103 ) MCS_label = N/A Priority = 404 Nice = 0 Account = classres QOS = normal JobState = PENDING Reason = Resources Dependency =( null ) Requeue = 0 Restarts = 0 BatchFlag = 1 Reboot = 0 ExitCode = 0 :0 RunTime = 00 :00:00 TimeLimit = 00 :01:00 TimeMin = N/A SubmitTime = 2018 -08-01T14:33:04 EligibleTime = 2018 -08-01T14:33:04 StartTime = Unknown EndTime = Unknown Deadline = N/A PreemptTime = None SuspendTime = None SecsPreSuspend = 0 LastSchedEval = 2018 -08-03T12:38:48 Partition = general-short-14,general-short-16,general-short-18,general-long-14,general-long-16,general-long-18,classres-14,classres-16 AllocNode:Sid = dev-intel18:4996 ReqNodeList =( null ) ExcNodeList =( null ) NodeList =( null ) NumNodes = 80 -80 NumCPUs = 160 NumTasks = 80 CPUs/Task = 2 ReqB:S:C:T = 0 :0:*:* TRES = cpu = 40 ,mem = 80G,node = 40 ,gres/gpu = 40 Socks/Node = * NtasksPerN:B:S:C = 0 :0:*:* CoreSpec = * MinCPUsNode = 2 MinMemoryNode = 2G MinTmpDiskNode = 0 Features = intel14 DelayBoot = 00 :00:00 Gres =( null ) Reservation =( null ) OverSubscribe = OK Contiguous = 0 Licenses =( null ) Network =( null ) Command = /mnt/home/changc81/GetExample/helloMPI/test WorkDir = /mnt/home/changc81/GetExample/helloMPI Comment = stdout = /mnt/home/changc81/GetExample/helloMPI/slurm-8929.out StdErr = /mnt/home/changc81/GetExample/helloMPI/slurm-8929.out StdIn = /dev/null StdOut = /mnt/home/changc81/GetExample/helloMPI/slurm-8929.out Power = You can check if the information is right for the job. If the job has not started to run and you would like change any specification, you can hold the job first by scontrol hold command: 1 2 3 4 5 $ scontrol hold 8929 $ squeue -l -u $USER Fri Aug 3 12 :26:57 2018 JOBID PARTITION NAME USER STATE TIME TIME_LIMI NODES NODELIST ( REASON ) 8929 general-s test nobody PENDING 0 :00 1 :00 80 ( JobHeldUser ) where you can see from the results of the squeue command, the job is pending due to the user's hold. Now, you can choose the parts of information you want to change in scontrol show results. Put them in scontrol update command and modify the information after symbol \"= \" . For example, the command line 1 $ scontrol update job 8929 NumNodes = 2 -2 NumTasks = 2 Features = intel16 will change the resource request of the job 8929 from 80 nodes and 80 tasks with intel14 nodes to 2 nodes and 2 tasks with intel16 nodes. After the update, you can use scontrol show command again to verify the job setting. Once you are done with the update work, you can release the job hold by command scontrol release : 1 2 3 4 5 $ scontrol release 8929 $ squeue -l -u $USER Fri Aug 3 13 :18:10 2018 JOBID PARTITION NAME USER STATE TIME TIME_LIMI NODES NODELIST ( REASON ) 8929 general-s test nobody RUNNING 0 :07 1 :00 2 lac- [ 386 -387 ] The job is now running due to the change of the resource request by command scontrol update . Again, we can check the running job by the command: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 $ scontrol show job 8929 JobId = 8929 JobName = test UserId = changc81 ( 804793 ) GroupId = helpdesk ( 2103 ) MCS_label = N/A Priority = 379 Nice = 0 Account = classres QOS = normal JobState = RUNNING Reason = None Dependency =( null ) Requeue = 0 Restarts = 0 BatchFlag = 1 Reboot = 0 ExitCode = 0 :0 RunTime = 00 :00:08 TimeLimit = 00 :01:00 TimeMin = N/A SubmitTime = 2018 -08-01T14:33:04 EligibleTime = 2018 -08-01T14:33:04 StartTime = 2018 -08-03T13:18:03 EndTime = 2018 -08-03T13:18:11 Deadline = N/A PreemptTime = None SuspendTime = None SecsPreSuspend = 0 LastSchedEval = 2018 -08-03T13:18:03 Partition = general-long-16 AllocNode:Sid = dev-intel18:4996 ReqNodeList =( null ) ExcNodeList =( null ) NodeList = lac- [ 386 -387 ] BatchHost = lac-386 NumNodes = 2 NumCPUs = 4 NumTasks = 2 CPUs/Task = 2 ReqB:S:C:T = 0 :0:*:* TRES = cpu = 4 ,mem = 4G,node = 2 ,billing = 4 Socks/Node = * NtasksPerN:B:S:C = 0 :0:*:* CoreSpec = * MinCPUsNode = 2 MinMemoryNode = 2G MinTmpDiskNode = 0 Features = intel16 DelayBoot = 00 :00:00 Gres =( null ) Reservation =( null ) OverSubscribe = OK Contiguous = 0 Licenses =( null ) Network =( null ) Command = /mnt/home/changc81/GetExample/helloMPI/test WorkDir = /mnt/home/changc81/GetExample/helloMPI Comment = stdout = /mnt/home/changc81/GetExample/helloMPI/slurm-8929.out StdErr = /mnt/home/changc81/GetExample/helloMPI/slurm-8929.out StdIn = /dev/null StdOut = /mnt/home/changc81/GetExample/helloMPI/slurm-8929.out Power = For a complete usage of scontrol command, please refer to the SLURM web site . scancel command If at any moment before the job complete, you would like to remove the job, you can use scancel command to cancel a job. For example, the command line 1 $ scancel 8929 cancel the running of the job 8929. For a complete usage of scancel command, please refer to the SLURM web site .","title":"SLURM Check, Modify and Cancel a Job by scontrol & scancel Command"},{"location":"SLURM_Check_Modify_and_Cancel_a_Job_by_scontrol_scancel_Command/#slurm-check-modify-and-cancel-a-job-by-scontrol-scancel-command","text":"","title":"SLURM Check, Modify and Cancel a Job by scontrol &amp; scancel Command"},{"location":"SLURM_Check_Modify_and_Cancel_a_Job_by_scontrol_scancel_Command/#scontrol-command","text":"Besides the brief listing of every jobs by squeue command, user can also see the detailed information of each job. Run SLURM command scontrol show with a job ID: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 $ scontrol show job 8929 JobId = 8929 JobName = test UserId = nobody ( 804293 ) GroupId = helpdesk ( 2103 ) MCS_label = N/A Priority = 404 Nice = 0 Account = classres QOS = normal JobState = PENDING Reason = Resources Dependency =( null ) Requeue = 0 Restarts = 0 BatchFlag = 1 Reboot = 0 ExitCode = 0 :0 RunTime = 00 :00:00 TimeLimit = 00 :01:00 TimeMin = N/A SubmitTime = 2018 -08-01T14:33:04 EligibleTime = 2018 -08-01T14:33:04 StartTime = Unknown EndTime = Unknown Deadline = N/A PreemptTime = None SuspendTime = None SecsPreSuspend = 0 LastSchedEval = 2018 -08-03T12:38:48 Partition = general-short-14,general-short-16,general-short-18,general-long-14,general-long-16,general-long-18,classres-14,classres-16 AllocNode:Sid = dev-intel18:4996 ReqNodeList =( null ) ExcNodeList =( null ) NodeList =( null ) NumNodes = 80 -80 NumCPUs = 160 NumTasks = 80 CPUs/Task = 2 ReqB:S:C:T = 0 :0:*:* TRES = cpu = 40 ,mem = 80G,node = 40 ,gres/gpu = 40 Socks/Node = * NtasksPerN:B:S:C = 0 :0:*:* CoreSpec = * MinCPUsNode = 2 MinMemoryNode = 2G MinTmpDiskNode = 0 Features = intel14 DelayBoot = 00 :00:00 Gres =( null ) Reservation =( null ) OverSubscribe = OK Contiguous = 0 Licenses =( null ) Network =( null ) Command = /mnt/home/changc81/GetExample/helloMPI/test WorkDir = /mnt/home/changc81/GetExample/helloMPI Comment = stdout = /mnt/home/changc81/GetExample/helloMPI/slurm-8929.out StdErr = /mnt/home/changc81/GetExample/helloMPI/slurm-8929.out StdIn = /dev/null StdOut = /mnt/home/changc81/GetExample/helloMPI/slurm-8929.out Power = You can check if the information is right for the job. If the job has not started to run and you would like change any specification, you can hold the job first by scontrol hold command: 1 2 3 4 5 $ scontrol hold 8929 $ squeue -l -u $USER Fri Aug 3 12 :26:57 2018 JOBID PARTITION NAME USER STATE TIME TIME_LIMI NODES NODELIST ( REASON ) 8929 general-s test nobody PENDING 0 :00 1 :00 80 ( JobHeldUser ) where you can see from the results of the squeue command, the job is pending due to the user's hold. Now, you can choose the parts of information you want to change in scontrol show results. Put them in scontrol update command and modify the information after symbol \"= \" . For example, the command line 1 $ scontrol update job 8929 NumNodes = 2 -2 NumTasks = 2 Features = intel16 will change the resource request of the job 8929 from 80 nodes and 80 tasks with intel14 nodes to 2 nodes and 2 tasks with intel16 nodes. After the update, you can use scontrol show command again to verify the job setting. Once you are done with the update work, you can release the job hold by command scontrol release : 1 2 3 4 5 $ scontrol release 8929 $ squeue -l -u $USER Fri Aug 3 13 :18:10 2018 JOBID PARTITION NAME USER STATE TIME TIME_LIMI NODES NODELIST ( REASON ) 8929 general-s test nobody RUNNING 0 :07 1 :00 2 lac- [ 386 -387 ] The job is now running due to the change of the resource request by command scontrol update . Again, we can check the running job by the command: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 $ scontrol show job 8929 JobId = 8929 JobName = test UserId = changc81 ( 804793 ) GroupId = helpdesk ( 2103 ) MCS_label = N/A Priority = 379 Nice = 0 Account = classres QOS = normal JobState = RUNNING Reason = None Dependency =( null ) Requeue = 0 Restarts = 0 BatchFlag = 1 Reboot = 0 ExitCode = 0 :0 RunTime = 00 :00:08 TimeLimit = 00 :01:00 TimeMin = N/A SubmitTime = 2018 -08-01T14:33:04 EligibleTime = 2018 -08-01T14:33:04 StartTime = 2018 -08-03T13:18:03 EndTime = 2018 -08-03T13:18:11 Deadline = N/A PreemptTime = None SuspendTime = None SecsPreSuspend = 0 LastSchedEval = 2018 -08-03T13:18:03 Partition = general-long-16 AllocNode:Sid = dev-intel18:4996 ReqNodeList =( null ) ExcNodeList =( null ) NodeList = lac- [ 386 -387 ] BatchHost = lac-386 NumNodes = 2 NumCPUs = 4 NumTasks = 2 CPUs/Task = 2 ReqB:S:C:T = 0 :0:*:* TRES = cpu = 4 ,mem = 4G,node = 2 ,billing = 4 Socks/Node = * NtasksPerN:B:S:C = 0 :0:*:* CoreSpec = * MinCPUsNode = 2 MinMemoryNode = 2G MinTmpDiskNode = 0 Features = intel16 DelayBoot = 00 :00:00 Gres =( null ) Reservation =( null ) OverSubscribe = OK Contiguous = 0 Licenses =( null ) Network =( null ) Command = /mnt/home/changc81/GetExample/helloMPI/test WorkDir = /mnt/home/changc81/GetExample/helloMPI Comment = stdout = /mnt/home/changc81/GetExample/helloMPI/slurm-8929.out StdErr = /mnt/home/changc81/GetExample/helloMPI/slurm-8929.out StdIn = /dev/null StdOut = /mnt/home/changc81/GetExample/helloMPI/slurm-8929.out Power = For a complete usage of scontrol command, please refer to the SLURM web site .","title":"scontrol command"},{"location":"SLURM_Check_Modify_and_Cancel_a_Job_by_scontrol_scancel_Command/#scancel-command","text":"If at any moment before the job complete, you would like to remove the job, you can use scancel command to cancel a job. For example, the command line 1 $ scancel 8929 cancel the running of the job 8929. For a complete usage of scancel command, please refer to the SLURM web site .","title":"scancel command"},{"location":"SLURM_overview/","text":"Overview of job scheduling and management HPCC uses SLURM (Simple Linux Utility for Resource Management) to manage users' jobs and computing resources. SLURM is an open-source, fault-tolerant, and highly scalable scheduling system. It has been employed by a large number of national and international computing centers. Users can submit a job by SLURM commands and request computing resources with specifications in a job script or on a command line. SLURM uses command-line commands to control jobs and clusters as well as show detailed information about jobs. The table below presents the most frequently used commands on HPCC. A complete list can be found at the SLURM documentation page . Command Description sacct displays accounting data for all jobs and job steps in the SLURM job accounting log or SLURM database. sbatch Used to submit batch jobis to SLURM job queue sacctmgr Used to view and modify SLURM account information scancel Used to signal jobs or job steps that are under the control of SLURM. scontrol Used view and modify SLURM configuration and state. sinfo view information about SLURM nodes and partitions. smap graphically view information about SLURM jobs, partitions, and set configurations parameters. sprio view the factors that comprise a job's scheduling priority. squeue view information about jobs located in the SLURM scheduling queue. srun Run parallel jobs.","title":"Overview"},{"location":"SLURM_overview/#overview-of-job-scheduling-and-management","text":"HPCC uses SLURM (Simple Linux Utility for Resource Management) to manage users' jobs and computing resources. SLURM is an open-source, fault-tolerant, and highly scalable scheduling system. It has been employed by a large number of national and international computing centers. Users can submit a job by SLURM commands and request computing resources with specifications in a job script or on a command line. SLURM uses command-line commands to control jobs and clusters as well as show detailed information about jobs. The table below presents the most frequently used commands on HPCC. A complete list can be found at the SLURM documentation page . Command Description sacct displays accounting data for all jobs and job steps in the SLURM job accounting log or SLURM database. sbatch Used to submit batch jobis to SLURM job queue sacctmgr Used to view and modify SLURM account information scancel Used to signal jobs or job steps that are under the control of SLURM. scontrol Used view and modify SLURM configuration and state. sinfo view information about SLURM nodes and partitions. smap graphically view information about SLURM jobs, partitions, and set configurations parameters. sprio view the factors that comprise a job's scheduling priority. squeue view information about jobs located in the SLURM scheduling queue. srun Run parallel jobs.","title":"Overview of job scheduling and management"},{"location":"SSH_Key-Based_Authentication/","text":"SSH Key-Based Authentication Typically, when someone uses a SSH client, that person needs to type a password for each new connection started. This can become bothersome if one is frequently making new connections or is in a situation where others may be physically present when the password is being typed. One alternative to typing the password so often is to use key-based authentication. There are several steps to setting up key-based authentication, but they are a one-time investment. Generating SSH keypairs A SSH keypair consists of a private key and a public key. Your private key is a secret in the same way that you password is a secret. And, your public key can be made publicly available in the same way that your name can be made publicly available. As with your password, you should not share your private key with anyone. SSH tool suites usually provide a utility for generating these keypairs. On HPCC systems and most other Unix systems, there is a command named ssh-keygen . If you install the full PuTTY SSH suite on Windows, then you will have a utility program called PuTTYgen , which performs a similar function. When you use these utilities, you will be given an option for protecting your private key with a passphrase. Please do this; it will prevent your private key from being used by a malicious individual if it is ever stolen. If you have not previously used ssh-keygen , then you can simply run the following command: 1 ssh-keygen -t rsa After you have set a passphrase and it has generated the keys, you will find the key files in the .ssh directory under your home directory. By default, these are named id_rsa and id_rsa.pub . id_rsa is your private key and id_rsa.pub is your public key. Note Use at least 2048 bits for an RSA key. This is usually the default key length, but can be specified with the '-b' option when generating your key, e.g. 'ssh-keygen -t rsa -b 2048' Distributing SSH public keys Some larger national HPC facilities, as well as sites which host free and open source software projects, only allow full access via key-based authentication. To access their machines, you need to first provide them with your public key. The MSU HPCC, of course, does not require key-based authentication, but we do provide it as an option. You can use it to connect from a home computer or personal workstation at work, or even from the computers of one facility to those of another. To get this to work, the machine that you are trying to login to must support key-based authentication (such as HPCC machines) and you must place your public key into what is known as the authorized keys file. If you are seeking to login to HPCC with key-based authentication from your local computer, then you will need to perform the following: Create ~/.ssh on gateway.hpcc.msu.edu , if you have not already done so. Copy the id_rsa.pub or equivalent file from your home or work machine over to gateway.hpcc.msu.edu as ~/.ssh/authorized_keys , if that file does not already exist. If ~/.ssh/authorized_keys does already exist on gateway.hpcc.msu.edu , then perform the following steps: Copy the id_rsa.pub or equivalent file from your home or work machine over to gateway.hpcc.msu.edu as ~/work-pubkey . cat ~/work-pubkey >> ~/.ssh/authorized_keys Set the permissions appropriately by running \" chmod 700 ~/.ssh \" and \" chmod 600 ~/.ssh/authorized_keys \" Copy id_rsa from your local computer to \\~/.ssh/ on the HPC gateway as well. ~/.ssh/authorized_keys is not a directory. It is a file. To store multiple authorized public keys in it, you will need to append the additional keys to the file. Running the SSH agent Although there are ways to use some private keys directly with your SSH client, you will likely want to run a SSH agent to manage any private keys that you have protected with passphrases. On HPCC and most other Unix systems, there is a program called ssh-agent for doing this. If you install the full PuTTY SSH suite on Windows, then you will have a similar utility called Pageant . If you are using Mac OSX, then see Adding a Private Key to Your Mac OSX Keychain. What an SSH agent does is cache your private keys and allow your SSH clients to refer to this cache when attempting to establish new sessions. When you attempt a key-based connection to a remote SSH server, that server will look up the public keys in your authorized keys file on that remote machine and then challenge the connecting client to prove that it has a matching private key by decrypting a message encrypted with a public key. Your client will refer to the cache of private keys maintained by your SSH agent, for the purpose of decrypting this challenge message. If it finds the matching private key and is thus able to decrypt the challenge message from the remote SSH server, then you will be allowed to login once the client has proven this to the server. This all happens without you noticing anything different... except that you no longer need to type in your password during login. To start ssh-agent on a Unix system using a Bourne shell-compatible shell (the default on HPCC), you can use the following command: 1 eval ` ssh-agent ` This starts the agent and sets some Unix environment variables that tell the SSH client, ssh , where to talk to the running agent process. The equivalent command for those using a C shell-compatible shell is: 1 eval ` ssh-agent -c ` Once it is running, you will want to load your private key(s) into the agent's cache. For example: 1 ssh-add will prompt your for the passphrase on ~/.ssh/id_rsa and load it into the agent's cache. If you are on some other system and have multiple keys pairs, then you may wish to load additional keys; for example: 1 ssh-add ~/.ssh/id_rsa_hpcc ~/.ssh/id_rsa_hpcc_vcs ~/.ssh/id_rsa_nersc Once you have ssh-agent running and have some private keys loaded, then you should be able to make key-based connections to other systems where you have placed your corresponding public keys into the authorized keys files... no password necessary. Try it. You don't want to leave stray ssh-agent processes running when you logout. To clean up after yourself, you can use: 1 eval ` ssh-agent -k ` with Bourne shells and 1 eval ` ssh-agent -c -k ` with C shells. If you find yourself using the agent frequently, then you may wish to consider adding the ssh-agent startup and shutdown commands to your shell control files. For Bourne shells, you would want to add 1 eval ` ssh-agent ` to ~/.bash_profile and 1 eval ` ssh-agent -k ` to ~/.bash_logout . For C shells, you would want to add 1 eval ` ssh-agent -c ` to your ~/.login file and 1 eval ` ssh-agent -c -k ` to your ~/.logout file. You should NOT add the ssh-add command to your shell initialization file. You should still plan on running this command by hand. As it issues prompts to your terminal, it is not a good idea to run it during shell initialization as the shell may not have fully configured your terminal yet. Another way of managing ssh-agent is via a program called keychain . You can learn more at the Keychain web site .","title":"SSH key"},{"location":"SSH_Key-Based_Authentication/#ssh-key-based-authentication","text":"Typically, when someone uses a SSH client, that person needs to type a password for each new connection started. This can become bothersome if one is frequently making new connections or is in a situation where others may be physically present when the password is being typed. One alternative to typing the password so often is to use key-based authentication. There are several steps to setting up key-based authentication, but they are a one-time investment.","title":"SSH Key-Based Authentication"},{"location":"SSH_Key-Based_Authentication/#generating-ssh-keypairs","text":"A SSH keypair consists of a private key and a public key. Your private key is a secret in the same way that you password is a secret. And, your public key can be made publicly available in the same way that your name can be made publicly available. As with your password, you should not share your private key with anyone. SSH tool suites usually provide a utility for generating these keypairs. On HPCC systems and most other Unix systems, there is a command named ssh-keygen . If you install the full PuTTY SSH suite on Windows, then you will have a utility program called PuTTYgen , which performs a similar function. When you use these utilities, you will be given an option for protecting your private key with a passphrase. Please do this; it will prevent your private key from being used by a malicious individual if it is ever stolen. If you have not previously used ssh-keygen , then you can simply run the following command: 1 ssh-keygen -t rsa After you have set a passphrase and it has generated the keys, you will find the key files in the .ssh directory under your home directory. By default, these are named id_rsa and id_rsa.pub . id_rsa is your private key and id_rsa.pub is your public key. Note Use at least 2048 bits for an RSA key. This is usually the default key length, but can be specified with the '-b' option when generating your key, e.g. 'ssh-keygen -t rsa -b 2048'","title":"Generating SSH keypairs"},{"location":"SSH_Key-Based_Authentication/#distributing-ssh-public-keys","text":"Some larger national HPC facilities, as well as sites which host free and open source software projects, only allow full access via key-based authentication. To access their machines, you need to first provide them with your public key. The MSU HPCC, of course, does not require key-based authentication, but we do provide it as an option. You can use it to connect from a home computer or personal workstation at work, or even from the computers of one facility to those of another. To get this to work, the machine that you are trying to login to must support key-based authentication (such as HPCC machines) and you must place your public key into what is known as the authorized keys file. If you are seeking to login to HPCC with key-based authentication from your local computer, then you will need to perform the following: Create ~/.ssh on gateway.hpcc.msu.edu , if you have not already done so. Copy the id_rsa.pub or equivalent file from your home or work machine over to gateway.hpcc.msu.edu as ~/.ssh/authorized_keys , if that file does not already exist. If ~/.ssh/authorized_keys does already exist on gateway.hpcc.msu.edu , then perform the following steps: Copy the id_rsa.pub or equivalent file from your home or work machine over to gateway.hpcc.msu.edu as ~/work-pubkey . cat ~/work-pubkey >> ~/.ssh/authorized_keys Set the permissions appropriately by running \" chmod 700 ~/.ssh \" and \" chmod 600 ~/.ssh/authorized_keys \" Copy id_rsa from your local computer to \\~/.ssh/ on the HPC gateway as well. ~/.ssh/authorized_keys is not a directory. It is a file. To store multiple authorized public keys in it, you will need to append the additional keys to the file.","title":"Distributing SSH public keys"},{"location":"SSH_Key-Based_Authentication/#running-the-ssh-agent","text":"Although there are ways to use some private keys directly with your SSH client, you will likely want to run a SSH agent to manage any private keys that you have protected with passphrases. On HPCC and most other Unix systems, there is a program called ssh-agent for doing this. If you install the full PuTTY SSH suite on Windows, then you will have a similar utility called Pageant . If you are using Mac OSX, then see Adding a Private Key to Your Mac OSX Keychain. What an SSH agent does is cache your private keys and allow your SSH clients to refer to this cache when attempting to establish new sessions. When you attempt a key-based connection to a remote SSH server, that server will look up the public keys in your authorized keys file on that remote machine and then challenge the connecting client to prove that it has a matching private key by decrypting a message encrypted with a public key. Your client will refer to the cache of private keys maintained by your SSH agent, for the purpose of decrypting this challenge message. If it finds the matching private key and is thus able to decrypt the challenge message from the remote SSH server, then you will be allowed to login once the client has proven this to the server. This all happens without you noticing anything different... except that you no longer need to type in your password during login. To start ssh-agent on a Unix system using a Bourne shell-compatible shell (the default on HPCC), you can use the following command: 1 eval ` ssh-agent ` This starts the agent and sets some Unix environment variables that tell the SSH client, ssh , where to talk to the running agent process. The equivalent command for those using a C shell-compatible shell is: 1 eval ` ssh-agent -c ` Once it is running, you will want to load your private key(s) into the agent's cache. For example: 1 ssh-add will prompt your for the passphrase on ~/.ssh/id_rsa and load it into the agent's cache. If you are on some other system and have multiple keys pairs, then you may wish to load additional keys; for example: 1 ssh-add ~/.ssh/id_rsa_hpcc ~/.ssh/id_rsa_hpcc_vcs ~/.ssh/id_rsa_nersc Once you have ssh-agent running and have some private keys loaded, then you should be able to make key-based connections to other systems where you have placed your corresponding public keys into the authorized keys files... no password necessary. Try it. You don't want to leave stray ssh-agent processes running when you logout. To clean up after yourself, you can use: 1 eval ` ssh-agent -k ` with Bourne shells and 1 eval ` ssh-agent -c -k ` with C shells. If you find yourself using the agent frequently, then you may wish to consider adding the ssh-agent startup and shutdown commands to your shell control files. For Bourne shells, you would want to add 1 eval ` ssh-agent ` to ~/.bash_profile and 1 eval ` ssh-agent -k ` to ~/.bash_logout . For C shells, you would want to add 1 eval ` ssh-agent -c ` to your ~/.login file and 1 eval ` ssh-agent -c -k ` to your ~/.logout file. You should NOT add the ssh-add command to your shell initialization file. You should still plan on running this command by hand. As it issues prompts to your terminal, it is not a good idea to run it during shell initialization as the shell may not have fully configured your terminal yet. Another way of managing ssh-agent is via a program called keychain . You can learn more at the Keychain web site .","title":"Running the SSH agent"},{"location":"Scavenger_Queue%202/","text":"Scavenger Queue The scavenger queue allows users to run preemptable jobs on idle cores. With few exceptions, each researcher using the HPCC is limited in the number of jobs or cores they can run at one time. Annually, non-buyin users are limited in the total number of CPU and GPU hours they can use. These limits do not apply to jobs submitted to the scavenger queue . Jobs in this queue can start on resources that would otherwise be left idle, improving research throughput. Similar to jobs submitted to the general-long queue, these jobs can request up to a 7-day wall time; however, jobs in the scavenger queue may be interrupted if resources are required for other non-scavenger jobs. The default behavior for interrupted jobs is to be re-queued, but users can opt for cancellation if it is more conducive to their workflow. We recommend that only users who can checkpoint and restart or have a workflow implemented that can manage jobs being canceled or requeued use this queue. To use the scavenger queue, add the following line to your job script: 1 #SBATCH --qos=scavenger To prevent your job from requeuing automatically if interrupted, add the following line to your job script: 1 #SBATCH --no-requeue","title":"Scavenger Queue"},{"location":"Scavenger_Queue%202/#scavenger-queue","text":"The scavenger queue allows users to run preemptable jobs on idle cores. With few exceptions, each researcher using the HPCC is limited in the number of jobs or cores they can run at one time. Annually, non-buyin users are limited in the total number of CPU and GPU hours they can use. These limits do not apply to jobs submitted to the scavenger queue . Jobs in this queue can start on resources that would otherwise be left idle, improving research throughput. Similar to jobs submitted to the general-long queue, these jobs can request up to a 7-day wall time; however, jobs in the scavenger queue may be interrupted if resources are required for other non-scavenger jobs. The default behavior for interrupted jobs is to be re-queued, but users can opt for cancellation if it is more conducive to their workflow. We recommend that only users who can checkpoint and restart or have a workflow implemented that can manage jobs being canceled or requeued use this queue. To use the scavenger queue, add the following line to your job script: 1 #SBATCH --qos=scavenger To prevent your job from requeuing automatically if interrupted, add the following line to your job script: 1 #SBATCH --no-requeue","title":"Scavenger Queue"},{"location":"Scavenger_Queue/","text":"Scavenger Queue The scavenger queue allows users to run preemptible jobs on idle cores. Jobs in the scavenger queue may be interrupted if resources are required for other non-scavenger jobs. With few exceptions, each researcher using the HPCC is limited in the number of jobs or cores they can run at one time. Annually, non-buyin users are limited in the total number of CPU and GPU hours they can use. These limits do not apply to jobs submitted to the scavenger queue . Jobs in this queue can start on resources that would otherwise be left idle, improving research throughput. Similar to jobs submitted to the general-long queue, these jobs can request up to a 7-day wall time. The default behavior for interrupted jobs is to be re-queued, but users can opt for cancellation if it is more conducive to their workflow. Note We recommend that only users who can checkpoint and restart or have a workflow implemented that can manage jobs being canceled or requeued use this queue. Usage To use the scavenger queue, add the following line to your job script: 1 #SBATCH --qos=scavenger To prevent your job from requeuing automatically if interrupted, add the following line to your job script: 1 #SBATCH --no-requeue The scavenger queue is not affected by the amount of wall time requested in your job script, e.g. 24 hours wall time is treated with the same scavenger queue priority as 4 hours wall time. Scavenger queue jobs will be automatically assigned to the scavenger account, regardless of the -A setting in your job script. Scheduling The scavenger queue runs using the backfill scheduler (see How Jobs are Scheduled ). Job scheduling may take on the order of minutes to occur, depending on the current load on the HPCC. Scavenger queue jobs run for a minimum of 1 minute before they can be preempted, but typically scavenger queue jobs run for approximately 1 hour before preemption.","title":"Scavenger Queue"},{"location":"Scavenger_Queue/#scavenger-queue","text":"The scavenger queue allows users to run preemptible jobs on idle cores. Jobs in the scavenger queue may be interrupted if resources are required for other non-scavenger jobs. With few exceptions, each researcher using the HPCC is limited in the number of jobs or cores they can run at one time. Annually, non-buyin users are limited in the total number of CPU and GPU hours they can use. These limits do not apply to jobs submitted to the scavenger queue . Jobs in this queue can start on resources that would otherwise be left idle, improving research throughput. Similar to jobs submitted to the general-long queue, these jobs can request up to a 7-day wall time. The default behavior for interrupted jobs is to be re-queued, but users can opt for cancellation if it is more conducive to their workflow. Note We recommend that only users who can checkpoint and restart or have a workflow implemented that can manage jobs being canceled or requeued use this queue.","title":"Scavenger Queue"},{"location":"Scavenger_Queue/#usage","text":"To use the scavenger queue, add the following line to your job script: 1 #SBATCH --qos=scavenger To prevent your job from requeuing automatically if interrupted, add the following line to your job script: 1 #SBATCH --no-requeue The scavenger queue is not affected by the amount of wall time requested in your job script, e.g. 24 hours wall time is treated with the same scavenger queue priority as 4 hours wall time. Scavenger queue jobs will be automatically assigned to the scavenger account, regardless of the -A setting in your job script.","title":"Usage"},{"location":"Scavenger_Queue/#scheduling","text":"The scavenger queue runs using the backfill scheduler (see How Jobs are Scheduled ). Job scheduling may take on the order of minutes to occur, depending on the current load on the HPCC. Scavenger queue jobs run for a minimum of 1 minute before they can be preempted, but typically scavenger queue jobs run for approximately 1 hour before preemption.","title":"Scheduling"},{"location":"Science_DMZ/","text":"Science DMZ What is a DMZ? In the cyberinfrastructure context, a Demilitarized Zone (DMZ) is a portion of the network designed to optimize high-performance for research applications. The Science DMZ enables researchers to disseminate terabytes or even petabytes of specialized data more easily and at speeds of 10 to 100 gigabits per second to other institutions and cloud providers. This ability to share data immeasurably increases its value, as the insights extrapolated from it by additional researchers have the potential to change society in significant and meaningful ways. What is the Science DMZ? A Science DMZ enables cyber-enabled big-data scientific research to be shared globally with ease. The Science DMZ at MSU will offer increased network speeds and reliability, broadly enhancing MSU\u2019s research and education cyberinfrastructure. All campus network users will benefit from the high-speed network connections that will be used for sharing data already stored at MSU\u2019s High Performance Computing System and on the NSF-funded OSIRIS storage infrastructure. The creation of a Science DMZ at MSU helps eliminate obstacles for better access to valuable data. By sharing resources and working together, researchers are better positioned to collaboratively find solutions to our biggest problems. This project also lays the foundation for a new relationship between MSU IT and the Office of Research and Innovation, strengthening collaboration and strategic planning as MSU develops cyberinfrastructure capabilities to enhance scientific research support. More general information on the Science DMZ at Michigan State University can be found at tech.msu.edu. How do I use the Science DMZ at MSU? Globus is the recommended method to transfer big data files using MSU\u2019s Science DMZ. For a general overview of Globus and information on setting up a Globus account, see Transferring data with Globus For walk-through training on using Globus, please self-enroll in ICER's DMZ Globus Training D2L course .","title":"Science DMZ"},{"location":"Science_DMZ/#science-dmz","text":"What is a DMZ? In the cyberinfrastructure context, a Demilitarized Zone (DMZ) is a portion of the network designed to optimize high-performance for research applications. The Science DMZ enables researchers to disseminate terabytes or even petabytes of specialized data more easily and at speeds of 10 to 100 gigabits per second to other institutions and cloud providers. This ability to share data immeasurably increases its value, as the insights extrapolated from it by additional researchers have the potential to change society in significant and meaningful ways. What is the Science DMZ? A Science DMZ enables cyber-enabled big-data scientific research to be shared globally with ease. The Science DMZ at MSU will offer increased network speeds and reliability, broadly enhancing MSU\u2019s research and education cyberinfrastructure. All campus network users will benefit from the high-speed network connections that will be used for sharing data already stored at MSU\u2019s High Performance Computing System and on the NSF-funded OSIRIS storage infrastructure. The creation of a Science DMZ at MSU helps eliminate obstacles for better access to valuable data. By sharing resources and working together, researchers are better positioned to collaboratively find solutions to our biggest problems. This project also lays the foundation for a new relationship between MSU IT and the Office of Research and Innovation, strengthening collaboration and strategic planning as MSU develops cyberinfrastructure capabilities to enhance scientific research support. More general information on the Science DMZ at Michigan State University can be found at tech.msu.edu. How do I use the Science DMZ at MSU? Globus is the recommended method to transfer big data files using MSU\u2019s Science DMZ. For a general overview of Globus and information on setting up a Globus account, see Transferring data with Globus For walk-through training on using Globus, please self-enroll in ICER's DMZ Globus Training D2L course .","title":"Science DMZ"},{"location":"Scratch_File_Systems/","text":"Scratch Space Each user is provided with a working directory know as scratch space . This space is intended for intensive input/output (I/O) operations i.e. , heavy reading and writing of data, involving very large files and/or a very large number of small files. Research groups may also request a scratch space . Unlike the home space and research space , the scratch space is not intended for long-term storage and cannot be accessed from a gateway node , with the exception of the rsync gateway used for file transfer . Data stored in a user's scratch space is not backed-up , and files in a scratch space with no I/O operations for 45 days will be automatically deleted to ensure the space is available to all users. The limit on stroage is 50TB and the initial limit on the number of files conatined in a research space is 1,000,000 files. A user's scratch space is available at /mnt/scratch/$USER , or use the bash environemntal variable $SCRATCH . Use the quota command to check a user's current space and file quotas. 1 2 3 4 5 6 7 $ quota Temporary Filesystems: --------------------------------------------------------------------------------------------------------------------------------------- /mnt/scratch (/mnt/gs21) Space Quota Space Used Space Free Space % Used Filess Quota Files Used Files Free Files % Used 51200G 0G 51200G 0% 1048576 1 1048575 0% Using a Scratch Space Scratch Space can sustain high data transfer rates and is a good choice for data files used in running parallel on multiple nodes with intensive I/O requirements. Jobs of this type will run much faster with data accessed from a scratch space and users should follow the proceedure below for best practice: Configure the job script and/or the main program for scratch space I/O using the path /mnt/scratch/$USER or the variable $SCRATCH Copy input data from the home space or research space to scratch space; to maintain data integrity keep the original data files in the home space or research space Schedule the job and confirm sucessful completion of the I/O operations Move the resulting output data back to either the home space or research space Delete that data from the scratch space Time Limits on Scratch Space Files in a scratch space with no I/O operations for 45 days will be automatically deleted to ensure the space is available to all users. To find files in a scratch space approaching the 45 day limit run the following command: find $SCRATCH -type f -mtime +40 Here the +40 arguement specifies files with no I/O for more than 40 days. Users may set this arguement to any number of days desired, upto the 45 day limit.","title":"Scratch space"},{"location":"Scratch_File_Systems/#scratch-space","text":"Each user is provided with a working directory know as scratch space . This space is intended for intensive input/output (I/O) operations i.e. , heavy reading and writing of data, involving very large files and/or a very large number of small files. Research groups may also request a scratch space . Unlike the home space and research space , the scratch space is not intended for long-term storage and cannot be accessed from a gateway node , with the exception of the rsync gateway used for file transfer . Data stored in a user's scratch space is not backed-up , and files in a scratch space with no I/O operations for 45 days will be automatically deleted to ensure the space is available to all users. The limit on stroage is 50TB and the initial limit on the number of files conatined in a research space is 1,000,000 files. A user's scratch space is available at /mnt/scratch/$USER , or use the bash environemntal variable $SCRATCH . Use the quota command to check a user's current space and file quotas. 1 2 3 4 5 6 7 $ quota Temporary Filesystems: --------------------------------------------------------------------------------------------------------------------------------------- /mnt/scratch (/mnt/gs21) Space Quota Space Used Space Free Space % Used Filess Quota Files Used Files Free Files % Used 51200G 0G 51200G 0% 1048576 1 1048575 0%","title":"Scratch Space"},{"location":"Scratch_File_Systems/#using-a-scratch-space","text":"Scratch Space can sustain high data transfer rates and is a good choice for data files used in running parallel on multiple nodes with intensive I/O requirements. Jobs of this type will run much faster with data accessed from a scratch space and users should follow the proceedure below for best practice: Configure the job script and/or the main program for scratch space I/O using the path /mnt/scratch/$USER or the variable $SCRATCH Copy input data from the home space or research space to scratch space; to maintain data integrity keep the original data files in the home space or research space Schedule the job and confirm sucessful completion of the I/O operations Move the resulting output data back to either the home space or research space Delete that data from the scratch space","title":"Using a Scratch Space"},{"location":"Scratch_File_Systems/#time-limits-on-scratch-space","text":"Files in a scratch space with no I/O operations for 45 days will be automatically deleted to ensure the space is available to all users. To find files in a scratch space approaching the 45 day limit run the following command: find $SCRATCH -type f -mtime +40 Here the +40 arguement specifies files with no I/O for more than 40 days. Users may set this arguement to any number of days desired, upto the 45 day limit.","title":"Time Limits on Scratch Space"},{"location":"Sensitive_Data_on_the_HPCC/","text":"Sensitive Data on the HPCC Sensitive Data Hosting: Data that are subject to external security requirements are considered sensitive data. These requirements may be imposed by regulations or contractual obligations. In either case, users that intend to store sensitive data on HPCC systems must work with the HPCC to ensure data security. Examples of sensitive data include: Personally Identifiable Information (PII) Protected Health Information (PHI) Controlled Unclassified Information (CUI) Data covered under the Heath Insurance Portability and Accountability Act (HIPAA) Data covered under the Federal Information Security Management Act (FISMA) Data covered under the Federal Education Rights and Privacy Act (FERPA) Data with security requirements set by the MSU Institutional Review Board (IRB) Controlled access data from the NIH Database of Genotypes and Phenotypes (dbGaP) Information about the HPCC's Sensitive Data Policy is available to users with MSU credentials. MSU users may also visit https://data-storage-finder.tech.msu.edu/ for more information about data storage choices on campusn (NOTE: The page only accessible from campus).","title":"Sensitive data storage"},{"location":"Sensitive_Data_on_the_HPCC/#sensitive-data-on-the-hpcc","text":"Sensitive Data Hosting: Data that are subject to external security requirements are considered sensitive data. These requirements may be imposed by regulations or contractual obligations. In either case, users that intend to store sensitive data on HPCC systems must work with the HPCC to ensure data security. Examples of sensitive data include: Personally Identifiable Information (PII) Protected Health Information (PHI) Controlled Unclassified Information (CUI) Data covered under the Heath Insurance Portability and Accountability Act (HIPAA) Data covered under the Federal Information Security Management Act (FISMA) Data covered under the Federal Education Rights and Privacy Act (FERPA) Data with security requirements set by the MSU Institutional Review Board (IRB) Controlled access data from the NIH Database of Genotypes and Phenotypes (dbGaP) Information about the HPCC's Sensitive Data Policy is available to users with MSU credentials. MSU users may also visit https://data-storage-finder.tech.msu.edu/ for more information about data storage choices on campusn (NOTE: The page only accessible from campus).","title":"Sensitive Data on the HPCC"},{"location":"Show_Job_Steps_by_sacct_and_srun_Commands/","text":"Showing job steps SLURM provides commands to show the execution information of each command line in a job script. This can be helpful for debugging and testing. In order to get such information, the wrapper command srun needs to be used. Let's take a look at the following job script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/bash #SBATCH -N 4 -n 4 -c 2 #SBATCH --time=00:05:00 #SBATCH --mem=1G module purge ; module load GCC/6.4.0-2.28 OpenMPI/2.1.2 module list mpicc mpi-hello.c -o hello.exe echo ; echo \"====== mpirun hello.exe ======\" mpirun hello.exe #0 Step echo ; echo \"====== srun hello.exe ======\" srun hello.exe #1 Step echo ; echo \"====== srun -n 8 -c 1 hello.exe ======\" srun -n 8 -c 1 hello.exe #2 Step echo ; echo \"====== srun ======\" srun NoSuchCommand #3 Step echo ; echo \"====== mpirun ======\" mpirun NoSuchCommand #4 Step echo ; echo \"====== scontrol show job $SLURM_JOB_ID ======\" srun -N 1 -n 1 -c 1 scontrol show job $SLURM_JOB_ID #5 Step Although there are many command lines, only 6 of them are executed with either mpirun or srun wrapper and marked with the comments (from step 0 to 5) in the end. SLURM can record each of the 6 executions as a job step. Once the job is submitted by sbatch command and starts running, you can use sacct command to check the steps: 1 2 3 4 5 6 7 8 9 10 11 12 $ sacct -j 10732 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 10732 test general-l+ classres 8 COMPLETED 0 :0 10732 .batch batch classres 2 COMPLETED 0 :0 10732 .extern extern classres 8 COMPLETED 0 :0 10732 .0 orted classres 6 COMPLETED 0 :0 10732 .1 hello.exe classres 8 COMPLETED 0 :0 10732 .2 hello.exe classres 8 COMPLETED 0 :0 10732 .3 NoSuchCom+ classres 8 FAILED 2 :0 10732 .4 orted classres 6 COMPLETED 0 :0 10732 .5 scontrol classres 1 COMPLETED 0 :0 where the Job has ID 10732 and the 6 steps are shown from JobID 10732.0 to 10732.5. We can also use a powertools command js to see more detailed information (such as memory usage and a list of used nodes) about the steps: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 $ js -j 10732 -C5 SLURM Job ID: 10732 =============================================================================================================================== JobID | 10732 | 10732 .batch | 10732 .extern | 10732 .0 | 10732 .1 | JobName | test | batch | extern | orted | hello.exe | User | changc81 | | | | | NodeList | lac- [ 380 -383 ] | lac-380 | lac- [ 380 -383 ] | lac- [ 381 -383 ] | lac- [ 380 -383 ] | NNodes | 4 | 1 | 4 | 3 | 4 | NTasks | | 1 | 4 | 3 | 4 | NCPUS | 8 | 2 | 8 | 6 | 8 | ReqMem | 1Gn | 1Gn | 1Gn | 1Gn | 1Gn | Timelimit | 00 :05:00 | | | | | Elapsed | 00 :00:16 | 00 :00:16 | 00 :00:16 | 00 :00:02 | 00 :00:01 | SystemCPU | 00 :03.283 | 00 :00.562 | 00 :00.001 | 00 :00.646 | 00 :00.572 | UserCPU | 00 :02.119 | 00 :00.753 | 00 :00.003 | 00 :00.396 | 00 :00.281 | TotalCPU | 00 :05.403 | 00 :01.316 | 00 :00.005 | 00 :01.042 | 00 :00.853 | AveCPULoad | 0 .337687 | 0 .08225 | 0 .0003125 | 0 .521 | 0 .853 | MaxRSS | | 10409K | 120K | 861K | 863K | MaxVMSize | | 652100K | 173968K | 324440K | 324436K | Start | 2018 -08-06T13:22:44 | 2018 -08-06T13:22:44 | 2018 -08-06T13:22:44 | 2018 -08-06T13:22:54 | 2018 -08-06T13:22:57 | End | 2018 -08-06T13:23:00 | 2018 -08-06T13:23:00 | 2018 -08-06T13:23:00 | 2018 -08-06T13:22:56 | 2018 -08-06T13:22:58 | ExitCode | 0 :0 | 0 :0 | 0 :0 | 0 :0 | 0 :0 | State | COMPLETED | COMPLETED | COMPLETED | COMPLETED | COMPLETED | =============================================================================================================================== JobID | 10732 .2 | 10732 .3 | 10732 .4 | 10732 .5 | JobName | hello.exe | NoSuchCommand | orted | scontrol | User | | | | | NodeList | lac- [ 380 -383 ] | lac- [ 380 -383 ] | lac- [ 381 -383 ] | lac-380 | NNodes | 4 | 4 | 3 | 1 | NTasks | 8 | 4 | 3 | 1 | NCPUS | 8 | 8 | 6 | 1 | ReqMem | 1Gn | 1Gn | 1Gn | 1Gn | Timelimit | | | | | Elapsed | 00 :00:01 | 00 :00:00 | 00 :00:01 | 00 :00:00 | SystemCPU | 00 :01.141 | 00 :00.051 | 00 :00.289 | 00 :00.017 | UserCPU | 00 :00.521 | 00 :00.031 | 00 :00.096 | 00 :00.035 | TotalCPU | 00 :01.663 | 00 :00.083 | 00 :00.385 | 00 :00.053 | AveCPULoad | 1 .663 | | 0 .385 | | MaxRSS | 34812K | 865K | 865K | 840K | MaxVMSize | 324436K | 324436K | 324440K | 324436K | Start | 2018 -08-06T13:22:58 | 2018 -08-06T13:22:59 | 2018 -08-06T13:22:59 | 2018 -08-06T13:23:00 | End | 2018 -08-06T13:22:59 | 2018 -08-06T13:22:59 | 2018 -08-06T13:23:00 | 2018 -08-06T13:23:00 | ExitCode | 0 :0 | 2 :0 | 0 :0 | 0 :0 | State | COMPLETED | FAILED | COMPLETED | COMPLETED | ========================================================================================================= From the results above, we can see the executions by mpirun are different from srun . First of all, for mpirun , the JobName only show \"orted\" no matter what commands are used in the steps 10732.0 and 10732.4. However, srun shows the correct commands in all of the steps (10732.1, 10732.2, 10732.3 and 10732.5). Secondly, mpirun results show only 3 tasks with 6 CPUs are used but srun results correctly show 4 tasks with 8 CPUs in step 10732.1, 8 tasks with 8 CPUs in step 10732.2 and 1 task with 1 CPU in 10732.5 step. Finally, both steps 10732.3 and 10732.4 ran the same command NoSuchCommand where there is no such file or directory and should cause an error execution. However, mpirun wrapper still consider it is complete without error. Only srun wrapper get the FAIL state with an exit code 2. From the job output in the following results, we see no difference between the outputs of the step 10732.0 ( mpirun hello.exe ) and the step 10732.1 ( srun hello.exe ). SLURM seems to get a good sacct information with srun but not with mpirun . If you wish to use the step information, do not forget to put srun in the command lines. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 Currently Loaded Modules: 1) GCCcore/6.4.0 2) binutils/2.28 3) GCC/6.4.0-2.28 4) OpenMPI/2.1.1 ====== mpirun hello.exe ====== Hello From: lac-380 I am the recieving processor 1 of 4 Hello From: lac-381 I am processor 2 of 4 Hello From: lac-382 I am processor 3 of 4 Hello From: lac-383 I am processor 4 of 4 ====== srun hello.exe ====== Hello From: lac-380 I am the recieving processor 1 of 4 Hello From: lac-381 I am processor 2 of 4 Hello From: lac-382 I am processor 3 of 4 Hello From: lac-383 I am processor 4 of 4 ====== srun -n 8 -c 1 hello.exe ====== Hello From: lac-380 I am the recieving processor 1 of 8 Hello From: lac-380 I am processor 2 of 8 Hello From: lac-381 I am processor 3 of 8 Hello From: lac-381 I am processor 4 of 8 Hello From: lac-382 I am processor 5 of 8 Hello From: lac-382 I am processor 6 of 8 Hello From: lac-383 I am processor 7 of 8 Hello From: lac-383 I am processor 8 of 8 ====== srun ====== slurmstepd: error: execve(): NoSuchCommand: No such file or directory slurmstepd: error: execve(): NoSuchCommand: No such file or directory slurmstepd: error: execve(): NoSuchCommand: No such file or directory srun: error: lac-381: task 1: Exited with exit code 2 srun: error: lac-383: task 3: Exited with exit code 2 srun: error: lac-382: task 2: Exited with exit code 2 slurmstepd: error: execve(): NoSuchCommand: No such file or directory srun: error: lac-380: task 0: Exited with exit code 2 ====== mpirun ====== -------------------------------------------------------------------------- mpirun was unable to find the specified executable file, and therefore did not launch the job. This error was first reported for process rank 0; it may have occurred for other processes as well. NOTE: A common cause for this error is misspelling a mpirun command line parameter option (remember that mpirun interprets the first unrecognized command line token as the executable). Node: lac-380 Executable: NoSuchCommand -------------------------------------------------------------------------- ====== scontrol show job 10732 ====== JobId=10732 JobName=test UserId=changc81(804793) GroupId=helpdesk(2103) MCS_label=N/A Priority=103 Nice=0 Account=classres QOS=normal JobState=RUNNING Reason=None Dependency=(null) Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0 RunTime=00:00:18 TimeLimit=00:05:00 TimeMin=N/A SubmitTime=2018-08-06T13:22:43 EligibleTime=2018-08-06T13:22:43 StartTime=2018-08-06T13:22:44 EndTime=2018-08-06T13:27:44 Deadline=N/A PreemptTime=None SuspendTime=None SecsPreSuspend=0 LastSchedEval=2018-08-06T13:22:44 Partition=general-long-16 AllocNode:Sid=lac-249:5133 ReqNodeList=(null) ExcNodeList=(null) NodeList=lac-[380-383] BatchHost=lac-380 NumNodes=4 NumCPUs=8 NumTasks=4 CPUs/Task=2 ReqB:S:C:T=0:0:*:* TRES=cpu=8,mem=4G,node=4,billing=8 Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=* MinCPUsNode=2 MinMemoryNode=1G MinTmpDiskNode=0 Features=(null) DelayBoot=00:00:00 Gres=(null) Reservation=(null) OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null) Command=/mnt/home/changc81/GetExample/helloMPI/test WorkDir=/mnt/home/changc81/GetExample/helloMPI Comment=stdout=/mnt/home/changc81/GetExample/helloMPI/slurm-10732.out StdErr=/mnt/home/changc81/GetExample/helloMPI/slurm-10732.out StdIn=/dev/null StdOut=/mnt/home/changc81/GetExample/helloMPI/slurm-10732.out Power= For a complete instruction of sacct command, please refer to the SLURM web site .","title":"Showing job steps"},{"location":"Show_Job_Steps_by_sacct_and_srun_Commands/#showing-job-steps","text":"SLURM provides commands to show the execution information of each command line in a job script. This can be helpful for debugging and testing. In order to get such information, the wrapper command srun needs to be used. Let's take a look at the following job script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/bash #SBATCH -N 4 -n 4 -c 2 #SBATCH --time=00:05:00 #SBATCH --mem=1G module purge ; module load GCC/6.4.0-2.28 OpenMPI/2.1.2 module list mpicc mpi-hello.c -o hello.exe echo ; echo \"====== mpirun hello.exe ======\" mpirun hello.exe #0 Step echo ; echo \"====== srun hello.exe ======\" srun hello.exe #1 Step echo ; echo \"====== srun -n 8 -c 1 hello.exe ======\" srun -n 8 -c 1 hello.exe #2 Step echo ; echo \"====== srun ======\" srun NoSuchCommand #3 Step echo ; echo \"====== mpirun ======\" mpirun NoSuchCommand #4 Step echo ; echo \"====== scontrol show job $SLURM_JOB_ID ======\" srun -N 1 -n 1 -c 1 scontrol show job $SLURM_JOB_ID #5 Step Although there are many command lines, only 6 of them are executed with either mpirun or srun wrapper and marked with the comments (from step 0 to 5) in the end. SLURM can record each of the 6 executions as a job step. Once the job is submitted by sbatch command and starts running, you can use sacct command to check the steps: 1 2 3 4 5 6 7 8 9 10 11 12 $ sacct -j 10732 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 10732 test general-l+ classres 8 COMPLETED 0 :0 10732 .batch batch classres 2 COMPLETED 0 :0 10732 .extern extern classres 8 COMPLETED 0 :0 10732 .0 orted classres 6 COMPLETED 0 :0 10732 .1 hello.exe classres 8 COMPLETED 0 :0 10732 .2 hello.exe classres 8 COMPLETED 0 :0 10732 .3 NoSuchCom+ classres 8 FAILED 2 :0 10732 .4 orted classres 6 COMPLETED 0 :0 10732 .5 scontrol classres 1 COMPLETED 0 :0 where the Job has ID 10732 and the 6 steps are shown from JobID 10732.0 to 10732.5. We can also use a powertools command js to see more detailed information (such as memory usage and a list of used nodes) about the steps: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 $ js -j 10732 -C5 SLURM Job ID: 10732 =============================================================================================================================== JobID | 10732 | 10732 .batch | 10732 .extern | 10732 .0 | 10732 .1 | JobName | test | batch | extern | orted | hello.exe | User | changc81 | | | | | NodeList | lac- [ 380 -383 ] | lac-380 | lac- [ 380 -383 ] | lac- [ 381 -383 ] | lac- [ 380 -383 ] | NNodes | 4 | 1 | 4 | 3 | 4 | NTasks | | 1 | 4 | 3 | 4 | NCPUS | 8 | 2 | 8 | 6 | 8 | ReqMem | 1Gn | 1Gn | 1Gn | 1Gn | 1Gn | Timelimit | 00 :05:00 | | | | | Elapsed | 00 :00:16 | 00 :00:16 | 00 :00:16 | 00 :00:02 | 00 :00:01 | SystemCPU | 00 :03.283 | 00 :00.562 | 00 :00.001 | 00 :00.646 | 00 :00.572 | UserCPU | 00 :02.119 | 00 :00.753 | 00 :00.003 | 00 :00.396 | 00 :00.281 | TotalCPU | 00 :05.403 | 00 :01.316 | 00 :00.005 | 00 :01.042 | 00 :00.853 | AveCPULoad | 0 .337687 | 0 .08225 | 0 .0003125 | 0 .521 | 0 .853 | MaxRSS | | 10409K | 120K | 861K | 863K | MaxVMSize | | 652100K | 173968K | 324440K | 324436K | Start | 2018 -08-06T13:22:44 | 2018 -08-06T13:22:44 | 2018 -08-06T13:22:44 | 2018 -08-06T13:22:54 | 2018 -08-06T13:22:57 | End | 2018 -08-06T13:23:00 | 2018 -08-06T13:23:00 | 2018 -08-06T13:23:00 | 2018 -08-06T13:22:56 | 2018 -08-06T13:22:58 | ExitCode | 0 :0 | 0 :0 | 0 :0 | 0 :0 | 0 :0 | State | COMPLETED | COMPLETED | COMPLETED | COMPLETED | COMPLETED | =============================================================================================================================== JobID | 10732 .2 | 10732 .3 | 10732 .4 | 10732 .5 | JobName | hello.exe | NoSuchCommand | orted | scontrol | User | | | | | NodeList | lac- [ 380 -383 ] | lac- [ 380 -383 ] | lac- [ 381 -383 ] | lac-380 | NNodes | 4 | 4 | 3 | 1 | NTasks | 8 | 4 | 3 | 1 | NCPUS | 8 | 8 | 6 | 1 | ReqMem | 1Gn | 1Gn | 1Gn | 1Gn | Timelimit | | | | | Elapsed | 00 :00:01 | 00 :00:00 | 00 :00:01 | 00 :00:00 | SystemCPU | 00 :01.141 | 00 :00.051 | 00 :00.289 | 00 :00.017 | UserCPU | 00 :00.521 | 00 :00.031 | 00 :00.096 | 00 :00.035 | TotalCPU | 00 :01.663 | 00 :00.083 | 00 :00.385 | 00 :00.053 | AveCPULoad | 1 .663 | | 0 .385 | | MaxRSS | 34812K | 865K | 865K | 840K | MaxVMSize | 324436K | 324436K | 324440K | 324436K | Start | 2018 -08-06T13:22:58 | 2018 -08-06T13:22:59 | 2018 -08-06T13:22:59 | 2018 -08-06T13:23:00 | End | 2018 -08-06T13:22:59 | 2018 -08-06T13:22:59 | 2018 -08-06T13:23:00 | 2018 -08-06T13:23:00 | ExitCode | 0 :0 | 2 :0 | 0 :0 | 0 :0 | State | COMPLETED | FAILED | COMPLETED | COMPLETED | ========================================================================================================= From the results above, we can see the executions by mpirun are different from srun . First of all, for mpirun , the JobName only show \"orted\" no matter what commands are used in the steps 10732.0 and 10732.4. However, srun shows the correct commands in all of the steps (10732.1, 10732.2, 10732.3 and 10732.5). Secondly, mpirun results show only 3 tasks with 6 CPUs are used but srun results correctly show 4 tasks with 8 CPUs in step 10732.1, 8 tasks with 8 CPUs in step 10732.2 and 1 task with 1 CPU in 10732.5 step. Finally, both steps 10732.3 and 10732.4 ran the same command NoSuchCommand where there is no such file or directory and should cause an error execution. However, mpirun wrapper still consider it is complete without error. Only srun wrapper get the FAIL state with an exit code 2. From the job output in the following results, we see no difference between the outputs of the step 10732.0 ( mpirun hello.exe ) and the step 10732.1 ( srun hello.exe ). SLURM seems to get a good sacct information with srun but not with mpirun . If you wish to use the step information, do not forget to put srun in the command lines. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 Currently Loaded Modules: 1) GCCcore/6.4.0 2) binutils/2.28 3) GCC/6.4.0-2.28 4) OpenMPI/2.1.1 ====== mpirun hello.exe ====== Hello From: lac-380 I am the recieving processor 1 of 4 Hello From: lac-381 I am processor 2 of 4 Hello From: lac-382 I am processor 3 of 4 Hello From: lac-383 I am processor 4 of 4 ====== srun hello.exe ====== Hello From: lac-380 I am the recieving processor 1 of 4 Hello From: lac-381 I am processor 2 of 4 Hello From: lac-382 I am processor 3 of 4 Hello From: lac-383 I am processor 4 of 4 ====== srun -n 8 -c 1 hello.exe ====== Hello From: lac-380 I am the recieving processor 1 of 8 Hello From: lac-380 I am processor 2 of 8 Hello From: lac-381 I am processor 3 of 8 Hello From: lac-381 I am processor 4 of 8 Hello From: lac-382 I am processor 5 of 8 Hello From: lac-382 I am processor 6 of 8 Hello From: lac-383 I am processor 7 of 8 Hello From: lac-383 I am processor 8 of 8 ====== srun ====== slurmstepd: error: execve(): NoSuchCommand: No such file or directory slurmstepd: error: execve(): NoSuchCommand: No such file or directory slurmstepd: error: execve(): NoSuchCommand: No such file or directory srun: error: lac-381: task 1: Exited with exit code 2 srun: error: lac-383: task 3: Exited with exit code 2 srun: error: lac-382: task 2: Exited with exit code 2 slurmstepd: error: execve(): NoSuchCommand: No such file or directory srun: error: lac-380: task 0: Exited with exit code 2 ====== mpirun ====== -------------------------------------------------------------------------- mpirun was unable to find the specified executable file, and therefore did not launch the job. This error was first reported for process rank 0; it may have occurred for other processes as well. NOTE: A common cause for this error is misspelling a mpirun command line parameter option (remember that mpirun interprets the first unrecognized command line token as the executable). Node: lac-380 Executable: NoSuchCommand -------------------------------------------------------------------------- ====== scontrol show job 10732 ====== JobId=10732 JobName=test UserId=changc81(804793) GroupId=helpdesk(2103) MCS_label=N/A Priority=103 Nice=0 Account=classres QOS=normal JobState=RUNNING Reason=None Dependency=(null) Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0 RunTime=00:00:18 TimeLimit=00:05:00 TimeMin=N/A SubmitTime=2018-08-06T13:22:43 EligibleTime=2018-08-06T13:22:43 StartTime=2018-08-06T13:22:44 EndTime=2018-08-06T13:27:44 Deadline=N/A PreemptTime=None SuspendTime=None SecsPreSuspend=0 LastSchedEval=2018-08-06T13:22:44 Partition=general-long-16 AllocNode:Sid=lac-249:5133 ReqNodeList=(null) ExcNodeList=(null) NodeList=lac-[380-383] BatchHost=lac-380 NumNodes=4 NumCPUs=8 NumTasks=4 CPUs/Task=2 ReqB:S:C:T=0:0:*:* TRES=cpu=8,mem=4G,node=4,billing=8 Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=* MinCPUsNode=2 MinMemoryNode=1G MinTmpDiskNode=0 Features=(null) DelayBoot=00:00:00 Gres=(null) Reservation=(null) OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null) Command=/mnt/home/changc81/GetExample/helloMPI/test WorkDir=/mnt/home/changc81/GetExample/helloMPI Comment=stdout=/mnt/home/changc81/GetExample/helloMPI/slurm-10732.out StdErr=/mnt/home/changc81/GetExample/helloMPI/slurm-10732.out StdIn=/dev/null StdOut=/mnt/home/changc81/GetExample/helloMPI/slurm-10732.out Power= For a complete instruction of sacct command, please refer to the SLURM web site .","title":"Showing job steps"},{"location":"Singularity_I._Introduction/","text":"Singularity: I. Introduction Singularity is installed on our HPCC. However, you may want to develop your own containers first on a local machine. Many HPC centers including MSU HPCC do not allow Docker containers through Docker. However, Singularity is compatible with Docker, and you can use Docker containers through Singularity. There are a few distinct difference between Docker and Singularity. Docker: Inside a Docker image, the user's privilege is escalated to root on the host system. This privilege is not supported by most HPCC including MSU HPCC. It means that Docker will not be installed on our system. Singularity: User has root privileges if elevated with \"sudo\" when a container runs. Can run and modify Docker images and containers These key difference make Singularity be installed on most HPCC. In addition, virtually all Docker containers can be run through Singularity, users can effectively run Docker on MSU HPCC. - - [Installation](#Singularity:I.Introduction-Installation) - [Check Installation](#Singularity:I.Introduction-CheckInstallation) - [Downloading pre-built images](#Singularity:I.Introduction-Downloadingpre-builtimages) - [Pulling an images from Sylabs cloud library](#Singularity:I.Introduction-PullinganimagesfromSylabscloudlibrary) - [Pulling an images from Docker hub](#Singularity:I.Introduction-PullinganimagesfromDockerhub) - [Interact with images](#Singularity:I.Introduction-Interactwithimages) - [shell](#Singularity:I.Introduction-shell) - [exec](#Singularity:I.Introduction-exec) - [run](#Singularity:I.Introduction-run) - [Cache setting](#Singularity:I.Introduction-Cachesetting) - [Creating writable containers with --sandbox options](#Singularity:I.Introduction-Creatingwritablecontainerswith--sandboxoptions) Installation Singularity exits as two major version, 2 and 3. Current version on MSU HPCC is 3.5.3. Therefore, in this tutorial, I will use version 3. To Install Singularity on your local machine, click here: https://www.sylabs.io/guides/3.0/user-guide/installation.html#installation The version of singularity on MSU HPCC is currently 3.4.1. The official documentation for this version is at https://www.sylabs.io/guides/3.4/user-guide/index.html . All singularity commands are built into the system such as 'singularity shell' and 'singularity exec', which means you can invoke these commands directly from the command line. Check Installation When you install Singularity on your local machine, then you can check the installation with 1 2 3 $ singularity pull shub://vsoch/hello-world INFO: Downloading shub image 59 .75 MiB / 59 .75 MiB [========================================================================================] 100 .00% 10 .46 MiB/s 5s In the above example, I used the Singularity Hub \u201cunique resource identifier,\u201d or uri , \" shub://\" which tells the software to run an image from Singularity Hub. To get help, you can use the help command which gives a general overview of Singularity options and subcommands as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 $ singularity --help Linux container platform optimized for High Performance Computing ( HPC ) and Enterprise Performance Computing ( EPC ) Usage: singularity [ global options... ] Description: Singularity containers provide an application virtualization layer enabling mobility of compute via both application and environment portability. With Singularity one is capable of building a root file system that runs on any other Linux system where Singularity is installed. Options: -d, --debug print debugging information ( highest verbosity ) -h, --help help for singularity --nocolor print without color output ( default False ) -q, --quiet suppress normal output -s, --silent only print errors -v, --verbose print additional information --version version for singularity Available Commands: build Build a Singularity image cache Manage the local cache capability Manage Linux capabilities for users and groups config Manage various singularity configuration ( root user only ) delete Deletes requested image from the library exec Run a command within a container help Help about any command inspect Show metadata for an image instance Manage containers running as services key Manage OpenPGP keys oci Manage OCI containers plugin Manage Singularity plugins pull Pull an image from a URI push Upload image to the provided URI remote Manage singularity remote endpoints run Run the user-defined default command within a container run-help Show the user-defined help for an image search Search a Container Library for images shell Run a shell within a container sif siftool is a program for Singularity Image Format ( SIF ) file manipulation sign Attach a cryptographic signature to an image test Run the user-defined tests within a container verify Verify cryptographic signatures attached to an image version Show the version for Singularity Examples: $ singularity help <command> [ <subcommand> ] $ singularity help build $ singularity help instance start For additional help or support, please visit https://www.sylabs.io/docs/ You can use the help command if you want to see the information about subcommands. For example, to see the pull command help, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 $ singularity help pullPull an image from a URI Usage: singularity pull [ pull options... ] [ output file ] <URI> Description: The 'pull' command allows you to download or build a container from a given URI. Supported URIs include: library: Pull an image from the currently configured library library://user/collection/container [ :tag ] docker: Pull an image from Docker Hub docker://user/image:tag shub: Pull an image from Singularity Hub shub://user/image:tag oras: Pull a SIF image from a supporting OCI registry oras://registry/namespace/image:tag http, https: Pull an image using the http ( s? ) protocol https://library.sylabs.io/v1/imagefile/library/default/alpine:latest Options: --arch string architecture to pull from library ( default \"amd64\" ) --dir string download images to the specific directory --disable-cache dont use cached images/blobs and dont create them --docker-login login to a Docker Repository interactively -F, --force overwrite an image file if it exists -h, --help help for pull --library string download images from the provided library ( default \"https://library.sylabs.io\" ) --no-cleanup do NOT clean up bundle after failed build, can be helpul for debugging --nohttps do NOT use HTTPS with the docker:// transport ( useful for local docker registries without a certificate ) Examples: From Sylabs cloud library $ singularity pull alpine.sif library://alpine:latest From Docker $ singularity pull tensorflow.sif docker://tensorflow/tensorflow:latest From Shub $ singularity pull singularity-images.sif shub://vsoch/singularity-images From supporting OCI registry ( e.g. Azure Container Registry ) $ singularity pull image.sif oras://<username>.azurecr.io/namespace/image:tag For additional help or support, please visit https://www.sylabs.io/docs/ Downloading pre-built images I already downloaded a pre-built image \"hello-world\" from shub, one of the registries, using pull command. This is the easiest way to use Singularity. You can use the pull command to download pre-built images from a number of Container Registries, here we\u2019ll be focusing on the Singularity-Hub or DockerHub . The following are some of container registries. library - images hosted on Sylabs Cloud shub - images hosted on Singularity Hub docker - images hosted on Docker Hub localimage - images saved on your machine yum - yum based systems such as CentOS and Scientific Linux debootstrap - apt based systems such as Debian and Ubuntu arch - Arch Linux busybox - BusyBox zypper - zypper based systems such as Suse and OpenSuse Pulling an images from Sylabs cloud library In this example, I will pull a base Alpine container from Sylabs cloud: 1 2 3 $ singularity pull library://sylabsed/linux/alpine INFO: Downloading library image 2 .08 MiB / 2 .08 MiB [===========================================================================================] 100 .00% 4 .74 MiB/s 0s You can rename the container using the \u2013name flag: 1 2 3 $ singularity pull --name my_alpine.sif library://sylabsed/linux/alpine INFO: Downloading library image 2 .08 MiB / 2 .08 MiB [===========================================================================================] 100 .00% 9 .65 MiB/s 0s The above example will save the image as \"my_alpine.sif\" Pulling an images from Docker hub This example pulls an Alpine image from Docker hub 1 2 3 4 5 6 7 8 9 10 11 $ singularity pull docker://alpine INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob df20fa9351a1 done Copying config 0f5f445df8 done Writing manifest to image destination Storing signatures 2020 /08/20 15 :53:52 info unpack layer: sha256:df20fa9351a15782c64e6dddb2d4a6f50bf6d3688060a34c4014b0d9a752eb4c INFO: Creating SIF file... INFO: Build complete: alpine_latest.sif Interact with images You can interact with images with shell , exec , and run commands. To learn how to interact with images, let's first pull an image \"lolcow_latest.sif\" from the libray. 1 $ singularity pull library://sylabsed/examples/lolcow shell The shell command allows you to spawn a new shell within your container and interact with it as if it is a virtual machine. 1 2 $singularity shell lolcow_latest.sif Singularity> The change in prompt indicates that you have entered the container. Once inside of a container, you are the same user as you are on the host system. 1 2 Singularity>whoami choiyj To exit from a container, type exit . 1 2 Singularity>exit $ exec The exec command allows you to execute a custom command within a container by specifying the image file. For instance, to execute the cowsay program within the lolcow_latest.sif container: 1 2 3 4 5 6 7 8 9 $ singularity exec lolcow_latest.sif cowsay container camp rocks ______________________ < container camp rocks > ---------------------- \\ ^__^ \\ ( oo ) \\_ ______ ( __ ) \\ ) \\/\\ || ----w | || || You can also use shell command to run the program in the container. 1 2 3 4 5 6 7 8 9 Singularity> cowsay container camp rocks ______________________ < container camp rocks > ---------------------- \\ ^__^ \\ ( oo ) \\_ ______ ( __ ) \\ ) \\/\\ || ----w | || || run Singularity containers contain runscripts. These are user defined scripts which define the actions of a container when user runs it. The runscript can be performed with the run command, or simply by calling the container as though it were an executable. 1 2 3 4 5 6 7 8 9 10 $ singularity run lolcow_latest.sif _________________________________________ / You ' re ugly and your mother dresses you \\ \\ funny. / ----------------------------------------- \\ ^__^ \\ ( oo ) \\_ ______ ( __ ) \\ ) \\/\\ || ----w | || || Cache setting By default, Singularity uses a temporary directory to save Docker files as tarballs: 1 2 3 4 5 6 7 8 $ ls ~/.singularity cache/ docker/ metadata/ $ ls .singularity/docker/ sha256:0efe07335a049e6afcd757db2d17ba37a12b717eb807acb03ddf3cd756b9fc2a.tar.gz sha256:43a98c1873995475a895f3d79f405232ef5230076b3f610c949c2e8341743af7.tar.gz sha256:6b1bb01b3a3b72463ae8ac5666d57b28f1a21d5256271910ac8df841aa04ecd1.tar.gz sha256:c2ca09a1934b951505ecc4d6b2e4ab7f9bf27bcdfb8999d0181deca74daf7683.tar.gz sha256:d6c3619d2153ffdefa4a9c19f15c5d566ce271b397a84537baa9ee45b24178f2.tar.gz You can change these by theses cache directories by specifying the location on your localhost as following: 1 2 3 $ mkdir -p $SCRATCH /singularity_tmp $ mkdir -p $SCRATCH /singularity_scratch $ SINGULARITY_TMPDIR = $SCRATCH /singularity_scratch SINGULARITY_CACHEDIR = $SCRATCH /singularity_tmp singularity --debug pull --name ubuntu-tmpdir.sif docker://ubuntu Creating writable containers with --sandbox options If you want to build a container within a writable directory (called a sandbox ), you can do that with --sandbox option. We\u2019ve already covered how you can pull an existing container from Docker Hub, but we can also build a Singularity container from docker using the build command: 1 $ singularity build --sandbox ubuntu-latest/ docker://ubuntu With --sandbox option, you can changes and write files within the directory, but those changes will not remain when you finished using the container. To make those changes persistent, use --writable option when you start a container. 1 $ singularity shell --writable ubuntu-latest/","title":"Singularity: I. Introduction"},{"location":"Singularity_I._Introduction/#singularity-i-introduction","text":"Singularity is installed on our HPCC. However, you may want to develop your own containers first on a local machine. Many HPC centers including MSU HPCC do not allow Docker containers through Docker. However, Singularity is compatible with Docker, and you can use Docker containers through Singularity. There are a few distinct difference between Docker and Singularity. Docker: Inside a Docker image, the user's privilege is escalated to root on the host system. This privilege is not supported by most HPCC including MSU HPCC. It means that Docker will not be installed on our system. Singularity: User has root privileges if elevated with \"sudo\" when a container runs. Can run and modify Docker images and containers These key difference make Singularity be installed on most HPCC. In addition, virtually all Docker containers can be run through Singularity, users can effectively run Docker on MSU HPCC. - - [Installation](#Singularity:I.Introduction-Installation) - [Check Installation](#Singularity:I.Introduction-CheckInstallation) - [Downloading pre-built images](#Singularity:I.Introduction-Downloadingpre-builtimages) - [Pulling an images from Sylabs cloud library](#Singularity:I.Introduction-PullinganimagesfromSylabscloudlibrary) - [Pulling an images from Docker hub](#Singularity:I.Introduction-PullinganimagesfromDockerhub) - [Interact with images](#Singularity:I.Introduction-Interactwithimages) - [shell](#Singularity:I.Introduction-shell) - [exec](#Singularity:I.Introduction-exec) - [run](#Singularity:I.Introduction-run) - [Cache setting](#Singularity:I.Introduction-Cachesetting) - [Creating writable containers with --sandbox options](#Singularity:I.Introduction-Creatingwritablecontainerswith--sandboxoptions)","title":"Singularity: I. Introduction"},{"location":"Singularity_I._Introduction/#installation","text":"Singularity exits as two major version, 2 and 3. Current version on MSU HPCC is 3.5.3. Therefore, in this tutorial, I will use version 3. To Install Singularity on your local machine, click here: https://www.sylabs.io/guides/3.0/user-guide/installation.html#installation The version of singularity on MSU HPCC is currently 3.4.1. The official documentation for this version is at https://www.sylabs.io/guides/3.4/user-guide/index.html . All singularity commands are built into the system such as 'singularity shell' and 'singularity exec', which means you can invoke these commands directly from the command line.","title":"Installation"},{"location":"Singularity_I._Introduction/#check-installation","text":"When you install Singularity on your local machine, then you can check the installation with 1 2 3 $ singularity pull shub://vsoch/hello-world INFO: Downloading shub image 59 .75 MiB / 59 .75 MiB [========================================================================================] 100 .00% 10 .46 MiB/s 5s In the above example, I used the Singularity Hub \u201cunique resource identifier,\u201d or uri , \" shub://\" which tells the software to run an image from Singularity Hub. To get help, you can use the help command which gives a general overview of Singularity options and subcommands as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 $ singularity --help Linux container platform optimized for High Performance Computing ( HPC ) and Enterprise Performance Computing ( EPC ) Usage: singularity [ global options... ] Description: Singularity containers provide an application virtualization layer enabling mobility of compute via both application and environment portability. With Singularity one is capable of building a root file system that runs on any other Linux system where Singularity is installed. Options: -d, --debug print debugging information ( highest verbosity ) -h, --help help for singularity --nocolor print without color output ( default False ) -q, --quiet suppress normal output -s, --silent only print errors -v, --verbose print additional information --version version for singularity Available Commands: build Build a Singularity image cache Manage the local cache capability Manage Linux capabilities for users and groups config Manage various singularity configuration ( root user only ) delete Deletes requested image from the library exec Run a command within a container help Help about any command inspect Show metadata for an image instance Manage containers running as services key Manage OpenPGP keys oci Manage OCI containers plugin Manage Singularity plugins pull Pull an image from a URI push Upload image to the provided URI remote Manage singularity remote endpoints run Run the user-defined default command within a container run-help Show the user-defined help for an image search Search a Container Library for images shell Run a shell within a container sif siftool is a program for Singularity Image Format ( SIF ) file manipulation sign Attach a cryptographic signature to an image test Run the user-defined tests within a container verify Verify cryptographic signatures attached to an image version Show the version for Singularity Examples: $ singularity help <command> [ <subcommand> ] $ singularity help build $ singularity help instance start For additional help or support, please visit https://www.sylabs.io/docs/ You can use the help command if you want to see the information about subcommands. For example, to see the pull command help, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 $ singularity help pullPull an image from a URI Usage: singularity pull [ pull options... ] [ output file ] <URI> Description: The 'pull' command allows you to download or build a container from a given URI. Supported URIs include: library: Pull an image from the currently configured library library://user/collection/container [ :tag ] docker: Pull an image from Docker Hub docker://user/image:tag shub: Pull an image from Singularity Hub shub://user/image:tag oras: Pull a SIF image from a supporting OCI registry oras://registry/namespace/image:tag http, https: Pull an image using the http ( s? ) protocol https://library.sylabs.io/v1/imagefile/library/default/alpine:latest Options: --arch string architecture to pull from library ( default \"amd64\" ) --dir string download images to the specific directory --disable-cache dont use cached images/blobs and dont create them --docker-login login to a Docker Repository interactively -F, --force overwrite an image file if it exists -h, --help help for pull --library string download images from the provided library ( default \"https://library.sylabs.io\" ) --no-cleanup do NOT clean up bundle after failed build, can be helpul for debugging --nohttps do NOT use HTTPS with the docker:// transport ( useful for local docker registries without a certificate ) Examples: From Sylabs cloud library $ singularity pull alpine.sif library://alpine:latest From Docker $ singularity pull tensorflow.sif docker://tensorflow/tensorflow:latest From Shub $ singularity pull singularity-images.sif shub://vsoch/singularity-images From supporting OCI registry ( e.g. Azure Container Registry ) $ singularity pull image.sif oras://<username>.azurecr.io/namespace/image:tag For additional help or support, please visit https://www.sylabs.io/docs/","title":"Check Installation"},{"location":"Singularity_I._Introduction/#downloading-pre-built-images","text":"I already downloaded a pre-built image \"hello-world\" from shub, one of the registries, using pull command. This is the easiest way to use Singularity. You can use the pull command to download pre-built images from a number of Container Registries, here we\u2019ll be focusing on the Singularity-Hub or DockerHub . The following are some of container registries. library - images hosted on Sylabs Cloud shub - images hosted on Singularity Hub docker - images hosted on Docker Hub localimage - images saved on your machine yum - yum based systems such as CentOS and Scientific Linux debootstrap - apt based systems such as Debian and Ubuntu arch - Arch Linux busybox - BusyBox zypper - zypper based systems such as Suse and OpenSuse","title":"Downloading pre-built images"},{"location":"Singularity_I._Introduction/#pulling-an-images-from-sylabs-cloud-library","text":"In this example, I will pull a base Alpine container from Sylabs cloud: 1 2 3 $ singularity pull library://sylabsed/linux/alpine INFO: Downloading library image 2 .08 MiB / 2 .08 MiB [===========================================================================================] 100 .00% 4 .74 MiB/s 0s You can rename the container using the \u2013name flag: 1 2 3 $ singularity pull --name my_alpine.sif library://sylabsed/linux/alpine INFO: Downloading library image 2 .08 MiB / 2 .08 MiB [===========================================================================================] 100 .00% 9 .65 MiB/s 0s The above example will save the image as \"my_alpine.sif\"","title":"Pulling an images from Sylabs cloud library"},{"location":"Singularity_I._Introduction/#pulling-an-images-from-docker-hub","text":"This example pulls an Alpine image from Docker hub 1 2 3 4 5 6 7 8 9 10 11 $ singularity pull docker://alpine INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob df20fa9351a1 done Copying config 0f5f445df8 done Writing manifest to image destination Storing signatures 2020 /08/20 15 :53:52 info unpack layer: sha256:df20fa9351a15782c64e6dddb2d4a6f50bf6d3688060a34c4014b0d9a752eb4c INFO: Creating SIF file... INFO: Build complete: alpine_latest.sif","title":"Pulling an images from Docker hub"},{"location":"Singularity_I._Introduction/#interact-with-images","text":"You can interact with images with shell , exec , and run commands. To learn how to interact with images, let's first pull an image \"lolcow_latest.sif\" from the libray. 1 $ singularity pull library://sylabsed/examples/lolcow","title":"Interact with images"},{"location":"Singularity_I._Introduction/#shell","text":"The shell command allows you to spawn a new shell within your container and interact with it as if it is a virtual machine. 1 2 $singularity shell lolcow_latest.sif Singularity> The change in prompt indicates that you have entered the container. Once inside of a container, you are the same user as you are on the host system. 1 2 Singularity>whoami choiyj To exit from a container, type exit . 1 2 Singularity>exit $","title":"shell"},{"location":"Singularity_I._Introduction/#exec","text":"The exec command allows you to execute a custom command within a container by specifying the image file. For instance, to execute the cowsay program within the lolcow_latest.sif container: 1 2 3 4 5 6 7 8 9 $ singularity exec lolcow_latest.sif cowsay container camp rocks ______________________ < container camp rocks > ---------------------- \\ ^__^ \\ ( oo ) \\_ ______ ( __ ) \\ ) \\/\\ || ----w | || || You can also use shell command to run the program in the container. 1 2 3 4 5 6 7 8 9 Singularity> cowsay container camp rocks ______________________ < container camp rocks > ---------------------- \\ ^__^ \\ ( oo ) \\_ ______ ( __ ) \\ ) \\/\\ || ----w | || ||","title":"exec"},{"location":"Singularity_I._Introduction/#run","text":"Singularity containers contain runscripts. These are user defined scripts which define the actions of a container when user runs it. The runscript can be performed with the run command, or simply by calling the container as though it were an executable. 1 2 3 4 5 6 7 8 9 10 $ singularity run lolcow_latest.sif _________________________________________ / You ' re ugly and your mother dresses you \\ \\ funny. / ----------------------------------------- \\ ^__^ \\ ( oo ) \\_ ______ ( __ ) \\ ) \\/\\ || ----w | || ||","title":"run"},{"location":"Singularity_I._Introduction/#cache-setting","text":"By default, Singularity uses a temporary directory to save Docker files as tarballs: 1 2 3 4 5 6 7 8 $ ls ~/.singularity cache/ docker/ metadata/ $ ls .singularity/docker/ sha256:0efe07335a049e6afcd757db2d17ba37a12b717eb807acb03ddf3cd756b9fc2a.tar.gz sha256:43a98c1873995475a895f3d79f405232ef5230076b3f610c949c2e8341743af7.tar.gz sha256:6b1bb01b3a3b72463ae8ac5666d57b28f1a21d5256271910ac8df841aa04ecd1.tar.gz sha256:c2ca09a1934b951505ecc4d6b2e4ab7f9bf27bcdfb8999d0181deca74daf7683.tar.gz sha256:d6c3619d2153ffdefa4a9c19f15c5d566ce271b397a84537baa9ee45b24178f2.tar.gz You can change these by theses cache directories by specifying the location on your localhost as following: 1 2 3 $ mkdir -p $SCRATCH /singularity_tmp $ mkdir -p $SCRATCH /singularity_scratch $ SINGULARITY_TMPDIR = $SCRATCH /singularity_scratch SINGULARITY_CACHEDIR = $SCRATCH /singularity_tmp singularity --debug pull --name ubuntu-tmpdir.sif docker://ubuntu","title":"Cache setting"},{"location":"Singularity_I._Introduction/#creating-writable-containers-with-sandbox-options","text":"If you want to build a container within a writable directory (called a sandbox ), you can do that with --sandbox option. We\u2019ve already covered how you can pull an existing container from Docker Hub, but we can also build a Singularity container from docker using the build command: 1 $ singularity build --sandbox ubuntu-latest/ docker://ubuntu With --sandbox option, you can changes and write files within the directory, but those changes will not remain when you finished using the container. To make those changes persistent, use --writable option when you start a container. 1 $ singularity shell --writable ubuntu-latest/","title":"Creating writable containers with --sandbox options"},{"location":"Singularity_II._Running_a_container_on_HPC/","text":"Singularity: II. Running a container on HPC Why use a container? Changes from the old CentOS 6 system General information of singularity on the HPCC Running a docker container Building containers Submitting a singularity job to our cluster Why use a container? Singularity allows users to run software inside of containers. A popular container system is 'Docker', which is interoperable with singularity. A Linux container provides an environment that's different from the Linux host server you may be running on. For example, you could run a different version of Linux (e.g., running Ubuntu on our CentOS system). One advantage of containers is if your software requires a newer version of system libraries (e.g. glibc) than is available in our operating system, then you can run your software in a container. The main reason for using containers is that all of the dependencies are pre-installed. Changes from the old CentOS 6 system First, 'module load singularity' is no longer required now on the CentOS 7 system. All singularity commands are built into the system such as 'singularity shell' and 'singularity exec', which means you can invoke these commands directly from the command line. Second, many software that used to require singularity images can now be installed without singularity after we have upgraded to CentOS 7. For example, you may use Rstudio without a singularity container. Use \"module spider\" to see if the software you need is installed (see more on module commands ). General information of singularity on the HPCC The version of singularity is currently 3.4.1. The official documentation for this version is at https://www.sylabs.io/guides/3.4/user-guide/index.html . In general, a singularity image file contains all the software you need to run a program, and you use the command \" singularity exec <imgfile> <command> \" to run your command inside that image. For example, if I have a special version of R, I can type the following command: 1 singularity exec r-special.simg Rscript myprogram.R Running a docker container Many programs are available as docker containers pre-built, and many of those are available on the docker hub https://hub.docker.com . For details about running a docker container with singularity, see user guide here . Here is a quick example of running a command in Ubuntu linux even though we use CentOS Linux: 1 2 singularity shell docker://ubuntu:latest # log-in to Ubuntu, use exit to log-out singularity run docker://ubuntu:latest uname -a # show details of the Linux version Building containers Building your own containers requires administrative access (e.g., root privileges) so you can't do this. However you may build them on your laptop following the singularity documentation and transfer the image file over to the HPCC and use it here. Submitting a singularity job to our cluster In general, running singularity commands is the same as running any kinds of programs when you prepare your SLURM script. Two typical situations are: (1) The program you are going to run inside of the container is as simple as one that runs on a single node. In this case, you put your singularity commands (singularity exec <imgfile> <command> ) right after all the sbatch directive lines. If the program needs to use multiple threads/cores on a node, say 8, you would request 8 cores by #SBATCH --cpus-per-task=8 as you would do with any regular program running. (2) You are running an MPI program within your container. In this case, you must use srun -n $SLURM_NTASKS before the singularity command to launch the processes on the cluster nodes. See a template script below. SLURM script 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/bash # Job name: #SBATCH --job-name=singularity-test # # Number of MPI tasks needed for use case: #SBATCH --ntasks=18 # # Processors per task: #SBATCH --cpus-per-task=1 # # Memory per CPU #SBATCH --mem-per-cpu=1G # # Wall clock limit: #SBATCH --time=30 # # Standard out and error: #SBATCH --output=%x-%j.SLURMout cd <directory containing the singularity image file ( .sif ) > srun -n $SLURM_NTASKS singularity exec xxx.sif <commands>","title":"Singularity II: Running a container"},{"location":"Singularity_II._Running_a_container_on_HPC/#singularity-ii-running-a-container-on-hpc","text":"Why use a container? Changes from the old CentOS 6 system General information of singularity on the HPCC Running a docker container Building containers Submitting a singularity job to our cluster","title":"Singularity: II. Running a container on HPC"},{"location":"Singularity_II._Running_a_container_on_HPC/#why-use-a-container","text":"Singularity allows users to run software inside of containers. A popular container system is 'Docker', which is interoperable with singularity. A Linux container provides an environment that's different from the Linux host server you may be running on. For example, you could run a different version of Linux (e.g., running Ubuntu on our CentOS system). One advantage of containers is if your software requires a newer version of system libraries (e.g. glibc) than is available in our operating system, then you can run your software in a container. The main reason for using containers is that all of the dependencies are pre-installed.","title":"Why use a container?"},{"location":"Singularity_II._Running_a_container_on_HPC/#changes-from-the-old-centos-6-system","text":"First, 'module load singularity' is no longer required now on the CentOS 7 system. All singularity commands are built into the system such as 'singularity shell' and 'singularity exec', which means you can invoke these commands directly from the command line. Second, many software that used to require singularity images can now be installed without singularity after we have upgraded to CentOS 7. For example, you may use Rstudio without a singularity container. Use \"module spider\" to see if the software you need is installed (see more on module commands ).","title":"Changes from the old CentOS 6 system"},{"location":"Singularity_II._Running_a_container_on_HPC/#general-information-of-singularity-on-the-hpcc","text":"The version of singularity is currently 3.4.1. The official documentation for this version is at https://www.sylabs.io/guides/3.4/user-guide/index.html . In general, a singularity image file contains all the software you need to run a program, and you use the command \" singularity exec <imgfile> <command> \" to run your command inside that image. For example, if I have a special version of R, I can type the following command: 1 singularity exec r-special.simg Rscript myprogram.R","title":"General information of singularity on the HPCC"},{"location":"Singularity_II._Running_a_container_on_HPC/#running-a-docker-container","text":"Many programs are available as docker containers pre-built, and many of those are available on the docker hub https://hub.docker.com . For details about running a docker container with singularity, see user guide here . Here is a quick example of running a command in Ubuntu linux even though we use CentOS Linux: 1 2 singularity shell docker://ubuntu:latest # log-in to Ubuntu, use exit to log-out singularity run docker://ubuntu:latest uname -a # show details of the Linux version","title":"Running a docker container"},{"location":"Singularity_II._Running_a_container_on_HPC/#building-containers","text":"Building your own containers requires administrative access (e.g., root privileges) so you can't do this. However you may build them on your laptop following the singularity documentation and transfer the image file over to the HPCC and use it here.","title":"Building containers"},{"location":"Singularity_II._Running_a_container_on_HPC/#submitting-a-singularity-job-to-our-cluster","text":"In general, running singularity commands is the same as running any kinds of programs when you prepare your SLURM script. Two typical situations are: (1) The program you are going to run inside of the container is as simple as one that runs on a single node. In this case, you put your singularity commands (singularity exec <imgfile> <command> ) right after all the sbatch directive lines. If the program needs to use multiple threads/cores on a node, say 8, you would request 8 cores by #SBATCH --cpus-per-task=8 as you would do with any regular program running. (2) You are running an MPI program within your container. In this case, you must use srun -n $SLURM_NTASKS before the singularity command to launch the processes on the cluster nodes. See a template script below. SLURM script 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/bash # Job name: #SBATCH --job-name=singularity-test # # Number of MPI tasks needed for use case: #SBATCH --ntasks=18 # # Processors per task: #SBATCH --cpus-per-task=1 # # Memory per CPU #SBATCH --mem-per-cpu=1G # # Wall clock limit: #SBATCH --time=30 # # Standard out and error: #SBATCH --output=%x-%j.SLURMout cd <directory containing the singularity image file ( .sif ) > srun -n $SLURM_NTASKS singularity exec xxx.sif <commands>","title":"Submitting a singularity job to our cluster"},{"location":"Singularity_III._Advanced_skills/","text":"Singularity: III. Advanced skills In this tutorial, we will learn how to create containers from a recipe file, called Singularity (which is equivalent to Dockerfile ) Cache setting Building containers Building containers from Singularity definition files Cache setting By default, Singularity uses a temporary directory to save Docker files as tarballs: 1 2 3 4 5 6 7 8 $ ls ~/.singularity cache/ docker/ metadata/ $ ls .singularity/docker/ sha256:0efe07335a049e6afcd757db2d17ba37a12b717eb807acb03ddf3cd756b9fc2a.tar.gz sha256:43a98c1873995475a895f3d79f405232ef5230076b3f610c949c2e8341743af7.tar.gz sha256:6b1bb01b3a3b72463ae8ac5666d57b28f1a21d5256271910ac8df841aa04ecd1.tar.gz sha256:c2ca09a1934b951505ecc4d6b2e4ab7f9bf27bcdfb8999d0181deca74daf7683.tar.gz sha256:d6c3619d2153ffdefa4a9c19f15c5d566ce271b397a84537baa9ee45b24178f2.tar.gz You can change these by theses cache directories by specifying the location on your localhost as following: 1 2 3 $ mkdir -p $SCRATCH /singularity_tmp $ mkdir -p $SCRATCH /singularity_scratch $ SINGULARITY_TMPDIR = $SCRATCH /singularity_scratch SINGULARITY_CACHEDIR = $SCRATCH /singularity_tmp singularity --debug pull --name ubuntu-tmpdir.sif docker://ubuntu Building containers Creating writable containers with --sandbox options If you want to build a container within a writable directory (called a sandbox ), you can do that with --sandbox option. We\u2019ve already covered how you can pull an existing container from Docker Hub, but we can also build a Singularity container from docker using the build command: 1 $ singularity build --sandbox ubuntu-latest/ docker://ubuntu With --sandbox option, you can changes and write files within the directory, but those changes will not remain when you finished using the container. To make those changes persistent, use --writable option when you start a container. 1 $ singularity shell --writable ubuntu-latest/ Building containers from Singularity definition files To build containers, Singularity uses a file Singularity which is equivalent to Dockerfile in Docker. You can use different name but it is better practice to put it in a director and name it Singularity because it will be helpful later on when developing on repositories. For detailed information on writing Singularity recipe files, please refer to the container definition docs. To create a container using a custom Singularity file, you use build command: 1 $ singularity build ubuntu-latest.sif Singularity You would get an error message because you do not have a Singularity file yet. Let's assume that you already have the following Singularity definition file, called \"lowcow.def\" and you want ot use it to build a SIF container. 1 2 3 4 5 6 7 8 9 10 11 12 13 Bootstrap: docker From: ubuntu:16.04 %post apt-get -y update apt-get -y install fortune cowsay lolcat %environment export LC_ALL = C export PATH = /usr/games: $PATH %runscript fortune | cowsay | lolcat","title":"Singularity III: Advanced"},{"location":"Singularity_III._Advanced_skills/#singularity-iii-advanced-skills","text":"In this tutorial, we will learn how to create containers from a recipe file, called Singularity (which is equivalent to Dockerfile ) Cache setting Building containers Building containers from Singularity definition files","title":"Singularity: III. Advanced skills"},{"location":"Singularity_III._Advanced_skills/#cache-setting","text":"By default, Singularity uses a temporary directory to save Docker files as tarballs: 1 2 3 4 5 6 7 8 $ ls ~/.singularity cache/ docker/ metadata/ $ ls .singularity/docker/ sha256:0efe07335a049e6afcd757db2d17ba37a12b717eb807acb03ddf3cd756b9fc2a.tar.gz sha256:43a98c1873995475a895f3d79f405232ef5230076b3f610c949c2e8341743af7.tar.gz sha256:6b1bb01b3a3b72463ae8ac5666d57b28f1a21d5256271910ac8df841aa04ecd1.tar.gz sha256:c2ca09a1934b951505ecc4d6b2e4ab7f9bf27bcdfb8999d0181deca74daf7683.tar.gz sha256:d6c3619d2153ffdefa4a9c19f15c5d566ce271b397a84537baa9ee45b24178f2.tar.gz You can change these by theses cache directories by specifying the location on your localhost as following: 1 2 3 $ mkdir -p $SCRATCH /singularity_tmp $ mkdir -p $SCRATCH /singularity_scratch $ SINGULARITY_TMPDIR = $SCRATCH /singularity_scratch SINGULARITY_CACHEDIR = $SCRATCH /singularity_tmp singularity --debug pull --name ubuntu-tmpdir.sif docker://ubuntu","title":"Cache setting"},{"location":"Singularity_III._Advanced_skills/#building-containers","text":"","title":"Building containers"},{"location":"Singularity_III._Advanced_skills/#creating-writable-containers-with-sandbox-options","text":"If you want to build a container within a writable directory (called a sandbox ), you can do that with --sandbox option. We\u2019ve already covered how you can pull an existing container from Docker Hub, but we can also build a Singularity container from docker using the build command: 1 $ singularity build --sandbox ubuntu-latest/ docker://ubuntu With --sandbox option, you can changes and write files within the directory, but those changes will not remain when you finished using the container. To make those changes persistent, use --writable option when you start a container. 1 $ singularity shell --writable ubuntu-latest/","title":"Creating writable containers with --sandbox options"},{"location":"Singularity_III._Advanced_skills/#building-containers-from-singularity-definition-files","text":"To build containers, Singularity uses a file Singularity which is equivalent to Dockerfile in Docker. You can use different name but it is better practice to put it in a director and name it Singularity because it will be helpful later on when developing on repositories. For detailed information on writing Singularity recipe files, please refer to the container definition docs. To create a container using a custom Singularity file, you use build command: 1 $ singularity build ubuntu-latest.sif Singularity You would get an error message because you do not have a Singularity file yet. Let's assume that you already have the following Singularity definition file, called \"lowcow.def\" and you want ot use it to build a SIF container. 1 2 3 4 5 6 7 8 9 10 11 12 13 Bootstrap: docker From: ubuntu:16.04 %post apt-get -y update apt-get -y install fortune cowsay lolcat %environment export LC_ALL = C export PATH = /usr/games: $PATH %runscript fortune | cowsay | lolcat","title":"Building containers from Singularity definition files"},{"location":"Slurm_Environment_Variables/","text":"SLURM Environment Variables The Slurm controller will set variables in the environment of the batch script. Below is a list of SLURM variables and the corresponding Torque/MOAB environment variables for the comparison. SLURM Variables Torque/MOAB Description SLURM_ARRAY_TASK_COUNT Total number of tasks in a job array SLURM_ARRAY_TASK_ID PBS_ARRAYID Job array ID (index) number SLURM_ARRAY_TASK_MAX Job array's maximum ID (index) number SLURM_ARRAY_TASK_MIN Job array's minimum ID (index) number SLURM_ARRAY_TASK_STEP Job array's index step size SLURM_ARRAY_JOB_ID PBS_JOBID Job array's master job ID number SLURM_CLUSTER_NAME Name of the cluster on which the job is executing SLURM_CPUS_ON_NODE Number of CPUS on the allocated node SLURM_CPUS_PER_TASK PBS_VNODENUM Number of cpus requested per task. Only set if the --cpus-per-task option is specified. SLURM_JOB_ACCOUNT Account name associated of the job allocation SLURM_JOBID, SLURM_JOB_ID PBS_JOBID The ID of the job allocation SLURM_JOB_CPUS_PER_NODE PBS_NUM_PPN Count of processors available to the job on this node. SLURM_JOB_DEPENDENCY Set to value of the --dependency option SLURM_JOB_NAME PBS_JOBNAME Name of the job SLURM_NODELIST, SLURM_JOB_NODELIST PBS_NODEFILE List of nodes allocated to the job SLURM_NNODES, SLURM_JOB_NUM_NODES Total number of different nodes in the job's resource allocation SLURM_MEM_PER_NODE Same as --mem SLURM_MEM_PER_CPU Same as --mem-per-cpu SLURM_NTASKS, SLURM_NPROCS PBS_NUM_NODES Same as -n , --ntasks SLURM_NTASKS_PER_NODE Number of tasks requested per node. Only set if the --ntasks-per-node option is specified. SLURM_NTASKS_PER_SOCKET Number of tasks requested per socket. Only set if the --ntasks-per-socket option is specified. SLURM_SUBMIT_DIR PBS_O_WORKDIR The directory from which sbatch was invoked SLURM_SUBMIT_HOST PBS_O_HOST The hostname of the computer from which sbatch was invoked SLURM_TASK_PID The process ID of the task being started SLURMD_NODENAME Name of the node running the job script SLURM_JOB_GPUS GPU IDs allocated to the job (if any). Using Variables in SLURM Jobs You may also set your own variables for use in your SLURM jobs. One way is to set them inside the script itself, but that requires modifying the script. It is possible to pass variables into a SLURM job when you submit the job using the --export flag. For example to pass the value of the variables REPS and X into the job script named jobs.sb you can use: 1 sbatch --export=REPS=500,X='test' jobs.sb These are then available in your jobs as $REPS and $X Using variables to set SLURM job name and output files SLURM does not support using variables in the #SBATCH lines within a job script (for example, #SBATCH -N=$REPS will NOT work). A very limited number of variables are available in the #SBATCH just as %j for JOB ID. However, values passed from the command line have precedence over values defined in the job script and you can set certain SLURM variables in the command line. For example, you could set the job name and output/error files can be passed on the sbatch command line: 1 2 3 RUNTYPE='test' RUNNUMBER=5 sbatch --job-name=$RUNTYPE.$RUNNUMBER.run --output=$RUNTYPE.$RUNUMBER.txt --export=A=$A,b=$b jobscript.sbatch However note in this example, the output file doesn't have the job ID, which is not available from the command line, only inside the sbatch shell script.","title":"SLURM environment variables"},{"location":"Slurm_Environment_Variables/#slurm-environment-variables","text":"The Slurm controller will set variables in the environment of the batch script. Below is a list of SLURM variables and the corresponding Torque/MOAB environment variables for the comparison. SLURM Variables Torque/MOAB Description SLURM_ARRAY_TASK_COUNT Total number of tasks in a job array SLURM_ARRAY_TASK_ID PBS_ARRAYID Job array ID (index) number SLURM_ARRAY_TASK_MAX Job array's maximum ID (index) number SLURM_ARRAY_TASK_MIN Job array's minimum ID (index) number SLURM_ARRAY_TASK_STEP Job array's index step size SLURM_ARRAY_JOB_ID PBS_JOBID Job array's master job ID number SLURM_CLUSTER_NAME Name of the cluster on which the job is executing SLURM_CPUS_ON_NODE Number of CPUS on the allocated node SLURM_CPUS_PER_TASK PBS_VNODENUM Number of cpus requested per task. Only set if the --cpus-per-task option is specified. SLURM_JOB_ACCOUNT Account name associated of the job allocation SLURM_JOBID, SLURM_JOB_ID PBS_JOBID The ID of the job allocation SLURM_JOB_CPUS_PER_NODE PBS_NUM_PPN Count of processors available to the job on this node. SLURM_JOB_DEPENDENCY Set to value of the --dependency option SLURM_JOB_NAME PBS_JOBNAME Name of the job SLURM_NODELIST, SLURM_JOB_NODELIST PBS_NODEFILE List of nodes allocated to the job SLURM_NNODES, SLURM_JOB_NUM_NODES Total number of different nodes in the job's resource allocation SLURM_MEM_PER_NODE Same as --mem SLURM_MEM_PER_CPU Same as --mem-per-cpu SLURM_NTASKS, SLURM_NPROCS PBS_NUM_NODES Same as -n , --ntasks SLURM_NTASKS_PER_NODE Number of tasks requested per node. Only set if the --ntasks-per-node option is specified. SLURM_NTASKS_PER_SOCKET Number of tasks requested per socket. Only set if the --ntasks-per-socket option is specified. SLURM_SUBMIT_DIR PBS_O_WORKDIR The directory from which sbatch was invoked SLURM_SUBMIT_HOST PBS_O_HOST The hostname of the computer from which sbatch was invoked SLURM_TASK_PID The process ID of the task being started SLURMD_NODENAME Name of the node running the job script SLURM_JOB_GPUS GPU IDs allocated to the job (if any).","title":"SLURM Environment Variables"},{"location":"Slurm_Environment_Variables/#using-variables-in-slurm-jobs","text":"You may also set your own variables for use in your SLURM jobs. One way is to set them inside the script itself, but that requires modifying the script. It is possible to pass variables into a SLURM job when you submit the job using the --export flag. For example to pass the value of the variables REPS and X into the job script named jobs.sb you can use: 1 sbatch --export=REPS=500,X='test' jobs.sb These are then available in your jobs as $REPS and $X","title":"Using Variables in SLURM Jobs"},{"location":"Slurm_Environment_Variables/#using-variables-to-set-slurm-job-name-and-output-files","text":"SLURM does not support using variables in the #SBATCH lines within a job script (for example, #SBATCH -N=$REPS will NOT work). A very limited number of variables are available in the #SBATCH just as %j for JOB ID. However, values passed from the command line have precedence over values defined in the job script and you can set certain SLURM variables in the command line. For example, you could set the job name and output/error files can be passed on the sbatch command line: 1 2 3 RUNTYPE='test' RUNNUMBER=5 sbatch --job-name=$RUNTYPE.$RUNNUMBER.run --output=$RUNTYPE.$RUNUMBER.txt --export=A=$A,b=$b jobscript.sbatch However note in this example, the output file doesn't have the job ID, which is not available from the command line, only inside the sbatch shell script.","title":"Using variables to set SLURM job name and output files"},{"location":"Software_Installation_by_EasyBuild/","text":"Software Installation by EasyBuild FOR CENTOS 7 ONLY EasyBuild is a python program installed on HPCC and can be used to do software installation. Since it is easy to use, HPCC users can load a EasyBuild module to build their software and module system, and they can work together with HPCC software system. To use EasyBuild after you log into Centos 7 nodes, please run 1 2 3 4 $ module purge ; module load EasyBuild ; module list Currently Loaded Modules: 1 ) EasyBuild/3.6.2 The commands remove any loaded module (by module purge ) and load the EasyBuild module of the current version. The purge of loaded modules is strongly suggested since any loaded module might affect software installation by Easybuild. (Warning will also show during installation if any module created by EasyBuild is loaded.) After the module is loaded, EasyBuild commands (such as eb, easy_install, ...) can function normally. By default, software is set to be built under each user's home directory with the path $HOME/software and module system built in $HOME/modules. (For users with helpdesk group, the default directories are /opt/software and /opt/modules respectively.) If you would like to choose different directories, you can reset the environment variables EASYBUILD_INSTALLPATH_SOFTWARE and EASYBUILD_INSTALLPATH_MODULES to your preferred places: 1 2 $ export EASYBUILD_INSTALLPATH_SOFTWARE = <path to the directory where software is installed> $ export EASYBUILD_INSTALLPATH_MODULES = <path to the directory where module system is installed> Please make sure the directories are built (by mkdir -p command) and run module use command so your module system ($EASYBUILD_INSTALLPATH_MODULES) is used with HPCC system (/opt/modules): 1 2 $ mkdir -p $EASYBUILD_INSTALLPATH_SOFTWARE $EASYBUILD_INSTALLPATH_MODULES $ module use $EASYBUILD_INSTALLPATH_MODULES /opt/modules In order to make the installation conveniently, three commands are created in our system and they are introduced here: ebF EasyBuild can install software according to the information from a eb file. EasyBuild has many eb files created for many different software installations with any possible compilers and libraries. If a eb file is provided without any path, EasyBuild will try to find it from EasyBuild default paths. Since the path names are long and many directories and files are inside, you can use ebF command to find eb files associated with a input software name: 1 2 3 4 5 6 7 8 9 $ ebF Intel ebF_PATH = /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs ====== $ebF_PATH /i/IntelClusterChecker/ IntelClusterChecker-2017.1.016.eb ====== $ebF_PATH /c/CrayIntel/ CrayIntel-2015.06.eb CrayIntel-2015.11.eb CrayIntel-2016.06.eb The command tries to find the software directories with the name containing the input keyword Intel and all eb files inside (including any possible compilers). If you are not sure the name of a software with lower or upper case, you can use -i option to see them all: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ ebF -i Intel ebF_PATH = /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs ====== $ebF_PATH /i/IntelClusterChecker/ IntelClusterChecker-2017.1.016.eb ====== $ebF_PATH /i/intelcuda/ intelcuda-2016.10.eb ====== $ebF_PATH /i/intel/ intel-2014.06.eb intel-2015.08.eb intel-2016.02-GCC-4.9.eb intel-2016a.eb intel-2017.09.eb intel-2018a.eb intel-2014.10.eb intel-2015a.eb intel-2016.02-GCC-5.3.eb intel-2016b.eb intel-2017a.eb intel-2014.11.eb intel-2015b.eb intel-2016.03-GCC-4.9.eb intel-2017.00.eb intel-2017b.eb intel-2014b.eb intel-2016.00.eb intel-2016.03-GCC-5.3.eb intel-2017.01.eb intel-2018.00.eb intel-2015.02.eb intel-2016.01.eb intel-2016.03-GCC-5.4.eb intel-2017.02.eb intel-2018.01.eb ====== $ebF_PATH /c/CrayIntel/ CrayIntel-2015.06.eb CrayIntel-2015.11.eb CrayIntel-2016.06.eb ====== $ebF_PATH /__archive__/i/intel-para/ intel-para-2014.12.eb where any software with name containing lower and upper case of the keyword Intel is shown. If you know the exact name of the software and would like to list less eb files, you can use -w option: 1 2 3 4 5 6 7 8 9 10 11 12 13 $ ebF -w intel ebF_PATH = /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs ====== $ebF_PATH /i/intel/ intel-2014.06.eb intel-2015.08.eb intel-2016.02-GCC-4.9.eb intel-2016a.eb intel-2017.09.eb intel-2018a.eb intel-2014.10.eb intel-2015a.eb intel-2016.02-GCC-5.3.eb intel-2016b.eb intel-2017a.eb intel-2014.11.eb intel-2015b.eb intel-2016.03-GCC-4.9.eb intel-2017.00.eb intel-2017b.eb intel-2014b.eb intel-2016.00.eb intel-2016.03-GCC-5.3.eb intel-2017.01.eb intel-2018.00.eb intel-2015.02.eb intel-2016.01.eb intel-2016.03-GCC-5.4.eb intel-2017.02.eb intel-2018.01.eb ====== $ebF_PATH /__archive__/i/intel-para/ intel-para-2014.12.eb The command now shows all eb files of the software related to the exact keyword intel . If you would like to see eb files just for the exact name intel only (no related software), you can use the symbol $ at the end of the software name: 1 2 3 4 5 6 7 8 9 10 $ ebF -w intel$ ebF_PATH = /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs ====== $ebF_PATH /i/intel/ intel-2014.06.eb intel-2015.08.eb intel-2016.02-GCC-4.9.eb intel-2016a.eb intel-2017.09.eb intel-2018a.eb intel-2014.10.eb intel-2015a.eb intel-2016.02-GCC-5.3.eb intel-2016b.eb intel-2017a.eb intel-2014.11.eb intel-2015b.eb intel-2016.03-GCC-4.9.eb intel-2017.00.eb intel-2017b.eb intel-2014b.eb intel-2016.00.eb intel-2016.03-GCC-5.3.eb intel-2017.01.eb intel-2018.00.eb intel-2015.02.eb intel-2016.01.eb intel-2016.03-GCC-5.4.eb intel-2017.02.eb intel-2018.01.eb where only eb files of the software name intel are show. If no eb file can be found for a particular software installation, you can also try to modify a similar one. Copy the file using the path name (leading with $ebF_PATH) and file name. Modify the installation information inside and give it a try. ebS After a eb file is found or created, you can use ebS command to install the software and create the module file. Since ebS will try to build or rebuild software (and its module file) no matter whether it exists in /opt/software, please check if you just need a module file before you run the command. If EasyBuild can not find a module file of a dependent software listed in the eb file, it will also try to built or rebuilt the dependence (and its module file) automatically. To use ebS command with input of a eb file found in the default paths, you can just use the name without the path: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 $ ebS intel-2018a.eb == temporary log file in case of crash /tmp/eb-lxUlvt/easybuild-maTXAl.log == resolving dependencies ... == processing EasyBuild easyconfig /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs/i/intel/intel-2018a.eb == building and installing Core/intel/2018a... == fetching files... == creating build dir, resetting environment... == unpacking... == patching... == preparing... == configuring... == building... == testing... == installing... == taking care of extensions... == postprocessing... == sanity checking... == cleaning up... == creating module... == permissions... == packaging... == COMPLETED: Installation ended successfully == Results of the build can be found in the log file ( s ) /opt/software/intel/2018a/easybuild/easybuild-intel-2018a-20180330.141146.log == Build succeeded for 1 out of 1 == Temporary log file ( s ) /tmp/eb-lxUlvt/easybuild-maTXAl.log* have been removed. == Temporary directory /tmp/eb-lxUlvt has been removed. If the eb file is not in the default path, please provide the path and file name to install the software. After it is installed successfully, you may check if the software is in /opt/software directory and if the module file is in the proper place by module spider command. ebM If a software is installed and functions properly but its module file is missed and can not be found by module spider command, you can use ebM command to recreate the module file. The command also needs input of a eb file name as ebS command: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 $ ebM intel-2018a.eb == temporary log file in case of crash /tmp/eb-ZTyyR3/easybuild-XiMfNO.log == resolving dependencies ... == processing EasyBuild easyconfig /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs/i/intel/intel-2018a.eb == building and installing Core/intel/2018a... == fetching files [ skipped ] == creating build dir, resetting environment... == backup of existing module file stored at /opt/modules/Core/intel/2018a.lua.bak_20180330141928 == unpacking [ skipped ] == patching [ skipped ] == preparing... == configuring [ skipped ] == building [ skipped ] == testing [ skipped ] == installing [ skipped ] == taking care of extensions [ skipped ] == postprocessing [ skipped ] == sanity checking [ skipped ] == cleaning up [ skipped ] == creating module... == comparing module file with backup /opt/modules/Core/intel/2018a.lua.bak_20180330141928 ; no differences found == permissions [ skipped ] == packaging [ skipped ] == COMPLETED: Installation ended successfully == Results of the build can be found in the log file ( s ) /opt/software/intel/2018a/easybuild/easybuild-intel-2018a-20180330.141933.log == Build succeeded for 1 out of 1 == Temporary log file ( s ) /tmp/eb-ZTyyR3/easybuild-XiMfNO.log* have been removed. == Temporary directory /tmp/eb-ZTyyR3 has been removed. It will also try to build or rebuilt the module file no matter whether it exists in the proper directory. While using ebM command, if you see an error message of failed to copy the new eb file to the software directory: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 $ ebM GCCcore-7.2.0.eb == temporary log file in case of crash /tmp/eb-W5U4vp/easybuild-F9E32g.log == resolving dependencies ... == processing EasyBuild easyconfig /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs/g/GCCcore/GCCcore-7.2.0.eb == building and installing Core/GCCcore/7.2.0... == fetching files [ skipped ] == creating build dir, resetting environment... == backup of existing module file stored at /opt/modules/Core/GCCcore/7.2.0.lua.bak_20180330143152 == unpacking [ skipped ] == patching [ skipped ] == preparing... == configuring [ skipped ] == building [ skipped ] == testing [ skipped ] == installing [ skipped ] == taking care of extensions [ skipped ] == postprocessing [ skipped ] == sanity checking [ skipped ] == cleaning up [ skipped ] == creating module... == comparing module file with backup /opt/modules/Core/GCCcore/7.2.0.lua.bak_20180330143152 ; diff is: --- /opt/modules/Core/GCCcore/7.2.0.lua.bak_20180330143152 +++ /opt/modules/Core/GCCcore/7.2.0.lua @@ -23,7 +23,6 @@ prepend_path ( \"CPATH\" , pathJoin ( root, \"include\" )) prepend_path ( \"LD_LIBRARY_PATH\" , pathJoin ( root, \"lib\" )) prepend_path ( \"LD_LIBRARY_PATH\" , pathJoin ( root, \"lib64\" )) -prepend_path ( \"LD_LIBRARY_PATH\" , pathJoin ( root, \"lib/gcc/x86_64-pc-linux-gnu/7.2.0\" )) prepend_path ( \"LIBRARY_PATH\" , pathJoin ( root, \"lib\" )) prepend_path ( \"LIBRARY_PATH\" , pathJoin ( root, \"lib64\" )) prepend_path ( \"MANPATH\" , pathJoin ( root, \"share/man\" )) == permissions [ skipped ] == packaging [ skipped ] ERROR: Traceback ( most recent call last ) : File \"/opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_framework-3.5.2-py2.7.egg/easybuild/main.py\" , line 128 , in build_and_install_software ( ec_res [ 'success' ] , app_log, err ) = build_and_install_one ( ec, init_env, hooks = hooks ) File \"/opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_framework-3.5.2-py2.7.egg/easybuild/framework/easyblock.py\" , line 2751 , in build_and_install_one copy_file ( spec, newspec ) File \"/opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_framework-3.5.2-py2.7.egg/easybuild/tools/filetools.py\" , line 1575 , in copy_file raise EasyBuildError ( \"Failed to copy file %s to %s: %s\" , path, target_path, err ) EasyBuildError: \"Failed to copy file /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs/g/GCCcore/GCCcore-7.2.0.eb to /opt/software/GCCcore/7.2.0/easybuild/GCCcore-7.2.0.eb: [Errno 1] Operation not permitted: '/opt/software/GCCcore/7.2.0/easybuild/GCCcore-7.2.0.eb'\" please remove the old eb file under the software directory: 1 $ rm /opt/software/GCCcore/7.2.0/easybuild/GCCcore-7.2.0.eb and try again. It should resolve the error. For more details about EasyBuild program, please refer to the EasyBuild documentation web site .","title":"Software Installation by EasyBuild"},{"location":"Software_Installation_by_EasyBuild/#software-installation-by-easybuild","text":"FOR CENTOS 7 ONLY EasyBuild is a python program installed on HPCC and can be used to do software installation. Since it is easy to use, HPCC users can load a EasyBuild module to build their software and module system, and they can work together with HPCC software system. To use EasyBuild after you log into Centos 7 nodes, please run 1 2 3 4 $ module purge ; module load EasyBuild ; module list Currently Loaded Modules: 1 ) EasyBuild/3.6.2 The commands remove any loaded module (by module purge ) and load the EasyBuild module of the current version. The purge of loaded modules is strongly suggested since any loaded module might affect software installation by Easybuild. (Warning will also show during installation if any module created by EasyBuild is loaded.) After the module is loaded, EasyBuild commands (such as eb, easy_install, ...) can function normally. By default, software is set to be built under each user's home directory with the path $HOME/software and module system built in $HOME/modules. (For users with helpdesk group, the default directories are /opt/software and /opt/modules respectively.) If you would like to choose different directories, you can reset the environment variables EASYBUILD_INSTALLPATH_SOFTWARE and EASYBUILD_INSTALLPATH_MODULES to your preferred places: 1 2 $ export EASYBUILD_INSTALLPATH_SOFTWARE = <path to the directory where software is installed> $ export EASYBUILD_INSTALLPATH_MODULES = <path to the directory where module system is installed> Please make sure the directories are built (by mkdir -p command) and run module use command so your module system ($EASYBUILD_INSTALLPATH_MODULES) is used with HPCC system (/opt/modules): 1 2 $ mkdir -p $EASYBUILD_INSTALLPATH_SOFTWARE $EASYBUILD_INSTALLPATH_MODULES $ module use $EASYBUILD_INSTALLPATH_MODULES /opt/modules In order to make the installation conveniently, three commands are created in our system and they are introduced here:","title":"Software Installation by EasyBuild"},{"location":"Software_Installation_by_EasyBuild/#ebf","text":"EasyBuild can install software according to the information from a eb file. EasyBuild has many eb files created for many different software installations with any possible compilers and libraries. If a eb file is provided without any path, EasyBuild will try to find it from EasyBuild default paths. Since the path names are long and many directories and files are inside, you can use ebF command to find eb files associated with a input software name: 1 2 3 4 5 6 7 8 9 $ ebF Intel ebF_PATH = /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs ====== $ebF_PATH /i/IntelClusterChecker/ IntelClusterChecker-2017.1.016.eb ====== $ebF_PATH /c/CrayIntel/ CrayIntel-2015.06.eb CrayIntel-2015.11.eb CrayIntel-2016.06.eb The command tries to find the software directories with the name containing the input keyword Intel and all eb files inside (including any possible compilers). If you are not sure the name of a software with lower or upper case, you can use -i option to see them all: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ ebF -i Intel ebF_PATH = /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs ====== $ebF_PATH /i/IntelClusterChecker/ IntelClusterChecker-2017.1.016.eb ====== $ebF_PATH /i/intelcuda/ intelcuda-2016.10.eb ====== $ebF_PATH /i/intel/ intel-2014.06.eb intel-2015.08.eb intel-2016.02-GCC-4.9.eb intel-2016a.eb intel-2017.09.eb intel-2018a.eb intel-2014.10.eb intel-2015a.eb intel-2016.02-GCC-5.3.eb intel-2016b.eb intel-2017a.eb intel-2014.11.eb intel-2015b.eb intel-2016.03-GCC-4.9.eb intel-2017.00.eb intel-2017b.eb intel-2014b.eb intel-2016.00.eb intel-2016.03-GCC-5.3.eb intel-2017.01.eb intel-2018.00.eb intel-2015.02.eb intel-2016.01.eb intel-2016.03-GCC-5.4.eb intel-2017.02.eb intel-2018.01.eb ====== $ebF_PATH /c/CrayIntel/ CrayIntel-2015.06.eb CrayIntel-2015.11.eb CrayIntel-2016.06.eb ====== $ebF_PATH /__archive__/i/intel-para/ intel-para-2014.12.eb where any software with name containing lower and upper case of the keyword Intel is shown. If you know the exact name of the software and would like to list less eb files, you can use -w option: 1 2 3 4 5 6 7 8 9 10 11 12 13 $ ebF -w intel ebF_PATH = /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs ====== $ebF_PATH /i/intel/ intel-2014.06.eb intel-2015.08.eb intel-2016.02-GCC-4.9.eb intel-2016a.eb intel-2017.09.eb intel-2018a.eb intel-2014.10.eb intel-2015a.eb intel-2016.02-GCC-5.3.eb intel-2016b.eb intel-2017a.eb intel-2014.11.eb intel-2015b.eb intel-2016.03-GCC-4.9.eb intel-2017.00.eb intel-2017b.eb intel-2014b.eb intel-2016.00.eb intel-2016.03-GCC-5.3.eb intel-2017.01.eb intel-2018.00.eb intel-2015.02.eb intel-2016.01.eb intel-2016.03-GCC-5.4.eb intel-2017.02.eb intel-2018.01.eb ====== $ebF_PATH /__archive__/i/intel-para/ intel-para-2014.12.eb The command now shows all eb files of the software related to the exact keyword intel . If you would like to see eb files just for the exact name intel only (no related software), you can use the symbol $ at the end of the software name: 1 2 3 4 5 6 7 8 9 10 $ ebF -w intel$ ebF_PATH = /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs ====== $ebF_PATH /i/intel/ intel-2014.06.eb intel-2015.08.eb intel-2016.02-GCC-4.9.eb intel-2016a.eb intel-2017.09.eb intel-2018a.eb intel-2014.10.eb intel-2015a.eb intel-2016.02-GCC-5.3.eb intel-2016b.eb intel-2017a.eb intel-2014.11.eb intel-2015b.eb intel-2016.03-GCC-4.9.eb intel-2017.00.eb intel-2017b.eb intel-2014b.eb intel-2016.00.eb intel-2016.03-GCC-5.3.eb intel-2017.01.eb intel-2018.00.eb intel-2015.02.eb intel-2016.01.eb intel-2016.03-GCC-5.4.eb intel-2017.02.eb intel-2018.01.eb where only eb files of the software name intel are show. If no eb file can be found for a particular software installation, you can also try to modify a similar one. Copy the file using the path name (leading with $ebF_PATH) and file name. Modify the installation information inside and give it a try.","title":"ebF"},{"location":"Software_Installation_by_EasyBuild/#ebs","text":"After a eb file is found or created, you can use ebS command to install the software and create the module file. Since ebS will try to build or rebuild software (and its module file) no matter whether it exists in /opt/software, please check if you just need a module file before you run the command. If EasyBuild can not find a module file of a dependent software listed in the eb file, it will also try to built or rebuilt the dependence (and its module file) automatically. To use ebS command with input of a eb file found in the default paths, you can just use the name without the path: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 $ ebS intel-2018a.eb == temporary log file in case of crash /tmp/eb-lxUlvt/easybuild-maTXAl.log == resolving dependencies ... == processing EasyBuild easyconfig /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs/i/intel/intel-2018a.eb == building and installing Core/intel/2018a... == fetching files... == creating build dir, resetting environment... == unpacking... == patching... == preparing... == configuring... == building... == testing... == installing... == taking care of extensions... == postprocessing... == sanity checking... == cleaning up... == creating module... == permissions... == packaging... == COMPLETED: Installation ended successfully == Results of the build can be found in the log file ( s ) /opt/software/intel/2018a/easybuild/easybuild-intel-2018a-20180330.141146.log == Build succeeded for 1 out of 1 == Temporary log file ( s ) /tmp/eb-lxUlvt/easybuild-maTXAl.log* have been removed. == Temporary directory /tmp/eb-lxUlvt has been removed. If the eb file is not in the default path, please provide the path and file name to install the software. After it is installed successfully, you may check if the software is in /opt/software directory and if the module file is in the proper place by module spider command.","title":"ebS"},{"location":"Software_Installation_by_EasyBuild/#ebm","text":"If a software is installed and functions properly but its module file is missed and can not be found by module spider command, you can use ebM command to recreate the module file. The command also needs input of a eb file name as ebS command: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 $ ebM intel-2018a.eb == temporary log file in case of crash /tmp/eb-ZTyyR3/easybuild-XiMfNO.log == resolving dependencies ... == processing EasyBuild easyconfig /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs/i/intel/intel-2018a.eb == building and installing Core/intel/2018a... == fetching files [ skipped ] == creating build dir, resetting environment... == backup of existing module file stored at /opt/modules/Core/intel/2018a.lua.bak_20180330141928 == unpacking [ skipped ] == patching [ skipped ] == preparing... == configuring [ skipped ] == building [ skipped ] == testing [ skipped ] == installing [ skipped ] == taking care of extensions [ skipped ] == postprocessing [ skipped ] == sanity checking [ skipped ] == cleaning up [ skipped ] == creating module... == comparing module file with backup /opt/modules/Core/intel/2018a.lua.bak_20180330141928 ; no differences found == permissions [ skipped ] == packaging [ skipped ] == COMPLETED: Installation ended successfully == Results of the build can be found in the log file ( s ) /opt/software/intel/2018a/easybuild/easybuild-intel-2018a-20180330.141933.log == Build succeeded for 1 out of 1 == Temporary log file ( s ) /tmp/eb-ZTyyR3/easybuild-XiMfNO.log* have been removed. == Temporary directory /tmp/eb-ZTyyR3 has been removed. It will also try to build or rebuilt the module file no matter whether it exists in the proper directory. While using ebM command, if you see an error message of failed to copy the new eb file to the software directory: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 $ ebM GCCcore-7.2.0.eb == temporary log file in case of crash /tmp/eb-W5U4vp/easybuild-F9E32g.log == resolving dependencies ... == processing EasyBuild easyconfig /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs/g/GCCcore/GCCcore-7.2.0.eb == building and installing Core/GCCcore/7.2.0... == fetching files [ skipped ] == creating build dir, resetting environment... == backup of existing module file stored at /opt/modules/Core/GCCcore/7.2.0.lua.bak_20180330143152 == unpacking [ skipped ] == patching [ skipped ] == preparing... == configuring [ skipped ] == building [ skipped ] == testing [ skipped ] == installing [ skipped ] == taking care of extensions [ skipped ] == postprocessing [ skipped ] == sanity checking [ skipped ] == cleaning up [ skipped ] == creating module... == comparing module file with backup /opt/modules/Core/GCCcore/7.2.0.lua.bak_20180330143152 ; diff is: --- /opt/modules/Core/GCCcore/7.2.0.lua.bak_20180330143152 +++ /opt/modules/Core/GCCcore/7.2.0.lua @@ -23,7 +23,6 @@ prepend_path ( \"CPATH\" , pathJoin ( root, \"include\" )) prepend_path ( \"LD_LIBRARY_PATH\" , pathJoin ( root, \"lib\" )) prepend_path ( \"LD_LIBRARY_PATH\" , pathJoin ( root, \"lib64\" )) -prepend_path ( \"LD_LIBRARY_PATH\" , pathJoin ( root, \"lib/gcc/x86_64-pc-linux-gnu/7.2.0\" )) prepend_path ( \"LIBRARY_PATH\" , pathJoin ( root, \"lib\" )) prepend_path ( \"LIBRARY_PATH\" , pathJoin ( root, \"lib64\" )) prepend_path ( \"MANPATH\" , pathJoin ( root, \"share/man\" )) == permissions [ skipped ] == packaging [ skipped ] ERROR: Traceback ( most recent call last ) : File \"/opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_framework-3.5.2-py2.7.egg/easybuild/main.py\" , line 128 , in build_and_install_software ( ec_res [ 'success' ] , app_log, err ) = build_and_install_one ( ec, init_env, hooks = hooks ) File \"/opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_framework-3.5.2-py2.7.egg/easybuild/framework/easyblock.py\" , line 2751 , in build_and_install_one copy_file ( spec, newspec ) File \"/opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_framework-3.5.2-py2.7.egg/easybuild/tools/filetools.py\" , line 1575 , in copy_file raise EasyBuildError ( \"Failed to copy file %s to %s: %s\" , path, target_path, err ) EasyBuildError: \"Failed to copy file /opt/software/EasyBuild/3.5.2/lib/python2.7/site-packages/easybuild_easyconfigs-3.5.2-py2.7.egg/easybuild/easyconfigs/g/GCCcore/GCCcore-7.2.0.eb to /opt/software/GCCcore/7.2.0/easybuild/GCCcore-7.2.0.eb: [Errno 1] Operation not permitted: '/opt/software/GCCcore/7.2.0/easybuild/GCCcore-7.2.0.eb'\" please remove the old eb file under the software directory: 1 $ rm /opt/software/GCCcore/7.2.0/easybuild/GCCcore-7.2.0.eb and try again. It should resolve the error. For more details about EasyBuild program, please refer to the EasyBuild documentation web site .","title":"ebM"},{"location":"Specifications_of_Job_submission/","text":"Specifications of Job submission The following is a comparison of submission options between PBS and SBATCH lines . It is only for help on the transition between the two systems. Important differences between SLURM and PBS Please be careful on the specifications --ntask= ( -n ) and --cpus-per-task= ( -c ) in SLURM since they are not in PBS specifications (and there is no CPUs per node or ppn in SLURM). The two requests are for software specifications in Parallel Computing . The number of tasks ( -n ) is the number of parallel processes in distributed memory (such as MPI model). The number of CPUs per task ( -c ) is for the number of threads in shared memory (such as OpenMP model). If you would like to specify how many different nodes (in hardware), please use --nodes= ( -N ) in SLURM job script or on sbatch command. Torque SLURM Description for Torque Torque Example SLURM Example #PBS #SBATCH Headof each line -A -A, --account= This option tells TORQUE to use the Account (not Username) credential specified. #PBS -A mybuyin #SBATCH -A mybuyin #SBATCH -- account=mybuyin -a -begin= This option tells PBS to run a job at the given time. #PBS -a 0615 #SBATCH --begin=06:15 -e -o -e, --error= pattern> -o, --output= pattern> Need a file name pattern. Can not be a directory name. For the details of valid filename pattern, check the manual of sbatch by \"man sbatch\" and look into filename pattern section. The location for Output_Path and Error_Path attributes as demonstrated below. Note that you only need to use one if you use the -j option. #PBS -e ~/ErrorFile #SBATCH -e ~/ErrorFile_%j_%u By default both standard output and standard error are directed to the same file. qsub -I salloc srun --pty /bin/bash Declares that the job is to be run \"interactively\". qsub -I qsub -I -X srun --pty /bin/bash salloc --x11 -j Using an eo option will combine STDOUT and STDERR in the file specified in Error_Path; oe will combine them in Output_Path. #PBS -j oe By default both standard output and standard error are directed to the same file. -l -N, --nodes=\\ -n, --ntasks=\\ --ntasks-per-node= -c, --cpus-per-task=\\ --gres=\\ -t, --time=\\ --mem=\\ -C, --constraint=\\ --tmp=\\ Separate them with \",\" nodes=#; Number and/or type of nodes to be reserved. ppn=# specify the number of processors per node requested. Defaults to 1. gpus=# specify the number of gpus to use. walltime= the total run time in the form: HH:MM:SS or DD:HH:MM:SS mem= Maximum amount of memory needed by a job. feature= the name of the type of compute noded related to our cluster configuration file= Maximum amount of local disk space needed by a job. #PBS -l walltime=01:00:00 #PBS -l mem=8gb #PBS -l feature=intel14|intel16 #PBS -l file=40GB (See more explanation in the start of this page) #SBATCH -n 4 -c 1 --gres=gpu:2 #SBATCH --time=01:00:00 #SBATCH --mem=2G #SBATCH -C NOAUTO:intel14|intel16 Constraints using \"|\" must be prepended with 'NOAUTO:'. Click here for more information . #SBATCH --tmp=40G -M --mail-user=\\ Emails account(s) to notify once a job changes states as specified by -m #PBS -M usr@msu.edu #SBATCH --mail-user=usr@msu.edu -m --mail-type=\\ a- sends mail when job is aborted by batch system b- sends mail when begins execution, e- sends mail when job ends, n- does not send mail #PBS -m abe #SBATCH --mail-type=FAIL,BEGIN,END NONE - does not send mail -N -J, --job-name=\\ Names the job #PBS -N MySuperComputing #SBATCH -J MySuperComputing -t -a, --array= Submits a Array Job with n identical tasks. Each task has the same \\$PBS_JOBID but different \\$PBS_ARRAYID variables. #PBS -t 5 #PBS -t 3-10 #SBATCH -a 5 #SBATCH --array=3-10 -V --export=<environment variables [ALL] NONE> Passes all current environment variables to the job. #PBS -V -v --export=<environment variables [ALL] NONE> Defines additional environment variables for the job. #PBS -v ev1=ph3,ev2=50 -W -L, --licenses=\\ Special Generic Resources such as software licenses can be requested using the -W option. This is most commonly used with matlab (see Matlab Licenses for more information.) #PBS -W gres:MATLAB #SBATCH -L matlab@27000@lm-01.i","title":"Specifications of job submission"},{"location":"Specifications_of_Job_submission/#specifications-of-job-submission","text":"The following is a comparison of submission options between PBS and SBATCH lines . It is only for help on the transition between the two systems. Important differences between SLURM and PBS Please be careful on the specifications --ntask= ( -n ) and --cpus-per-task= ( -c ) in SLURM since they are not in PBS specifications (and there is no CPUs per node or ppn in SLURM). The two requests are for software specifications in Parallel Computing . The number of tasks ( -n ) is the number of parallel processes in distributed memory (such as MPI model). The number of CPUs per task ( -c ) is for the number of threads in shared memory (such as OpenMP model). If you would like to specify how many different nodes (in hardware), please use --nodes= ( -N ) in SLURM job script or on sbatch command. Torque SLURM Description for Torque Torque Example SLURM Example #PBS #SBATCH Headof each line -A -A, --account= This option tells TORQUE to use the Account (not Username) credential specified. #PBS -A mybuyin #SBATCH -A mybuyin #SBATCH -- account=mybuyin -a -begin= This option tells PBS to run a job at the given time. #PBS -a 0615 #SBATCH --begin=06:15 -e -o -e, --error= pattern> -o, --output= pattern> Need a file name pattern. Can not be a directory name. For the details of valid filename pattern, check the manual of sbatch by \"man sbatch\" and look into filename pattern section. The location for Output_Path and Error_Path attributes as demonstrated below. Note that you only need to use one if you use the -j option. #PBS -e ~/ErrorFile #SBATCH -e ~/ErrorFile_%j_%u By default both standard output and standard error are directed to the same file. qsub -I salloc srun --pty /bin/bash Declares that the job is to be run \"interactively\". qsub -I qsub -I -X srun --pty /bin/bash salloc --x11 -j Using an eo option will combine STDOUT and STDERR in the file specified in Error_Path; oe will combine them in Output_Path. #PBS -j oe By default both standard output and standard error are directed to the same file. -l -N, --nodes=\\ -n, --ntasks=\\ --ntasks-per-node= -c, --cpus-per-task=\\ --gres=\\ -t, --time=\\ --mem=\\ -C, --constraint=\\ --tmp=\\ Separate them with \",\" nodes=#; Number and/or type of nodes to be reserved. ppn=# specify the number of processors per node requested. Defaults to 1. gpus=# specify the number of gpus to use. walltime= the total run time in the form: HH:MM:SS or DD:HH:MM:SS mem= Maximum amount of memory needed by a job. feature= the name of the type of compute noded related to our cluster configuration file= Maximum amount of local disk space needed by a job. #PBS -l walltime=01:00:00 #PBS -l mem=8gb #PBS -l feature=intel14|intel16 #PBS -l file=40GB (See more explanation in the start of this page) #SBATCH -n 4 -c 1 --gres=gpu:2 #SBATCH --time=01:00:00 #SBATCH --mem=2G #SBATCH -C NOAUTO:intel14|intel16 Constraints using \"|\" must be prepended with 'NOAUTO:'. Click here for more information . #SBATCH --tmp=40G -M --mail-user=\\ Emails account(s) to notify once a job changes states as specified by -m #PBS -M usr@msu.edu #SBATCH --mail-user=usr@msu.edu -m --mail-type=\\ a- sends mail when job is aborted by batch system b- sends mail when begins execution, e- sends mail when job ends, n- does not send mail #PBS -m abe #SBATCH --mail-type=FAIL,BEGIN,END NONE - does not send mail -N -J, --job-name=\\ Names the job #PBS -N MySuperComputing #SBATCH -J MySuperComputing -t -a, --array= Submits a Array Job with n identical tasks. Each task has the same \\$PBS_JOBID but different \\$PBS_ARRAYID variables. #PBS -t 5 #PBS -t 3-10 #SBATCH -a 5 #SBATCH --array=3-10 -V --export=<environment variables [ALL] NONE> Passes all current environment variables to the job. #PBS -V -v --export=<environment variables [ALL] NONE> Defines additional environment variables for the job. #PBS -v ev1=ph3,ev2=50 -W -L, --licenses=\\ Special Generic Resources such as software licenses can be requested using the -W option. This is most commonly used with matlab (see Matlab Licenses for more information.) #PBS -W gres:MATLAB #SBATCH -L matlab@27000@lm-01.i","title":"Specifications of Job submission"},{"location":"Stata/","text":"Stata Many versions of Stata are installed on the ICER HPC. When you log-in, Stata is not available by default, but it may be load easily using this command (note you must type Stata with a capital 'S' run stata batch 1 [ hpc@dev-intel18 ~ ] $ module load Stata This loads Stata SE version 15. This is equivalent to using the command 1 [hpc@dev-intel18 ~]$ module load Stata/15.0.SE Stata has a command line version and a GUI (windowed) version. To use the command line, type stata at the prompt. You will see this : Using Stata command line 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [ hpc@dev-intel18 ~ ] $ module load Stata [ hpc@dev-intel18 ~ ] $ stata ___ ____ ____ ____ ____ ( R ) /__ / ____/ / ____/ ___/ / /___/ / /___/ 15 .0 Copyright 1985 -2017 StataCorp LLC Statistics/Data Analysis StataCorp 4905 Lakeway Drive College Station, Texas 77845 USA 800 -STATA-PC http://www.stata.com 979 -696-4600 stata@stata.com 979 -696-4601 ( fax ) 15 -user Stata network perpetual license: Serial number: 401506213245 Licensed to: iCER / HPCC at Michigan State University East Lansing, MI Notes: 1 . Unicode is supported ; see help unicode_advice. . In which you may type Stata commands. Type 'exit' to quit this version. To run a Stata do file from the command line in 'batch', you use the syntax run stata batch 1 [ hpc@dev-intel18 ~ ] $stata -b do dofilename.do Versions Stata comes in several versions: IC, SE, and MP; see https://www.stata.com/products/which-stata-is-right-for-me/ for details for the differences. Stata/IC has limitations on the numbers of variables that affect most users but has no licensing restrictions (see below). While the default version of Stata available when you load the module is Stata/SE, you currently have to use the command 'stata-se' to start the 'SE' version run stata-se interactive session 1 2 3 [ hpc@dev-intel18 ~ ] $module load Stata/15.0.SE [ hpc@dev-intel18 ~ ] $stata -se # to run the interactive version [ hpc@dev-intel18 ~ ] $stata -se -b do dofilename.do # to run a do file However, to use the \"MP\" Version, you must load it explicitly. run stata-mp interactive session 1 2 [ hpc@dev-intel18 ~ ] $ module load Stata/15.0.MP [ hpc@dev-intel18 ~ ] $ stata-mp Note that even if you load Stata/MP or SE module, as above, if you just use the command 'stata' it will load the IC version. To see which version of Stata you are current in, use the \"about\" command at the dot prompt. to use these special versions to run a bach do file, use stata-se and stata-mp instead of plain stata For Stata/SE There are 15 licenses available, so 15 users may use it, for MP there are 5 user licenses of 8-cores each. Please exit the program when you are finished with it. GUI version To use the GUI version, you must first be connected to HPCC with X11 forwarding ( MobaXterm for Windows, XQuartz for Mac - see instruction on installing SSH client ) or using a web-base connection to HPCC (see instruction on Connecting via web site ). Once an X11 or remote desktop client is connected, you can run xstata on a dev node: run stata-mp interactive session 1 2 [ hpc@dev-intel18 ~ ] $ module load Stata [ hpc@dev-intel18 ~ ] $ xstata Variables Limits Even if you load the MP or SE versions, Stata limits the number of variables to 5000 unless you tell it otherwise. For information use the help set_maxvar command at the dot prompt. You can set the maxvar for your session or in your do file with (for example to 6000) run stata batch 1 . set maxvar 6000 There are other settings related to memory usage which are important as Stata attempts to be very conservative. For more information use the Stata \"memory\" command Running Jobs Note you must use the command line version inside a sb script when running jobs. To copy a working example of Stata job file into your home directory, you can use our getexample tool Getting Stata Example 1 2 3 4 5 6 7 module load powertools cd ~ getexample STATA_example cd STATA_example # look into the README file in this folder for details cat README More helps For questions requiring deeper knowledge of statistics, users could contact CSTAT services at https://cstat.msu.edu/cstat-services and use the \"schedule a meeting\" link to submit an intake form.","title":"Stata"},{"location":"Stata/#stata","text":"Many versions of Stata are installed on the ICER HPC. When you log-in, Stata is not available by default, but it may be load easily using this command (note you must type Stata with a capital 'S' run stata batch 1 [ hpc@dev-intel18 ~ ] $ module load Stata This loads Stata SE version 15. This is equivalent to using the command 1 [hpc@dev-intel18 ~]$ module load Stata/15.0.SE Stata has a command line version and a GUI (windowed) version. To use the command line, type stata at the prompt. You will see this : Using Stata command line 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [ hpc@dev-intel18 ~ ] $ module load Stata [ hpc@dev-intel18 ~ ] $ stata ___ ____ ____ ____ ____ ( R ) /__ / ____/ / ____/ ___/ / /___/ / /___/ 15 .0 Copyright 1985 -2017 StataCorp LLC Statistics/Data Analysis StataCorp 4905 Lakeway Drive College Station, Texas 77845 USA 800 -STATA-PC http://www.stata.com 979 -696-4600 stata@stata.com 979 -696-4601 ( fax ) 15 -user Stata network perpetual license: Serial number: 401506213245 Licensed to: iCER / HPCC at Michigan State University East Lansing, MI Notes: 1 . Unicode is supported ; see help unicode_advice. . In which you may type Stata commands. Type 'exit' to quit this version. To run a Stata do file from the command line in 'batch', you use the syntax run stata batch 1 [ hpc@dev-intel18 ~ ] $stata -b do dofilename.do","title":"Stata"},{"location":"Stata/#versions","text":"Stata comes in several versions: IC, SE, and MP; see https://www.stata.com/products/which-stata-is-right-for-me/ for details for the differences. Stata/IC has limitations on the numbers of variables that affect most users but has no licensing restrictions (see below). While the default version of Stata available when you load the module is Stata/SE, you currently have to use the command 'stata-se' to start the 'SE' version run stata-se interactive session 1 2 3 [ hpc@dev-intel18 ~ ] $module load Stata/15.0.SE [ hpc@dev-intel18 ~ ] $stata -se # to run the interactive version [ hpc@dev-intel18 ~ ] $stata -se -b do dofilename.do # to run a do file However, to use the \"MP\" Version, you must load it explicitly. run stata-mp interactive session 1 2 [ hpc@dev-intel18 ~ ] $ module load Stata/15.0.MP [ hpc@dev-intel18 ~ ] $ stata-mp Note that even if you load Stata/MP or SE module, as above, if you just use the command 'stata' it will load the IC version. To see which version of Stata you are current in, use the \"about\" command at the dot prompt. to use these special versions to run a bach do file, use stata-se and stata-mp instead of plain stata For Stata/SE There are 15 licenses available, so 15 users may use it, for MP there are 5 user licenses of 8-cores each. Please exit the program when you are finished with it.","title":"Versions"},{"location":"Stata/#gui-version","text":"To use the GUI version, you must first be connected to HPCC with X11 forwarding ( MobaXterm for Windows, XQuartz for Mac - see instruction on installing SSH client ) or using a web-base connection to HPCC (see instruction on Connecting via web site ). Once an X11 or remote desktop client is connected, you can run xstata on a dev node: run stata-mp interactive session 1 2 [ hpc@dev-intel18 ~ ] $ module load Stata [ hpc@dev-intel18 ~ ] $ xstata","title":"GUI version"},{"location":"Stata/#variables-limits","text":"Even if you load the MP or SE versions, Stata limits the number of variables to 5000 unless you tell it otherwise. For information use the help set_maxvar command at the dot prompt. You can set the maxvar for your session or in your do file with (for example to 6000) run stata batch 1 . set maxvar 6000 There are other settings related to memory usage which are important as Stata attempts to be very conservative. For more information use the Stata \"memory\" command","title":"Variables Limits"},{"location":"Stata/#running-jobs","text":"Note you must use the command line version inside a sb script when running jobs. To copy a working example of Stata job file into your home directory, you can use our getexample tool Getting Stata Example 1 2 3 4 5 6 7 module load powertools cd ~ getexample STATA_example cd STATA_example # look into the README file in this folder for details cat README","title":"Running Jobs"},{"location":"Stata/#more-helps","text":"For questions requiring deeper knowledge of statistics, users could contact CSTAT services at https://cstat.msu.edu/cstat-services and use the \"schedule a meeting\" link to submit an intake form.","title":"More helps"},{"location":"Submitting-multiple-jobs-simultaneously_40337501.html/","text":"Teaching : Submitting multiple jobs simultaneously Make a copy of the job script multi_seq.sb and name it multi_sim.sb Edit multi_sim.sb to simultaneously run python_script.py and r_script.R Be sure to update the resources required to run multi_sim.sb if necessary. Submit job to compute node Answer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #Make a copy of the job script multi_seq.sb and name it multi-sim.sb cp multi_seq.sb multi_sim.sb #Edit multi_sim.sb to simultaneously run python_script.py and r_script.R gedit multi_sim.sb python3 python_script.py& Rscript r_script.R& wait #Note: Be sure to use \u201c&\u201d (otherwise run in sequential) and \u201cwait\u201d (otherwise job exit immediately) #Be sure to update the resources required to run multi-sim.sb if necessary. Note, the number of nodes/cores requested should be the sum of the nodes/cores needed to run each job. In this case since each job uses one node and one core. Requesting one node is sufficient. #Submit job to compute node. Type the following at the command line: sbatch multi_sim.sb","title":"Submitting multiple jobs simultaneously 40337501.html"},{"location":"Submitting-multiple-jobs-simultaneously_40337501.html/#teaching-submitting-multiple-jobs-simultaneously","text":"Make a copy of the job script multi_seq.sb and name it multi_sim.sb Edit multi_sim.sb to simultaneously run python_script.py and r_script.R Be sure to update the resources required to run multi_sim.sb if necessary. Submit job to compute node Answer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #Make a copy of the job script multi_seq.sb and name it multi-sim.sb cp multi_seq.sb multi_sim.sb #Edit multi_sim.sb to simultaneously run python_script.py and r_script.R gedit multi_sim.sb python3 python_script.py& Rscript r_script.R& wait #Note: Be sure to use \u201c&\u201d (otherwise run in sequential) and \u201cwait\u201d (otherwise job exit immediately) #Be sure to update the resources required to run multi-sim.sb if necessary. Note, the number of nodes/cores requested should be the sum of the nodes/cores needed to run each job. In this case since each job uses one node and one core. Requesting one node is sufficient. #Submit job to compute node. Type the following at the command line: sbatch multi_sim.sb","title":"Teaching : Submitting multiple jobs simultaneously"},{"location":"Submitting_a_TensorFlow_job/","text":"Submitting a TensorFlow job We assume that you've installed your TensorFlow virtual environment on a GPU dev-node (dev-intel16-k80). The Python code we are going to run in the SLURM job script is named matmul.py , with content being matmul.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import sys import numpy as np import tensorflow as tf from datetime import datetime device_name = sys . argv [ 1 ] # Choose device from cmd line. Options: gpu or cpu shape = ( int ( sys . argv [ 2 ]), int ( sys . argv [ 2 ])) if device_name == \"gpu\" : device_name = \"/gpu:6\" else : device_name = \"/cpu:0\" with tf . device ( device_name ): random_matrix = tf . random_uniform ( shape = shape , minval = 0 , maxval = 1 ) dot_operation = tf . matmul ( random_matrix , tf . transpose ( random_matrix )) sum_operation = tf . reduce_sum ( dot_operation ) startTime = datetime . now () with tf . Session ( config = tf . ConfigProto ( allow_soft_placement = True , log_device_placement = True )) as session : result = session . run ( sum_operation ) print ( result ) print ( \" \\n \" * 5 ) print ( \"Shape:\" , shape , \"Device:\" , device_name ) print ( \"Time taken:\" , datetime . now () - startTime ) print ( \" \\n \" * 5 ) Now we write a SLURM script to submit the job to the cluster: Submitting GPU TensorFlow job: test_matmul.sbatch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #!/bin/bash #SBATCH --job-name=test_matmul #SBATCH --gres=gpu:1 #SBATCH --mem-per-cpu=20G #SBATCH --time=20 #SBATCH --output=%x-%j.SLURMout echo $CUDA_VISIBLE_DEVICES module purge module load GCC/6.4.0-2.28 OpenMPI/2.1.2 module load CUDA/10.0.130 cuDNN/7.5.0.56-CUDA-10.0.130 module load Python/3.6.4 source ~/tf-1.13.1-env/bin/activate export TF_CPP_MIN_LOG_LEVEL = 2 # disables the warning, doesn't enable AVX/FMA. srun python matmul.py gpu 1500 To submit it, simply run 1 sbatch test_matmul.sbatch The final result will be written to file \" test_matmul-<jobid>.SLURMout \".","title":"Submitting a TF job"},{"location":"Submitting_a_TensorFlow_job/#submitting-a-tensorflow-job","text":"We assume that you've installed your TensorFlow virtual environment on a GPU dev-node (dev-intel16-k80). The Python code we are going to run in the SLURM job script is named matmul.py , with content being matmul.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import sys import numpy as np import tensorflow as tf from datetime import datetime device_name = sys . argv [ 1 ] # Choose device from cmd line. Options: gpu or cpu shape = ( int ( sys . argv [ 2 ]), int ( sys . argv [ 2 ])) if device_name == \"gpu\" : device_name = \"/gpu:6\" else : device_name = \"/cpu:0\" with tf . device ( device_name ): random_matrix = tf . random_uniform ( shape = shape , minval = 0 , maxval = 1 ) dot_operation = tf . matmul ( random_matrix , tf . transpose ( random_matrix )) sum_operation = tf . reduce_sum ( dot_operation ) startTime = datetime . now () with tf . Session ( config = tf . ConfigProto ( allow_soft_placement = True , log_device_placement = True )) as session : result = session . run ( sum_operation ) print ( result ) print ( \" \\n \" * 5 ) print ( \"Shape:\" , shape , \"Device:\" , device_name ) print ( \"Time taken:\" , datetime . now () - startTime ) print ( \" \\n \" * 5 ) Now we write a SLURM script to submit the job to the cluster: Submitting GPU TensorFlow job: test_matmul.sbatch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #!/bin/bash #SBATCH --job-name=test_matmul #SBATCH --gres=gpu:1 #SBATCH --mem-per-cpu=20G #SBATCH --time=20 #SBATCH --output=%x-%j.SLURMout echo $CUDA_VISIBLE_DEVICES module purge module load GCC/6.4.0-2.28 OpenMPI/2.1.2 module load CUDA/10.0.130 cuDNN/7.5.0.56-CUDA-10.0.130 module load Python/3.6.4 source ~/tf-1.13.1-env/bin/activate export TF_CPP_MIN_LOG_LEVEL = 2 # disables the warning, doesn't enable AVX/FMA. srun python matmul.py gpu 1500 To submit it, simply run 1 sbatch test_matmul.sbatch The final result will be written to file \" test_matmul-<jobid>.SLURMout \".","title":"Submitting a TensorFlow job"},{"location":"System_Commands/","text":"System Commands Please check the table below to find the SLURM correspondence of a Torque command: Command Description Torque SLURM Batch job submission qsub \\<Job File Name> sbatch \\<Job File Name> Interactive job submission qsub -I salloc | srun --pty /bin/bash Job list qstat squeue -l Job list by users qstat -u \\<User Name> squeue -l -u \\<User Name> Job deletion qdel \\<Job ID> scancel \\<Job ID> Job hold qhold \\<Job ID> scontrol hold \\<Job ID> Job release qrls \\<Job ID> scontrol release \\<Job ID> Job update qalter \\<Job ID> scontrol update job \\<Job ID> Job details qstat -f \\<Job ID> scontrol show job \\<Job ID> Node list pbsnodes -l sinfo -N Node details pbsnodes scontrol show nodes Please check the table below to find the SLURM correspondence of a Moab command: Command Description Moab SLURM Job start time showstart \\<Job ID> squeue --start -j \\<Job ID> Status of nodes mdiag -n sinfo -N -l User's account mdiag -u \\<User Name> sacctmgr show association user=\\<User Name> Account members mdiag -a \\<Account Name> sacctmgr show assoc account=\\<Account Name> Nodes of accounts mdiag -s sinfo -a Finally, it is suggested to use srun as the parallel command wrapper although mpirun can still be used in SLRUM system. Command Description OpenMPI SLURM Parallel wrapper mpirun srun","title":"System commands"},{"location":"System_Commands/#system-commands","text":"Please check the table below to find the SLURM correspondence of a Torque command: Command Description Torque SLURM Batch job submission qsub \\<Job File Name> sbatch \\<Job File Name> Interactive job submission qsub -I salloc | srun --pty /bin/bash Job list qstat squeue -l Job list by users qstat -u \\<User Name> squeue -l -u \\<User Name> Job deletion qdel \\<Job ID> scancel \\<Job ID> Job hold qhold \\<Job ID> scontrol hold \\<Job ID> Job release qrls \\<Job ID> scontrol release \\<Job ID> Job update qalter \\<Job ID> scontrol update job \\<Job ID> Job details qstat -f \\<Job ID> scontrol show job \\<Job ID> Node list pbsnodes -l sinfo -N Node details pbsnodes scontrol show nodes Please check the table below to find the SLURM correspondence of a Moab command: Command Description Moab SLURM Job start time showstart \\<Job ID> squeue --start -j \\<Job ID> Status of nodes mdiag -n sinfo -N -l User's account mdiag -u \\<User Name> sacctmgr show association user=\\<User Name> Account members mdiag -a \\<Account Name> sacctmgr show assoc account=\\<Account Name> Nodes of accounts mdiag -s sinfo -a Finally, it is suggested to use srun as the parallel command wrapper although mpirun can still be used in SLRUM system. Command Description OpenMPI SLURM Parallel wrapper mpirun srun","title":"System Commands"},{"location":"Targeting_Cluster_Architectures/","text":"Targeting Cluster Architectures While all HPCC nodes are the x86_64 architecture, some newer processors have features that are not supported by older processors. A program compiled on a newer processor may not run on an older processor and may result in an 'Illegal Instruction' error. This can be corrected by specifying compilers parameters that control which processor instruction are used. Use the following options for your compiler to ensure your programs will run on all HPCC nodes. Compiler Type Min Version Max Version Arguments GCC 6.4 N/A -march=core-avx-i -mtune=skylake-avx512 GCC 4.9 \\< 6.4 -march=core-avx-i -mtune=silvermont GCC 4.8 \\< 4.9 -march=core-avx-i -mtune=core-avx2 GCC 4.6 \\< 4.8 -march=core-avx-i GCC 4.3 \\< 4.6 -march=core2 GCC 3.3 \\< 4.3 -march=nocona Intel 2015.1 N/A -mAVX -axCORE-AVX-I,CORE-AVX2,CORE-AVX512 The new amd20 cluster does not support AVX-512.","title":"Targeting Cluster Architectures"},{"location":"Targeting_Cluster_Architectures/#targeting-cluster-architectures","text":"While all HPCC nodes are the x86_64 architecture, some newer processors have features that are not supported by older processors. A program compiled on a newer processor may not run on an older processor and may result in an 'Illegal Instruction' error. This can be corrected by specifying compilers parameters that control which processor instruction are used. Use the following options for your compiler to ensure your programs will run on all HPCC nodes. Compiler Type Min Version Max Version Arguments GCC 6.4 N/A -march=core-avx-i -mtune=skylake-avx512 GCC 4.9 \\< 6.4 -march=core-avx-i -mtune=silvermont GCC 4.8 \\< 4.9 -march=core-avx-i -mtune=core-avx2 GCC 4.6 \\< 4.8 -march=core-avx-i GCC 4.3 \\< 4.6 -march=core2 GCC 3.3 \\< 4.3 -march=nocona Intel 2015.1 N/A -mAVX -axCORE-AVX-I,CORE-AVX2,CORE-AVX512 The new amd20 cluster does not support AVX-512.","title":"Targeting Cluster Architectures"},{"location":"TensorFlow_2.5_installation_as_of_Jul_2021_/","text":"TensorFlow 2.5 installation Warning CUDA version >= 11.1 is currently disabled on the HPCC GPU nodes. It'll be enabled again in Jan 2022 after system upgrade. Meanwhile, you can either install TF with a version older than 2.5 or use HPCC-installed TF (run \"getexample TensorFlow_example\"). To run TF-GPU , we need to log into a GPU dev-node such as dev-amd20-v100. To install 1 2 3 4 5 6 7 8 9 module purge module load GCC/8.2.0-2.31.1 module load cuDNN/8.0.5.39-CUDA-11.1.1 module load Python virtualenv -p python3 tf2env-Jul2021 source tf2env-Jul2021/bin/activate pip install --upgrade tensorflow deactivate To test 1 2 3 4 5 6 7 8 9 module purge module load GCC/8.2.0-2.31.1 module load cuDNN/8.0.5.39-CUDA-11.1.1 module load Python source tf2env-Jul2021/bin/activate python >>> import tensorflow as tf >>> print (tf.__version__) >>> print(\"Num GPUs Available:\", len(tf.config.experimental.list_physical_devices('GPU')))","title":"Installing TF 2.5"},{"location":"TensorFlow_2.5_installation_as_of_Jul_2021_/#tensorflow-25-installation","text":"Warning CUDA version >= 11.1 is currently disabled on the HPCC GPU nodes. It'll be enabled again in Jan 2022 after system upgrade. Meanwhile, you can either install TF with a version older than 2.5 or use HPCC-installed TF (run \"getexample TensorFlow_example\"). To run TF-GPU , we need to log into a GPU dev-node such as dev-amd20-v100. To install 1 2 3 4 5 6 7 8 9 module purge module load GCC/8.2.0-2.31.1 module load cuDNN/8.0.5.39-CUDA-11.1.1 module load Python virtualenv -p python3 tf2env-Jul2021 source tf2env-Jul2021/bin/activate pip install --upgrade tensorflow deactivate To test 1 2 3 4 5 6 7 8 9 module purge module load GCC/8.2.0-2.31.1 module load cuDNN/8.0.5.39-CUDA-11.1.1 module load Python source tf2env-Jul2021/bin/activate python >>> import tensorflow as tf >>> print (tf.__version__) >>> print(\"Num GPUs Available:\", len(tf.config.experimental.list_physical_devices('GPU')))","title":"TensorFlow 2.5 installation"},{"location":"Torque_vs._SLURM/","text":"Torque vs. SLURM For users familiar with Torque/MOAB system, please check the following sections for comparisons between using Torque and using SLURM system. This could help you to use SLURM commands and transfer your job script from PBS to SBATCH. System Commands Specifications of Job submission Environment Variables Please also notice that the installation of a parallel compiler (such as OpenMPI, MVAPICH, ...) needs to be configured with job scheduler. If users would like to use their own installations of parallel compilers, please make sure they are compiled with appropriate settings.","title":"Overview"},{"location":"Torque_vs._SLURM/#torque-vs-slurm","text":"For users familiar with Torque/MOAB system, please check the following sections for comparisons between using Torque and using SLURM system. This could help you to use SLURM commands and transfer your job script from PBS to SBATCH.","title":"Torque vs. SLURM"},{"location":"Torque_vs._SLURM/#system-commands","text":"","title":"System Commands"},{"location":"Torque_vs._SLURM/#specifications-of-job-submission","text":"","title":"Specifications of Job submission"},{"location":"Torque_vs._SLURM/#environment-variables","text":"Please also notice that the installation of a parallel compiler (such as OpenMPI, MVAPICH, ...) needs to be configured with job scheduler. If users would like to use their own installations of parallel compilers, please make sure they are compiled with appropriate settings.","title":"Environment Variables"},{"location":"Transferring_data_with_Globus/","text":"Transferring data with Globus What is Globus? Globus is a free service to the MSU community for secure, reliable research data management. Globus gives users the ability to move and share data regardless of user or file location through a single web browser-based interface. Users can manage data from any device ( e.g. ,\u202fsupercomputer, tape archive, lab cluster or equipment, public cloud, or personal computer/laptop) from anywhere in the world using their existing institutional identities. Use of Globus removes data management roadblocks by providing unified access to all storage locations, making it easy to work with data while ensuring reliability and security. For more information on Globus, check out the Globus website here Why should I use Globus? Globus is ideal for moving large files and data transfers between ICER\u2019s HPCC or external research collaborators because of its truly fire-and-forget method of transferring data. After you initiate a file transfer, Globus will work on your behalf to optimize transfer performance, monitor for transfer completion and correctness, and recover from network errors, credential expiration, and collection downtime without restarting the transfer. This allows you to navigate away from File Manager, close the browser window, and even logout. Transferring data with Globus The HPCC has a Globus data transfer endpoint, msu#hpcc. This can be used to do large data transfers to/from your personal computer, to/from collaborators or to/from external HPC sites. You can also use it to share data. With Globus, you can create a transfer (to or from) between your computer and a folder on the HPCC which you have access to. This could be a folder in your home, research or scratch spaces. You can also create Globus \"shares\" to your external colleagues. They can use your created link to access the HPCC directory. You can not create a globus share on any folder in your home directory. You can only share folders in scratch space or any research space you have access to. To use Globus Online, please perform the following steps: If you do not have a globus account, create one at https://www.globus.org Log into the MSU Globus Online portal, https://globus.msu.edu and set up a free Globus Online account. If you wish to use Globus to transfer data to/from your local computer, install the Globus Connect Personal tool . On the Globus Start Transfer page, enter msu#hpcc as the end point on one side. This will pull up an authentication window. Use your MSU NetID for the username, and your MSU NetID password for the passphrase. This will set up an authenticated session that will last as long as specified in the Credential Lifetime field, up to a limit of 2 weeks. Select and authenticate with the other endpoint for your transfer, and initiate your transfer. An example can be seen from How To Transfer Files with Globus . To share data in a HPCC folder accessible to you with other persons, please check How To Share Data Using Globus . More information about using Globus can be found on Data transfer and sharing using Globus or Globus support page. For further training on the Science DMZ and how to use Globus, please self-enroll in ICER's DMZ Globus Training D2L course . ICER's DMZ Globus Training - Globus Walkthrough Documents (optional) Find and Connect to HPCC using Globus Transfer from PC to HPCC using Globus Transferring Data between Endpoints Using Globus Transferring Data with Google using Globus Sharing Data using Globus","title":"Using Globus"},{"location":"Transferring_data_with_Globus/#transferring-data-with-globus","text":"What is Globus? Globus is a free service to the MSU community for secure, reliable research data management. Globus gives users the ability to move and share data regardless of user or file location through a single web browser-based interface. Users can manage data from any device ( e.g. ,\u202fsupercomputer, tape archive, lab cluster or equipment, public cloud, or personal computer/laptop) from anywhere in the world using their existing institutional identities. Use of Globus removes data management roadblocks by providing unified access to all storage locations, making it easy to work with data while ensuring reliability and security. For more information on Globus, check out the Globus website here Why should I use Globus? Globus is ideal for moving large files and data transfers between ICER\u2019s HPCC or external research collaborators because of its truly fire-and-forget method of transferring data. After you initiate a file transfer, Globus will work on your behalf to optimize transfer performance, monitor for transfer completion and correctness, and recover from network errors, credential expiration, and collection downtime without restarting the transfer. This allows you to navigate away from File Manager, close the browser window, and even logout. Transferring data with Globus The HPCC has a Globus data transfer endpoint, msu#hpcc. This can be used to do large data transfers to/from your personal computer, to/from collaborators or to/from external HPC sites. You can also use it to share data. With Globus, you can create a transfer (to or from) between your computer and a folder on the HPCC which you have access to. This could be a folder in your home, research or scratch spaces. You can also create Globus \"shares\" to your external colleagues. They can use your created link to access the HPCC directory. You can not create a globus share on any folder in your home directory. You can only share folders in scratch space or any research space you have access to. To use Globus Online, please perform the following steps: If you do not have a globus account, create one at https://www.globus.org Log into the MSU Globus Online portal, https://globus.msu.edu and set up a free Globus Online account. If you wish to use Globus to transfer data to/from your local computer, install the Globus Connect Personal tool . On the Globus Start Transfer page, enter msu#hpcc as the end point on one side. This will pull up an authentication window. Use your MSU NetID for the username, and your MSU NetID password for the passphrase. This will set up an authenticated session that will last as long as specified in the Credential Lifetime field, up to a limit of 2 weeks. Select and authenticate with the other endpoint for your transfer, and initiate your transfer. An example can be seen from How To Transfer Files with Globus . To share data in a HPCC folder accessible to you with other persons, please check How To Share Data Using Globus . More information about using Globus can be found on Data transfer and sharing using Globus or Globus support page. For further training on the Science DMZ and how to use Globus, please self-enroll in ICER's DMZ Globus Training D2L course . ICER's DMZ Globus Training - Globus Walkthrough Documents (optional) Find and Connect to HPCC using Globus Transfer from PC to HPCC using Globus Transferring Data between Endpoints Using Globus Transferring Data with Google using Globus Sharing Data using Globus","title":"Transferring data with Globus"},{"location":"Trimmomatic/","text":"Trimmomatic Trimmomatic is a tool for trimming Illumina FASTQ data and removing adapters. When data is sequenced on Illumina, adapters are added for the fragments to attach to the beads. If these adapters are not removed they can result in false assembly or other issues. Additionally, the quality of the sequences varies across the length of the read, and poorer quality regions can be trimmed using Trimmomatic. Running Trimmomatic is a good first step in quality filtering your Illumina data. To run it on the HPCC (for example trimming paired-end reads): 1 java -jar /opt/software/Trimmomatic/0.36-Java-1.8.0_92/trimmomatic-0.36.jar PE [-threads <threads] [-phred33 | -phred64] [-trimlog <logFile>] <input 1> <input 2> <paired output 1> <unpaired output 1> <paired output 2> <unpaired output 2> <step 1> ... Read the manual for how to use it. Note that the adapter sequence files are in /opt/software/Trimmomatic/0.36-Java-1.8.0_92/adapters/ . If you couldn't find the adapters you need in that directory, you will need to obtain them from elsewhere (for example asking the person who ran the library prep for you).","title":"Trimmomatic"},{"location":"Trimmomatic/#trimmomatic","text":"Trimmomatic is a tool for trimming Illumina FASTQ data and removing adapters. When data is sequenced on Illumina, adapters are added for the fragments to attach to the beads. If these adapters are not removed they can result in false assembly or other issues. Additionally, the quality of the sequences varies across the length of the read, and poorer quality regions can be trimmed using Trimmomatic. Running Trimmomatic is a good first step in quality filtering your Illumina data. To run it on the HPCC (for example trimming paired-end reads): 1 java -jar /opt/software/Trimmomatic/0.36-Java-1.8.0_92/trimmomatic-0.36.jar PE [-threads <threads] [-phred33 | -phred64] [-trimlog <logFile>] <input 1> <input 2> <paired output 1> <unpaired output 1> <paired output 2> <unpaired output 2> <step 1> ... Read the manual for how to use it. Note that the adapter sequence files are in /opt/software/Trimmomatic/0.36-Java-1.8.0_92/adapters/ . If you couldn't find the adapters you need in that directory, you will need to obtain them from elsewhere (for example asking the person who ran the library prep for you).","title":"Trimmomatic"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/","text":"Trinity for RNA-seq de novo assembly Loading module Take loading Trinity 2.6.6 as an example, we run: 1 2 module purge module load icc/2017.4.196-GCC-6.4.0-2.28 impi/2017.3.196 Trinity/2.6.6 Most basic run (transcript assembly) A typical Trinity command for assembling strand-specific paired-end RNA-seq data would look like: A typical run of Trinity 1 2 3 4 5 6 7 Trinity \\ --seqType fq \\ --max_memory 2G \\ --left reads.left.fq \\ --right reads.right.fq \\ --SS_lib_type RF \\ --CPU 10 This will generate output files in a new directory \" trinity_out_dir \" in the working directory. Among them, the assembled transcripts file is \" Trinity.fasta \". For more detail, check out https://github.com/trinityrnaseq/trinityrnaseq/wiki When you submit the above command as a job to the cluster, you need to request 10 CPUs in the sbatch script with the following lines (in addition to your other sbatch directives): sbatch code snippet 1 2 3 #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=10 Transcript quantification Trinity provides abundant utility scripts for post-assembly analysis, such as quality assessment, transcript quantification and differential expression tests. For some of them, external software tools need to be installed separately (that is, they are not bundled with Trinity). For example, for the transcript quantification step, we will need one of RSEM, eXpress, kalllisto and salmon (cf. https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-Transcript-Quantification ). We have made all these four available on the HPCC. As instructed by Trinity, \" the tools should be available via your PATH setting \". So, in the next example where we choose to use RSEM to align reads to the assembled transcript and then quantify transcript abundance, we first set the PATH variable so that RSEM can be automatically searched for by trinity. Using RSEM for transcript quantification 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Assuming # 1) you've loaded Trinity module already and # 2) your current working directory is trinity_out_dir generated from the previous assembly step. export PATH=/opt/software/RSEM/1.3.1-GCCcore-6.4.0/usr/local/bin:$PATH /opt/software/Trinity/2.6.6/util/align_and_estimate_abundance.pl --seqType fq --transcripts Trinity.fasta \\ --est_method RSEM \\ --left ../reads.left.fq \\ --right ../reads.right.fq \\ --SS_lib_type RF \\ --aln_method bowtie \\ --trinity_mode \\ --prep_reference \\ --thread_count 10 \\ --output_dir RSEM_out The RSEM computation generates two primary output files containing estimated abundances in the subdirectory RSEM_out as specified in the command above: RSEM.isoforms.results (transcript level) and RSEM.genes.results (gene level). More utilities Please consult https://github.com/trinityrnaseq/trinityrnaseq/wiki for detail. Note that a few R packages are needed for differential expression analysis ( https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-Differential-Expression ). These have been installed in R/4.0.2 which can be loaded by 1 module purge; module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 Version note The latest version is 2.91. After loading it, you may load R 4.0.2 for DE analysis. module purge module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 Trinity/2.9.1 module load R/4.0.2","title":"Trinity for RNA-seq de novo assembly"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#trinity-for-rna-seq-de-novo-assembly","text":"","title":"Trinity for RNA-seq de novo assembly"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#loading-module","text":"Take loading Trinity 2.6.6 as an example, we run: 1 2 module purge module load icc/2017.4.196-GCC-6.4.0-2.28 impi/2017.3.196 Trinity/2.6.6","title":"Loading module"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#most-basic-run-transcript-assembly","text":"A typical Trinity command for assembling strand-specific paired-end RNA-seq data would look like: A typical run of Trinity 1 2 3 4 5 6 7 Trinity \\ --seqType fq \\ --max_memory 2G \\ --left reads.left.fq \\ --right reads.right.fq \\ --SS_lib_type RF \\ --CPU 10 This will generate output files in a new directory \" trinity_out_dir \" in the working directory. Among them, the assembled transcripts file is \" Trinity.fasta \". For more detail, check out https://github.com/trinityrnaseq/trinityrnaseq/wiki When you submit the above command as a job to the cluster, you need to request 10 CPUs in the sbatch script with the following lines (in addition to your other sbatch directives): sbatch code snippet 1 2 3 #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=10","title":"Most basic run (transcript assembly)"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#transcript-quantification","text":"Trinity provides abundant utility scripts for post-assembly analysis, such as quality assessment, transcript quantification and differential expression tests. For some of them, external software tools need to be installed separately (that is, they are not bundled with Trinity). For example, for the transcript quantification step, we will need one of RSEM, eXpress, kalllisto and salmon (cf. https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-Transcript-Quantification ). We have made all these four available on the HPCC. As instructed by Trinity, \" the tools should be available via your PATH setting \". So, in the next example where we choose to use RSEM to align reads to the assembled transcript and then quantify transcript abundance, we first set the PATH variable so that RSEM can be automatically searched for by trinity. Using RSEM for transcript quantification 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Assuming # 1) you've loaded Trinity module already and # 2) your current working directory is trinity_out_dir generated from the previous assembly step. export PATH=/opt/software/RSEM/1.3.1-GCCcore-6.4.0/usr/local/bin:$PATH /opt/software/Trinity/2.6.6/util/align_and_estimate_abundance.pl --seqType fq --transcripts Trinity.fasta \\ --est_method RSEM \\ --left ../reads.left.fq \\ --right ../reads.right.fq \\ --SS_lib_type RF \\ --aln_method bowtie \\ --trinity_mode \\ --prep_reference \\ --thread_count 10 \\ --output_dir RSEM_out The RSEM computation generates two primary output files containing estimated abundances in the subdirectory RSEM_out as specified in the command above: RSEM.isoforms.results (transcript level) and RSEM.genes.results (gene level).","title":"Transcript quantification"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#more-utilities","text":"Please consult https://github.com/trinityrnaseq/trinityrnaseq/wiki for detail. Note that a few R packages are needed for differential expression analysis ( https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-Differential-Expression ). These have been installed in R/4.0.2 which can be loaded by 1 module purge; module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2","title":"More utilities"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#version-note","text":"The latest version is 2.91. After loading it, you may load R 4.0.2 for DE analysis. module purge module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 Trinity/2.9.1 module load R/4.0.2","title":"Version note"},{"location":"Tutorials-and-Exercises_34963691.html/","text":"Teaching : Tutorials and Exercises File Permission in Research Space How to make users able to access files or directories in their research space? HPCC Job Submission Workflow Understand basic workflow for job submission to the clusters. Running multiple jobs sequentially Submitting multiple jobs simultaneously Job Arrays - Run multiple similar jobs simultaneously","title":"Tutorials and Exercises 34963691.html"},{"location":"Tutorials-and-Exercises_34963691.html/#teaching-tutorials-and-exercises","text":"","title":"Teaching : Tutorials and Exercises"},{"location":"Tutorials-and-Exercises_34963691.html/#file-permission-in-research-space","text":"How to make users able to access files or directories in their research space?","title":"File Permission in Research Space"},{"location":"Tutorials-and-Exercises_34963691.html/#hpcc-job-submission-workflow","text":"Understand basic workflow for job submission to the clusters.","title":"HPCC Job Submission Workflow"},{"location":"Tutorials-and-Exercises_34963691.html/#running-multiple-jobs-sequentially","text":"","title":"Running multiple jobs sequentially"},{"location":"Tutorials-and-Exercises_34963691.html/#submitting-multiple-jobs-simultaneously","text":"","title":"Submitting multiple jobs simultaneously"},{"location":"Tutorials-and-Exercises_34963691.html/#job-arrays-run-multiple-similar-jobs-simultaneously","text":"","title":"Job Arrays - Run multiple similar jobs simultaneously"},{"location":"User_Created_Modules/","text":"User Created Modules This is for advanced usage, since creating your own module files for software access is usually not necessary. If you develop or install your own software, you might consider writing a modulefile to help manage your environment variables. HPCC presently uses the LMOD module package, developed at TACC. The following is a typical module file with comments. Name your files with a .lua extension. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 -- -*- lua -*- help ( [[ Describe your software here. ]]) -- comments are prefaced with two dashes whatis ( \"Description: Name of software\" ) whatis ( \"URL: www.ucc.org \" ) local install_path = \"/mnt/home/ongbw/opt/mysoftware\" -- set an environment variable setenv ( \"MYSOFTWARE_HOME\" ,install_path ) -- add to PATH variable prepend_path ( 'PATH' , pathJoin ( install_path, \"bin\" )) -- add Library Paths prepend_path ( 'LD_LIBRARY_PATH' ,pathJoin ( install_path, \"lib\" )) prepend_path ( 'LIBRARY_PATH' ,pathJoin ( install_path, \"lib\" )) -- add include paths prepend_path ( 'INCLUDE' ,pathJoin ( install_path, \"include\" ))","title":"User created modules"},{"location":"User_Created_Modules/#user-created-modules","text":"This is for advanced usage, since creating your own module files for software access is usually not necessary. If you develop or install your own software, you might consider writing a modulefile to help manage your environment variables. HPCC presently uses the LMOD module package, developed at TACC. The following is a typical module file with comments. Name your files with a .lua extension. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 -- -*- lua -*- help ( [[ Describe your software here. ]]) -- comments are prefaced with two dashes whatis ( \"Description: Name of software\" ) whatis ( \"URL: www.ucc.org \" ) local install_path = \"/mnt/home/ongbw/opt/mysoftware\" -- set an environment variable setenv ( \"MYSOFTWARE_HOME\" ,install_path ) -- add to PATH variable prepend_path ( 'PATH' , pathJoin ( install_path, \"bin\" )) -- add Library Paths prepend_path ( 'LD_LIBRARY_PATH' ,pathJoin ( install_path, \"lib\" )) prepend_path ( 'LIBRARY_PATH' ,pathJoin ( install_path, \"lib\" )) -- add include paths prepend_path ( 'INCLUDE' ,pathJoin ( install_path, \"include\" ))","title":"User Created Modules"},{"location":"Using_Git_from_a_Unix_Shell/","text":"Using Git from a Unix Shell Overview This document applies to people attempting to use the standard Git client to access vcs.icer.msu.edu from a Unix shell, such as provided on gateway.hpcc.msu.edu . This document also applies to people using Git in Cygwin or MSysGit in MSys on Windows, or using Git from the Fink project or the Darwin Ports project and running in a Mac OS X terminal window. Anonymous Access A project which has enabled anonymous access to its repository can be cloned via the following command: git clone http://vcs.icer.msu.edu/git-repos/ project .git where project is the name of the project. A repository cloned in this manner cannot be used to push to the repository from which it was cloned, but can be used to repeatedly pull in updates from it. Authenticated Access via HTTPS A designated developer for a project can clone the project's repository with the following command: git clone https:// username @vcs.icer.msu.edu/git-repos/ project .git where username is either the MSU Net ID or other specially assigned ID of the developer and project is the name of the project. The person attempting to access a repository via this method will be prompted for a password. Note that, when needed, the person's password will be transmitted over a secure channel. A repository cloned in this manner can be used to push updates to and pull updates from the repository from which it was cloned. Authenticated Access via SSH A designated developer for a project can clone the project's repository with the following command: git clone ssh:// username @vcs.icer.msu.edu/git-repos/ project .git where username is the MSU Net ID of a designated developer and project is the name of the project. Note that designated develoeprs not having valid MSU Net IDs cannot presently access a repository with this command; such developers should use the HTTPS access method instead. The person attempting to access a repository via this method will be prompted for a password, unless that person has previously arranged for key-based authentication. Note that, when needed, the person's password will be transmitted over a secure channel. A repository cloned in this manner can be used to push updates to and pull updates from the repository from which it was cloned. **HELPFUL TIP:** If you are accessing your git repository from the HPCC, change **vcs.icer.msu.edu** to **vcs-00-dmz.dmz** in your git clone command. This change will allow you to access your repository without retyping your password. Note: this alternative URL will not work from anywhere but the HPCC. If you have already have a clone of your repository you can manually change the URL by editing the .git/config file from within your repository.","title":"Using Git from a Unix Shell"},{"location":"Using_Git_from_a_Unix_Shell/#using-git-from-a-unix-shell","text":"","title":"Using Git from a Unix Shell"},{"location":"Using_Git_from_a_Unix_Shell/#overview","text":"This document applies to people attempting to use the standard Git client to access vcs.icer.msu.edu from a Unix shell, such as provided on gateway.hpcc.msu.edu . This document also applies to people using Git in Cygwin or MSysGit in MSys on Windows, or using Git from the Fink project or the Darwin Ports project and running in a Mac OS X terminal window.","title":"Overview"},{"location":"Using_Git_from_a_Unix_Shell/#anonymous-access","text":"A project which has enabled anonymous access to its repository can be cloned via the following command: git clone http://vcs.icer.msu.edu/git-repos/ project .git where project is the name of the project. A repository cloned in this manner cannot be used to push to the repository from which it was cloned, but can be used to repeatedly pull in updates from it.","title":"Anonymous Access"},{"location":"Using_Git_from_a_Unix_Shell/#authenticated-access-via-https","text":"A designated developer for a project can clone the project's repository with the following command: git clone https:// username @vcs.icer.msu.edu/git-repos/ project .git where username is either the MSU Net ID or other specially assigned ID of the developer and project is the name of the project. The person attempting to access a repository via this method will be prompted for a password. Note that, when needed, the person's password will be transmitted over a secure channel. A repository cloned in this manner can be used to push updates to and pull updates from the repository from which it was cloned.","title":"Authenticated Access via HTTPS"},{"location":"Using_Git_from_a_Unix_Shell/#authenticated-access-via-ssh","text":"A designated developer for a project can clone the project's repository with the following command: git clone ssh:// username @vcs.icer.msu.edu/git-repos/ project .git where username is the MSU Net ID of a designated developer and project is the name of the project. Note that designated develoeprs not having valid MSU Net IDs cannot presently access a repository with this command; such developers should use the HTTPS access method instead. The person attempting to access a repository via this method will be prompted for a password, unless that person has previously arranged for key-based authentication. Note that, when needed, the person's password will be transmitted over a secure channel. A repository cloned in this manner can be used to push updates to and pull updates from the repository from which it was cloned. **HELPFUL TIP:** If you are accessing your git repository from the HPCC, change **vcs.icer.msu.edu** to **vcs-00-dmz.dmz** in your git clone command. This change will allow you to access your repository without retyping your password. Note: this alternative URL will not work from anywhere but the HPCC. If you have already have a clone of your repository you can manually change the URL by editing the .git/config file from within your repository.","title":"Authenticated Access via SSH"},{"location":"Using_Python_in_HPCC_with_virtualenv/","text":"Using Python in HPCC with virtualenv Python applications usually use packages and modules that require specific version of libraries. This means one installed application may conflict with another application due to using the same library but with different versions. It is difficult to meet the requirements of every application by one global Python installation. To resolve this issue, users can create an isolated virtual environment with a particular version of Python in a self-contained directory of their home or research space. Any package and the dependent libraries installed inside the directory can be available only through the virtual environment. Different applications can then use different virtual environments to avoid any conflict. Create and use virtual environments To create python virtual environments, please make sure your preferred version of Python is loaded. It is also a good idea to create a directory of the python version to store different environments and their applications: 1 2 3 4 5 6 7 8 9 10 11 [UserName@dev-intel18 ~]$ module list Python Currently Loaded Modules Matching: Python 1) Python/3.6.4 [UserName@dev-intel18 ~]$ which python /opt/software/Python/3.6.4-foss-2018a/bin/python [UserName@dev-intel18 ~]$ mkdir Python3.6.4 [UserName@dev-intel18 ~]$ cd Python3.6.4 [UserName@dev-intel18 Python3.6.4]$ Currently, two common tools can be used to create Python virtual environments. Please use only one of them: venv is available for Python 3.3 and later by default. The application pip and setuptools should be ready to use in HPCC system. A virtual environment can be created by running \" python3 -m venv <DIR> \", where <DIR> is the directory of the created environment. Following is an example of the command, and the directory tutorial is created for the virtual environment. 1 2 3 [UserName@dev-intel18 Python3.6.4]$ python3 -m venv tutorial [UserName@dev-intel18 Python3.6.4]$ ls tutorial bin include lib lib64 pyvenv.cfg virtualenv supports all Python versions. By default, HPCC system has pip , setuptools and wheel installed and available. Similarly to venv, a virtual environment can be created by executing \"virtualenv \\<DIR>\", where applications of the virtual environment are installed in tutorial directory. [UserName@dev-intel18 Python3.6.4]$ virtualenv tutorial Using base prefix '/opt/software/Python/3.6.4-foss-2018a' New python executable in /mnt/home/UserName/Python3.6.4/tutorial/bin/python Installing setuptools, pip, wheel...done. [UserName@dev-intel18 Python3.6.4]$ ls tutorial bin include lib lib64 pip-selfcheck.json Please use one of them to create the virtual environment. The created tutorial environment can now be used by sourcing the script file activate under the bin directory: 1 2 [UserName@dev-intel18 Python3.6.4]$ source tutorial/bin/activate (tutorial) [UserName@dev-intel18 Python3.6.4]$ where the name inside the parentheses (tutorial) in front of the prompt line shows the current Python environment. To leave the environment, just run \"deactivate\": 1 2 (tutorial) [UserName@dev-intel18 Python3.6.4]$ deactivate [UserName@dev-intel18 Python3.6.4]$ and the parentheses disappear. Any time, users want to use tutorial environment. Simply source the file again: \" source ~/Python3.6.4/tutorial/bin/activate \" and the environment is back. More information can be found about venv or virtualenv . Install packages from PyPI using pip The most common usage of pip is to install python packages from the Python Package Index with a requirement specifier . Users can also check other usages with pip. Below, some of the common usage scenarios are introduced. To install the latest version of a python package, users can run \" pip install <Package Name> \", for example, using sympy as <Package Name> : 1 2 3 4 5 6 (tutorial) [UserName@dev-intel18 Python3.6.4]$ pip install \"sympy\" Collecting sympy Using cached https://files.pythonhosted.org/packages/21/21/f4105795ca7f35c541d82c5b06be684dd2f5cb4f508fb487cd7aea4de776/sympy-1.4-py2.py3-none-any.whl Collecting mpmath>=0.19 (from sympy) Installing collected packages: mpmath, sympy Successfully installed mpmath-1.1.0 sympy-1.4 To install a specific version of a python package, please run \" pip install <Package Name>==<Version Number> \". For example, install numpy with <version number> as 1.16.2 : 1 2 3 4 5 6 (tutorial) [UserName@dev-intel18 Python3.6.4]$ pip install \"numpy==1.16.2\" Collecting numpy==1.16.2 Downloading https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl (17.3MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17.3MB 10.5MB/s Installing collected packages: numpy Successfully installed numpy-1.16.2 To upgrade an already installed package to the latest from PyPI, users can run \" pip install --upgrade \". For example, upgrade the installed package numpy : 1 2 3 4 5 6 7 8 (tutorial) [UserName@dev-intel18 Python3.6.4]$ pip install --upgrade numpy Collecting numpy Using cached https://files.pythonhosted.org/packages/e5/e6/c3fdc53aed9fa19d6ff3abf97dfad768ae3afce1b7431f7500000816bda5/numpy-1.17.2-cp36-cp36m-manylinux1_x86_64.whl Installing collected packages: numpy Found existing installation: numpy 1.16.2 Uninstalling numpy-1.16.2: Successfully uninstalled numpy-1.16.2 Successfully installed numpy-1.17.2 With pip, you can also list all installed packages and their versions with the command \" pip freeze \": 1 2 3 4 (tutorial) [UserName@dev-intel18 Python3.6.4]$ pip freeze mpmath==1.1.0 numpy==1.17.2 sympy==1.4 For more detail, see the pip docs , which includes a complete Reference Guide . More examples of using virtualenv and pip install can be found in TensorFlow page . PYTHONPATH environment variable You can use the environment variable PYTHONPATH to include the packages already installed by the same python version in other directories. By adding the site-packages paths to PYTHONPATH environment variable and separating them by \" : \" sign: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 (tutorial) [UserName@dev-intel18 Python3.6.4]$ export PYTHONPATH=~/Python3.6.4/tutorial/lib/python3.6/site-packages:/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages (tutorial) [UserName@dev-intel18 Python3.6.4]$ pip freeze absl-py==0.5.0 alabaster==0.7.12 appdirs==1.4.3 artemis==0.1.4 ... nose==1.3.7 numpy==1.17.2 numpydoc==0.8.0 ... suspenders==0.2.6 sympy==1.4 tensorboard==1.10.0 tensorflow==1.10.1 ... virtualenv==15.1.0 wcwidth==0.1.7 Werkzeug==0.14.1 xopen==0.3.5 all packages inside the paths are now ready to use. Please make sure the site-packages path of the current environment is set the first in PYTHONPATH variable. If a package is installed in more than one path (possibly with different versions), the package of the first path showing in the variable ( PYTHONPATH ) will be used.","title":"Using virtual env"},{"location":"Using_Python_in_HPCC_with_virtualenv/#using-python-in-hpcc-with-virtualenv","text":"Python applications usually use packages and modules that require specific version of libraries. This means one installed application may conflict with another application due to using the same library but with different versions. It is difficult to meet the requirements of every application by one global Python installation. To resolve this issue, users can create an isolated virtual environment with a particular version of Python in a self-contained directory of their home or research space. Any package and the dependent libraries installed inside the directory can be available only through the virtual environment. Different applications can then use different virtual environments to avoid any conflict.","title":"Using Python in HPCC with virtualenv"},{"location":"Using_Python_in_HPCC_with_virtualenv/#create-and-use-virtual-environments","text":"To create python virtual environments, please make sure your preferred version of Python is loaded. It is also a good idea to create a directory of the python version to store different environments and their applications: 1 2 3 4 5 6 7 8 9 10 11 [UserName@dev-intel18 ~]$ module list Python Currently Loaded Modules Matching: Python 1) Python/3.6.4 [UserName@dev-intel18 ~]$ which python /opt/software/Python/3.6.4-foss-2018a/bin/python [UserName@dev-intel18 ~]$ mkdir Python3.6.4 [UserName@dev-intel18 ~]$ cd Python3.6.4 [UserName@dev-intel18 Python3.6.4]$ Currently, two common tools can be used to create Python virtual environments. Please use only one of them: venv is available for Python 3.3 and later by default. The application pip and setuptools should be ready to use in HPCC system. A virtual environment can be created by running \" python3 -m venv <DIR> \", where <DIR> is the directory of the created environment. Following is an example of the command, and the directory tutorial is created for the virtual environment. 1 2 3 [UserName@dev-intel18 Python3.6.4]$ python3 -m venv tutorial [UserName@dev-intel18 Python3.6.4]$ ls tutorial bin include lib lib64 pyvenv.cfg virtualenv supports all Python versions. By default, HPCC system has pip , setuptools and wheel installed and available. Similarly to venv, a virtual environment can be created by executing \"virtualenv \\<DIR>\", where applications of the virtual environment are installed in tutorial directory. [UserName@dev-intel18 Python3.6.4]$ virtualenv tutorial Using base prefix '/opt/software/Python/3.6.4-foss-2018a' New python executable in /mnt/home/UserName/Python3.6.4/tutorial/bin/python Installing setuptools, pip, wheel...done. [UserName@dev-intel18 Python3.6.4]$ ls tutorial bin include lib lib64 pip-selfcheck.json Please use one of them to create the virtual environment. The created tutorial environment can now be used by sourcing the script file activate under the bin directory: 1 2 [UserName@dev-intel18 Python3.6.4]$ source tutorial/bin/activate (tutorial) [UserName@dev-intel18 Python3.6.4]$ where the name inside the parentheses (tutorial) in front of the prompt line shows the current Python environment. To leave the environment, just run \"deactivate\": 1 2 (tutorial) [UserName@dev-intel18 Python3.6.4]$ deactivate [UserName@dev-intel18 Python3.6.4]$ and the parentheses disappear. Any time, users want to use tutorial environment. Simply source the file again: \" source ~/Python3.6.4/tutorial/bin/activate \" and the environment is back. More information can be found about venv or virtualenv .","title":"Create and use virtual environments"},{"location":"Using_Python_in_HPCC_with_virtualenv/#install-packages-from-pypi-using-pip","text":"The most common usage of pip is to install python packages from the Python Package Index with a requirement specifier . Users can also check other usages with pip. Below, some of the common usage scenarios are introduced. To install the latest version of a python package, users can run \" pip install <Package Name> \", for example, using sympy as <Package Name> : 1 2 3 4 5 6 (tutorial) [UserName@dev-intel18 Python3.6.4]$ pip install \"sympy\" Collecting sympy Using cached https://files.pythonhosted.org/packages/21/21/f4105795ca7f35c541d82c5b06be684dd2f5cb4f508fb487cd7aea4de776/sympy-1.4-py2.py3-none-any.whl Collecting mpmath>=0.19 (from sympy) Installing collected packages: mpmath, sympy Successfully installed mpmath-1.1.0 sympy-1.4 To install a specific version of a python package, please run \" pip install <Package Name>==<Version Number> \". For example, install numpy with <version number> as 1.16.2 : 1 2 3 4 5 6 (tutorial) [UserName@dev-intel18 Python3.6.4]$ pip install \"numpy==1.16.2\" Collecting numpy==1.16.2 Downloading https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl (17.3MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17.3MB 10.5MB/s Installing collected packages: numpy Successfully installed numpy-1.16.2 To upgrade an already installed package to the latest from PyPI, users can run \" pip install --upgrade \". For example, upgrade the installed package numpy : 1 2 3 4 5 6 7 8 (tutorial) [UserName@dev-intel18 Python3.6.4]$ pip install --upgrade numpy Collecting numpy Using cached https://files.pythonhosted.org/packages/e5/e6/c3fdc53aed9fa19d6ff3abf97dfad768ae3afce1b7431f7500000816bda5/numpy-1.17.2-cp36-cp36m-manylinux1_x86_64.whl Installing collected packages: numpy Found existing installation: numpy 1.16.2 Uninstalling numpy-1.16.2: Successfully uninstalled numpy-1.16.2 Successfully installed numpy-1.17.2 With pip, you can also list all installed packages and their versions with the command \" pip freeze \": 1 2 3 4 (tutorial) [UserName@dev-intel18 Python3.6.4]$ pip freeze mpmath==1.1.0 numpy==1.17.2 sympy==1.4 For more detail, see the pip docs , which includes a complete Reference Guide . More examples of using virtualenv and pip install can be found in TensorFlow page .","title":"Install packages from PyPI using pip"},{"location":"Using_Python_in_HPCC_with_virtualenv/#pythonpath-environment-variable","text":"You can use the environment variable PYTHONPATH to include the packages already installed by the same python version in other directories. By adding the site-packages paths to PYTHONPATH environment variable and separating them by \" : \" sign: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 (tutorial) [UserName@dev-intel18 Python3.6.4]$ export PYTHONPATH=~/Python3.6.4/tutorial/lib/python3.6/site-packages:/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages (tutorial) [UserName@dev-intel18 Python3.6.4]$ pip freeze absl-py==0.5.0 alabaster==0.7.12 appdirs==1.4.3 artemis==0.1.4 ... nose==1.3.7 numpy==1.17.2 numpydoc==0.8.0 ... suspenders==0.2.6 sympy==1.4 tensorboard==1.10.0 tensorflow==1.10.1 ... virtualenv==15.1.0 wcwidth==0.1.7 Werkzeug==0.14.1 xopen==0.3.5 all packages inside the paths are now ready to use. Please make sure the site-packages path of the current environment is set the first in PYTHONPATH variable. If a package is installed in more than one path (possibly with different versions), the package of the first path showing in the variable ( PYTHONPATH ) will be used.","title":"PYTHONPATH environment variable"},{"location":"Using_Version_Control_Systems/","text":"Using Version Control Systems Overview ICER provides git and svn to assist researchers in collaboratively developing code. We recommend hosting the repositories at http://gitlab.msu.edu. The following site provides a nice 15 minute, hands-on tutorial for using git: http://try.github.com/ The following git manual is much more comprehensive: http://git-scm.com/docs Accessing Repositories with SSH Key-Based Authentication Using Git from a Unix Shell","title":"Using Version Control Systems"},{"location":"Using_Version_Control_Systems/#using-version-control-systems","text":"","title":"Using Version Control Systems"},{"location":"Using_Version_Control_Systems/#overview","text":"ICER provides git and svn to assist researchers in collaboratively developing code. We recommend hosting the repositories at http://gitlab.msu.edu. The following site provides a nice 15 minute, hands-on tutorial for using git: http://try.github.com/ The following git manual is much more comprehensive: http://git-scm.com/docs","title":"Overview"},{"location":"Using_Version_Control_Systems/#accessing-repositories-with-ssh-key-based-authentication","text":"","title":"Accessing Repositories with SSH Key-Based Authentication"},{"location":"Using_Version_Control_Systems/#using-git-from-a-unix-shell","text":"","title":"Using Git from a Unix Shell"},{"location":"Using_conda/","text":"Using conda This wiki is based on conda version prior to 4.6. It may not apply to 4.6 and later versions (where, for example, conda environment is activated by conda activate ). Check the manual before you start using conda. Introduction Anaconda is a distribution of python that contains a free, easy-to-install package manager, environment manager and Python distribution with a collection of 1,000+ open source packages with free community support. Conda is the package and environment manager that installs and updates packages and their dependencies, included in all versions of Anaconda. Anaconda needs to be installed in your home or project directory and so does any software which will be installed using it. This allows users to have full control of their programs. This wiki serves only as a mini-version of conda tutorial. You are highly recommended to read the User Guide. conda to install and run a program called \"macs2\". First we go to Anaconda Cloud to search for it. Once it's found, we pick a source and click on the link to get the installation command (i.e., conda install -c bioconda macs2 below) Installation of macs2 1 2 3 4 5 export PATH=$PATH:$HOME/anaconda3/bin conda create --name macs2 source activate macs2 conda install -c bioconda macs2 source deactivate To launch it later on (for example, in your SLURM job submission script): Using it 1 2 3 4 export PATH=$PATH:$HOME/anaconda3/bin source activate macs2 #[your commands] source deactivate Installation of Anaconda Visit https://docs.anaconda.com/anaconda/install/linux for installation instruction. And Linux installers are available from https://www.anaconda.com/download/#linux Note: 1) Choose 64-Bit (x86) Installer. 2) Version 2 or 3? You should choose python2 or python3 depending on what your software requirements are. Some libraries/packages only work on Python 2 so you should check with the python package you ultimately want to use. If you don't know, select the latest version 3. 3) Anaconda now partners with Microsoft and asks if you want to install \"VSCode\" THIS WILL NOT WORK. The installation requires administrative privileges that you don't have. You can download the install file directly into your home directory by first getting the URL of the download (right-click on the download link and select \"copy link address\" or \"copy link location\"). Then open a terminal to log onto a dev-node, and use this command: curl -O <download link> An example link can be: https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh Once the file Anaconda3-2022.05-Linux-x86_64.sh is downloaded, you can run the command: bash Anaconda3-2022.05-Linux-x86_64.sh During Installation the installer will ask you Accept licence agreement : you must type yes to agree. We can not install for the who system, you must install yourself and agree to this license Where to install : you can install in any location in your home directory, the default is Anaconda2 or Anaconda3. Note you can have multiple installations of Anaconda Initialize Anaconda (run conda init?) : please answer \"no\". A more careful setup is suggested. Please follow the instruction in the next section after this installation. Install VS Code? This is a new bundle that comes with Anaconda. This won't work and you must answer \"no\" Please remember where Anaconda is installed in the step 2 above. The installation path will be used for module setup in the next section. In case your installed Anaconda is auto-activated on startup, please run the command: conda config --set auto_activate_base false after close and re-open your current shell. Please read the next section for the setup and the start-up of your Anaconda. Module Setup for Anaconda In order to have no conflict between user-installed Anaconda and system-installed Python versions, a setup in the $HOME/.bashrc file for module system is necessary. The .bashrc file under your home directory can be used to do environment setting for every time you log in to a HPCC node. You can modify the file by an editor program (such as nano, vim) for Anaconda setup: (1) Please check if any conda initialization or setup script is in the ~/.bashrc file. The script starts with # >>> conda init >>> and ends with # <<< conda init <<< : 1 2 3 4 5 6 7 # >>> conda init >>> # !! Contents within this block are managed by 'conda init' !! __conda_setup= ... ... ... ... ... ... unset __conda_setup # <<< conda init <<< If there is, please remove or comment all of the command lines out (by adding the sign \" # \" in its front). If there is any export command line to add the installed Anaconda bin path to PATH environment variable, please also comment it out. (2) If your installed Anaconda path is ~/anaconda3 or ~/anaconda2 , you can skip this step and go to the next one. If not, please set the variable CONDA3PATH to be the place of installed Anaconda3 by adding the command line: 1 export CONDA3PATH=<your installed Anaconda3 path> in the ~/.bashrc file. If you have Anaconda2 installed, you may set the variable CONDA2PATH to be the installed path of Anaconda2. After the environment setup, please save the ~/.bashrc file. Log out and log into a dev node. (3) Whenever you want to use Anaconda3 commands, just run: 1 module load Conda/3 to initiate conda and CONDA3PATH is in your PATH variable. Loading the module will also replace any loaded Python module so there is no conflict. If CONDA2PATH variable is set up, please load Conda/2 module for using Anaconda2. (4) If you would like to have your installed Anaconda3 to be the default Python program when your session starts up, you can add the command: 1 module load Conda/3 2> /dev/null to the ~/.bashrc file after export CONDA3PATH=... command line. Using Conda module can also avoid a conflict with Web-based Remote Desktop gateway. If anaconda2 is installed, you can use Conda/2 instead of Conda/3 module. Managing conda To manage conda, you should use a Terminal window to check the following. You can begin by checking if conda is installed by using the command: Checking the version 1 conda --version If it is installed, the number of the version you have installed should show up after the previous command in Terminal. Version example 1 conda 4 .2.9 If there\u2019s an error message, check to see if you\u2019re logged into the same account that you used to install Anaconda or Miniconda. You can update conda to the current version by typing in to your command line the following in line 1: Updating conda 1 2 conda update conda Proceed ([ y ] /n ) ? Line 2 asks you if you wish to proceed. Type \u2018y\u2019 and hit Enter to update. Managing Environments You will use a Terminal window for the following steps. To create an environment, use the following command: Creating an environment 1 conda create --name testing biopython Two things to note from this: you can replace \u2018testing\u2019 with whatever name you want, and biopython is just the program that was chosen for this. If your program is not yet installed, you will be prompted to install a new set of packages with a similar message to how you are prompted if you want to update conda. Type \u2018y\u2019 and hit \"Enter\" to confirm that you want to install the package. To activate this new environment, choose the appropriate command (keep in mind that our environment name is \u2018testing\u2019): Activate an environment 1 source activate testing To deactivate an environment and return to the root, use the following command: Deactivating an environment 1 source deactivate testing Now let\u2019s say you want to create a new environment and install a different version of Python along with a couple of packages. We will use the Astroid and Babel packages. Run this command: Environment with multiple packages 1 conda create --name test2 python = 3 .5 astroid babel Your new environment is named \u2018test2\u2019 (which can be interchanged for whatever name you like when you\u2019re running an environment) and it has Python 3, Astroid, and Babel installed. If you wish to find out more about the create command, run: create help 1 conda create --help To display your environments, use the command: Displaying environments 1 conda info --envs Your environments should be outputted, and there will be an asterisk next to the active environment. Your current environment should be in (parentheses) or [brackets] in front of your prompt. To switch to another environment, just use the activate command with the environment name. If you wish to make a copy of an environment, use the following command: Copying an environment 1 conda create --name hello --clone testing You can verify that the clone was made by displaying your environments. You can delete an environment by using: Deleting an environment 1 conda remove --name hello --all Since we don\u2019t want a clone of testing, we decided to delete it. You can check that it is deleted by displaying your environments. Managing Python Use a Terminal window again for these steps. You can search for which versions of Python are available to be installed by using the command: Searching for available Python versions 1 conda search --full-name python \u201c--full-name\u201d lists only the packages whose full name is exactly \u2018python\u2019. If you want to search packages which contain the text \u2018python\u2019, but that is not its full name, use the command: Versions with the word \"python\" 1 conda search python We will go over how to install a new verison of Python without overwriting our original Python version. Let\u2019s start off by creating a new environment called py3. Use the create command: Environment for installing a new version of Python 1 conda create --name py3 python = 3 If you get prompted to install a new package, type \u2018y\u2019 and hit \"Enter\". Display your list of environments with Displaying new environment 1 conda info --envs to check if py3 was successfully created, and activate py3 by using: Activating py3 1 source activate py3 Now check the version of Python being used with: Checking py3's Python version 1 python --version Now we can switch to one of our previous environments that we first made when installing conda. Let\u2019s switch back to testing. Using the command: Switching to an environment with a different Python version 1 source activate testing Now you check which version we are using with the same used earlier. This should show the same version that was used when you first installed conda. Now let\u2019s deactivate testing with the command: Deactivating testing 1 source deactivate testing Managing Packages Use the Terminal window for this section. The following command will allow you to see the packages installed in your environment: Listing packages command 1 conda list This website provides a list of packages available to install: http://docs.continuum.io/anaconda/pkg-docs.html The package that we will install to practice is called \u201cbeautifulsoup4\u201d \u2013 this is a package which provides a Python library designed for screen-scraping/MIT. To search if this package is available to be installed, try to use the following command: Searching for a package 1 conda search beautifulsoup4 If it displays the package, then it is available to install. Use the following command to install a package: Install a Package 1 conda install --name test2 beautifulsoup4 You will be prompted if you want to proceed. Type \u2018y\u2019 and hit \"Enter\" Now activate the environment that this package was installed in, in this case test2: Activating environment with the new package 1 source activate test2 Now let\u2019s check if the package is installed by using: Listing the packages 1 conda list It should be listed if it is installed. Installing Packages Use the Terminal window and an internet browser for this section. Not all packages can be installed with conda install, so we can pull packages from anaconda.org . The following website has other packages that we can use and install: http://anaconda.org . For the example, we will install a package called \u2018bottleneck\u2019. Go to this page and use the search bar in the upper left corner to search for \u2018bottleneck\u2019. Choose the most frequently downloaded version of bottleneck. The format of the text should be something similar to \u2018conda-forge/bottleneck 1.2.1 \u2019. This is split into two places that you can click on. The first is \u2018conda-forge\u2019, which is the owner of the package. We can ignore this one and click on \u2018bottleneck 1.2.1 \u2019. At the middle to bottom portion of the page, there should be a command listed to install this package. In this case, the command is: 1 conda install -c conda-forge bottleneck Activate your test2 environment and enter this command to install bottleneck. Installing a package from Anaconda.org 1 2 source activate test2 conda install -c conda-forge bottleneck You will be prompted if you want to proceed, again type \u2018y\u2019 and hit \"Enter\". We can now check if the new package is installed. 1 conda list If it is installed, it show in the Terminal window after using the previous command. Installing a Package with pip Pip is used to install packages that are not available from conda or anaconda.org . It is only used as a package manager and does not manage environments. If you don\u2019t have the environment \u2018test2\u2019 (or whatever environment you want to install a package in) active, activate it now. We will install a program named \u2018see\u2019 into this environment. The command for this is: Installing with pip 1 pip install see Now use the listing packages command to check if \u2018see\u2019 is installed. Note: You can replace \u2018see\u2019 with whatever pip package that you are trying to install. This is just an example. Installing Commercial Packages If you wish to install a commercial package, follow the format for installing packages from conda, which was described earlier in this document in the \"Managing Packages\" section. Removing Packages, Environments, or conda To remove a package from an environment, use the following command: Removing a package from an environment 1 conda remove --name ENVIRONMENT_NAME PACKAGE_NAME You would replace ENIVRONMENT_NAME with the name of your environment, and also replace PACKAGE_NAME with the name of the package. For an example, we will remove the \u2018astroid\u2019 package from test2: Ex. Removing a package 1 conda remove --name test2 astroid You will be prompted if you want to proceed, type \u2018y\u2019 and hit \"Enter\". Now use the list command to check if \u2018astroid\u2019 has been removed. To remove an environment, use the command: Removing an environment 1 conda remove --name ENVIRONMENT_NAME --all Replace ENVIRONMENT_NAME with the environment you want to remove. For an example, we will create a new environment called \u2018trash\u2019. Remembering how to create an environment from before, use the command: Ex. Removing an Environment 1 conda create --name trash python = 3 .5 Check that this environment exists by using: Ex. Removing an Environment 1 conda info --envs Now we run: Ex. Removing an Environment 1 conda remove --name trash --all You will be prompted to proceed, type \u2018y\u2019 and hit \"Enter\". Now check if the environment exists by looking at all of your environments again with the same command used a moment ago. The environment \u2018trash\u2019 should no longer be there. If you want to remove anaconda or miniconda, you can use the following commands: Removing anaconda or miniconda 1 2 rm -rf ~/miniconda rm -rf ~/anaconda","title":"Using Conda"},{"location":"Using_conda/#using-conda","text":"This wiki is based on conda version prior to 4.6. It may not apply to 4.6 and later versions (where, for example, conda environment is activated by conda activate ). Check the manual before you start using conda.","title":"Using conda"},{"location":"Using_conda/#introduction","text":"Anaconda is a distribution of python that contains a free, easy-to-install package manager, environment manager and Python distribution with a collection of 1,000+ open source packages with free community support. Conda is the package and environment manager that installs and updates packages and their dependencies, included in all versions of Anaconda. Anaconda needs to be installed in your home or project directory and so does any software which will be installed using it. This allows users to have full control of their programs. This wiki serves only as a mini-version of conda tutorial. You are highly recommended to read the User Guide. conda to install and run a program called \"macs2\". First we go to Anaconda Cloud to search for it. Once it's found, we pick a source and click on the link to get the installation command (i.e., conda install -c bioconda macs2 below) Installation of macs2 1 2 3 4 5 export PATH=$PATH:$HOME/anaconda3/bin conda create --name macs2 source activate macs2 conda install -c bioconda macs2 source deactivate To launch it later on (for example, in your SLURM job submission script): Using it 1 2 3 4 export PATH=$PATH:$HOME/anaconda3/bin source activate macs2 #[your commands] source deactivate","title":"Introduction"},{"location":"Using_conda/#installation-of-anaconda","text":"Visit https://docs.anaconda.com/anaconda/install/linux for installation instruction. And Linux installers are available from https://www.anaconda.com/download/#linux Note: 1) Choose 64-Bit (x86) Installer. 2) Version 2 or 3? You should choose python2 or python3 depending on what your software requirements are. Some libraries/packages only work on Python 2 so you should check with the python package you ultimately want to use. If you don't know, select the latest version 3. 3) Anaconda now partners with Microsoft and asks if you want to install \"VSCode\" THIS WILL NOT WORK. The installation requires administrative privileges that you don't have. You can download the install file directly into your home directory by first getting the URL of the download (right-click on the download link and select \"copy link address\" or \"copy link location\"). Then open a terminal to log onto a dev-node, and use this command: curl -O <download link> An example link can be: https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh Once the file Anaconda3-2022.05-Linux-x86_64.sh is downloaded, you can run the command: bash Anaconda3-2022.05-Linux-x86_64.sh During Installation the installer will ask you Accept licence agreement : you must type yes to agree. We can not install for the who system, you must install yourself and agree to this license Where to install : you can install in any location in your home directory, the default is Anaconda2 or Anaconda3. Note you can have multiple installations of Anaconda Initialize Anaconda (run conda init?) : please answer \"no\". A more careful setup is suggested. Please follow the instruction in the next section after this installation. Install VS Code? This is a new bundle that comes with Anaconda. This won't work and you must answer \"no\" Please remember where Anaconda is installed in the step 2 above. The installation path will be used for module setup in the next section. In case your installed Anaconda is auto-activated on startup, please run the command: conda config --set auto_activate_base false after close and re-open your current shell. Please read the next section for the setup and the start-up of your Anaconda.","title":"Installation of Anaconda"},{"location":"Using_conda/#module-setup-for-anaconda","text":"In order to have no conflict between user-installed Anaconda and system-installed Python versions, a setup in the $HOME/.bashrc file for module system is necessary. The .bashrc file under your home directory can be used to do environment setting for every time you log in to a HPCC node. You can modify the file by an editor program (such as nano, vim) for Anaconda setup: (1) Please check if any conda initialization or setup script is in the ~/.bashrc file. The script starts with # >>> conda init >>> and ends with # <<< conda init <<< : 1 2 3 4 5 6 7 # >>> conda init >>> # !! Contents within this block are managed by 'conda init' !! __conda_setup= ... ... ... ... ... ... unset __conda_setup # <<< conda init <<< If there is, please remove or comment all of the command lines out (by adding the sign \" # \" in its front). If there is any export command line to add the installed Anaconda bin path to PATH environment variable, please also comment it out. (2) If your installed Anaconda path is ~/anaconda3 or ~/anaconda2 , you can skip this step and go to the next one. If not, please set the variable CONDA3PATH to be the place of installed Anaconda3 by adding the command line: 1 export CONDA3PATH=<your installed Anaconda3 path> in the ~/.bashrc file. If you have Anaconda2 installed, you may set the variable CONDA2PATH to be the installed path of Anaconda2. After the environment setup, please save the ~/.bashrc file. Log out and log into a dev node. (3) Whenever you want to use Anaconda3 commands, just run: 1 module load Conda/3 to initiate conda and CONDA3PATH is in your PATH variable. Loading the module will also replace any loaded Python module so there is no conflict. If CONDA2PATH variable is set up, please load Conda/2 module for using Anaconda2. (4) If you would like to have your installed Anaconda3 to be the default Python program when your session starts up, you can add the command: 1 module load Conda/3 2> /dev/null to the ~/.bashrc file after export CONDA3PATH=... command line. Using Conda module can also avoid a conflict with Web-based Remote Desktop gateway. If anaconda2 is installed, you can use Conda/2 instead of Conda/3 module.","title":"Module Setup for Anaconda"},{"location":"Using_conda/#managing-conda","text":"To manage conda, you should use a Terminal window to check the following. You can begin by checking if conda is installed by using the command: Checking the version 1 conda --version If it is installed, the number of the version you have installed should show up after the previous command in Terminal. Version example 1 conda 4 .2.9 If there\u2019s an error message, check to see if you\u2019re logged into the same account that you used to install Anaconda or Miniconda. You can update conda to the current version by typing in to your command line the following in line 1: Updating conda 1 2 conda update conda Proceed ([ y ] /n ) ? Line 2 asks you if you wish to proceed. Type \u2018y\u2019 and hit Enter to update.","title":"Managing conda"},{"location":"Using_conda/#managing-environments","text":"You will use a Terminal window for the following steps. To create an environment, use the following command: Creating an environment 1 conda create --name testing biopython Two things to note from this: you can replace \u2018testing\u2019 with whatever name you want, and biopython is just the program that was chosen for this. If your program is not yet installed, you will be prompted to install a new set of packages with a similar message to how you are prompted if you want to update conda. Type \u2018y\u2019 and hit \"Enter\" to confirm that you want to install the package. To activate this new environment, choose the appropriate command (keep in mind that our environment name is \u2018testing\u2019): Activate an environment 1 source activate testing To deactivate an environment and return to the root, use the following command: Deactivating an environment 1 source deactivate testing Now let\u2019s say you want to create a new environment and install a different version of Python along with a couple of packages. We will use the Astroid and Babel packages. Run this command: Environment with multiple packages 1 conda create --name test2 python = 3 .5 astroid babel Your new environment is named \u2018test2\u2019 (which can be interchanged for whatever name you like when you\u2019re running an environment) and it has Python 3, Astroid, and Babel installed. If you wish to find out more about the create command, run: create help 1 conda create --help To display your environments, use the command: Displaying environments 1 conda info --envs Your environments should be outputted, and there will be an asterisk next to the active environment. Your current environment should be in (parentheses) or [brackets] in front of your prompt. To switch to another environment, just use the activate command with the environment name. If you wish to make a copy of an environment, use the following command: Copying an environment 1 conda create --name hello --clone testing You can verify that the clone was made by displaying your environments. You can delete an environment by using: Deleting an environment 1 conda remove --name hello --all Since we don\u2019t want a clone of testing, we decided to delete it. You can check that it is deleted by displaying your environments.","title":"Managing Environments"},{"location":"Using_conda/#managing-python","text":"Use a Terminal window again for these steps. You can search for which versions of Python are available to be installed by using the command: Searching for available Python versions 1 conda search --full-name python \u201c--full-name\u201d lists only the packages whose full name is exactly \u2018python\u2019. If you want to search packages which contain the text \u2018python\u2019, but that is not its full name, use the command: Versions with the word \"python\" 1 conda search python We will go over how to install a new verison of Python without overwriting our original Python version. Let\u2019s start off by creating a new environment called py3. Use the create command: Environment for installing a new version of Python 1 conda create --name py3 python = 3 If you get prompted to install a new package, type \u2018y\u2019 and hit \"Enter\". Display your list of environments with Displaying new environment 1 conda info --envs to check if py3 was successfully created, and activate py3 by using: Activating py3 1 source activate py3 Now check the version of Python being used with: Checking py3's Python version 1 python --version Now we can switch to one of our previous environments that we first made when installing conda. Let\u2019s switch back to testing. Using the command: Switching to an environment with a different Python version 1 source activate testing Now you check which version we are using with the same used earlier. This should show the same version that was used when you first installed conda. Now let\u2019s deactivate testing with the command: Deactivating testing 1 source deactivate testing","title":"Managing Python"},{"location":"Using_conda/#managing-packages","text":"Use the Terminal window for this section. The following command will allow you to see the packages installed in your environment: Listing packages command 1 conda list This website provides a list of packages available to install: http://docs.continuum.io/anaconda/pkg-docs.html The package that we will install to practice is called \u201cbeautifulsoup4\u201d \u2013 this is a package which provides a Python library designed for screen-scraping/MIT. To search if this package is available to be installed, try to use the following command: Searching for a package 1 conda search beautifulsoup4 If it displays the package, then it is available to install. Use the following command to install a package: Install a Package 1 conda install --name test2 beautifulsoup4 You will be prompted if you want to proceed. Type \u2018y\u2019 and hit \"Enter\" Now activate the environment that this package was installed in, in this case test2: Activating environment with the new package 1 source activate test2 Now let\u2019s check if the package is installed by using: Listing the packages 1 conda list It should be listed if it is installed.","title":"Managing Packages"},{"location":"Using_conda/#installing-packages","text":"Use the Terminal window and an internet browser for this section. Not all packages can be installed with conda install, so we can pull packages from anaconda.org . The following website has other packages that we can use and install: http://anaconda.org . For the example, we will install a package called \u2018bottleneck\u2019. Go to this page and use the search bar in the upper left corner to search for \u2018bottleneck\u2019. Choose the most frequently downloaded version of bottleneck. The format of the text should be something similar to \u2018conda-forge/bottleneck 1.2.1 \u2019. This is split into two places that you can click on. The first is \u2018conda-forge\u2019, which is the owner of the package. We can ignore this one and click on \u2018bottleneck 1.2.1 \u2019. At the middle to bottom portion of the page, there should be a command listed to install this package. In this case, the command is: 1 conda install -c conda-forge bottleneck Activate your test2 environment and enter this command to install bottleneck. Installing a package from Anaconda.org 1 2 source activate test2 conda install -c conda-forge bottleneck You will be prompted if you want to proceed, again type \u2018y\u2019 and hit \"Enter\". We can now check if the new package is installed. 1 conda list If it is installed, it show in the Terminal window after using the previous command.","title":"Installing Packages"},{"location":"Using_conda/#installing-a-package-with-pip","text":"Pip is used to install packages that are not available from conda or anaconda.org . It is only used as a package manager and does not manage environments. If you don\u2019t have the environment \u2018test2\u2019 (or whatever environment you want to install a package in) active, activate it now. We will install a program named \u2018see\u2019 into this environment. The command for this is: Installing with pip 1 pip install see Now use the listing packages command to check if \u2018see\u2019 is installed. Note: You can replace \u2018see\u2019 with whatever pip package that you are trying to install. This is just an example.","title":"Installing a Package with pip"},{"location":"Using_conda/#installing-commercial-packages","text":"If you wish to install a commercial package, follow the format for installing packages from conda, which was described earlier in this document in the \"Managing Packages\" section.","title":"Installing Commercial Packages"},{"location":"Using_conda/#removing-packages-environments-or-conda","text":"To remove a package from an environment, use the following command: Removing a package from an environment 1 conda remove --name ENVIRONMENT_NAME PACKAGE_NAME You would replace ENIVRONMENT_NAME with the name of your environment, and also replace PACKAGE_NAME with the name of the package. For an example, we will remove the \u2018astroid\u2019 package from test2: Ex. Removing a package 1 conda remove --name test2 astroid You will be prompted if you want to proceed, type \u2018y\u2019 and hit \"Enter\". Now use the list command to check if \u2018astroid\u2019 has been removed. To remove an environment, use the command: Removing an environment 1 conda remove --name ENVIRONMENT_NAME --all Replace ENVIRONMENT_NAME with the environment you want to remove. For an example, we will create a new environment called \u2018trash\u2019. Remembering how to create an environment from before, use the command: Ex. Removing an Environment 1 conda create --name trash python = 3 .5 Check that this environment exists by using: Ex. Removing an Environment 1 conda info --envs Now we run: Ex. Removing an Environment 1 conda remove --name trash --all You will be prompted to proceed, type \u2018y\u2019 and hit \"Enter\". Now check if the environment exists by looking at all of your environments again with the same command used a moment ago. The environment \u2018trash\u2019 should no longer be there. If you want to remove anaconda or miniconda, you can use the following commands: Removing anaconda or miniconda 1 2 rm -rf ~/miniconda rm -rf ~/anaconda","title":"Removing Packages, Environments, or conda"},{"location":"Virtual_Terminals/","text":"Virtual Terminals GNU Screen GNU Screen is a program that allows you to create a virtual terminal session inside a single terminal window. It is useful for dealing with multiple programs from a command line interface and for separating programs from the Unix shell that started the program. To start screen, simply type 1 screen at the command prompt. Even if it looks like nothing has happened, you are now in a new window within screen. Describing the details of how screen works is beyond the scope of this entry, but it allows you to leave a session running even after you've logged out (or disconnected because of network issues). One challenge with the 'screen' command is that by default you can't load any modules or other system commands. That's because 'screen' does not run the shell profile commands (in /etc/profile and /etc/profile.d). HPCC configures the modules system and several other system variables/settings in these profile scripts. Without running them, the module system won't work. However, you can ask screen to run this profile by including the line shell=-$SHELL in the screen config file \".screenrc\" in your home directory. To make a screen config file, try this (provided you don't already have a .screenrc file) create default .screenrc 1 echo 'shell -$SHELL' >> ~/.screenrc You'll have to exit and restart screen to see the changes. Screen Commands To send commands to screen (instead of the window you're working in), you preface them with Ctrl-a, i.e., you type the Ctrl key and \"a\" together, release both keys, then type the next letter to invoke the command. Ctrl-A then: ? Display available screen commands \" List running screen windows N Display number of current window c Open a new window [number] Switch to window [number] [ Copy (so you can paste to another window) ] Paste k Kill the current window Ctrl-\\ Quit and close all screen windows A Label the screen window M Monitor for activity d Detach the current screen session Startup commands: screen -ls List detached screen sessions on the server screen -S (name for new screen) Create a screen with specified name screen --r [id] Reattach the specified screen Detaching/Reattaching Screen Sessions To detach a running screen session, type Ctrl-A d This will detach the screen session with all of its windows but leave all of the related processes running. You can start and detach as many screen sessions as you wish, each with its own windows and processes. You can even logout of the server, and your screen sessions will continue running while you're gone. To reattach a screen session, find the id of the one you want with screen -ls then reattach it with screen -r id If you don't remember which screen sessions you have opened on our cluster, you can type: 1 2 module load powertools userinfo <uid> If you are running a licensed software (such as MATLAB) within a screen session, please remember to kill it when you're done developing for the day. This will help us better manage licenses for the MSU research community. Screen customizations The attached file (click here) includes customizations to screen that places an information bar at the bottom of the terminal. Many people find it very usefu. View the file and copy selected or all lines to your \".screenrc\" file. See also http://tmux.sourceforge.net/ . Tmux tmux is intended to be the modern, BSD-licensed alternative to screen. Both programs are available on HPCC. It allows splitting a window horizontally and vertically, and copying and pasting between multiple buffers. More information is available at http://tmux.sourceforge.net .","title":"Virtual Terminals"},{"location":"Virtual_Terminals/#virtual-terminals","text":"","title":"Virtual Terminals"},{"location":"Virtual_Terminals/#gnu-screen","text":"GNU Screen is a program that allows you to create a virtual terminal session inside a single terminal window. It is useful for dealing with multiple programs from a command line interface and for separating programs from the Unix shell that started the program. To start screen, simply type 1 screen at the command prompt. Even if it looks like nothing has happened, you are now in a new window within screen. Describing the details of how screen works is beyond the scope of this entry, but it allows you to leave a session running even after you've logged out (or disconnected because of network issues). One challenge with the 'screen' command is that by default you can't load any modules or other system commands. That's because 'screen' does not run the shell profile commands (in /etc/profile and /etc/profile.d). HPCC configures the modules system and several other system variables/settings in these profile scripts. Without running them, the module system won't work. However, you can ask screen to run this profile by including the line shell=-$SHELL in the screen config file \".screenrc\" in your home directory. To make a screen config file, try this (provided you don't already have a .screenrc file) create default .screenrc 1 echo 'shell -$SHELL' >> ~/.screenrc You'll have to exit and restart screen to see the changes.","title":"GNU Screen"},{"location":"Virtual_Terminals/#screen-commands","text":"To send commands to screen (instead of the window you're working in), you preface them with Ctrl-a, i.e., you type the Ctrl key and \"a\" together, release both keys, then type the next letter to invoke the command. Ctrl-A then: ? Display available screen commands \" List running screen windows N Display number of current window c Open a new window [number] Switch to window [number] [ Copy (so you can paste to another window) ] Paste k Kill the current window Ctrl-\\ Quit and close all screen windows A Label the screen window M Monitor for activity d Detach the current screen session Startup commands: screen -ls List detached screen sessions on the server screen -S (name for new screen) Create a screen with specified name screen --r [id] Reattach the specified screen","title":"Screen Commands"},{"location":"Virtual_Terminals/#detachingreattaching-screen-sessions","text":"To detach a running screen session, type Ctrl-A d This will detach the screen session with all of its windows but leave all of the related processes running. You can start and detach as many screen sessions as you wish, each with its own windows and processes. You can even logout of the server, and your screen sessions will continue running while you're gone. To reattach a screen session, find the id of the one you want with screen -ls then reattach it with screen -r id If you don't remember which screen sessions you have opened on our cluster, you can type: 1 2 module load powertools userinfo <uid> If you are running a licensed software (such as MATLAB) within a screen session, please remember to kill it when you're done developing for the day. This will help us better manage licenses for the MSU research community.","title":"Detaching/Reattaching Screen Sessions"},{"location":"Virtual_Terminals/#screen-customizations","text":"The attached file (click here) includes customizations to screen that places an information bar at the bottom of the terminal. Many people find it very usefu. View the file and copy selected or all lines to your \".screenrc\" file. See also http://tmux.sourceforge.net/ .","title":"Screen customizations"},{"location":"Virtual_Terminals/#tmux","text":"tmux is intended to be the modern, BSD-licensed alternative to screen. Both programs are available on HPCC. It allows splitting a window horizontally and vertically, and copying and pasting between multiple buffers. More information is available at http://tmux.sourceforge.net .","title":"Tmux"},{"location":"Web-based_Remote_Desktop_Protocol/","text":"Web-based Remote Desktop Protocol HPCC offers a way for users to connect to the main systems using the Web-based Remote Desktop Protocol for users who are more comfortable using a desktop environment, or are otherwise having issues connecting via SSH. Users only need to have a browser install on their machines in order for this feature to function. Connecting to HPCC via web browser First, users need to open a web browser on their local machine, and connecting to the web site https://webrdp.hpcc.msu.edu. It will ask for user name and password. Input your MSU NetID and password and then click on \"Log In\" button to get connection. After you are connected, the page showing the available session types look like the following at the first time. Click on \"Launch Session\" button, a pop-up window show the several choices of sessions: Mate Desktop Xterm terminal (Termial only) Firefox (Browser only) You can select your choice by click on the icon, then click the button \"Launch\" to launch the session. Warning Opening both Mate desktop and XFCE desktop at the same time will cause the sessions to crash as well as possibly causing your web browser to crash. Desktops Available Mate Desktop: Note Mate desktop is intended to be the default desktop environment. The terminal quick launch icon is configured to start Xterm by default, so that the users complete profile will load. If you would like to use one of the other available terminals you will need to enable the login shell option. Please see below for instructions. Other Bookmarks: Xterm (Terminal Only): This option will open a single terminal only. You may then launch any futher applications you may need from the terminal. Firefox (Web Browser only): This optoin will open a web browser only. Session Actions Detatching from a session The web based remote desktop will allow for detatching from a session, while still leaving the session open. This allows for you to resume your work, or continue your work if you are disconnected. To detatch from a session simply close your web browser window containing that session Note Sessions will automatically close after 12 hours, if a user is not attached to the session Reattaching to a session To reattach to a session, log into the web remote desktop if needed, in the sessions browser tab click the session you would like to access. Ending a session To end a desktop session simply logout of the desktop environment To end a terminal session type \"exit\" or \"logoff\" To end a web browser only session simply close the web browser Terminating a session If any session will not close or you no longer need a session it can be terminated from the sessions web broser tab Select the checkbox next to the session or sessions you would like to terminate Click the Actions button Select terminate Note Please note that it may take thirty seconds to a minute for some sessions to terminate Enabling a login shell on a terminal program: Mate Terminal: Open MATE Terminal from the applications menu. Select Edit from the terminals menu bar Select Profile Preferences from the drop down menu Select the Title and Command tab Make sure the checkbox next to \"Run command as a login shell\" is checked. Close terminal and restart Features of the Web-based Remote Desktop The Web-based Remote Desktop functions as the same as the Remote Desktop Client: You can run Graphical-based programs without needing to setup a X11 instance on your local machine. You can submit jobs from the RDP Gateway just like our regular Gateway. If your connection is interrupted, your session will remain active in the background. When you reconnect, you will be placed right back into your remote session. Conflict issue with Conda If you have Anaconda (or Miniconda) installed in your home space, the dbus commands in Anaconda (or Miniconda) bin path will conflicts with those in /usr/bin . If your Anaconda is set auto-activated, launch to Remote Desktop will fail with error messages. In order to avoid this issue, please run the command: 1 conda config --set auto_activate_base false on a dev node to stop activating Anaconda automatically. Please also refer to Module Setup for Anaconda for how to set up and start your installed Anaconda. Troubleshooting: If you are getting errors when starting a session please open a ticket and provide the errors you received.","title":"Web-based Remote Desktop Protocol"},{"location":"Web-based_Remote_Desktop_Protocol/#web-based-remote-desktop-protocol","text":"HPCC offers a way for users to connect to the main systems using the Web-based Remote Desktop Protocol for users who are more comfortable using a desktop environment, or are otherwise having issues connecting via SSH. Users only need to have a browser install on their machines in order for this feature to function.","title":"Web-based Remote Desktop Protocol"},{"location":"Web-based_Remote_Desktop_Protocol/#connecting-to-hpcc-via-web-browser","text":"First, users need to open a web browser on their local machine, and connecting to the web site https://webrdp.hpcc.msu.edu. It will ask for user name and password. Input your MSU NetID and password and then click on \"Log In\" button to get connection. After you are connected, the page showing the available session types look like the following at the first time. Click on \"Launch Session\" button, a pop-up window show the several choices of sessions: Mate Desktop Xterm terminal (Termial only) Firefox (Browser only) You can select your choice by click on the icon, then click the button \"Launch\" to launch the session. Warning Opening both Mate desktop and XFCE desktop at the same time will cause the sessions to crash as well as possibly causing your web browser to crash.","title":"Connecting to HPCC via web browser"},{"location":"Web-based_Remote_Desktop_Protocol/#desktops-available","text":"","title":"Desktops Available"},{"location":"Web-based_Remote_Desktop_Protocol/#mate-desktop","text":"Note Mate desktop is intended to be the default desktop environment. The terminal quick launch icon is configured to start Xterm by default, so that the users complete profile will load. If you would like to use one of the other available terminals you will need to enable the login shell option. Please see below for instructions.","title":"Mate Desktop:"},{"location":"Web-based_Remote_Desktop_Protocol/#other-bookmarks","text":"","title":"Other Bookmarks:"},{"location":"Web-based_Remote_Desktop_Protocol/#xterm-terminal-only","text":"This option will open a single terminal only. You may then launch any futher applications you may need from the terminal.","title":"Xterm (Terminal Only):"},{"location":"Web-based_Remote_Desktop_Protocol/#firefox-web-browser-only","text":"This optoin will open a web browser only.","title":"Firefox (Web Browser only):"},{"location":"Web-based_Remote_Desktop_Protocol/#session-actions","text":"","title":"Session Actions"},{"location":"Web-based_Remote_Desktop_Protocol/#detatching-from-a-session","text":"The web based remote desktop will allow for detatching from a session, while still leaving the session open. This allows for you to resume your work, or continue your work if you are disconnected. To detatch from a session simply close your web browser window containing that session Note Sessions will automatically close after 12 hours, if a user is not attached to the session","title":"Detatching from a session"},{"location":"Web-based_Remote_Desktop_Protocol/#reattaching-to-a-session","text":"To reattach to a session, log into the web remote desktop if needed, in the sessions browser tab click the session you would like to access.","title":"Reattaching to a session"},{"location":"Web-based_Remote_Desktop_Protocol/#ending-a-session","text":"To end a desktop session simply logout of the desktop environment To end a terminal session type \"exit\" or \"logoff\" To end a web browser only session simply close the web browser","title":"Ending a session"},{"location":"Web-based_Remote_Desktop_Protocol/#terminating-a-session","text":"If any session will not close or you no longer need a session it can be terminated from the sessions web broser tab Select the checkbox next to the session or sessions you would like to terminate Click the Actions button Select terminate Note Please note that it may take thirty seconds to a minute for some sessions to terminate","title":"Terminating a session"},{"location":"Web-based_Remote_Desktop_Protocol/#enabling-a-login-shell-on-a-terminal-program","text":"","title":"Enabling a login shell on a terminal program:"},{"location":"Web-based_Remote_Desktop_Protocol/#mate-terminal","text":"Open MATE Terminal from the applications menu. Select Edit from the terminals menu bar Select Profile Preferences from the drop down menu Select the Title and Command tab Make sure the checkbox next to \"Run command as a login shell\" is checked. Close terminal and restart","title":"Mate Terminal:"},{"location":"Web-based_Remote_Desktop_Protocol/#features-of-the-web-based-remote-desktop","text":"The Web-based Remote Desktop functions as the same as the Remote Desktop Client: You can run Graphical-based programs without needing to setup a X11 instance on your local machine. You can submit jobs from the RDP Gateway just like our regular Gateway. If your connection is interrupted, your session will remain active in the background. When you reconnect, you will be placed right back into your remote session.","title":"Features of the Web-based Remote Desktop"},{"location":"Web-based_Remote_Desktop_Protocol/#conflict-issue-with-conda","text":"If you have Anaconda (or Miniconda) installed in your home space, the dbus commands in Anaconda (or Miniconda) bin path will conflicts with those in /usr/bin . If your Anaconda is set auto-activated, launch to Remote Desktop will fail with error messages. In order to avoid this issue, please run the command: 1 conda config --set auto_activate_base false on a dev node to stop activating Anaconda automatically. Please also refer to Module Setup for Anaconda for how to set up and start your installed Anaconda.","title":"Conflict issue with Conda"},{"location":"Web-based_Remote_Desktop_Protocol/#troubleshooting","text":"If you are getting errors when starting a session please open a ticket and provide the errors you received.","title":"Troubleshooting:"},{"location":"Web_Site_Access_to_HPCC/","tags":["reference"],"text":"Web browser access to the HPCC In addition to connecting by ssh , HPCC users may also access the system via a web browser. You need a valid MSU NetID. We offer two ways for web access: Open OnDemand To connect, log into https://ondemand.hpcc.msu.edu/. Web-based Remote Desktop Protocol (webrdp) To connect, log into https://webrdp.hpcc.msu.edu/. Note that webrdp is planned to retire by the end of 2022 and tech support is limited. We highly recommend choosing OnDemand for your web access to the HPCC.","title":"Web access"},{"location":"Web_Site_Access_to_HPCC/#web-browser-access-to-the-hpcc","text":"In addition to connecting by ssh , HPCC users may also access the system via a web browser. You need a valid MSU NetID. We offer two ways for web access: Open OnDemand To connect, log into https://ondemand.hpcc.msu.edu/. Web-based Remote Desktop Protocol (webrdp) To connect, log into https://webrdp.hpcc.msu.edu/. Note that webrdp is planned to retire by the end of 2022 and tech support is limited. We highly recommend choosing OnDemand for your web access to the HPCC.","title":"Web browser access to the HPCC"},{"location":"Why_Use_HPCC/","text":"Why Use HPCC Let us make a comparison between your pc and our HPCC: Hardwares Laptop/Desktop HPCC Clusters Number of Nodes 1 979 Sockets per node 1 2, 8 Cores per node 4,8,or 16 20,28,40,128 or 144 Cores total 4,8,or 16 50,084 Core Speed 2.7 - 3.5 ghz 2.5-3 ghz RAM memory 8, 16 or 32 GB 64, 128, 92, 500 GB or 6TB File Storage 250, 500 GB or 1TB 1TB(Home), 50TB(Scratch) Connection to other computers Campus ethernet1 Gbit/sec \"Infiniband\" 100 Gbit/sec Users 1 ~1,500 Schedule On Demand 24/7 via queue Software You can use more than 500 titles and 3,000 different versions of software installed in HPCC: Compilers \u2014 GNU, intel, PGI, CUDA, ... Parallel \u2014 OpenMPI, IMPI, MVAPICH2, ... Bioinformatics \u2014 BLAST, Trinity, Mothur, Samtools, Trimmomatic, ... Libraries \u2014 MKL, OpenBLAS, LAPACK, FFTW, ... Commerical \u2014 MATLAB, ANSYS, COMSOL, STATA, GAUSSIAN, ... for FREE !!!","title":"Why Use HPCC"},{"location":"Why_Use_HPCC/#why-use-hpcc","text":"Let us make a comparison between your pc and our HPCC:","title":"Why Use HPCC"},{"location":"Why_Use_HPCC/#hardwares","text":"Laptop/Desktop HPCC Clusters Number of Nodes 1 979 Sockets per node 1 2, 8 Cores per node 4,8,or 16 20,28,40,128 or 144 Cores total 4,8,or 16 50,084 Core Speed 2.7 - 3.5 ghz 2.5-3 ghz RAM memory 8, 16 or 32 GB 64, 128, 92, 500 GB or 6TB File Storage 250, 500 GB or 1TB 1TB(Home), 50TB(Scratch) Connection to other computers Campus ethernet1 Gbit/sec \"Infiniband\" 100 Gbit/sec Users 1 ~1,500 Schedule On Demand 24/7 via queue","title":"Hardwares"},{"location":"Why_Use_HPCC/#software","text":"You can use more than 500 titles and 3,000 different versions of software installed in HPCC: Compilers \u2014 GNU, intel, PGI, CUDA, ... Parallel \u2014 OpenMPI, IMPI, MVAPICH2, ... Bioinformatics \u2014 BLAST, Trinity, Mothur, Samtools, Trimmomatic, ... Libraries \u2014 MKL, OpenBLAS, LAPACK, FFTW, ... Commerical \u2014 MATLAB, ANSYS, COMSOL, STATA, GAUSSIAN, ... for FREE !!!","title":"Software"},{"location":"bi/","tags":["reference"],"text":"bi Information of Buy-In Account 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ bi -h Usage: bi bi -h bi [ -a <account> | -u <user> ] bi -d [ -u <user> ] bi -l [ -u <user> ] -h | --help Display this help message -a | --account <account> Display users and nodes of a buyin account -u | --user <user> Display buyin accounts for this user -d | --default Display only the user's default buyin account -l | --list List jobs and usage status of buyin nodes","title":"SLURM - buyin information"},{"location":"bi/#bi","text":"Information of Buy-In Account 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ bi -h Usage: bi bi -h bi [ -a <account> | -u <user> ] bi -d [ -u <user> ] bi -l [ -u <user> ] -h | --help Display this help message -a | --account <account> Display users and nodes of a buyin account -u | --user <user> Display buyin accounts for this user -d | --default Display only the user's default buyin account -l | --list List jobs and usage status of buyin nodes","title":"bi"},{"location":"development_nodes/","text":"Development nodes HPCC has several development nodes that are available for users to compile their code, and do short (CPU time less than 2 hours) runs to estimate run-time and memory usage. These development nodes run the latest operating system and have similar configuration and environment setup to the compute nodes of the same clusters. Please use these development nodes to compile your program and test the work flow of your job script. For running long-time or large-resource computations, please submit jobs to use compute nodes. To access a certain one, for example, dev-intel14, please log into a gateway node and run: ssh dev-intel14 Node Hostname Cores Memory Notes dev-intel14 20 256GB Large memory intel14 node dev-intel14-k20 20 128GB Two Nvidia K20 GPUs dev-intel16 28 128GB Two 2.4Ghz 14-core Intel Xeon E5-2680v4 (28 cores total) dev-intel16-k80 28 256GB Intel16 node with 4 Nvidia Tesla K80 GPUs dev-intel18 40 377GB Two 2.4Ghz 20-core Intel Xeon Gold 6148 CPU (40 cores total) dev-amd20-v100 48 187GB Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz and 4 Tesla V100S dev-amd20 128 960GB AMD EPYC 7H12 64-Core Processor @ 2.6GHz Nodes with -k20,-k80,or -v100 suffixes have GPU cards required by GPU-enabled software, but may be used for any software. Note there is not a development node containing the AMD20 A100 GPUs. Once your program is compiled and job script is tested, users can submit it to the SLURM queue by specifying various constraints such as job duration, memory usage, number of CPUs, software license reservations and so on. Warning Any long running (total CPU run time > 2hrs ) jobs on dev nodes will be killed automatically without advance notice.","title":"Development nodes"},{"location":"development_nodes/#development-nodes","text":"HPCC has several development nodes that are available for users to compile their code, and do short (CPU time less than 2 hours) runs to estimate run-time and memory usage. These development nodes run the latest operating system and have similar configuration and environment setup to the compute nodes of the same clusters. Please use these development nodes to compile your program and test the work flow of your job script. For running long-time or large-resource computations, please submit jobs to use compute nodes. To access a certain one, for example, dev-intel14, please log into a gateway node and run: ssh dev-intel14 Node Hostname Cores Memory Notes dev-intel14 20 256GB Large memory intel14 node dev-intel14-k20 20 128GB Two Nvidia K20 GPUs dev-intel16 28 128GB Two 2.4Ghz 14-core Intel Xeon E5-2680v4 (28 cores total) dev-intel16-k80 28 256GB Intel16 node with 4 Nvidia Tesla K80 GPUs dev-intel18 40 377GB Two 2.4Ghz 20-core Intel Xeon Gold 6148 CPU (40 cores total) dev-amd20-v100 48 187GB Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz and 4 Tesla V100S dev-amd20 128 960GB AMD EPYC 7H12 64-Core Processor @ 2.6GHz Nodes with -k20,-k80,or -v100 suffixes have GPU cards required by GPU-enabled software, but may be used for any software. Note there is not a development node containing the AMD20 A100 GPUs. Once your program is compiled and job script is tested, users can submit it to the SLURM queue by specifying various constraints such as job duration, memory usage, number of CPUs, software license reservations and so on. Warning Any long running (total CPU run time > 2hrs ) jobs on dev nodes will be killed automatically without advance notice.","title":"Development nodes"},{"location":"filesystem_overview/","text":"Overview of HPCC file systems Home, Research and Scratch are network file systems, meaning that each node in the cluster must go through the network switch to access these spaces. Please refer to individual wiki pages ( Home , Research , and Scratch ) in this section for how to use each of them. /tmp and /mnt/local are locally accessible in the hard drive of each node. The space is not affected by the network and has larger size compared with the RAMDISK /dev/shm which is located inside the node's RAM. However, /dev/shm is the closest storage location for files. Files stored here take up some of the node's memory space. For more information, please have a look at Local file system and Guidelines for Choosing File Storage and I/O . The ufs18 file system is a 2018 IBM General Parallel File System (GPFS) installed for the HPCC to store the home and research spaces. While it is faster and more stable, users need to learn its differences from other file systems and understand how to use it. Please see UFS18 file system . Data that are subject to external security requirements are considered sensitive data. These requirements may be imposed by regulations or contractual obligations. In either case, users that intend to store sensitive data on HPCC systems must work with us to ensure data security. For more information, check out our Sensitive data hosting wiki page.","title":"Overview"},{"location":"filesystem_overview/#overview-of-hpcc-file-systems","text":"Home, Research and Scratch are network file systems, meaning that each node in the cluster must go through the network switch to access these spaces. Please refer to individual wiki pages ( Home , Research , and Scratch ) in this section for how to use each of them. /tmp and /mnt/local are locally accessible in the hard drive of each node. The space is not affected by the network and has larger size compared with the RAMDISK /dev/shm which is located inside the node's RAM. However, /dev/shm is the closest storage location for files. Files stored here take up some of the node's memory space. For more information, please have a look at Local file system and Guidelines for Choosing File Storage and I/O . The ufs18 file system is a 2018 IBM General Parallel File System (GPFS) installed for the HPCC to store the home and research spaces. While it is faster and more stable, users need to learn its differences from other file systems and understand how to use it. Please see UFS18 file system . Data that are subject to external security requirements are considered sensitive data. These requirements may be imposed by regulations or contractual obligations. In either case, users that intend to store sensitive data on HPCC systems must work with us to ensure data security. For more information, check out our Sensitive data hosting wiki page.","title":"Overview of HPCC file systems"},{"location":"getexample/","text":"getexample Download user examples. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 $ getexample Download an HPC example: usage: getexample <examplename> Where <examplename> is the name of the example you want to download. This will create a directory named examplename which you can cd into and read the README file for details (if one is avaliable) or read .qsub or .sb file for how the example is run. You may submit the example with 'qsub *.qsub' or 'sbatch *.sb'. For Example: getexample helloworld Possible example names: abaqus_example cuda_hybrid Intro2Linux_Oct_2019 MATLAB_threadPool PETSc_example ABySS DDT_examples Job_dependency MKL_benchmark Python_MPI ADMB_example dmtcp_longjob LAPACK_example MKL_c_eigenvalues Python_numpy affinity espresso_benchmark magma_example MKL_Example QuantumESPRESSO Amber_example FFTW makefile_example MKL_FFTW R_parallel_examples basic_array_job FI491 maker_MPI MKL_parallel SAS_example blast fluentMPI MATLAB_basic mothur_example ScaLAPACK blender_farm fortran_openmp MATLAB_compiler MPI_OpenMP_GPU simpleMatlab BOOST_example FreeSurfer MATLAB_GPU MPI_pi STATA_array bowtie gmp_mpfr MATLAB_many_jobs multi_variable STATA_example brother_test helloHPCC MATLAB_parallel NAMD_example tbb_example Clang_example helloMPI MATLAB_parameter_sweep OpenACC_example TotalView_MPI_example CMakePackageExamples helloworld MATLAB_parfor openmp_exercise VASP_example cuda intro2hpcc MATLAB_patternsearch PC2HPC XSEDE_MPI_WORKSHOP","title":"Get software usage examples"},{"location":"getexample/#getexample","text":"Download user examples. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 $ getexample Download an HPC example: usage: getexample <examplename> Where <examplename> is the name of the example you want to download. This will create a directory named examplename which you can cd into and read the README file for details (if one is avaliable) or read .qsub or .sb file for how the example is run. You may submit the example with 'qsub *.qsub' or 'sbatch *.sb'. For Example: getexample helloworld Possible example names: abaqus_example cuda_hybrid Intro2Linux_Oct_2019 MATLAB_threadPool PETSc_example ABySS DDT_examples Job_dependency MKL_benchmark Python_MPI ADMB_example dmtcp_longjob LAPACK_example MKL_c_eigenvalues Python_numpy affinity espresso_benchmark magma_example MKL_Example QuantumESPRESSO Amber_example FFTW makefile_example MKL_FFTW R_parallel_examples basic_array_job FI491 maker_MPI MKL_parallel SAS_example blast fluentMPI MATLAB_basic mothur_example ScaLAPACK blender_farm fortran_openmp MATLAB_compiler MPI_OpenMP_GPU simpleMatlab BOOST_example FreeSurfer MATLAB_GPU MPI_pi STATA_array bowtie gmp_mpfr MATLAB_many_jobs multi_variable STATA_example brother_test helloHPCC MATLAB_parallel NAMD_example tbb_example Clang_example helloMPI MATLAB_parameter_sweep OpenACC_example TotalView_MPI_example CMakePackageExamples helloworld MATLAB_parfor openmp_exercise VASP_example cuda intro2hpcc MATLAB_patternsearch PC2HPC XSEDE_MPI_WORKSHOP","title":"getexample"},{"location":"install_ssh_client/","text":"Install SSH Client In order to use HPCC system robustly by command lines, please read the instruction to have a SSH client and X11 server installed in your computer. For Windows machines We recommend installing MobaXterm Home Edition (Installer edition). This program provides both an SSH client (command line) and an X-Windows (X11 server) system for running graphical user interface (GUI) programs on HPCC. (See MoabXTerm SSH Tutorial & SFTP Tutorial.) For Mac OS machines You can use Terminal program (installed with the operation system) on the task bar (or search for terminal) as your SSH Client. However, for running graphical user interface (GUI) programs on HPCC, you will need to install the X11-server program XQuartz. See https://www.xquartz.org/ for download instructions.","title":"Install an SSH client"},{"location":"install_ssh_client/#install-ssh-client","text":"In order to use HPCC system robustly by command lines, please read the instruction to have a SSH client and X11 server installed in your computer.","title":"Install SSH Client"},{"location":"install_ssh_client/#for-windows-machines","text":"We recommend installing MobaXterm Home Edition (Installer edition). This program provides both an SSH client (command line) and an X-Windows (X11 server) system for running graphical user interface (GUI) programs on HPCC. (See MoabXTerm SSH Tutorial & SFTP Tutorial.)","title":"For Windows machines"},{"location":"install_ssh_client/#for-mac-os-machines","text":"You can use Terminal program (installed with the operation system) on the task bar (or search for terminal) as your SSH Client. However, for running graphical user interface (GUI) programs on HPCC, you will need to install the X11-server program XQuartz. See https://www.xquartz.org/ for download instructions.","title":"For Mac OS machines"},{"location":"job_policies/","text":"Job Policies The following limits apply generally to all MSU users of the HPCC. Those at affiliate institutions may be working under slightly different policies. The limits are in place to help our large user community share the HPC. However, if these policies are an impediment to completing your research, please contact us. CPU and GPU usage limits HPCC users who do not have a buy-in account are given a 'general' SLURM account. The general account is limited to 500,000 CPU hours (30,000,000 minutes) and 10,000 GPU hours (600,000 minutes) every year (from January 1st to December 31st) starting from 2021. There is no yearly usage limit on CPU or GPU time with a buy-in account. If you have a buy-in account, your jobs will be run under that account by default, unless the manager of the buy-in account has chosen to opt-in (jobs submitted with the -A flag) instead of opt-out. Users with general account can use the powertools command SLURMUsage to check their used CPU and GPU time (in minutes) and left CPU and GPU time (in hours): $ ml powertools # run this command if powertools not loaded $ SLURMUsage If users without a buy-in account needs more CPU or GPU time due to running out of the limits, they can request additional CPU/GPU hours by filling out the CPU/GPU Increase Request online form . Limits on job resource requests Time: Users can schedule jobs and run for at most 7 days (168 hours) ( --time=168:00:00 ) CPU: Users can utilize up to a total of 1040 cores or 520 jobs running at any one time. (Buyin groups who have purchased more than 1040 cores can exceed this limit) Queue: The maximum number of jobs that can be queued or running per user is 1000 jobs. Buy-in program Faculty can purchase nodes via our buy-in program . The program guarantees jobs submitted with a buy-in group will start running on their buy-in nodes in 4 hours. However, due to contention between buy-in group jobs, the guarantee might not be fulfilled if requested resources are occupied or reserved by other jobs of the buy-in group. Policy summary Jobs that run under 4 hours are able to run on the largest set of nodes ( the combination of community + specialized hardware + buy-in nodes. see below for details) Jobs that request more resources (processors or RAM) have priorities over smaller jobs because these jobs are more difficult to schedule. Jobs accrue priority based on how long they have been queued. The scheduler will attempt to balance usage among all users. (See Fairshare Policy below.) It is against our fair use policy to artificially increase the priority of a job in the queue (e.g. by requesting more resources which will not be used). Jobs found to be manipulating the scheduler will be canceled, and users continuing to attempt this will be suspended. More about queue time Fairshare As jobs wait in the queue, they accrue priority to run. Another factor that contributes to a job's priority value is fairshare. The scheduler will attempt to ensure fair resource utilization of all HPCC users by adjusting the initial priorities of the users who have recently used HPCC resources. Due to the policy, if users had jobs running with many resources recently, their current pending jobs might wait longer than before. Users can find the FAIRSHARE contribution to a job priority by running command \" sprio -u $USER \": 1 2 3 4 [ UserID@dev-intel18 UserID ] $ sprio -u $USER JOBID PARTITION USER PRIORITY SITE AGE FAIRSHARE QOS TRES 53381467 general-l UserID 49432 0 0 49318 0 cpu = 100 ,mem = 15 53381467 general-s UserID 49432 0 0 49318 0 cpu = 100 ,mem = 15 where it is found under FAIRSHARE column and the values are between 60,000 and 0. More resources your jobs used recently, the less the values become and so are the priorities of your jobs. For other contributions of sprio results, please check Job Priority Factors . Shorter jobs can run on more nodes Jobs that request a total running (wall-clock) time of four hours or less can run on any available buy-in and specialized nodes. Because they can access any nodes, they are likely to start running more quickly than the jobs which have to wait for the general-long partition nodes. Bigger jobs are prioritized & small jobs are backfilled The scheduler attempts to gather resources for large jobs and then backfill smaller jobs around them. The size of the job is determined by the number of CPUs and amount of memory requested. The scheduler packs small jobs together to allow more resources to be gathered for multi-core jobs. Resource requests are monitored. Abusive resource requests may violate MSU policy.","title":"Job policies"},{"location":"job_policies/#job-policies","text":"The following limits apply generally to all MSU users of the HPCC. Those at affiliate institutions may be working under slightly different policies. The limits are in place to help our large user community share the HPC. However, if these policies are an impediment to completing your research, please contact us.","title":"Job Policies"},{"location":"job_policies/#cpu-and-gpu-usage-limits","text":"HPCC users who do not have a buy-in account are given a 'general' SLURM account. The general account is limited to 500,000 CPU hours (30,000,000 minutes) and 10,000 GPU hours (600,000 minutes) every year (from January 1st to December 31st) starting from 2021. There is no yearly usage limit on CPU or GPU time with a buy-in account. If you have a buy-in account, your jobs will be run under that account by default, unless the manager of the buy-in account has chosen to opt-in (jobs submitted with the -A flag) instead of opt-out. Users with general account can use the powertools command SLURMUsage to check their used CPU and GPU time (in minutes) and left CPU and GPU time (in hours): $ ml powertools # run this command if powertools not loaded $ SLURMUsage If users without a buy-in account needs more CPU or GPU time due to running out of the limits, they can request additional CPU/GPU hours by filling out the CPU/GPU Increase Request online form .","title":"CPU and GPU usage limits"},{"location":"job_policies/#limits-on-job-resource-requests","text":"Time: Users can schedule jobs and run for at most 7 days (168 hours) ( --time=168:00:00 ) CPU: Users can utilize up to a total of 1040 cores or 520 jobs running at any one time. (Buyin groups who have purchased more than 1040 cores can exceed this limit) Queue: The maximum number of jobs that can be queued or running per user is 1000 jobs.","title":"Limits on job resource requests"},{"location":"job_policies/#buy-in-program","text":"Faculty can purchase nodes via our buy-in program . The program guarantees jobs submitted with a buy-in group will start running on their buy-in nodes in 4 hours. However, due to contention between buy-in group jobs, the guarantee might not be fulfilled if requested resources are occupied or reserved by other jobs of the buy-in group.","title":"Buy-in program"},{"location":"job_policies/#policy-summary","text":"Jobs that run under 4 hours are able to run on the largest set of nodes ( the combination of community + specialized hardware + buy-in nodes. see below for details) Jobs that request more resources (processors or RAM) have priorities over smaller jobs because these jobs are more difficult to schedule. Jobs accrue priority based on how long they have been queued. The scheduler will attempt to balance usage among all users. (See Fairshare Policy below.) It is against our fair use policy to artificially increase the priority of a job in the queue (e.g. by requesting more resources which will not be used). Jobs found to be manipulating the scheduler will be canceled, and users continuing to attempt this will be suspended.","title":"Policy summary"},{"location":"job_policies/#more-about-queue-time","text":"","title":"More about queue time"},{"location":"job_policies/#fairshare","text":"As jobs wait in the queue, they accrue priority to run. Another factor that contributes to a job's priority value is fairshare. The scheduler will attempt to ensure fair resource utilization of all HPCC users by adjusting the initial priorities of the users who have recently used HPCC resources. Due to the policy, if users had jobs running with many resources recently, their current pending jobs might wait longer than before. Users can find the FAIRSHARE contribution to a job priority by running command \" sprio -u $USER \": 1 2 3 4 [ UserID@dev-intel18 UserID ] $ sprio -u $USER JOBID PARTITION USER PRIORITY SITE AGE FAIRSHARE QOS TRES 53381467 general-l UserID 49432 0 0 49318 0 cpu = 100 ,mem = 15 53381467 general-s UserID 49432 0 0 49318 0 cpu = 100 ,mem = 15 where it is found under FAIRSHARE column and the values are between 60,000 and 0. More resources your jobs used recently, the less the values become and so are the priorities of your jobs. For other contributions of sprio results, please check Job Priority Factors .","title":"Fairshare"},{"location":"job_policies/#shorter-jobs-can-run-on-more-nodes","text":"Jobs that request a total running (wall-clock) time of four hours or less can run on any available buy-in and specialized nodes. Because they can access any nodes, they are likely to start running more quickly than the jobs which have to wait for the general-long partition nodes.","title":"Shorter jobs can run on more nodes"},{"location":"job_policies/#bigger-jobs-are-prioritized-small-jobs-are-backfilled","text":"The scheduler attempts to gather resources for large jobs and then backfill smaller jobs around them. The size of the job is determined by the number of CPUs and amount of memory requested. The scheduler packs small jobs together to allow more resources to be gathered for multi-core jobs. Resource requests are monitored. Abusive resource requests may violate MSU policy.","title":"Bigger jobs are prioritized &amp; small jobs are backfilled"},{"location":"job_priority_factors/","text":"Job Priority Factors Multiple factors contribute to the priority of SLURM jobs. The jobs age, the resources it requests, the submitting user\u2019s cluster utilization, and whether it was submitted to a buy-in account all contribute to the job\u2019s priority. Priority Factor Description Maximum Contribution to Priority Age Starts at zero at job submission, then increases linearly to a maximum of 60000 after 30 days 60000 after 30 days Fairshare Starts at 60000 and decreases and users' recent usage goes up. Usage for this calculation is decayed 50% each day 60000 for no recent cluster usage Size Scales linearly with the amount of CPU and memory requested by a job. 100 per CPU, 20 per GB. 52000+ depending on memory requested QOS Adds 1500 to buy-in jobs to ensure they are always above backfill schedulers minimum priority for reserving resources 1500 Backfill Scheduler Requirements to Reserve Resources Minimum Priority of 1500 The backfill scheduler will not reserve future resources for jobs with an overall priority less than 1500. Jobs with a priority less than 1500 may be started immediately by the backfill scheduler if resources are available. Jobs submitted to buy-in accounts are always above this threshold. Minimum Age of 30 Minutes The backfill scheduler will not reserve future resources for jobs that have been queued for less than 30 minutes. Jobs queued for less than 30 minutes may be started immediately by the backfill scheduler if resources are available.","title":"Job priority factors"},{"location":"job_priority_factors/#job-priority-factors","text":"Multiple factors contribute to the priority of SLURM jobs. The jobs age, the resources it requests, the submitting user\u2019s cluster utilization, and whether it was submitted to a buy-in account all contribute to the job\u2019s priority. Priority Factor Description Maximum Contribution to Priority Age Starts at zero at job submission, then increases linearly to a maximum of 60000 after 30 days 60000 after 30 days Fairshare Starts at 60000 and decreases and users' recent usage goes up. Usage for this calculation is decayed 50% each day 60000 for no recent cluster usage Size Scales linearly with the amount of CPU and memory requested by a job. 100 per CPU, 20 per GB. 52000+ depending on memory requested QOS Adds 1500 to buy-in jobs to ensure they are always above backfill schedulers minimum priority for reserving resources 1500","title":"Job Priority Factors"},{"location":"job_priority_factors/#backfill-scheduler-requirements-to-reserve-resources","text":"","title":"Backfill Scheduler Requirements to Reserve Resources"},{"location":"job_priority_factors/#minimum-priority-of-1500","text":"The backfill scheduler will not reserve future resources for jobs with an overall priority less than 1500. Jobs with a priority less than 1500 may be started immediately by the backfill scheduler if resources are available. Jobs submitted to buy-in accounts are always above this threshold.","title":"Minimum Priority of 1500"},{"location":"job_priority_factors/#minimum-age-of-30-minutes","text":"The backfill scheduler will not reserve future resources for jobs that have been queued for less than 30 minutes. Jobs queued for less than 30 minutes may be started immediately by the backfill scheduler if resources are available.","title":"Minimum Age of 30 Minutes"},{"location":"js/","text":"js The SLURM command sacct can be used to show the job steps of a job: 1 2 3 4 5 6 7 8 sacct -j 40410 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 40410 job classres-+ classres 28 COMPLETED 0 :0 40410 .batch batch classres 28 COMPLETED 0 :0 40410 .extern extern classres 28 COMPLETED 0 :0 40410 .0 pw.x classres 28 COMPLETED 0 :0 40410 .1 ph.x classres 28 COMPLETED 0 :0 and the resource usages after it finished running. However, to display your desired results, it might take you some time to look into the web site and learn how to use the command. Here we introduce the powertools command \" js \" to display the resource usages of your jobs. [ Display Usage Info of a Job ] [ Display a List of Jobs ] [ More Selections of js Command ] Display Usage Info of a Job Users can simply run the powertools command \" js \" and it gives you most of the useful resource usages. To see the resource usages of a job, just use the command \" js -j <JobID> \", e.g., 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ js -j 45251 # powertools command SLURM Job ID: 45251 WrkDir = /mnt/home/changc81/GetExample/GaAs stdout = /mnt/home/changc81/GetExample/GaAs/slurm-45251.out ========================================================================================================= JobID | 45251 | 45251 .batch | 45251 .extern | 45251 .0 | JobName | job | batch | extern | pw.x | User | UserName | | | | NodeList | lac-421 | lac-421 | lac-421 | lac-421 | NNodes | 1 | 1 | 1 | 1 | NTasks | | 1 | 1 | 28 | NCPUS | 28 | 28 | 28 | 28 | ReqMem | 112Gn | 112Gn | 112Gn | 112Gn | Timelimit | 04 :00:00 | | | | Elapsed | 00 :00:16 | 00 :00:16 | 00 :00:16 | 00 :00:14 | TotalCPU | 05 :39.544 | 00 :00.999 | 00 :00.001 | 05 :38.543 | AveCPULoad | 21 .2215 | 0 .0624375 | 6 .25e-05 | 24 .1816 | MaxRSS | | | 20K | 60296K | MaxVMSize | | 189200K | 4184K | 605104K | Start | 2018 -08-28T20:27:40 | 2018 -08-28T20:27:40 | 2018 -08-28T20:27:40 | 2018 -08-28T20:27:41 | End | 2018 -08-28T20:27:56 | 2018 -08-28T20:27:56 | 2018 -08-28T20:27:56 | 2018 -08-28T20:27:55 | ExitCode | 0 :0 | 0 :0 | 0 :0 | 0 :0 | State | COMPLETED | COMPLETED | COMPLETED | COMPLETED | ========================================================================================================= If you would like to show more data of a job, you can also use the specification -F: 1 $ js -j <Job ID> -F # powertools command to list all stored data of the job steps. Display a List of Jobs If users would like to know a list of jobs submitted before, they can use \" js -z \" command. Simply provide a period of time when job was running with -S (start time of the period) and -E (end time of the period) options: 1 $ js -z -S <Start Time> -E <End Time> and a list of the jobs with their properties and resource usages is displayed. For example, user can run the command: 1 2 3 4 5 6 7 8 $ js -z -S 2021 -04-12 -E 2021 -04-19 JobID JobName NNo NTas NCPU Timelimit Elapsed AveCPU MaxRSS Stat Exit Start NodeList ------------ ---------- --- ---- ---- ----------- ----------- ------- ---------- ---- ---- ------------------- ----------------- 21043834 ondemand/+ 1 1 1 01 :00:00 01 :00:16 0 .05487 467 .41M TIM+ 0 :0 2021 -04-12T08:59:10 css-033 21127831 fi_info 1 1 2 00 :05:00 00 :03:19 1 .18321 58191 .45M COM+ 0 :0 2021 -04-16T10:14:33 skl-033 21158898 hello.exe 1 8 1 00 :20:00 00 :01:09 0 .236 644 .61M COM+ 0 :0 2021 -04-17T20:17:57 amr-133 21158916 interacti+ 1 1 1 03 :00:00 02 :00:04 0 .77325 1244 .61M COM+ 0 :0 2021 -04-18T20:18:29 css-033 21158973 SPAdes 1 4 8 09 :30:00 09 :00:04 7 .254 13 .20G FAI+ 1 :0 2021 -04-19T20:20:44 lac-421 to see a list of jobs running between April 12th 2021 and April 19th 2021. If any one of the options -S or -E is not specified, the time will be considered as the current time of \" js \" execution. More Selections of js Command To see all possible usages of the command, please use the specification -h: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 $ js -h js [ <OPTION> ] Valid <OPTION> values are: -a, --allusers: Display jobs for all users. By default, only the current user 's jobs are displayed. If ran by user root this is the default. -A, --accounts: Use this comma separated list of accounts to select jobs to display. By default, all accounts are selected. -b, --brief: Equivalent to ' --format = jobstep,state,error '. -c, --completion: Use job completion instead of accounting data. --delimiter: ASCII characters used to separate the fields when specifying the -p or -P options. The default delimiter is a ' | '. This options is ignored if -p or -P options are not specified. -C: Display results in columns rather than rows. Each column shows all data of a job step. A number can be specified after -C for how many columns in a row. -D, --duplicates: If Slurm job ids are reset, some job numbers may appear more than once referring to different jobs. Without this option only the most recent jobs will be displayed. -e, --helpformat: Print a list of fields that can be specified with the ' --format ' option -E, --endtime: Select jobs eligible before this time. If states are given with the -s option return jobs in this state before this period. --federation: Report jobs from federation if a member of a one. -f, --file=file: Read data from the specified file, rather than Slurm' s current accounting log file. ( Only appliciable when running the filetxt plugin. ) -F: Display data of all fields ( --format = ALL ) in columns. By default, three columns are shown in a row. See -C to change the default column number. -g, --gid, --group: Use this comma separated list of gids or group names to select jobs to display. By default, all groups are selected. -h, --help: Print this description of use. -i, --nnodes = N: Return jobs which ran on this many nodes ( N = min [ -max ]) -I, --ncpus = N: Return jobs which ran on this many cpus ( N = min [ -max ]) -j, --jobs: Format is <job ( .step ) >. Display information about this job or comma-separated list of jobs. The default is all jobs. Adding .step will display the specific job step of that job. ( A step id of 'batch' will display the information about the batch step. ) -k, --timelimit-min: Only send data about jobs with this timelimit. If used with timelimit_max this will be the minimum timelimit of the range. Default is no restriction. -K, --timelimit-max: Ignored by itself, but if timelimit_min is set this will be the maximum timelimit of the range. Default is no restriction. --local Report information only about jobs on the local cluster. Overrides --federation. -l, --long: Equivalent to specifying '--format=jobid,jobname,partition,maxvmsize,maxvmsizenode, maxvmsizetask,avevmsize,maxrss,maxrssnode, maxrsstask,averss,maxpages,maxpagesnode, maxpagestask,avepages,mincpu,mincpunode, mincputask,avecpu,ntasks,alloccpus,elapsed, state,exitcode,avecpufreq,reqcpufreqmin, reqcpufreqmax,reqcpufreqgov,consumedenergy, maxdiskread,maxdiskreadnode,maxdiskreadtask, avediskread,maxdiskwrite,maxdiskwritenode, maxdiskwritetask,avediskread,allocgres,reqgres -L, --allclusters: Display jobs ran on all clusters. By default, only jobs ran on the cluster from where sacct is called are displayed. -M, --clusters: Only send data about these clusters. Use \"all\" for all clusters. -n, --noheader: No header will be added to the beginning of output. The default is to print a header. --noconvert: Don' t convert units from their original type ( e.g. 2048M won 't be converted to 2G). -N, --nodelist: Display jobs that ran on any of these nodes, can be one or more using a ranged string. --name: Display jobs that have any of these name(s). -o, --format: Comma separated list of fields. (use \"--helpformat\" for a list of available fields). -p, --parsable: output will be ' | ' delimited with a ' | ' at the end -P, --parsable2: output will be ' | ' delimited without a ' | ' at the end -q, --qos: Only send data about jobs using these qos. Default is all. -r, --partition: Comma separated list of partitions to select jobs and job steps from. The default is all partitions. -s, --state: Select jobs based on their current state or the state they were in during the time period given: running (r), completed (cd), failed (f), timeout (to), resizing (rs), deadline (dl) and node_fail (nf). -S, --starttime: Select jobs eligible after this time. Default is 00:00:00 of the current day, unless ' -s ' is set then the default is ' now '. -T, --truncate: Truncate time. So if a job started before --starttime the start time would be truncated to --starttime. The same for end time and --endtime. -u, --uid, --user: Use this comma separated list of uids or user names to select jobs to display. By default, the running user' s uid is used. --units =[ KMGTP ] : Display values in specified unit type. Takes precedence over --noconvert option. --usage: Display brief usage message. -v, --verbose: Primarily for debugging purposes, report the state of various variables during processing. -V, --version: Print version. -W, --wckeys: Only send data about these wckeys. Default is all. --whole-hetjob =[ yes | no ] : If set to 'yes' ( or not set ) , then information about all the heterogeneous components will be retrieved. If set to 'no' only the specific filtered components will be retrieved. -x, --associations: Only send data about these association id. Default is all. -X, --allocations: Only show statistics relevant to the job allocation itself, not taking steps into consideration. -z: Show simple summary data only. Note, valid start/end time formats are... HH:MM [ :SS ] [ AM | PM ] MMDD [ YY ] or MM/DD [ /YY ] or MM.DD [ .YY ] MM/DD [ /YY ] -HH:MM [ :SS ] YYYY-MM-DD [ THH:MM [ :SS ]]","title":"js"},{"location":"js/#js","text":"The SLURM command sacct can be used to show the job steps of a job: 1 2 3 4 5 6 7 8 sacct -j 40410 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 40410 job classres-+ classres 28 COMPLETED 0 :0 40410 .batch batch classres 28 COMPLETED 0 :0 40410 .extern extern classres 28 COMPLETED 0 :0 40410 .0 pw.x classres 28 COMPLETED 0 :0 40410 .1 ph.x classres 28 COMPLETED 0 :0 and the resource usages after it finished running. However, to display your desired results, it might take you some time to look into the web site and learn how to use the command. Here we introduce the powertools command \" js \" to display the resource usages of your jobs. [ Display Usage Info of a Job ] [ Display a List of Jobs ] [ More Selections of js Command ]","title":"js"},{"location":"js/#display-usage-info-of-a-job","text":"Users can simply run the powertools command \" js \" and it gives you most of the useful resource usages. To see the resource usages of a job, just use the command \" js -j <JobID> \", e.g., 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ js -j 45251 # powertools command SLURM Job ID: 45251 WrkDir = /mnt/home/changc81/GetExample/GaAs stdout = /mnt/home/changc81/GetExample/GaAs/slurm-45251.out ========================================================================================================= JobID | 45251 | 45251 .batch | 45251 .extern | 45251 .0 | JobName | job | batch | extern | pw.x | User | UserName | | | | NodeList | lac-421 | lac-421 | lac-421 | lac-421 | NNodes | 1 | 1 | 1 | 1 | NTasks | | 1 | 1 | 28 | NCPUS | 28 | 28 | 28 | 28 | ReqMem | 112Gn | 112Gn | 112Gn | 112Gn | Timelimit | 04 :00:00 | | | | Elapsed | 00 :00:16 | 00 :00:16 | 00 :00:16 | 00 :00:14 | TotalCPU | 05 :39.544 | 00 :00.999 | 00 :00.001 | 05 :38.543 | AveCPULoad | 21 .2215 | 0 .0624375 | 6 .25e-05 | 24 .1816 | MaxRSS | | | 20K | 60296K | MaxVMSize | | 189200K | 4184K | 605104K | Start | 2018 -08-28T20:27:40 | 2018 -08-28T20:27:40 | 2018 -08-28T20:27:40 | 2018 -08-28T20:27:41 | End | 2018 -08-28T20:27:56 | 2018 -08-28T20:27:56 | 2018 -08-28T20:27:56 | 2018 -08-28T20:27:55 | ExitCode | 0 :0 | 0 :0 | 0 :0 | 0 :0 | State | COMPLETED | COMPLETED | COMPLETED | COMPLETED | ========================================================================================================= If you would like to show more data of a job, you can also use the specification -F: 1 $ js -j <Job ID> -F # powertools command to list all stored data of the job steps.","title":"Display Usage Info of a Job"},{"location":"js/#display-a-list-of-jobs","text":"If users would like to know a list of jobs submitted before, they can use \" js -z \" command. Simply provide a period of time when job was running with -S (start time of the period) and -E (end time of the period) options: 1 $ js -z -S <Start Time> -E <End Time> and a list of the jobs with their properties and resource usages is displayed. For example, user can run the command: 1 2 3 4 5 6 7 8 $ js -z -S 2021 -04-12 -E 2021 -04-19 JobID JobName NNo NTas NCPU Timelimit Elapsed AveCPU MaxRSS Stat Exit Start NodeList ------------ ---------- --- ---- ---- ----------- ----------- ------- ---------- ---- ---- ------------------- ----------------- 21043834 ondemand/+ 1 1 1 01 :00:00 01 :00:16 0 .05487 467 .41M TIM+ 0 :0 2021 -04-12T08:59:10 css-033 21127831 fi_info 1 1 2 00 :05:00 00 :03:19 1 .18321 58191 .45M COM+ 0 :0 2021 -04-16T10:14:33 skl-033 21158898 hello.exe 1 8 1 00 :20:00 00 :01:09 0 .236 644 .61M COM+ 0 :0 2021 -04-17T20:17:57 amr-133 21158916 interacti+ 1 1 1 03 :00:00 02 :00:04 0 .77325 1244 .61M COM+ 0 :0 2021 -04-18T20:18:29 css-033 21158973 SPAdes 1 4 8 09 :30:00 09 :00:04 7 .254 13 .20G FAI+ 1 :0 2021 -04-19T20:20:44 lac-421 to see a list of jobs running between April 12th 2021 and April 19th 2021. If any one of the options -S or -E is not specified, the time will be considered as the current time of \" js \" execution.","title":"Display a List of Jobs"},{"location":"js/#more-selections-of-js-command","text":"To see all possible usages of the command, please use the specification -h: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 $ js -h js [ <OPTION> ] Valid <OPTION> values are: -a, --allusers: Display jobs for all users. By default, only the current user 's jobs are displayed. If ran by user root this is the default. -A, --accounts: Use this comma separated list of accounts to select jobs to display. By default, all accounts are selected. -b, --brief: Equivalent to ' --format = jobstep,state,error '. -c, --completion: Use job completion instead of accounting data. --delimiter: ASCII characters used to separate the fields when specifying the -p or -P options. The default delimiter is a ' | '. This options is ignored if -p or -P options are not specified. -C: Display results in columns rather than rows. Each column shows all data of a job step. A number can be specified after -C for how many columns in a row. -D, --duplicates: If Slurm job ids are reset, some job numbers may appear more than once referring to different jobs. Without this option only the most recent jobs will be displayed. -e, --helpformat: Print a list of fields that can be specified with the ' --format ' option -E, --endtime: Select jobs eligible before this time. If states are given with the -s option return jobs in this state before this period. --federation: Report jobs from federation if a member of a one. -f, --file=file: Read data from the specified file, rather than Slurm' s current accounting log file. ( Only appliciable when running the filetxt plugin. ) -F: Display data of all fields ( --format = ALL ) in columns. By default, three columns are shown in a row. See -C to change the default column number. -g, --gid, --group: Use this comma separated list of gids or group names to select jobs to display. By default, all groups are selected. -h, --help: Print this description of use. -i, --nnodes = N: Return jobs which ran on this many nodes ( N = min [ -max ]) -I, --ncpus = N: Return jobs which ran on this many cpus ( N = min [ -max ]) -j, --jobs: Format is <job ( .step ) >. Display information about this job or comma-separated list of jobs. The default is all jobs. Adding .step will display the specific job step of that job. ( A step id of 'batch' will display the information about the batch step. ) -k, --timelimit-min: Only send data about jobs with this timelimit. If used with timelimit_max this will be the minimum timelimit of the range. Default is no restriction. -K, --timelimit-max: Ignored by itself, but if timelimit_min is set this will be the maximum timelimit of the range. Default is no restriction. --local Report information only about jobs on the local cluster. Overrides --federation. -l, --long: Equivalent to specifying '--format=jobid,jobname,partition,maxvmsize,maxvmsizenode, maxvmsizetask,avevmsize,maxrss,maxrssnode, maxrsstask,averss,maxpages,maxpagesnode, maxpagestask,avepages,mincpu,mincpunode, mincputask,avecpu,ntasks,alloccpus,elapsed, state,exitcode,avecpufreq,reqcpufreqmin, reqcpufreqmax,reqcpufreqgov,consumedenergy, maxdiskread,maxdiskreadnode,maxdiskreadtask, avediskread,maxdiskwrite,maxdiskwritenode, maxdiskwritetask,avediskread,allocgres,reqgres -L, --allclusters: Display jobs ran on all clusters. By default, only jobs ran on the cluster from where sacct is called are displayed. -M, --clusters: Only send data about these clusters. Use \"all\" for all clusters. -n, --noheader: No header will be added to the beginning of output. The default is to print a header. --noconvert: Don' t convert units from their original type ( e.g. 2048M won 't be converted to 2G). -N, --nodelist: Display jobs that ran on any of these nodes, can be one or more using a ranged string. --name: Display jobs that have any of these name(s). -o, --format: Comma separated list of fields. (use \"--helpformat\" for a list of available fields). -p, --parsable: output will be ' | ' delimited with a ' | ' at the end -P, --parsable2: output will be ' | ' delimited without a ' | ' at the end -q, --qos: Only send data about jobs using these qos. Default is all. -r, --partition: Comma separated list of partitions to select jobs and job steps from. The default is all partitions. -s, --state: Select jobs based on their current state or the state they were in during the time period given: running (r), completed (cd), failed (f), timeout (to), resizing (rs), deadline (dl) and node_fail (nf). -S, --starttime: Select jobs eligible after this time. Default is 00:00:00 of the current day, unless ' -s ' is set then the default is ' now '. -T, --truncate: Truncate time. So if a job started before --starttime the start time would be truncated to --starttime. The same for end time and --endtime. -u, --uid, --user: Use this comma separated list of uids or user names to select jobs to display. By default, the running user' s uid is used. --units =[ KMGTP ] : Display values in specified unit type. Takes precedence over --noconvert option. --usage: Display brief usage message. -v, --verbose: Primarily for debugging purposes, report the state of various variables during processing. -V, --version: Print version. -W, --wckeys: Only send data about these wckeys. Default is all. --whole-hetjob =[ yes | no ] : If set to 'yes' ( or not set ) , then information about all the heterogeneous components will be retrieved. If set to 'no' only the specific filtered components will be retrieved. -x, --associations: Only send data about these association id. Default is all. -X, --allocations: Only show statistics relevant to the job allocation itself, not taking steps into consideration. -z: Show simple summary data only. Note, valid start/end time formats are... HH:MM [ :SS ] [ AM | PM ] MMDD [ YY ] or MM/DD [ /YY ] or MM.DD [ .YY ] MM/DD [ /YY ] -HH:MM [ :SS ] YYYY-MM-DD [ THH:MM [ :SS ]]","title":"More Selections of js Command"},{"location":"make/","text":"Makefile Makeifles are a simple way to organize code compilation. This tutorial offers a very basic idea of what is possible using make. For this tutorial, please download three files, a main program (hello.c), a functional code (hellofunc.c), and an include file (hello.h) [hello.c, hellofunc.c, hello.h] under the 'hello' directory. To compile these codes, you would use the following command: 1 gcc -o hello hello.c hellofunc.c -I. This command compiles the two c files, and names the executable hello. With the '-I.' flag, gcc will look in the current directory for the include file 'hello.h'. With only two .c files, it is easy to compile with the above approach, but with more files, it is more likely having typos. In addition, if you are only making changes to one .c file, the above approach recompiles all of .c files every time which is time-consuming and inefficient. So it is time to learn how makefile will be helpful for such cases. First, create a file which has the following two lines. (The filename should be makefile or Makefile ), and put it under the 'hello' directory. 1 2 hello: hello.c hellofunc.c gcc -o hello hello.c hellofunc.c -I. Now, type make on the terminal and check if the executable is created. The make command will execute the compile command as you have written it in the makefile. Note that make with no arguments executes the first rule in the file. Furthermore, by putting the list of files on which the command depends on the first line after the ':', make knows that the rule hello needs to be executed if any of those files change. One very important thing to note is that there should be a tab before the gcc command in the makefile (multiple spaces do not work!). There must be a tab at the beginning of any command, otherwise, you will get a lot of errors. Can we make it a little bit more efficient? Let's modify our makefile as following: 1 2 3 4 CC=gcc CFLAGS=-I. hello: hello.o hellofunc.o $(CC) -o hello hello.o hellofunc.o In this makefile, we define CC and CFLAGS, which are special macros communicating to make how we want to compile the files hello.c and hellofunc.c. In particular, CC is for the C compiler, and CFLAGS is the list of flags to pass to C compiler. By putting the object files (hello.o and hellofunc.o) in the dependency list and in the rule, make knows it must first compile the .c files individually, and then build the executable hello. If your project is small, like consisting of a few separate codes, this form of makefile is enough to handle the set of codes. However, this makefile misses include files. For example, if you made a change to 'hello.h' make would not recompile the .c files, even though they needed to be. In order to fix this problem, we need to tell make that all .c files depend on certain .h files. It can be done by writing a simple rule and adding it to the makefile. 1 2 3 4 5 6 7 8 9 CC=gcc CFLAGS=-I. DEPS = hello.h %.o: %.c $(DEPS) $(CC) -c -o $@ $< $(CFLAGS) hello: hello.o hellofunc.o $(CC) -o hello hello.o hellofunc.o This addition first creates the macro DEPS (the macro name does not have to be DEPS. You can use any name.), which is the set of .h files on which the .c files depend. Then we define a rule for all .o files. The rule says that the .o file depends on the .c files, and the .h files which are included in the DEPS. Next, the rule says that to generate the .o file, make needs to compile the .c file using the compiler defined in the CC. The -c flag says to generate the object file, the -o \\$@ says to put the output of the compilation in the file named on the left side of the :, the \\$< is the first item in the dependencies list, and the CFLAGS macro is defined on the 2nd line. For the simplification, you can use special macros \\$@ and \\$^ , which are the left and right sides of the : , respectively, to make the overall compilation rule more general. In the example below, all of the include files should be listed as part of the macro DEPS, and all of the object files should be listed as part of the macro OBJ. 1 2 3 4 5 6 7 8 9 10 CC=gcc CFLAGS=-I. DEPS = hello.h OBJ = hello.o hellofunc.o %.o: %.c $(DEPS) $(CC) -c -o $@ $< $(CFLAGS) hello: $(OBJ) $(CC) -o $@ $^ $(CFLAGS) Now you have a good sense of makefile. For more information on makefiles and the make function, check out the GNU Make Manual , which will tell you everything on makefile. You can download some makefile examples using getexample in our HPC.","title":"make"},{"location":"make/#makefile","text":"Makeifles are a simple way to organize code compilation. This tutorial offers a very basic idea of what is possible using make. For this tutorial, please download three files, a main program (hello.c), a functional code (hellofunc.c), and an include file (hello.h) [hello.c, hellofunc.c, hello.h] under the 'hello' directory. To compile these codes, you would use the following command: 1 gcc -o hello hello.c hellofunc.c -I. This command compiles the two c files, and names the executable hello. With the '-I.' flag, gcc will look in the current directory for the include file 'hello.h'. With only two .c files, it is easy to compile with the above approach, but with more files, it is more likely having typos. In addition, if you are only making changes to one .c file, the above approach recompiles all of .c files every time which is time-consuming and inefficient. So it is time to learn how makefile will be helpful for such cases. First, create a file which has the following two lines. (The filename should be makefile or Makefile ), and put it under the 'hello' directory. 1 2 hello: hello.c hellofunc.c gcc -o hello hello.c hellofunc.c -I. Now, type make on the terminal and check if the executable is created. The make command will execute the compile command as you have written it in the makefile. Note that make with no arguments executes the first rule in the file. Furthermore, by putting the list of files on which the command depends on the first line after the ':', make knows that the rule hello needs to be executed if any of those files change. One very important thing to note is that there should be a tab before the gcc command in the makefile (multiple spaces do not work!). There must be a tab at the beginning of any command, otherwise, you will get a lot of errors. Can we make it a little bit more efficient? Let's modify our makefile as following: 1 2 3 4 CC=gcc CFLAGS=-I. hello: hello.o hellofunc.o $(CC) -o hello hello.o hellofunc.o In this makefile, we define CC and CFLAGS, which are special macros communicating to make how we want to compile the files hello.c and hellofunc.c. In particular, CC is for the C compiler, and CFLAGS is the list of flags to pass to C compiler. By putting the object files (hello.o and hellofunc.o) in the dependency list and in the rule, make knows it must first compile the .c files individually, and then build the executable hello. If your project is small, like consisting of a few separate codes, this form of makefile is enough to handle the set of codes. However, this makefile misses include files. For example, if you made a change to 'hello.h' make would not recompile the .c files, even though they needed to be. In order to fix this problem, we need to tell make that all .c files depend on certain .h files. It can be done by writing a simple rule and adding it to the makefile. 1 2 3 4 5 6 7 8 9 CC=gcc CFLAGS=-I. DEPS = hello.h %.o: %.c $(DEPS) $(CC) -c -o $@ $< $(CFLAGS) hello: hello.o hellofunc.o $(CC) -o hello hello.o hellofunc.o This addition first creates the macro DEPS (the macro name does not have to be DEPS. You can use any name.), which is the set of .h files on which the .c files depend. Then we define a rule for all .o files. The rule says that the .o file depends on the .c files, and the .h files which are included in the DEPS. Next, the rule says that to generate the .o file, make needs to compile the .c file using the compiler defined in the CC. The -c flag says to generate the object file, the -o \\$@ says to put the output of the compilation in the file named on the left side of the :, the \\$< is the first item in the dependencies list, and the CFLAGS macro is defined on the 2nd line. For the simplification, you can use special macros \\$@ and \\$^ , which are the left and right sides of the : , respectively, to make the overall compilation rule more general. In the example below, all of the include files should be listed as part of the macro DEPS, and all of the object files should be listed as part of the macro OBJ. 1 2 3 4 5 6 7 8 9 10 CC=gcc CFLAGS=-I. DEPS = hello.h OBJ = hello.o hellofunc.o %.o: %.c $(DEPS) $(CC) -c -o $@ $< $(CFLAGS) hello: $(OBJ) $(CC) -o $@ $^ $(CFLAGS) Now you have a good sense of makefile. For more information on makefiles and the make function, check out the GNU Make Manual , which will tell you everything on makefile. You can download some makefile examples using getexample in our HPC.","title":"Makefile"},{"location":"node_status/","text":"node_status Display a list of compute nodes and their properties. 1 2 3 4 5 6 7 8 9 10 11 $ node_status -h node_status: Display status of all compute nodes or nodes with following specifications. -h | --help Display this help message -f | --feature Selected features ( csm,csn,csp,css,qml,lac,vim,nvl,skl,intel14,intel16,intel18,gpu,k80 ) -w | --nodelist List of nodes ( conflict with -f ) -A | --account Account names ( such as general,classres, ... ) -c | --cpus Available number of CPU -m | --mem Available memory ( in Mb ) -g | --gpus Available number of GPU 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 $ node_status Wed Apr 22 11 :14:40 EDT 2020 NodeName Account State CPU ( Load:Aloc Idl:Tot ) Mem ( Aval:Tot ) Mb GPU ( I:T ) Reason ---------------------------------------------------------------------------------------------------------- csm-001 general ALLOCATED 13 .61: 20 0 : 20 45186 : 246640 N/A csm-002 albrecht MIXED 10 .14: 15 5 : 20 1072 : 246640 N/A csm-003 colej ALLOCATED 7 .45: 20 0 : 20 50032 : 246640 N/A csm-004 colej ALLOCATED 6 .24: 20 0 : 20 50032 : 246640 N/A csm-005 colej MIXED 9 .02: 16 4 : 20 29552 : 246640 N/A csm-007 colej MIXED 3 .85: 16 4 : 20 29552 : 246640 N/A csm-008 horticulture DOWN* N/A: 0 20 : 20 246640 : 246640 N/A DEAD - to be removed [ climer@2020-04-08T12:48:08 ] csm-009 horticulture DOWN* N/A: 0 20 : 20 246640 : 246640 N/A Climer - Need power reset 4 -21-20 [ climer@2020-04-21T07:59:30 ] csm-010 ged ALLOCATED 6 .34: 20 0 : 20 50032 : 246640 N/A csm-017 eisenlohr DOWN*+DRAIN N/A: 0 20 : 20 246640 : 246640 N/A Parks testing ssh puppet module [ parksjo@2020-04-02T10:10:52 ] csm-018 eisenlohr IDLE+DRAIN 0 .06: 0 20 : 20 246640 : 246640 N/A Parks testing ssh puppet module [ parksjo@2020-04-02T10:10:40 ] csm-019 eisenlohr DOWN+DRAIN 0 .01: 0 20 : 20 246640 : 246640 N/A DEAD - to be removed [ climer@2020-04-08T12:48:45 ] csm-020 eisenlohr ALLOCATED 6 .75: 20 0 : 20 50032 : 246640 N/A csm-021 dworkin ALLOCATED 6 .68: 20 0 : 20 65392 : 246640 N/A csm-022 mitchmcg ALLOCATED 6 .08: 20 0 : 20 65392 : 246640 N/A csn-001 general MIXED 9 .76: 18 2 : 20 18208 : 118012 k20 ( 0 :2 ) csn-002 general MIXED 1 .64: 8 12 : 20 61628 : 118012 k20 ( 0 :2 ) csn-003 general MIXED 2 .29: 6 14 : 20 80878 : 118012 k20 ( 0 :2 ) csn-004 general MIXED 9 .10: 9 11 : 20 9742 : 118012 k20 ( 0 :2 ) csn-005 general MIXED 9 .92: 12 8 : 20 16160 : 118012 k20 ( 0 :2 ) csn-006 general MIXED 5 .51: 6 14 : 20 252 : 118012 k20 ( 0 :2 ) csn-007 general MIXED 1 .16: 14 6 : 20 95964 : 118012 k20 ( 0 :2 ) csn-008 general MIXED 1 .67: 8 12 : 20 61628 : 118012 k20 ( 0 :2 ) csn-009 general MIXED 1 .29: 7 13 : 20 78012 : 118012 k20 ( 0 :2 ) csn-010 general MIXED 1 .59: 8 12 : 20 61628 : 118012 k20 ( 0 :2 ) csn-011 general MIXED 1 .57: 8 12 : 20 61628 : 118012 k20 ( 0 :2 ) csn-012 general MIXED 4 .54: 11 9 : 20 12476 : 118012 k20 ( 0 :2 ) csn-013 general MIXED 4 .53: 11 9 : 20 12476 : 118012 k20 ( 0 :2 ) csn-014 general MIXED 4 .75: 11 9 : 20 12476 : 118012 k20 ( 0 :2 ) csn-015 general MIXED 4 .59: 11 9 : 20 12476 : 118012 k20 ( 0 :2 ) csn-016 general MIXED 0 .62: 7 13 : 20 78012 : 118012 k20 ( 0 :2 ) csn-017 general MIXED 1 .77: 8 12 : 20 61628 : 118012 k20 ( 0 :2 ) csn-018 general MIXED 1 .16: 5 15 : 20 89820 : 118012 k20 ( 0 :2 ) csn-019 general ALLOCATED 3 .98: 20 0 : 20 8718 : 118012 k20 ( 0 :2 ) csn-020 general IDLE 10 .51: 0 20 : 20 118012 : 118012 k20 ( 2 :2 ) csn-021 general MIXED 6 .05: 6 14 : 20 16160 : 118012 k20 ( 0 :2 ) csn-022 general MIXED 1 .73: 8 12 : 20 26812 : 118012 k20 ( 0 :2 ) csn-023 general MIXED 0 .35: 8 12 : 20 78012 : 118012 k20 ( 0 :2 ) csn-024 general MIXED 1 .46: 8 12 : 20 61628 : 118012 k20 ( 0 :2 ) csn-025 general MIXED 1 .73: 5 15 : 20 57052 : 118012 k20 ( 0 :2 ) csn-026 general MIXED 1 .67: 15 5 : 20 12476 : 118012 k20 ( 0 :2 ) csn-027 general ALLOCATED 12 .84: 20 0 : 20 1276 : 118012 k20 ( 0 :2 ) csn-028 general MIXED 1 .97: 8 12 : 20 78012 : 118012 k20 ( 0 :2 ) csn-029 general MIXED 0 .68: 7 13 : 20 78012 : 118012 k20 ( 0 :2 ) csn-030 general MIXED 1 .77: 8 12 : 20 61628 : 118012 k20 ( 0 :2 ) csn-031 general MIXED 2 .08: 8 12 : 20 78012 : 118012 k20 ( 0 :2 ) csn-032 general MIXED 2 .14: 6 14 : 20 31726 : 118012 k20 ( 0 :2 ) csn-033 general MIXED 2 .15: 6 14 : 20 73436 : 118012 k20 ( 0 :2 ) csn-034 general MIXED 1 .85: 8 12 : 20 61628 : 118012 k20 ( 0 :2 ) csn-035 general MIXED 4 .82: 11 9 : 20 28860 : 118012 k20 ( 0 :2 ) csn-036 general MIXED 0 .63: 8 12 : 20 78012 : 118012 k20 ( 0 :2 ) csn-037 eisenlohr MIXED 1 .16: 9 11 : 20 37052 : 118012 k20 ( 0 :2 ) csn-038 christlibuyin MIXED 1 .43: 15 5 : 20 12476 : 118012 k20 ( 0 :2 ) csn-039 christlibuyin MIXED 2 .11: 15 5 : 20 12476 : 118012 k20 ( 0 :2 ) csp-006 general ALLOCATED 20 .14: 20 0 : 20 2304 : 118012 N/A csp-007 general DOWN* N/A: 0 20 : 20 118012 : 118012 N/A DEAD - to be removed [ climer@2020-04-08T12:48:21 ] csp-016 general ALLOCATED 20 .02: 20 0 : 20 38140 : 118012 N/A csp-017 general ALLOCATED 20 .08: 20 0 : 20 1276 : 118012 N/A csp-018 general ALLOCATED 20 .09: 20 0 : 20 30972 : 118012 N/A csp-019 christlibuyin MIXED 8 .12: 9 11 : 20 68860 : 118012 N/A csp-020 general MIXED 7 .01: 7 13 : 20 252 : 118012 N/A csp-025 general ALLOCATED 12 .64: 20 0 : 20 1276 : 118012 N/A csp-026 christlibuyin MIXED 3 .98: 4 16 : 20 3324 : 118012 N/A css-001 colej MIXED 3 .72: 19 1 : 20 38704 : 246640 N/A css-002 colej MIXED 1 .08: 1 19 : 20 77052 : 118012 N/A css-003 plzbuyin MIXED 12 .19: 14 6 : 20 11516 : 118012 N/A css-007 albrecht MIXED 13 .28: 13 7 : 20 2928 : 246640 N/A css-008 general ALLOCATED 22 .18: 20 0 : 20 19658 : 246640 N/A css-009 general ALLOCATED 15 .18: 20 0 : 20 7024 : 246640 N/A css-010 general ALLOCATED 19 .25: 20 0 : 20 45936 : 246640 N/A css-011 general ALLOCATED 20 .17: 20 0 : 20 16518 : 246640 N/A css-012 general ALLOCATED 21 .14: 20 0 : 20 4846 : 246640 N/A css-013 general ALLOCATED 7 .22: 20 0 : 20 2178 : 246640 N/A css-014 general DOWN* N/A: 0 20 : 20 246640 : 246640 N/A Climer - Need power reset 4 -21-20 [ climer@2020-04-21T07:56:26 ] css-016 general ALLOCATED 21 .24: 20 0 : 20 41032 : 246640 N/A css-017 general ALLOCATED 20 .14: 20 0 : 20 17682 : 246640 N/A css-018 general MIXED 18 .01: 18 2 : 20 134 : 246640 N/A css-019 general ALLOCATED 20 .03: 20 0 : 20 8322 : 246640 N/A css-020 yueqibuyin MIXED 15 .87: 16 4 : 20 7420 : 118012 N/A css-023 general ALLOCATED 13 .98: 20 0 : 20 718 : 118012 N/A css-032 general ALLOCATED 21 .08: 20 0 : 20 36262 : 118012 N/A css-033 classres MIXED 8 .98: 11 9 : 20 5564 : 118012 N/A css-034 general MIXED 12 .98: 13 7 : 20 1276 : 118012 N/A css-035 general MIXED 6 .25: 5 15 : 20 3324 : 118012 N/A css-036 general ALLOCATED 7 .85: 20 0 : 20 174 : 53248 N/A css-038 general ALLOCATED 20 .26: 20 0 : 20 23552 : 53248 N/A css-039 general MIXED 17 .10: 18 2 : 20 4 : 53248 N/A css-040 general MIXED 14 .06: 19 1 : 20 548 : 53248 N/A css-041 general ALLOCATED 19 .42: 20 0 : 20 1850 : 53248 N/A css-042 general MIXED 5 .92: 18 2 : 20 274 : 53248 N/A css-043 general MIXED 7 .10: 9 11 : 20 346 : 53248 N/A css-044 general ALLOCATED 20 .13: 20 0 : 20 12028 : 53248 N/A css-045 general ALLOCATED 19 .02: 20 0 : 20 4240 : 53248 N/A css-047 general MIXED 8 .10: 18 2 : 20 2048 : 53248 N/A css-048 general ALLOCATED 6 .98: 20 0 : 20 5544 : 53248 N/A css-049 general MIXED 12 .82: 11 9 : 20 24374 : 53248 N/A css-050 general MIXED 5 .01: 14 6 : 20 0 : 53248 N/A css-052 general MIXED 16 .22: 19 1 : 20 12188 : 53248 N/A css-053 general DOWN+DRAIN 0 .01: 0 20 : 20 53248 : 53248 N/A DEAD - to be removed [ climer@2020-04-08T12:48:35 ] css-054 general MIXED 17 .07: 17 3 : 20 288 : 53248 N/A css-055 general MIXED 21 .75: 8 12 : 20 11336 : 53248 N/A css-056 general MIXED 20 .96: 16 4 : 20 10932 : 53248 N/A css-057 general ALLOCATED 20 .17: 20 0 : 20 19532 : 53248 N/A css-058 general ALLOCATED 19 .07: 20 0 : 20 14344 : 53248 N/A css-059 general MIXED 7 .10: 7 13 : 20 346 : 53248 N/A css-060 general ALLOCATED 16 .04: 20 0 : 20 8740 : 53248 N/A css-061 general MIXED 3 .06: 3 17 : 20 1298 : 53248 N/A css-062 general ALLOCATED 20 .14: 20 0 : 20 12912 : 53248 N/A css-063 general ALLOCATED 15 .98: 20 0 : 20 144 : 53248 N/A css-064 general ALLOCATED 20 .10: 20 0 : 20 37152 : 53248 N/A css-065 general ALLOCATED 10 .01: 20 0 : 20 2668 : 53248 N/A css-066 general MIXED 18 .05: 19 1 : 20 552 : 53248 N/A css-067 yueqibuyin MIXED 1 .01: 1 19 : 20 12288 : 53248 N/A css-071 general DOWN*+DRAIN N/A: 0 20 : 20 215232 : 215232 N/A DEAD - to be removed [ climer@2020-04-08T12:47:23 ] css-072 colej MIXED 6 .85: 7 13 : 20 0 : 53248 N/A css-074 yueqibuyin MIXED 2 .82: 3 17 : 20 4096 : 53248 N/A css-075 general ALLOCATED 19 .32: 20 0 : 20 32798 : 53248 N/A css-076 general DOWN* N/A: 0 20 : 20 53248 : 53248 N/A DEAD - to be removed [ climer@2020-04-08T12:47:14 ] css-079 general DOWN*+DRAIN N/A: 0 20 : 20 53248 : 53248 N/A Climer - Need power reset - 4 -8-20 [ climer@2020-04-08T12:46:08 ] css-080 general DOWN*+DRAIN N/A: 0 20 : 20 53248 : 53248 N/A Climer - Need power reset - 4 -8-20 [ climer@2020-04-08T12:46:20 ] css-081 toulson MIXED 6 .32: 9 11 : 20 7360 : 53248 N/A css-082 yueqibuyin DOWN* N/A: 0 20 : 20 53248 : 53248 N/A Climer - waiting Power reset 4 -8-20 [ climer@2020-04-08T12:51:51 ] css-083 general MIXED 10 .40: 18 2 : 20 424 : 53248 N/A css-084 toulson MIXED 3 .06: 3 17 : 20 4096 : 53248 N/A css-085 toulson MIXED 2 .98: 3 17 : 20 4096 : 53248 N/A css-087 general ALLOCATED 16 .09: 20 0 : 20 1644 : 53248 N/A css-088 general ALLOCATED 20 .27: 20 0 : 20 1374 : 53248 N/A css-089 general MIXED 10 .10: 16 4 : 20 1442 : 53248 N/A css-090 manning MIXED 6 .33: 9 11 : 20 7360 : 53248 N/A css-091 manning MIXED 2 .98: 3 17 : 20 4096 : 53248 N/A css-092 general MIXED 14 .08: 18 2 : 20 20 : 53248 N/A css-093 general ALLOCATED 19 .10: 20 0 : 20 42798 : 53248 N/A css-094 general MIXED 18 .16: 17 3 : 20 698 : 53248 N/A css-095 general MIXED 5 .02: 15 5 : 20 0 : 53248 N/A css-097 general ALLOCATED 20 .11: 20 0 : 20 24576 : 53248 N/A css-098 general MIXED 15 .08: 15 5 : 20 692 : 53248 N/A css-099 general ALLOCATED 17 .07: 20 0 : 20 9764 : 53248 N/A css-100 cukier MIXED 3 .00: 3 17 : 20 4096 : 53248 N/A css-101 cukier MIXED 3 .04: 3 17 : 20 4096 : 53248 N/A css-102 black MIXED 2 .82: 3 17 : 20 4096 : 53248 N/A css-103 black MIXED 6 .82: 7 13 : 20 0 : 53248 N/A css-106 yueqibuyin MIXED 2 .13: 2 18 : 20 20480 : 53248 N/A css-107 general ALLOCATED 14 .03: 20 0 : 20 8196 : 53248 N/A css-108 yueqibuyin MIXED 14 .01: 14 6 : 20 0 : 53248 N/A css-109 yueqibuyin MIXED 1 .01: 3 17 : 20 4288 : 53248 N/A css-111 yueqibuyin MIXED 14 .00: 14 6 : 20 0 : 53248 N/A css-112 yueqibuyin MIXED 1 .03: 1 19 : 20 12288 : 53248 N/A css-113 yueqibuyin MIXED 2 .93: 3 17 : 20 4096 : 53248 N/A css-114 yueqibuyin MIXED 14 .22: 14 6 : 20 0 : 53248 N/A css-115 yueqibuyin MIXED 14 .03: 14 6 : 20 0 : 53248 N/A css-116 yueqibuyin MIXED 1 .02: 1 19 : 20 12288 : 53248 N/A css-117 yueqibuyin MIXED 1 .08: 1 19 : 20 12288 : 53248 N/A css-118 general MIXED 17 .15: 19 1 : 20 92 : 53248 N/A css-119 yueqibuyin MIXED 14 .01: 14 6 : 20 0 : 53248 N/A css-120 yueqibuyin MIXED 5 .78: 8 12 : 20 20480 : 53248 N/A css-121 general MIXED 14 .01: 16 4 : 20 1442 : 53248 N/A css-122 baek MIXED 2 .98: 3 17 : 20 4096 : 53248 N/A css-123 baek MIXED 1 .10: 1 19 : 20 12288 : 53248 N/A css-124 general ALLOCATED 20 .26: 20 0 : 20 4312 : 53248 N/A css-125 aeimit MIXED 5 .06: 5 15 : 20 8192 : 53248 N/A css-126 general ALLOCATED 16 .13: 20 0 : 20 6418 : 53248 N/A css-127 toulson MIXED 1 .02: 1 19 : 20 12288 : 53248 N/A cs* = > 33 .3% ( buyin ) 91 .4% ( 162 ) 43 .6%: 59 .5% ( 3240 ) 69 .9% ( 17 .0Tb ) 97 % ( 78 ) Usage% ( Total ) lac-000 deyoungbuyin ALLOCATED 0 .06: 28 0 : 28 6012 : 118012 N/A lac-001 deyoungbuyin ALLOCATED 0 .07: 28 0 : 28 6012 : 118012 N/A lac-002 deyoungbuyin ALLOCATED 0 .11: 28 0 : 28 6012 : 118012 N/A lac-003 deyoungbuyin MIXED 25 .86: 26 2 : 28 5372 : 118012 N/A lac-004 deyoungbuyin ALLOCATED 0 .15: 28 0 : 28 6012 : 118012 N/A lac-005 deyoungbuyin ALLOCATED 0 .11: 28 0 : 28 6012 : 118012 N/A lac-006 deyoungbuyin DOWN+DRAIN 1 .00: 0 28 : 28 118012 : 118012 N/A NHC: check_hw_ib: No IB port is ACTIVE ( LinkUp 56 Gb/sec ) . [ root@2020-04-20T08:18:40 ] lac-007 deyoungbuyin ALLOCATED 0 .15: 28 0 : 28 6012 : 118012 N/A lac-008 deyoungbuyin ALLOCATED 0 .17: 28 0 : 28 6012 : 118012 N/A lac-009 deyoungbuyin ALLOCATED 0 .12: 28 0 : 28 6012 : 118012 N/A lac-010 deyoungbuyin ALLOCATED 0 .08: 28 0 : 28 6012 : 118012 N/A lac-011 deyoungbuyin ALLOCATED 0 .08: 28 0 : 28 6012 : 118012 N/A lac-012 deyoungbuyin ALLOCATED 0 .22: 28 0 : 28 6012 : 118012 N/A lac-013 deyoungbuyin ALLOCATED 0 .07: 28 0 : 28 6012 : 118012 N/A lac-014 deyoungbuyin ALLOCATED 0 .07: 28 0 : 28 6012 : 118012 N/A lac-015 deyoungbuyin ALLOCATED 0 .08: 28 0 : 28 6012 : 118012 N/A lac-016 deyoungbuyin ALLOCATED 0 .17: 28 0 : 28 6012 : 118012 N/A lac-017 deyoungbuyin ALLOCATED 0 .05: 28 0 : 28 6012 : 118012 N/A lac-018 deyoungbuyin ALLOCATED 0 .09: 28 0 : 28 6012 : 118012 N/A lac-019 deyoungbuyin ALLOCATED 0 .10: 28 0 : 28 6012 : 118012 N/A lac-020 deyoungbuyin ALLOCATED 0 .09: 28 0 : 28 6012 : 118012 N/A lac-021 deyoungbuyin ALLOCATED 0 .10: 28 0 : 28 6012 : 118012 N/A lac-022 deyoungbuyin ALLOCATED 0 .11: 28 0 : 28 6012 : 118012 N/A lac-023 deyoungbuyin ALLOCATED 0 .15: 28 0 : 28 6012 : 118012 N/A lac-024 deyoungbuyin MIXED 1 .39: 10 18 : 28 86640 : 246640 k80 ( 4 :8 ) lac-025 DicksonLab MIXED 6 .53: 16 12 : 28 6768 : 246640 k80 ( 1 :8 ) lac-026 DicksonLab MIXED 6 .98: 16 12 : 28 6768 : 246640 k80 ( 1 :8 ) lac-027 DicksonLab MIXED 6 .73: 16 12 : 28 6768 : 246640 k80 ( 1 :8 ) lac-028 DicksonLab MIXED 6 .79: 16 12 : 28 6768 : 246640 k80 ( 1 :8 ) lac-029 DicksonLab MIXED 3 .62: 13 15 : 28 55920 : 246640 k80 ( 1 :8 ) lac-030 feig-covid MIXED 1 .24: 14 14 : 28 66640 : 246640 k80 ( 3 :8 ) lac-031 merzjrke MIXED 4 .10: 10 18 : 28 197488 : 246640 k80 ( 0 :8 ) lac-032 allenmc MIXED 5 .15: 5 23 : 28 15612 : 118012 N/A lac-033 allenmc MIXED 5 .10: 5 23 : 28 15612 : 118012 N/A lac-034 allenmc MIXED 0 .29: 26 2 : 28 5372 : 118012 N/A lac-035 allenmc MIXED 7 .98: 26 2 : 28 5372 : 118012 N/A lac-036 allenmc ALLOCATED 28 .32: 28 0 : 28 60668 : 118012 N/A lac-037 allenmc ALLOCATED 28 .19: 28 0 : 28 60668 : 118012 N/A lac-038 general ALLOCATED 23 .36: 28 0 : 28 33028 : 118012 N/A lac-039 general ALLOCATED 24 .67: 28 0 : 28 260 : 118012 N/A lac-040 general MIXED 21 .14: 26 2 : 28 252 : 118012 N/A lac-041 general MIXED 9 .10: 9 19 : 28 252 : 118012 N/A lac-042 general MIXED 22 .86: 25 3 : 28 252 : 118012 N/A lac-043 general MIXED 20 .15: 21 7 : 28 252 : 118012 N/A lac-044 general MIXED 11 .51: 21 7 : 28 4540 : 118012 N/A lac-045 ptg MIXED 5 .22: 7 21 : 28 7612 : 118012 N/A lac-046 ptg MIXED 25 .51: 26 2 : 28 5372 : 118012 N/A lac-047 ptg MIXED 5 .28: 7 21 : 28 7612 : 118012 N/A lac-048 ptg MIXED 12 .23: 12 16 : 28 8444 : 118012 N/A lac-049 ptg MIXED 12 .37: 14 14 : 28 444 : 118012 N/A lac-050 ptg MIXED 11 .05: 13 15 : 28 1468 : 118012 N/A lac-051 ptg MIXED 5 .09: 5 23 : 28 15612 : 118012 N/A lac-052 ptg MIXED 5 .06: 7 21 : 28 7612 : 118012 N/A lac-053 ptg MIXED 5 .10: 5 23 : 28 15612 : 118012 N/A lac-054 ptg MIXED 12 .20: 12 16 : 28 8444 : 118012 N/A lac-055 ptg MIXED 12 .10: 14 14 : 28 444 : 118012 N/A lac-056 ptg MIXED 12 .41: 14 14 : 28 444 : 118012 N/A lac-057 ptg MIXED 12 .21: 14 14 : 28 444 : 118012 N/A lac-058 ptg MIXED 7 .08: 9 19 : 28 5564 : 118012 N/A lac-059 ptg ALLOCATED 23 .79: 28 0 : 28 15612 : 118012 N/A lac-060 ptg ALLOCATED 12 .93: 28 0 : 28 15612 : 118012 N/A lac-061 ptg ALLOCATED 13 .39: 28 0 : 28 15612 : 118012 N/A lac-062 ptg ALLOCATED 10 .32: 28 0 : 28 15612 : 118012 N/A lac-063 ptg ALLOCATED 12 .18: 28 0 : 28 15612 : 118012 N/A lac-064 ptg ALLOCATED 10 .46: 28 0 : 28 15612 : 118012 N/A lac-065 ptg ALLOCATED 16 .00: 28 0 : 28 15612 : 118012 N/A lac-066 ptg ALLOCATED 13 .01: 28 0 : 28 15612 : 118012 N/A lac-067 ptg MIXED 5 .36: 5 23 : 28 15612 : 118012 N/A lac-068 ptg ALLOCATED 21 .58: 28 0 : 28 15612 : 118012 N/A lac-069 ptg MIXED 12 .17: 14 14 : 28 444 : 118012 N/A lac-070 ptg MIXED 12 .19: 12 16 : 28 8444 : 118012 N/A lac-071 ptg MIXED 12 .08: 12 16 : 28 8444 : 118012 N/A lac-072 ptg MIXED 12 .17: 14 14 : 28 444 : 118012 N/A lac-073 ptg MIXED 7 .14: 7 21 : 28 13564 : 118012 N/A lac-074 ptg MIXED 5 .08: 8 20 : 28 2492 : 118012 N/A lac-075 ptg MIXED 5 .04: 7 21 : 28 7612 : 118012 N/A lac-076 ptg ALLOCATED 25 .55: 28 0 : 28 15612 : 118012 N/A lac-077 ptg MIXED 5 .18: 7 21 : 28 7612 : 118012 N/A lac-078 general MIXED 11 .38: 8 20 : 28 69884 : 118012 N/A lac-079 ptg ALLOCATED 22 .37: 28 0 : 28 15612 : 118012 N/A lac-080 merzjrke MIXED 2 .48: 16 12 : 28 50032 : 246640 k80 ( 0 :8 ) lac-081 merzjrke MIXED 13 .85: 23 5 : 28 9072 : 246640 k80 ( 0 :8 ) lac-082 merzjrke MIXED 5 .85: 18 10 : 28 131952 : 246640 k80 ( 0 :8 ) lac-083 merzjrke MIXED 4 .74: 19 9 : 28 880 : 246640 k80 ( 0 :8 ) lac-084 merzjrke MIXED 3 .43: 4 24 : 28 238448 : 246640 k80 ( 4 :8 ) lac-085 merzjrke MIXED 4 .68: 19 9 : 28 880 : 246640 k80 ( 0 :8 ) lac-086 merzjrke MIXED 1 .20: 16 12 : 28 50032 : 246640 k80 ( 0 :8 ) lac-087 general MIXED 18 .91: 19 9 : 28 9072 : 246640 k80 ( 0 :8 ) lac-088 ptg MIXED 4 .03: 6 22 : 28 7612 : 118012 N/A lac-089 ptg MIXED 4 .04: 6 22 : 28 7612 : 118012 N/A lac-090 ptg MIXED 4 .06: 6 22 : 28 7612 : 118012 N/A lac-091 ptg MIXED 4 .01: 6 22 : 28 7612 : 118012 N/A lac-092 ptg MIXED 4 .06: 6 22 : 28 7612 : 118012 N/A lac-093 ptg ALLOCATED 9 .42: 28 0 : 28 15612 : 118012 N/A lac-094 ptg ALLOCATED 14 .45: 28 0 : 28 15612 : 118012 N/A lac-095 ptg ALLOCATED 12 .50: 28 0 : 28 15612 : 118012 N/A lac-096 ptg ALLOCATED 6 .97: 28 0 : 28 15612 : 118012 N/A lac-097 ptg ALLOCATED 8 .46: 28 0 : 28 15612 : 118012 N/A lac-098 ptg ALLOCATED 6 .59: 28 0 : 28 15612 : 118012 N/A lac-099 ptg ALLOCATED 7 .12: 28 0 : 28 15612 : 118012 N/A lac-100 ptg ALLOCATED 19 .14: 28 0 : 28 15612 : 118012 N/A lac-101 ptg ALLOCATED 17 .22: 28 0 : 28 15612 : 118012 N/A lac-102 ptg MIXED 11 .19: 13 15 : 28 444 : 118012 N/A lac-103 ptg MIXED 7 .07: 9 19 : 28 4540 : 118012 N/A lac-104 ptg MIXED 5 .12: 7 21 : 28 444 : 118012 N/A lac-105 ptg MIXED 8 .21: 8 20 : 28 5372 : 118012 N/A lac-106 ptg ALLOCATED 20 .78: 28 0 : 28 15612 : 118012 N/A lac-107 ptg MIXED 11 .02: 13 15 : 28 444 : 118012 N/A lac-108 ptg MIXED 11 .22: 13 15 : 28 444 : 118012 N/A lac-109 ptg MIXED 11 .15: 11 17 : 28 8444 : 118012 N/A lac-110 ptg MIXED 11 .18: 13 15 : 28 444 : 118012 N/A lac-111 ptg MIXED 11 .17: 11 17 : 28 8444 : 118012 N/A lac-112 ptg MIXED 11 .09: 13 15 : 28 444 : 118012 N/A lac-113 ptg MIXED 11 .23: 13 15 : 28 444 : 118012 N/A lac-114 ptg MIXED 11 .01: 11 17 : 28 8444 : 118012 N/A lac-115 ptg MIXED 8 .07: 10 18 : 28 3516 : 118012 N/A lac-116 ptg MIXED 4 .89: 7 21 : 28 444 : 118012 N/A lac-117 ptg MIXED 7 .20: 7 21 : 28 37116 : 118012 N/A lac-118 ptg MIXED 11 .05: 13 15 : 28 444 : 118012 N/A lac-119 ptg MIXED 11 .16: 13 15 : 28 444 : 118012 N/A lac-120 ptg MIXED 4 .19: 4 24 : 28 40188 : 118012 N/A lac-121 ptg MIXED 5 .08: 7 21 : 28 6588 : 118012 N/A lac-122 ptg MIXED 4 .99: 5 23 : 28 8444 : 118012 N/A lac-123 general ALLOCATED 29 .19: 28 0 : 28 38904 : 118012 N/A lac-124 ptg MIXED 5 .18: 5 23 : 28 15612 : 118012 N/A lac-125 ptg MIXED 5 .07: 7 21 : 28 7612 : 118012 N/A lac-126 ptg MIXED 5 .29: 7 21 : 28 7612 : 118012 N/A lac-127 ptg ALLOCATED 22 .09: 28 0 : 28 15612 : 118012 N/A lac-128 ptg ALLOCATED 24 .32: 28 0 : 28 15612 : 118012 N/A lac-129 ptg MIXED 5 .15: 7 21 : 28 7612 : 118012 N/A lac-130 ptg ALLOCATED 24 .86: 28 0 : 28 15612 : 118012 N/A lac-131 ptg ALLOCATED 10 .80: 28 0 : 28 15612 : 118012 N/A lac-132 ptg MIXED 12 .01: 14 14 : 28 444 : 118012 N/A lac-133 ptg MIXED 12 .22: 14 14 : 28 444 : 118012 N/A lac-134 ptg MIXED 11 .25: 13 15 : 28 1468 : 118012 N/A lac-135 ptg MIXED 5 .20: 7 21 : 28 7612 : 118012 N/A lac-136 merzjrke MIXED+DRAIN 56 .36: 4 24 : 28 238448 : 246640 k80 ( 4 :8 ) NHC: Script timed out while executing \"icer_check_gpu_count 8 syslog die\" . [ root@2020-04-22T04:03:33 ] lac-137 feig-covid MIXED 3 .69: 13 15 : 28 74480 : 246640 k80 ( 3 :8 ) lac-138 merzjrke MIXED 9 .92: 1 27 : 28 181104 : 246640 k80 ( 4 :8 ) lac-139 merzjrke MIXED 17 .77: 11 17 : 28 66416 : 246640 k80 ( 0 :8 ) lac-140 merzjrke MIXED 15 .97: 24 4 : 28 206256 : 246640 k80 ( 0 :8 ) lac-141 merzjrke MIXED 5 .56: 8 20 : 28 181104 : 246640 k80 ( 0 :8 ) lac-142 merzjrke MIXED 17 .78: 8 20 : 28 230256 : 246640 k80 ( 0 :8 ) lac-143 general MIXED 11 .22: 11 17 : 28 7024 : 246640 k80 ( 7 :8 ) lac-144 ptg MIXED 4 .84: 7 21 : 28 444 : 118012 N/A lac-145 ptg MIXED 5 .07: 7 21 : 28 444 : 118012 N/A lac-146 ptg MIXED 5 .11: 5 23 : 28 8444 : 118012 N/A lac-147 ptg MIXED 4 .62: 7 21 : 28 444 : 118012 N/A lac-148 ptg MIXED 5 .05: 7 21 : 28 444 : 118012 N/A lac-149 ptg MIXED 5 .10: 7 21 : 28 444 : 118012 N/A lac-150 ptg MIXED 5 .03: 7 21 : 28 444 : 118012 N/A lac-151 ptg MIXED 5 .15: 5 23 : 28 8444 : 118012 N/A lac-152 ptg MIXED 5 .09: 7 21 : 28 444 : 118012 N/A lac-153 ptg MIXED 5 .02: 7 21 : 28 444 : 118012 N/A lac-154 ptg MIXED 5 .04: 7 21 : 28 444 : 118012 N/A lac-155 ptg MIXED 10 .08: 13 15 : 28 444 : 118012 N/A lac-156 ptg MIXED 5 .51: 7 21 : 28 444 : 118012 N/A lac-157 ptg MIXED 5 .06: 7 21 : 28 444 : 118012 N/A lac-158 ptg MIXED 5 .27: 5 23 : 28 8444 : 118012 N/A lac-159 ptg MIXED 5 .01: 7 21 : 28 444 : 118012 N/A lac-160 ptg MIXED 4 .26: 6 22 : 28 7612 : 118012 N/A lac-161 ptg MIXED 6 .34: 6 22 : 28 7420 : 118012 N/A lac-162 ptg MIXED 11 .08: 13 15 : 28 444 : 118012 N/A lac-163 ptg MIXED 11 .07: 13 15 : 28 444 : 118012 N/A lac-164 ptg MIXED 11 .26: 11 17 : 28 8444 : 118012 N/A lac-165 ptg MIXED 11 .11: 13 15 : 28 444 : 118012 N/A lac-166 ptg MIXED 11 .10: 13 15 : 28 444 : 118012 N/A lac-167 ptg MIXED 11 .07: 11 17 : 28 8444 : 118012 N/A lac-168 ptg MIXED 11 .37: 13 15 : 28 444 : 118012 N/A lac-169 ptg MIXED 11 .21: 13 15 : 28 444 : 118012 N/A lac-170 ptg MIXED 11 .15: 13 15 : 28 444 : 118012 N/A lac-171 ptg MIXED 11 .28: 13 15 : 28 444 : 118012 N/A lac-172 ptg MIXED 11 .06: 13 15 : 28 444 : 118012 N/A lac-173 ptg MIXED 11 .28: 13 15 : 28 444 : 118012 N/A lac-174 ptg MIXED 11 .21: 13 15 : 28 444 : 118012 N/A lac-175 ptg MIXED 11 .04: 13 15 : 28 444 : 118012 N/A lac-176 ptg MIXED 11 .07: 13 15 : 28 444 : 118012 N/A lac-177 ptg MIXED 12 .03: 14 14 : 28 444 : 118012 N/A lac-178 ptg MIXED 11 .10: 11 17 : 28 8444 : 118012 N/A lac-179 ptg MIXED 11 .04: 13 15 : 28 444 : 118012 N/A lac-180 ptg MIXED 11 .07: 13 15 : 28 444 : 118012 N/A lac-181 ptg MIXED 10 .06: 10 18 : 28 9468 : 118012 N/A lac-182 ptg MIXED 4 .24: 6 22 : 28 7612 : 118012 N/A lac-183 ptg MIXED 4 .01: 6 22 : 28 7612 : 118012 N/A lac-184 ptg MIXED 4 .05: 6 22 : 28 7612 : 118012 N/A lac-185 ptg MIXED 4 .05: 6 22 : 28 7612 : 118012 N/A lac-186 ptg MIXED 4 .04: 6 22 : 28 7612 : 118012 N/A lac-187 ptg MIXED 4 .23: 6 22 : 28 7612 : 118012 N/A lac-188 ptg MIXED 4 .24: 6 22 : 28 7612 : 118012 N/A lac-189 ptg ALLOCATED 8 .34: 28 0 : 28 15612 : 118012 N/A lac-190 ptg ALLOCATED 7 .25: 28 0 : 28 15612 : 118012 N/A lac-191 ptg ALLOCATED 8 .49: 28 0 : 28 15612 : 118012 N/A lac-192 general MIXED 5 .92: 15 13 : 28 23280 : 246640 k80 ( 2 :8 ) lac-193 guowei-search MIXED 2 .09: 22 6 : 28 33872 : 246640 k80 ( 3 :8 ) lac-194 guowei-search MIXED 6 .74: 20 8 : 28 2320 : 246640 k80 ( 5 :8 ) lac-195 guowei-search MIXED 7 .76: 17 11 : 28 1840 : 246640 k80 ( 6 :8 ) lac-196 guowei-search MIXED 3 .95: 17 11 : 28 13392 : 246640 k80 ( 3 :8 ) lac-197 guowei-search MIXED 3 .65: 17 11 : 28 13392 : 246640 k80 ( 3 :8 ) lac-198 general MIXED 0 .78: 10 18 : 28 86640 : 246640 k80 ( 4 :8 ) lac-199 general MIXED 1 .98: 11 17 : 28 66160 : 246640 k80 ( 3 :8 ) lac-200 ptg MIXED 11 .07: 13 15 : 28 444 : 118012 N/A lac-201 ptg MIXED 11 .21: 13 15 : 28 444 : 118012 N/A lac-202 ptg MIXED 11 .10: 13 15 : 28 444 : 118012 N/A lac-203 ptg MIXED 11 .08: 13 15 : 28 444 : 118012 N/A lac-204 ptg MIXED 11 .05: 13 15 : 28 444 : 118012 N/A lac-205 ptg MIXED 11 .21: 13 15 : 28 444 : 118012 N/A lac-206 ptg MIXED 11 .01: 13 15 : 28 444 : 118012 N/A lac-207 ptg MIXED 11 .01: 13 15 : 28 444 : 118012 N/A lac-208 ptg MIXED 8 .06: 10 18 : 28 3516 : 118012 N/A lac-209 general MIXED 3 .43: 10 18 : 28 27900 : 118012 N/A lac-210 ptg ALLOCATED 8 .52: 28 0 : 28 15612 : 118012 N/A lac-211 ptg ALLOCATED 8 .60: 28 0 : 28 15612 : 118012 N/A lac-212 ptg ALLOCATED 7 .35: 28 0 : 28 15612 : 118012 N/A lac-213 ptg ALLOCATED 7 .67: 28 0 : 28 15612 : 118012 N/A lac-214 ptg MIXED 26 .31: 26 2 : 28 5372 : 118012 N/A lac-215 tonggao MIXED 22 .49: 26 2 : 28 7612 : 118012 N/A lac-216 weilai ALLOCATED 24 .65: 28 0 : 28 5372 : 118012 N/A lac-217 general ALLOCATED 23 .03: 28 0 : 28 4420 : 118012 N/A lac-218 weilai ALLOCATED 25 .05: 28 0 : 28 5372 : 118012 N/A lac-219 weilai ALLOCATED 25 .12: 28 0 : 28 5372 : 118012 N/A lac-220 weilai MIXED 25 .44: 25 3 : 28 5372 : 118012 N/A lac-221 weilai ALLOCATED 25 .26: 28 0 : 28 5372 : 118012 N/A lac-222 weilai MIXED 16 .52: 26 2 : 28 5372 : 118012 N/A lac-223 weilai MIXED 8 .72: 9 19 : 28 1276 : 118012 N/A lac-224 weilai ALLOCATED 25 .00: 28 0 : 28 134000 : 246640 N/A lac-225 general MIXED 12 .28: 23 5 : 28 2928 : 246640 N/A lac-228 general MIXED 16 .26: 16 12 : 28 62320 : 246640 N/A lac-229 bazil MIXED 23 .93: 24 4 : 28 880 : 246640 N/A lac-230 general MIXED 41 .60: 18 10 : 28 131952 : 246640 N/A lac-231 general ALLOCATED 14 .49: 28 0 : 28 13168 : 246640 N/A lac-232 general MIXED 23 .93: 24 4 : 28 884 : 246640 N/A lac-233 general MIXED 26 .96: 24 4 : 28 8052 : 246640 N/A lac-234 general MIXED 2 .12: 2 26 : 28 164720 : 246640 N/A lac-235 general MIXED 5 .08: 5 23 : 28 74608 : 246640 N/A lac-236 cmich ALLOCATED 0 .53: 28 0 : 28 134640 : 246640 N/A lac-237 cmich ALLOCATED 4 .72: 28 0 : 28 34032 : 246640 N/A lac-238 cmich ALLOCATED 14 .00: 28 0 : 28 34800 : 246640 N/A lac-239 cmich MIXED 14 .06: 18 10 : 28 1264 : 246640 N/A lac-240 cmich ALLOCATED 8 .02: 28 0 : 28 35568 : 246640 N/A lac-241 cmich MIXED 15 .06: 15 13 : 28 880 : 246640 N/A lac-242 cmich MIXED 15 .28: 15 13 : 28 880 : 246640 N/A lac-243 cmich MIXED 15 .10: 15 13 : 28 880 : 246640 N/A lac-244 cmich MIXED 6 .06: 6 22 : 28 148336 : 246640 N/A lac-245 cmich MIXED 8 .08: 8 20 : 28 115568 : 246640 N/A lac-246 general ALLOCATED 16 .25: 28 0 : 28 17264 : 246640 N/A lac-247 general MIXED 6 .10: 6 22 : 28 79728 : 246640 N/A lac-248 iceradmin DOWN+DRAIN 0 .06: 0 28 : 28 246640 : 246640 N/A NHC: check_fs_mount: /mnt/gs18 not mounted [ root@2020-04-14T11:10:52 ] lac-250 lirac ALLOCATED 22 .19: 28 0 : 28 37152 : 503520 N/A lac-251 wenhuang ALLOCATED 26 .65: 28 0 : 28 382880 : 503520 N/A lac-252 general MIXED 0 .20: 18 10 : 28 431520 : 503520 N/A lac-253 general MIXED 14 .81: 15 13 : 28 257760 : 503520 N/A lac-254 beacon MIXED 6 .78: 7 21 : 28 3324 : 118012 N/A lac-255 chomiuk MIXED 10 .33: 13 15 : 28 3324 : 118012 N/A lac-256 quantgen ALLOCATED 4 .10: 28 0 : 28 290912 : 503520 N/A lac-257 hirn ALLOCATED 17 .81: 28 0 : 28 279200 : 503520 N/A lac-258 hirn ALLOCATED 28 .15: 28 0 : 28 216800 : 503520 N/A lac-259 quantgen ALLOCATED 28 .28: 28 0 : 28 216800 : 503520 N/A lac-260 quantgen ALLOCATED 5 .17: 28 0 : 28 391520 : 503520 N/A lac-261 ccg MIXED 25 .99: 25 3 : 28 260000 : 503520 N/A lac-276 general MIXED 15 .60: 16 12 : 28 598 : 118012 N/A lac-277 general ALLOCATED 20 .14: 28 0 : 28 56 : 118012 N/A lac-278 general MIXED 6 .11: 6 22 : 28 880 : 246640 N/A lac-279 general MIXED 15 .65: 25 3 : 28 2928 : 246640 N/A lac-280 general MIXED 18 .05: 15 13 : 28 880 : 246640 N/A lac-281 general MIXED 26 .14: 27 1 : 28 48 : 246640 N/A lac-282 general MIXED 6 .05: 6 22 : 28 880 : 246640 N/A lac-283 general MIXED 5 .02: 5 23 : 28 74608 : 246640 N/A lac-284 general ALLOCATED 25 .07: 28 0 : 28 4980 : 246640 N/A lac-285 qian MIXED 6 .10: 27 1 : 28 880 : 246640 N/A lac-286 scbbuyin ALLOCATED 7 .98: 28 0 : 28 17264 : 246640 k80 ( 8 :8 ) lac-287 general MIXED 1 .97: 11 17 : 28 70256 : 246640 k80 ( 3 :8 ) lac-288 general ALLOCATED 5 .97: 28 0 : 28 52080 : 246640 k80 ( 6 :8 ) lac-289 general MIXED 1 .02: 10 18 : 28 86640 : 246640 k80 ( 4 :8 ) lac-290 general MIXED 1 .10: 10 18 : 28 86640 : 246640 k80 ( 4 :8 ) lac-291 merzjrke MIXED 5 .79: 8 20 : 28 181104 : 246640 k80 ( 0 :8 ) lac-292 general MIXED 0 .86: 10 18 : 28 86640 : 246640 k80 ( 4 :8 ) lac-293 general MIXED 0 .65: 10 18 : 28 86640 : 246640 k80 ( 4 :8 ) lac-294 wang-krishnan MIXED 16 .83: 27 1 : 28 4976 : 246640 N/A lac-295 wang-krishnan MIXED 15 .76: 26 2 : 28 21360 : 246640 N/A lac-296 wang-krishnan MIXED 16 .59: 26 2 : 28 21360 : 246640 N/A lac-297 wang-krishnan MIXED 17 .39: 27 1 : 28 4976 : 246640 N/A lac-298 wang-krishnan MIXED 17 .44: 26 2 : 28 21360 : 246640 N/A lac-299 wang-krishnan MIXED 15 .17: 26 2 : 28 21360 : 246640 N/A lac-300 general MIXED 3 .06: 3 25 : 28 123760 : 246640 N/A lac-301 general MIXED 16 .10: 16 12 : 28 181104 : 246640 N/A lac-302 ccg MIXED 24 .21: 24 4 : 28 12000 : 503520 N/A lac-303 ccg MIXED 24 .06: 24 4 : 28 12000 : 503520 N/A lac-304 ccg MIXED 24 .13: 24 4 : 28 12000 : 503520 N/A lac-305 ccg MIXED 24 .05: 24 4 : 28 12000 : 503520 N/A lac-306 general ALLOCATED 28 .24: 28 0 : 28 216800 : 503520 N/A lac-307 ccg MIXED 24 .09: 26 2 : 28 4000 : 503520 N/A lac-308 ccg MIXED 24 .08: 26 2 : 28 4000 : 503520 N/A lac-309 ccg MIXED 24 .13: 26 2 : 28 4000 : 503520 N/A lac-310 ccg ALLOCATED 7 .82: 28 0 : 28 224992 : 503520 N/A lac-311 oakland-universi MIXED 24 .11: 24 4 : 28 12000 : 503520 N/A lac-312 oakland-universi MIXED 24 .10: 24 4 : 28 12000 : 503520 N/A lac-313 oakland-universi MIXED 24 .29: 24 4 : 28 12000 : 503520 N/A lac-314 oakland-universi ALLOCATED 2 .17: 28 0 : 28 349920 : 503520 N/A lac-315 ccg MIXED 24 .10: 24 4 : 28 12000 : 503520 N/A lac-316 cmich ALLOCATED 7 .50: 28 0 : 28 358112 : 503520 N/A lac-317 cmich ALLOCATED 0 .55: 28 0 : 28 391520 : 503520 N/A lac-318 cmich MIXED 21 .51: 26 2 : 28 5372 : 118012 N/A lac-319 cmich MIXED 15 .65: 16 12 : 28 252 : 118012 N/A lac-320 cmich MIXED 10 .17: 10 18 : 28 252 : 118012 N/A lac-321 cmich MIXED 10 .09: 10 18 : 28 252 : 118012 N/A lac-322 cmich MIXED 8 .04: 8 20 : 28 2300 : 118012 N/A lac-323 cmich MIXED 4 .15: 16 12 : 28 4476 : 118012 N/A lac-324 cmich MIXED 12 .69: 26 2 : 28 5372 : 118012 N/A lac-325 cmich ALLOCATED 0 .41: 28 0 : 28 6012 : 118012 N/A lac-326 cmich MIXED 7 .04: 7 21 : 28 3324 : 118012 N/A lac-327 cmich MIXED 0 .25: 26 2 : 28 5372 : 118012 N/A lac-328 cmich MIXED 12 .52: 26 2 : 28 5372 : 118012 N/A lac-329 cmich MIXED 9 .17: 13 15 : 28 636 : 118012 N/A lac-330 beacon MIXED 12 .16: 14 14 : 28 444 : 118012 N/A lac-331 beacon MIXED 12 .24: 14 14 : 28 444 : 118012 N/A lac-332 beacon MIXED 12 .12: 14 14 : 28 444 : 118012 N/A lac-333 beacon MIXED 11 .12: 13 15 : 28 1468 : 118012 N/A lac-334 ptg ALLOCATED 1 .82: 28 0 : 28 15612 : 118012 N/A lac-335 ptg ALLOCATED 1 .92: 28 0 : 28 15612 : 118012 N/A lac-336 general MIXED 20 .78: 21 7 : 28 3324 : 118012 N/A lac-337 general MIXED 12 .59: 26 2 : 28 252 : 118012 N/A lac-338 general MIXED 15 .39: 27 1 : 28 3328 : 118012 N/A lac-339 general ALLOCATED 13 .06: 28 0 : 28 29948 : 118012 N/A lac-340 ptg ALLOCATED 1 .87: 28 0 : 28 15612 : 118012 N/A lac-341 ptg ALLOCATED 1 .64: 28 0 : 28 15612 : 118012 N/A lac-342 general MIXED 1 .49: 14 14 : 28 66640 : 246640 k80 ( 3 :8 ) lac-343 merzjrke MIXED 33 .27: 25 3 : 28 124336 : 246640 k80 ( 0 :8 ) lac-344 merzjrke MIXED 12 .17: 22 6 : 28 25456 : 246640 k80 ( 0 :8 ) lac-345 merzjrke MIXED+COMPL 16 .13: 22 6 : 28 173488 : 246640 k80 ( 2 :8 ) lac-346 merzjrke MIXED 14 .60: 8 20 : 28 181104 : 246640 k80 ( 0 :8 ) lac-347 merzjrke DOWN* N/A: 0 28 : 28 246640 : 246640 k80 ( 8 :8 ) Climer - Need power reset 4 -21-20 [ climer@2020-04-21T07:58:48 ] lac-348 feig-covid MIXED 1 .12: 14 14 : 28 66640 : 246640 k80 ( 3 :8 ) lac-349 merzjrke MIXED+DRAIN 59 .62: 4 24 : 28 238448 : 246640 k80 ( 4 :8 ) NHC: Script timed out while executing \"icer_check_gpu_count 8 syslog die\" . [ root@2020-04-22T03:38:40 ] lac-350 phani MIXED 16 .54: 26 2 : 28 5372 : 118012 N/A lac-351 phani MIXED 15 .30: 26 2 : 28 5372 : 118012 N/A lac-352 phani MIXED 0 .36: 26 2 : 28 5372 : 118012 N/A lac-353 general ALLOCATED 27 .20: 28 0 : 28 1564 : 118012 N/A lac-354 general MIXED 12 .16: 15 13 : 28 7616 : 118012 N/A lac-355 general MIXED 8 .23: 23 5 : 28 252 : 118012 N/A lac-356 general MIXED 20 .96: 20 8 : 28 8444 : 118012 N/A lac-357 general MIXED 23 .99: 23 5 : 28 3328 : 118012 N/A lac-358 general MIXED 21 .31: 21 7 : 28 7420 : 118012 N/A lac-359 general MIXED 20 .98: 21 7 : 28 252 : 118012 N/A lac-360 general MIXED 12 .09: 17 11 : 28 2300 : 118012 N/A lac-361 junlin MIXED 10 .38: 13 15 : 28 3324 : 118012 N/A lac-362 junlin MIXED 6 .62: 7 21 : 28 3324 : 118012 N/A lac-363 general ALLOCATED 18 .79: 28 0 : 28 16640 : 118012 N/A lac-364 general MIXED 13 .36: 26 2 : 28 11790 : 118012 N/A lac-365 SPG ALLOCATED 28 .32: 28 0 : 28 15612 : 118012 N/A lac-366 SPG ALLOCATED 28 .36: 28 0 : 28 15612 : 118012 N/A lac-367 SPG ALLOCATED 28 .12: 28 0 : 28 15612 : 118012 N/A lac-368 hirn MIXED 4 .85: 26 2 : 28 5372 : 118012 N/A lac-369 hirn MIXED 19 .90: 26 2 : 28 5372 : 118012 N/A lac-372 general MIXED 22 .19: 26 2 : 28 1352 : 118012 N/A lac-374 general MIXED 20 .98: 22 6 : 28 256 : 118012 N/A lac-375 general MIXED 24 .19: 27 1 : 28 328 : 118012 N/A lac-376 general MIXED 23 .12: 23 5 : 28 5372 : 118012 N/A lac-377 general MIXED 26 .12: 26 2 : 28 4766 : 118012 N/A lac-378 general MIXED 11 .36: 13 15 : 28 1280 : 118012 N/A lac-379 general MIXED 13 .28: 14 14 : 28 51452 : 118012 N/A lac-380 general MIXED 18 .07: 26 2 : 28 328 : 118012 N/A lac-381 general MIXED 22 .88: 20 8 : 28 9208 : 118012 N/A lac-382 general ALLOCATED 15 .15: 28 0 : 28 1276 : 118012 N/A lac-383 general MIXED 7 .11: 16 12 : 28 85244 : 118012 N/A lac-384 general MIXED 2 .30: 2 26 : 28 75004 : 118012 N/A lac-385 general ALLOCATED 13 .14: 28 0 : 28 3612 : 118012 N/A lac-386 general ALLOCATED 24 .06: 28 0 : 28 11054 : 118012 N/A lac-387 general MIXED 16 .07: 27 1 : 28 13564 : 118012 N/A lac-388 general ALLOCATED 24 .46: 28 0 : 28 5396 : 118012 N/A lac-389 general MIXED 25 .71: 24 4 : 28 44284 : 118012 N/A lac-390 general MIXED 21 .11: 25 3 : 28 252 : 118012 N/A lac-391 general MIXED 26 .68: 27 1 : 28 6420 : 118012 N/A lac-392 general MIXED 16 .20: 16 12 : 28 256 : 118012 N/A lac-393 general MIXED 14 .26: 20 8 : 28 12540 : 118012 N/A lac-394 general MIXED 5 .98: 24 4 : 28 252 : 118012 N/A lac-395 general ALLOCATED 15 .00: 28 0 : 28 3324 : 118012 N/A lac-396 general MIXED 20 .06: 25 3 : 28 1280 : 118012 N/A lac-397 general MIXED 6 .43: 25 3 : 28 252 : 118012 N/A lac-398 general MIXED 17 .20: 17 11 : 28 11516 : 118012 N/A lac-399 general MIXED 20 .98: 25 3 : 28 6396 : 118012 N/A lac-400 tsang DOWN+DRAIN 0 .02: 0 28 : 28 118012 : 118012 N/A Testing climer-2-12-2- [ climer@2020-02-12T07:43:21 ] lac-401 general MIXED 29 .11: 24 4 : 28 4290 : 118012 N/A lac-402 general ALLOCATED 26 .10: 28 0 : 28 1300 : 118012 N/A lac-403 general MIXED 22 .28: 25 3 : 28 8444 : 118012 N/A lac-404 general ALLOCATED 25 .60: 28 0 : 28 1276 : 118012 N/A lac-405 general MIXED 9 .17: 15 13 : 28 3324 : 118012 N/A lac-406 general MIXED 17 .90: 25 3 : 28 3324 : 118012 N/A lac-407 general MIXED 21 .03: 25 3 : 28 252 : 118012 N/A lac-408 general MIXED 17 .83: 18 10 : 28 252 : 118012 N/A lac-409 general MIXED 26 .63: 24 4 : 28 3324 : 118012 N/A lac-410 general MIXED 15 .10: 18 10 : 28 252 : 118012 N/A lac-411 general MIXED 17 .07: 22 6 : 28 256 : 118012 N/A lac-412 general MIXED 17 .11: 27 1 : 28 252 : 118012 N/A lac-413 general MIXED 16 .18: 16 12 : 28 256 : 118012 N/A lac-414 general MIXED 17 .79: 18 10 : 28 252 : 118012 N/A lac-415 general MIXED 16 .32: 18 10 : 28 252 : 118012 N/A lac-416 general MIXED 9 .03: 9 19 : 28 252 : 118012 N/A lac-417 general MIXED 10 .28: 18 10 : 28 1276 : 118012 N/A lac-418 general MIXED 6 .25: 6 22 : 28 252 : 118012 N/A lac-419 general ALLOCATED 19 .17: 28 0 : 28 2300 : 118012 N/A lac-420 general ALLOCATED 14 .05: 28 0 : 28 8444 : 118012 N/A lac-421 classres MIXED 9 .21: 17 11 : 28 3324 : 118012 N/A lac-422 general MIXED 18 .12: 26 2 : 28 1016 : 118012 N/A lac-423 general MIXED 20 .79: 24 4 : 28 4348 : 118012 N/A lac-424 general MIXED 18 .08: 27 1 : 28 4348 : 118012 N/A lac-425 general MIXED 12 .11: 25 3 : 28 252 : 118012 N/A lac-426 general MIXED 12 .41: 19 9 : 28 1276 : 118012 N/A lac-427 general MIXED 9 .20: 9 19 : 28 252 : 118012 N/A lac-428 general DOWN* N/A: 0 28 : 28 118012 : 118012 N/A Climer - waiting repair 4 -8-20 [ climer@2020-04-08T12:49:48 ] lac-429 general MIXED 15 .89: 27 1 : 28 1276 : 118012 N/A lac-430 general MIXED 13 .12: 18 10 : 28 256 : 118012 N/A lac-431 general MIXED 24 .13: 26 2 : 28 1276 : 118012 N/A lac-432 general MIXED 5 .13: 11 17 : 28 252 : 118012 N/A lac-433 general MIXED 25 .08: 26 2 : 28 742 : 118012 N/A lac-434 general MIXED 22 .59: 26 2 : 28 2300 : 118012 N/A lac-435 general MIXED 17 .03: 24 4 : 28 252 : 118012 N/A lac-436 general MIXED 20 .95: 21 7 : 28 3328 : 118012 N/A lac-437 general MIXED 20 .11: 21 7 : 28 1280 : 118012 N/A lac-438 general MIXED 13 .43: 20 8 : 28 37116 : 118012 N/A lac-439 general MIXED 27 .93: 25 3 : 28 3324 : 118012 N/A lac-440 general ALLOCATED 24 .46: 28 0 : 28 2300 : 118012 N/A lac-441 general MIXED 13 .90: 14 14 : 28 252 : 118012 N/A lac-442 general ALLOCATED 28 .16: 28 0 : 28 3266 : 118012 N/A lac-443 general MIXED 9 .47: 17 11 : 28 252 : 118012 N/A lac-444 general MIXED 8 .46: 26 2 : 28 252 : 118012 N/A lac-445 general ALLOCATED 28 .01: 28 0 : 28 7444 : 118012 N/A lac = > 69 .0% ( buyin ) 98 .8% ( 426 ) 43 .5%: 65 .7% ( 11928 ) 81 .6% ( 68 .2Tb ) 70 % ( 384 ) Usage% ( Total ) nvl-000 piermaro MIXED 11 .84: 18 22 : 40 183394 : 376162 v100 ( 3 :8 ) nvl-001 cmse MIXED 39 .45: 36 4 : 40 228706 : 376162 v100 ( 8 :8 ) nvl-002 DicksonLab ALLOCATED 36 .61: 40 0 : 40 208706 : 376162 v100 ( 7 :8 ) nvl-003 DicksonLab ALLOCATED 36 .77: 40 0 : 40 208706 : 376162 v100 ( 7 :8 ) nvl-004 DicksonLab MIXED 10 .23: 18 22 : 40 183394 : 376162 v100 ( 3 :8 ) nvl-005 alexrd-covid MIXED 7 .60: 8 32 : 40 1378 : 376162 v100 ( 1 :8 ) nvl-006 alexrd-covid MIXED 10 .77: 24 16 : 40 239090 : 376162 v100 ( 0 :8 ) nvl-007 general MIXED 8 .04: 8 32 : 40 370162 : 376162 v100 ( 0 :8 ) nvl = > 87 .5% ( buyin ) 100 .0% ( 8 ) 50 .4%: 60 .0% ( 320 ) 46 .0% ( 2 .87Tb ) 55 % ( 64 ) Usage% ( Total ) qml-000 general ALLOCATED 40 .04: 48 0 : 48 446368 :3067808 N/A qml-001 gmiaslab ALLOCATED 48 .19: 48 0 : 48 1037264 :1528784 N/A qml-002 ged ALLOCATED 48 .05: 48 0 : 48 1037264 :1528784 N/A qml-003 horticulture ALLOCATED 43 .91: 48 0 : 48 909300 :1015776 N/A qml-004 mitchmcg ALLOCATED 48 .27: 48 0 : 48 1037264 :1528784 N/A qml-005 general MIXED 69 .44: 74 22 : 96 3198784 :6145856 N/A qml = > 66 .7% ( buyin ) 100 .0% ( 6 ) 88 .7%: 93 .5% ( 336 ) 48 .3% ( 14 .1Tb ) N/A ( 0 ) Usage% ( Total ) skl-000 devolab MIXED 17 .59: 5 35 : 40 35682 : 85858 N/A skl-001 devolab MIXED 11 .33: 7 33 : 40 33634 : 85858 N/A skl-002 devolab MIXED 5 .62: 4 36 : 40 36706 : 85858 N/A skl-003 devolab MIXED 12 .21: 12 28 : 40 3938 : 85858 N/A skl-004 tsangm ALLOCATED 3 .16: 40 0 : 40 3938 : 85858 N/A skl-005 plzbuyin MIXED 5 .22: 12 28 : 40 3938 : 85858 N/A skl-006 allenmc MIXED 9 .52: 6 34 : 40 61282 : 85858 N/A skl-007 allenmc MIXED 8 .55: 6 34 : 40 61282 : 85858 N/A skl-008 allenmc MIXED 9 .07: 9 31 : 40 12130 : 85858 N/A skl-009 allenmc MIXED 6 .03: 6 34 : 40 61282 : 85858 N/A skl-010 allenmc MIXED 12 .01: 12 28 : 40 3938 : 85858 N/A skl-011 allenmc MIXED 12 .03: 12 28 : 40 3938 : 85858 N/A skl-012 seiswei MIXED 12 .12: 12 28 : 40 3938 : 85858 N/A skl-013 junlin MIXED 24 .62: 6 34 : 40 61282 : 85858 N/A skl-014 junlin MIXED 6 .03: 6 34 : 40 61282 : 85858 N/A skl-015 junlin MIXED 34 .88: 34 6 : 40 3938 : 85858 N/A skl-016 junlin MIXED 5 .66: 12 28 : 40 3938 : 85858 N/A skl-017 klausner ALLOCATED 40 .05: 40 0 : 40 3938 : 85858 N/A skl-018 klausner MIXED 12 .16: 12 28 : 40 3938 : 85858 N/A skl-019 klausner MIXED 12 .34: 12 28 : 40 3938 : 85858 N/A skl-020 klausner MIXED 12 .18: 12 28 : 40 3938 : 85858 N/A skl-021 cbcclab ALLOCATED 32 .85: 40 0 : 40 21858 : 85858 N/A skl-022 cbcclab MIXED 12 .28: 12 28 : 40 3938 : 85858 N/A skl-023 guowei-search MIXED 5 .18: 12 28 : 40 3938 : 85858 N/A skl-024 edgerpat MIXED 7 .65: 38 2 : 40 3938 : 85858 N/A skl-025 edgerpat MIXED 12 .18: 12 28 : 40 3938 : 85858 N/A skl-026 guowei-search MIXED 5 .48: 12 28 : 40 3938 : 85858 N/A skl-027 guowei-search ALLOCATED 38 .56: 40 0 : 40 3938 : 85858 N/A skl-028 guowei-search MIXED 5 .07: 12 28 : 40 3938 : 85858 N/A skl-029 guowei-search MIXED 5 .14: 12 28 : 40 3938 : 85858 N/A skl-030 guowei-search MIXED 5 .11: 12 28 : 40 3938 : 85858 N/A skl-031 guowei-search MIXED 5 .07: 12 28 : 40 3938 : 85858 N/A skl-032 guowei-search MIXED 5 .12: 12 28 : 40 3938 : 85858 N/A skl-033 guowei-search MIXED 5 .06: 12 28 : 40 3938 : 85858 N/A skl-034 guowei-search MIXED 5 .23: 12 28 : 40 3938 : 85858 N/A skl-035 guowei-search MIXED 5 .17: 12 28 : 40 3938 : 85858 N/A skl-036 guowei-search MIXED 5 .12: 12 28 : 40 3938 : 85858 N/A skl-037 guowei-search MIXED 28 .03: 28 12 : 40 28514 : 85858 N/A skl-038 guowei-search MIXED 5 .27: 12 28 : 40 3938 : 85858 N/A skl-039 guowei-search MIXED 5 .23: 12 28 : 40 3938 : 85858 N/A skl-040 guowei-search MIXED 5 .13: 12 28 : 40 3938 : 85858 N/A skl-041 guowei-search MIXED 5 .07: 12 28 : 40 3938 : 85858 N/A skl-042 guowei-search MIXED 5 .08: 12 28 : 40 3938 : 85858 N/A skl-043 guowei-search MIXED 5 .07: 12 28 : 40 3938 : 85858 N/A skl-044 guowei-search MIXED 5 .10: 12 28 : 40 3938 : 85858 N/A skl-045 guowei-search MIXED 5 .10: 12 28 : 40 3938 : 85858 N/A skl-046 guowei-search MIXED 5 .14: 13 27 : 40 1890 : 85858 N/A skl-047 guowei-search MIXED 5 .06: 13 27 : 40 1890 : 85858 N/A skl-048 guowei-search MIXED 5 .16: 13 27 : 40 1890 : 85858 N/A skl-049 guowei-search MIXED 5 .07: 13 27 : 40 1890 : 85858 N/A skl-050 guowei-search MIXED 28 .02: 28 12 : 40 28514 : 85858 N/A skl-051 guowei-search MIXED 28 .83: 29 11 : 40 8034 : 85858 N/A skl-052 guowei-search MIXED 5 .21: 25 15 : 40 1890 : 85858 N/A skl-053 guowei-search MIXED 4 .79: 25 15 : 40 1890 : 85858 N/A skl-054 guowei-search MIXED 3 .27: 19 21 : 40 14178 : 85858 N/A skl-055 guowei-search MIXED 2 .68: 16 24 : 40 20322 : 85858 N/A skl-056 guowei-search MIXED 2 .66: 16 24 : 40 20322 : 85858 N/A skl-057 general ALLOCATED 77 .50: 40 0 : 40 2466 : 85858 N/A skl-058 general MIXED 21 .20: 27 13 : 40 874 : 85858 N/A skl-059 general MIXED 20 .06: 31 9 : 40 870 : 85858 N/A skl-060 general MIXED 8 .15: 7 33 : 40 866 : 85858 N/A skl-061 general MIXED 19 .24: 18 22 : 40 12130 : 85858 N/A skl-062 general MIXED 36 .90: 38 2 : 40 1914 : 85858 N/A skl-063 general MIXED 33 .71: 36 4 : 40 890 : 85858 N/A skl-064 general MIXED 25 .42: 26 14 : 40 890 : 85858 N/A skl-065 general MIXED 38 .71: 39 1 : 40 890 : 85858 N/A skl-066 general ALLOCATED 40 .02: 40 0 : 40 3938 : 85858 N/A skl-067 general ALLOCATED 40 .09: 40 0 : 40 3938 : 85858 N/A skl-068 general MIXED 15 .94: 25 15 : 40 1890 : 85858 N/A skl-069 general MIXED 21 .71: 23 17 : 40 3942 : 85858 N/A skl-070 general ALLOCATED 22 .31: 40 0 : 40 1894 : 85858 N/A skl-071 general MIXED 16 .65: 30 10 : 40 1890 : 85858 N/A skl-072 general MIXED 22 .15: 22 18 : 40 866 : 85858 N/A skl-073 general ALLOCATED 40 .09: 40 0 : 40 3938 : 85858 N/A skl-074 general MIXED 19 .91: 22 18 : 40 870 : 85858 N/A skl-075 general MIXED 37 .57: 36 4 : 40 4514 : 85858 N/A skl-076 general ALLOCATED 38 .08: 40 0 : 40 1442 : 85858 N/A skl-077 general MIXED 23 .94: 34 6 : 40 866 : 85858 N/A skl-078 general MIXED 25 .81: 28 12 : 40 870 : 85858 N/A skl-079 general MIXED 22 .93: 36 4 : 40 866 : 85858 N/A skl-080 general MIXED 16 .86: 20 20 : 40 866 : 85858 N/A skl-081 general MIXED 35 .24: 35 5 : 40 418 : 85858 N/A skl-082 general MIXED 24 .07: 30 10 : 40 866 : 85858 N/A skl-083 general MIXED 2 .61: 6 34 : 40 866 : 85858 N/A skl-084 general MIXED 21 .03: 23 17 : 40 866 : 85858 N/A skl-085 general MIXED 26 .66: 25 15 : 40 866 : 85858 N/A skl-086 general MIXED 39 .28: 39 1 : 40 1442 : 85858 N/A skl-087 general MIXED 17 .81: 22 18 : 40 1890 : 85858 N/A skl-088 general MIXED 5 .93: 2 38 : 40 3938 : 85858 N/A skl-089 general MIXED 16 .11: 15 25 : 40 866 : 85858 N/A skl-090 general MIXED 5 .74: 6 34 : 40 866 : 85858 N/A skl-091 general MIXED 8 .68: 14 26 : 40 866 : 85858 N/A skl-092 general MIXED 16 .46: 19 21 : 40 1890 : 85858 N/A skl-093 general MIXED 22 .51: 22 18 : 40 870 : 85858 N/A skl-094 general MIXED 31 .44: 39 1 : 40 1906 : 85858 N/A skl-095 general MIXED 27 .04: 29 11 : 40 870 : 85858 N/A skl-096 general MIXED 9 .76: 10 30 : 40 866 : 85858 N/A skl-097 general MIXED 9 .76: 10 30 : 40 866 : 85858 N/A skl-098 general MIXED 21 .58: 22 18 : 40 866 : 85858 N/A skl-099 general MIXED 24 .98: 36 4 : 40 870 : 85858 N/A skl-100 general MIXED 20 .48: 21 19 : 40 1890 : 85858 N/A skl-101 general MIXED 18 .06: 21 19 : 40 866 : 85858 N/A skl-102 general MIXED 7 .36: 15 25 : 40 866 : 85858 N/A skl-103 general MIXED 39 .21: 39 1 : 40 1442 : 85858 N/A skl-104 general MIXED 18 .07: 27 13 : 40 1890 : 85858 N/A skl-105 general ALLOCATED 9 .77: 40 0 : 40 3938 : 85858 N/A skl-106 general MIXED 10 .78: 6 34 : 40 1890 : 85858 N/A skl-107 general MIXED 16 .02: 18 22 : 40 870 : 85858 N/A skl-108 general MIXED 22 .08: 33 7 : 40 866 : 85858 N/A skl-109 general MIXED 6 .94: 11 29 : 40 866 : 85858 N/A skl-110 general MIXED 9 .05: 6 34 : 40 3938 : 85858 N/A skl-111 general MIXED 37 .53: 37 3 : 40 418 : 85858 N/A skl-112 general MIXED 5 .12: 4 36 : 40 866 : 85858 N/A skl-113 zayernouri_fmath MIXED 9 .68: 16 24 : 40 2402 : 182626 N/A skl-114 zayernouri_fmath MIXED 9 .21: 16 24 : 40 2402 : 182626 N/A skl-115 zayernouri_fmath MIXED 9 .39: 16 24 : 40 2402 : 182626 N/A skl-116 niederhu MIXED 9 .19: 16 24 : 40 2402 : 182626 N/A skl-117 daylab MIXED 12 .15: 12 28 : 40 2402 : 182626 N/A skl-118 junlin MIXED 12 .01: 12 28 : 40 2402 : 182626 N/A skl-119 pollyhsu MIXED 24 .21: 24 16 : 40 2402 : 182626 N/A skl-120 yueqibuyin ALLOCATED 40 .23: 40 0 : 40 18786 : 182626 N/A skl-121 yueqibuyin ALLOCATED 40 .05: 40 0 : 40 18786 : 182626 N/A skl-122 yueqibuyin ALLOCATED 40 .09: 40 0 : 40 18786 : 182626 N/A skl-123 yueqibuyin ALLOCATED 40 .09: 40 0 : 40 18786 : 182626 N/A skl-124 plzbuyin MIXED 12 .01: 12 28 : 40 2402 : 182626 N/A skl-125 hcy MIXED 12 .06: 12 28 : 40 2402 : 182626 N/A skl-126 hcy MIXED 12 .02: 12 28 : 40 2402 : 182626 N/A skl-127 hcy MIXED 10 .16: 10 30 : 40 10594 : 182626 N/A skl-128 junlin MIXED 37 .04: 37 3 : 40 14690 : 182626 N/A skl-129 zayernouri_fmath MIXED 11 .95: 30 10 : 40 20834 : 182626 N/A skl-130 zayernouri_fmath MIXED 4 .10: 4 36 : 40 133474 : 182626 N/A skl-131 zayernouri_fmath MIXED 6 .21: 12 28 : 40 67938 : 182626 N/A skl-132 qian MIXED 13 .45: 34 6 : 40 7522 : 376162 N/A skl-133 vmante IDLE+DRAIN 0 .01: 0 40 : 40 376162 : 376162 N/A Low RealMemory [ slurm@2020-04-22T09:05:12 ] skl-134 liulab MIXED 9 .15: 9 31 : 40 7522 : 376162 N/A skl-135 chenlab MIXED 11 .39: 24 16 : 40 81250 : 376162 N/A skl-136 junlin MIXED 18 .01: 18 22 : 40 7522 : 376162 N/A skl-137 mitchmcg MIXED 13 .91: 20 20 : 40 97634 : 376162 N/A skl-138 eisenlohr MIXED 8 .38: 8 32 : 40 245090 : 376162 N/A skl-139 davidroy MIXED 17 .32: 24 16 : 40 15714 : 376162 N/A skl-140 shadeash-colej MIXED 35 .14: 35 5 : 40 79202 : 763234 N/A skl-141 shadeash-colej ALLOCATED 40 .17: 40 0 : 40 566626 : 763234 N/A skl-142 cbcclab ALLOCATED 34 .51: 40 0 : 40 699234 : 763234 N/A skl-143 general MIXED 32 .10: 32 8 : 40 370018 : 763234 N/A skl-144 guowei-search MIXED 7 .21: 35 5 : 40 152930 : 763234 N/A skl-145 general ALLOCATED 32 .83: 40 0 : 40 304482 : 763234 N/A skl-146 guowei-search MIXED 36 .05: 36 4 : 40 615778 : 763234 N/A skl-147 general MIXED 3 .03: 13 27 : 40 42338 : 763234 N/A skl-148 davidroy MIXED 19 .03: 19 21 : 40 19810 : 376162 N/A skl-149 davidroy MIXED 19 .02: 19 21 : 40 19810 : 376162 N/A skl-150 davidroy MIXED 35 .41: 35 5 : 40 3426 : 376162 N/A skl-151 davidroy MIXED 18 .07: 37 3 : 40 42338 : 376162 N/A skl-152 davidroy MIXED 19 .02: 19 21 : 40 19810 : 376162 N/A skl-153 davidroy ALLOCATED 40 .38: 40 0 : 40 179554 : 376162 N/A skl-154 davidroy MIXED 35 .09: 35 5 : 40 3426 : 376162 N/A skl-155 davidroy MIXED 35 .24: 35 5 : 40 3426 : 376162 N/A skl-156 davidroy MIXED 19 .02: 19 21 : 40 19810 : 376162 N/A skl-157 davidroy MIXED 23 .04: 23 17 : 40 3426 : 376162 N/A skl-158 davidroy MIXED 23 .06: 23 17 : 40 3426 : 376162 N/A skl-159 davidroy ALLOCATED 40 .11: 40 0 : 40 179554 : 376162 N/A skl-160 davidroy ALLOCATED 40 .29: 40 0 : 40 146786 : 376162 N/A skl-161 zayernouri_fmath MIXED 22 .03: 22 18 : 40 3426 : 376162 N/A skl-162 general MIXED 37 .09: 37 3 : 40 3426 : 376162 N/A skl-163 general ALLOCATED 40 .48: 40 0 : 40 107874 : 376162 N/A skl-164 general ALLOCATED 40 .10: 40 0 : 40 83298 : 376162 N/A skl-165 general ALLOCATED 2 .42: 40 0 : 40 179554 : 376162 N/A skl-166 guowei-search MIXED 11 .29: 31 9 : 40 3426 : 376162 N/A skl-167 guowei-search MIXED 11 .12: 30 10 : 40 23906 : 376162 N/A skl = > 62 .5% ( buyin ) 99 .4% ( 168 ) 45 .6%: 55 .6% ( 6720 ) 80 .3% ( 28 .4Tb ) N/A ( 0 ) Usage% ( Total ) vim-000 hsu MIXED 1474 .82: 23 41 : 64 53152 :3067808 N/A vim-001 general MIXED 10 .42: 31 33 : 64 970656 :3067808 N/A vim-002 ccg MIXED 66 .14: 63 81 :144 5427008 :6145856 N/A intel14 = > 34 .5% ( buyin ) 91 .7% ( 168 ) 47 .8%: 62 .7% ( 3576 ) 60 .1% ( 31 .1Tb ) 97 % ( 78 ) Usage% ( Total ) intel16 = > 69 .0% ( buyin ) 98 .8% ( 429 ) 55 .2%: 65 .1% ( 12200 ) 76 .6% ( 79 .9Tb ) 70 % ( 384 ) Usage% ( Total ) intel18 = > 63 .6% ( buyin ) 99 .4% ( 176 ) 45 .8%: 55 .8% ( 7040 ) 77 .1% ( 31 .3Tb ) 55 % ( 64 ) Usage% ( Total ) Summary = > 60 .3% ( buyin ) 97 .4% ( 773 ) 51 .2%: 61 .9% ( 22816 ) 73 .1% ( 142Tb ) 72 % ( 526 ) Usage% ( Total )","title":"node_status"},{"location":"node_status/#node_status","text":"Display a list of compute nodes and their properties. 1 2 3 4 5 6 7 8 9 10 11 $ node_status -h node_status: Display status of all compute nodes or nodes with following specifications. -h | --help Display this help message -f | --feature Selected features ( csm,csn,csp,css,qml,lac,vim,nvl,skl,intel14,intel16,intel18,gpu,k80 ) -w | --nodelist List of nodes ( conflict with -f ) -A | --account Account names ( such as general,classres, ... ) -c | --cpus Available number of CPU -m | --mem Available memory ( in Mb ) -g | --gpus Available number of GPU 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 $ node_status Wed Apr 22 11 :14:40 EDT 2020 NodeName Account State CPU ( Load:Aloc Idl:Tot ) Mem ( Aval:Tot ) Mb GPU ( I:T ) Reason ---------------------------------------------------------------------------------------------------------- csm-001 general ALLOCATED 13 .61: 20 0 : 20 45186 : 246640 N/A csm-002 albrecht MIXED 10 .14: 15 5 : 20 1072 : 246640 N/A csm-003 colej ALLOCATED 7 .45: 20 0 : 20 50032 : 246640 N/A csm-004 colej ALLOCATED 6 .24: 20 0 : 20 50032 : 246640 N/A csm-005 colej MIXED 9 .02: 16 4 : 20 29552 : 246640 N/A csm-007 colej MIXED 3 .85: 16 4 : 20 29552 : 246640 N/A csm-008 horticulture DOWN* N/A: 0 20 : 20 246640 : 246640 N/A DEAD - to be removed [ climer@2020-04-08T12:48:08 ] csm-009 horticulture DOWN* N/A: 0 20 : 20 246640 : 246640 N/A Climer - Need power reset 4 -21-20 [ climer@2020-04-21T07:59:30 ] csm-010 ged ALLOCATED 6 .34: 20 0 : 20 50032 : 246640 N/A csm-017 eisenlohr DOWN*+DRAIN N/A: 0 20 : 20 246640 : 246640 N/A Parks testing ssh puppet module [ parksjo@2020-04-02T10:10:52 ] csm-018 eisenlohr IDLE+DRAIN 0 .06: 0 20 : 20 246640 : 246640 N/A Parks testing ssh puppet module [ parksjo@2020-04-02T10:10:40 ] csm-019 eisenlohr DOWN+DRAIN 0 .01: 0 20 : 20 246640 : 246640 N/A DEAD - to be removed [ climer@2020-04-08T12:48:45 ] csm-020 eisenlohr ALLOCATED 6 .75: 20 0 : 20 50032 : 246640 N/A csm-021 dworkin ALLOCATED 6 .68: 20 0 : 20 65392 : 246640 N/A csm-022 mitchmcg ALLOCATED 6 .08: 20 0 : 20 65392 : 246640 N/A csn-001 general MIXED 9 .76: 18 2 : 20 18208 : 118012 k20 ( 0 :2 ) csn-002 general MIXED 1 .64: 8 12 : 20 61628 : 118012 k20 ( 0 :2 ) csn-003 general MIXED 2 .29: 6 14 : 20 80878 : 118012 k20 ( 0 :2 ) csn-004 general MIXED 9 .10: 9 11 : 20 9742 : 118012 k20 ( 0 :2 ) csn-005 general MIXED 9 .92: 12 8 : 20 16160 : 118012 k20 ( 0 :2 ) csn-006 general MIXED 5 .51: 6 14 : 20 252 : 118012 k20 ( 0 :2 ) csn-007 general MIXED 1 .16: 14 6 : 20 95964 : 118012 k20 ( 0 :2 ) csn-008 general MIXED 1 .67: 8 12 : 20 61628 : 118012 k20 ( 0 :2 ) csn-009 general MIXED 1 .29: 7 13 : 20 78012 : 118012 k20 ( 0 :2 ) csn-010 general MIXED 1 .59: 8 12 : 20 61628 : 118012 k20 ( 0 :2 ) csn-011 general MIXED 1 .57: 8 12 : 20 61628 : 118012 k20 ( 0 :2 ) csn-012 general MIXED 4 .54: 11 9 : 20 12476 : 118012 k20 ( 0 :2 ) csn-013 general MIXED 4 .53: 11 9 : 20 12476 : 118012 k20 ( 0 :2 ) csn-014 general MIXED 4 .75: 11 9 : 20 12476 : 118012 k20 ( 0 :2 ) csn-015 general MIXED 4 .59: 11 9 : 20 12476 : 118012 k20 ( 0 :2 ) csn-016 general MIXED 0 .62: 7 13 : 20 78012 : 118012 k20 ( 0 :2 ) csn-017 general MIXED 1 .77: 8 12 : 20 61628 : 118012 k20 ( 0 :2 ) csn-018 general MIXED 1 .16: 5 15 : 20 89820 : 118012 k20 ( 0 :2 ) csn-019 general ALLOCATED 3 .98: 20 0 : 20 8718 : 118012 k20 ( 0 :2 ) csn-020 general IDLE 10 .51: 0 20 : 20 118012 : 118012 k20 ( 2 :2 ) csn-021 general MIXED 6 .05: 6 14 : 20 16160 : 118012 k20 ( 0 :2 ) csn-022 general MIXED 1 .73: 8 12 : 20 26812 : 118012 k20 ( 0 :2 ) csn-023 general MIXED 0 .35: 8 12 : 20 78012 : 118012 k20 ( 0 :2 ) csn-024 general MIXED 1 .46: 8 12 : 20 61628 : 118012 k20 ( 0 :2 ) csn-025 general MIXED 1 .73: 5 15 : 20 57052 : 118012 k20 ( 0 :2 ) csn-026 general MIXED 1 .67: 15 5 : 20 12476 : 118012 k20 ( 0 :2 ) csn-027 general ALLOCATED 12 .84: 20 0 : 20 1276 : 118012 k20 ( 0 :2 ) csn-028 general MIXED 1 .97: 8 12 : 20 78012 : 118012 k20 ( 0 :2 ) csn-029 general MIXED 0 .68: 7 13 : 20 78012 : 118012 k20 ( 0 :2 ) csn-030 general MIXED 1 .77: 8 12 : 20 61628 : 118012 k20 ( 0 :2 ) csn-031 general MIXED 2 .08: 8 12 : 20 78012 : 118012 k20 ( 0 :2 ) csn-032 general MIXED 2 .14: 6 14 : 20 31726 : 118012 k20 ( 0 :2 ) csn-033 general MIXED 2 .15: 6 14 : 20 73436 : 118012 k20 ( 0 :2 ) csn-034 general MIXED 1 .85: 8 12 : 20 61628 : 118012 k20 ( 0 :2 ) csn-035 general MIXED 4 .82: 11 9 : 20 28860 : 118012 k20 ( 0 :2 ) csn-036 general MIXED 0 .63: 8 12 : 20 78012 : 118012 k20 ( 0 :2 ) csn-037 eisenlohr MIXED 1 .16: 9 11 : 20 37052 : 118012 k20 ( 0 :2 ) csn-038 christlibuyin MIXED 1 .43: 15 5 : 20 12476 : 118012 k20 ( 0 :2 ) csn-039 christlibuyin MIXED 2 .11: 15 5 : 20 12476 : 118012 k20 ( 0 :2 ) csp-006 general ALLOCATED 20 .14: 20 0 : 20 2304 : 118012 N/A csp-007 general DOWN* N/A: 0 20 : 20 118012 : 118012 N/A DEAD - to be removed [ climer@2020-04-08T12:48:21 ] csp-016 general ALLOCATED 20 .02: 20 0 : 20 38140 : 118012 N/A csp-017 general ALLOCATED 20 .08: 20 0 : 20 1276 : 118012 N/A csp-018 general ALLOCATED 20 .09: 20 0 : 20 30972 : 118012 N/A csp-019 christlibuyin MIXED 8 .12: 9 11 : 20 68860 : 118012 N/A csp-020 general MIXED 7 .01: 7 13 : 20 252 : 118012 N/A csp-025 general ALLOCATED 12 .64: 20 0 : 20 1276 : 118012 N/A csp-026 christlibuyin MIXED 3 .98: 4 16 : 20 3324 : 118012 N/A css-001 colej MIXED 3 .72: 19 1 : 20 38704 : 246640 N/A css-002 colej MIXED 1 .08: 1 19 : 20 77052 : 118012 N/A css-003 plzbuyin MIXED 12 .19: 14 6 : 20 11516 : 118012 N/A css-007 albrecht MIXED 13 .28: 13 7 : 20 2928 : 246640 N/A css-008 general ALLOCATED 22 .18: 20 0 : 20 19658 : 246640 N/A css-009 general ALLOCATED 15 .18: 20 0 : 20 7024 : 246640 N/A css-010 general ALLOCATED 19 .25: 20 0 : 20 45936 : 246640 N/A css-011 general ALLOCATED 20 .17: 20 0 : 20 16518 : 246640 N/A css-012 general ALLOCATED 21 .14: 20 0 : 20 4846 : 246640 N/A css-013 general ALLOCATED 7 .22: 20 0 : 20 2178 : 246640 N/A css-014 general DOWN* N/A: 0 20 : 20 246640 : 246640 N/A Climer - Need power reset 4 -21-20 [ climer@2020-04-21T07:56:26 ] css-016 general ALLOCATED 21 .24: 20 0 : 20 41032 : 246640 N/A css-017 general ALLOCATED 20 .14: 20 0 : 20 17682 : 246640 N/A css-018 general MIXED 18 .01: 18 2 : 20 134 : 246640 N/A css-019 general ALLOCATED 20 .03: 20 0 : 20 8322 : 246640 N/A css-020 yueqibuyin MIXED 15 .87: 16 4 : 20 7420 : 118012 N/A css-023 general ALLOCATED 13 .98: 20 0 : 20 718 : 118012 N/A css-032 general ALLOCATED 21 .08: 20 0 : 20 36262 : 118012 N/A css-033 classres MIXED 8 .98: 11 9 : 20 5564 : 118012 N/A css-034 general MIXED 12 .98: 13 7 : 20 1276 : 118012 N/A css-035 general MIXED 6 .25: 5 15 : 20 3324 : 118012 N/A css-036 general ALLOCATED 7 .85: 20 0 : 20 174 : 53248 N/A css-038 general ALLOCATED 20 .26: 20 0 : 20 23552 : 53248 N/A css-039 general MIXED 17 .10: 18 2 : 20 4 : 53248 N/A css-040 general MIXED 14 .06: 19 1 : 20 548 : 53248 N/A css-041 general ALLOCATED 19 .42: 20 0 : 20 1850 : 53248 N/A css-042 general MIXED 5 .92: 18 2 : 20 274 : 53248 N/A css-043 general MIXED 7 .10: 9 11 : 20 346 : 53248 N/A css-044 general ALLOCATED 20 .13: 20 0 : 20 12028 : 53248 N/A css-045 general ALLOCATED 19 .02: 20 0 : 20 4240 : 53248 N/A css-047 general MIXED 8 .10: 18 2 : 20 2048 : 53248 N/A css-048 general ALLOCATED 6 .98: 20 0 : 20 5544 : 53248 N/A css-049 general MIXED 12 .82: 11 9 : 20 24374 : 53248 N/A css-050 general MIXED 5 .01: 14 6 : 20 0 : 53248 N/A css-052 general MIXED 16 .22: 19 1 : 20 12188 : 53248 N/A css-053 general DOWN+DRAIN 0 .01: 0 20 : 20 53248 : 53248 N/A DEAD - to be removed [ climer@2020-04-08T12:48:35 ] css-054 general MIXED 17 .07: 17 3 : 20 288 : 53248 N/A css-055 general MIXED 21 .75: 8 12 : 20 11336 : 53248 N/A css-056 general MIXED 20 .96: 16 4 : 20 10932 : 53248 N/A css-057 general ALLOCATED 20 .17: 20 0 : 20 19532 : 53248 N/A css-058 general ALLOCATED 19 .07: 20 0 : 20 14344 : 53248 N/A css-059 general MIXED 7 .10: 7 13 : 20 346 : 53248 N/A css-060 general ALLOCATED 16 .04: 20 0 : 20 8740 : 53248 N/A css-061 general MIXED 3 .06: 3 17 : 20 1298 : 53248 N/A css-062 general ALLOCATED 20 .14: 20 0 : 20 12912 : 53248 N/A css-063 general ALLOCATED 15 .98: 20 0 : 20 144 : 53248 N/A css-064 general ALLOCATED 20 .10: 20 0 : 20 37152 : 53248 N/A css-065 general ALLOCATED 10 .01: 20 0 : 20 2668 : 53248 N/A css-066 general MIXED 18 .05: 19 1 : 20 552 : 53248 N/A css-067 yueqibuyin MIXED 1 .01: 1 19 : 20 12288 : 53248 N/A css-071 general DOWN*+DRAIN N/A: 0 20 : 20 215232 : 215232 N/A DEAD - to be removed [ climer@2020-04-08T12:47:23 ] css-072 colej MIXED 6 .85: 7 13 : 20 0 : 53248 N/A css-074 yueqibuyin MIXED 2 .82: 3 17 : 20 4096 : 53248 N/A css-075 general ALLOCATED 19 .32: 20 0 : 20 32798 : 53248 N/A css-076 general DOWN* N/A: 0 20 : 20 53248 : 53248 N/A DEAD - to be removed [ climer@2020-04-08T12:47:14 ] css-079 general DOWN*+DRAIN N/A: 0 20 : 20 53248 : 53248 N/A Climer - Need power reset - 4 -8-20 [ climer@2020-04-08T12:46:08 ] css-080 general DOWN*+DRAIN N/A: 0 20 : 20 53248 : 53248 N/A Climer - Need power reset - 4 -8-20 [ climer@2020-04-08T12:46:20 ] css-081 toulson MIXED 6 .32: 9 11 : 20 7360 : 53248 N/A css-082 yueqibuyin DOWN* N/A: 0 20 : 20 53248 : 53248 N/A Climer - waiting Power reset 4 -8-20 [ climer@2020-04-08T12:51:51 ] css-083 general MIXED 10 .40: 18 2 : 20 424 : 53248 N/A css-084 toulson MIXED 3 .06: 3 17 : 20 4096 : 53248 N/A css-085 toulson MIXED 2 .98: 3 17 : 20 4096 : 53248 N/A css-087 general ALLOCATED 16 .09: 20 0 : 20 1644 : 53248 N/A css-088 general ALLOCATED 20 .27: 20 0 : 20 1374 : 53248 N/A css-089 general MIXED 10 .10: 16 4 : 20 1442 : 53248 N/A css-090 manning MIXED 6 .33: 9 11 : 20 7360 : 53248 N/A css-091 manning MIXED 2 .98: 3 17 : 20 4096 : 53248 N/A css-092 general MIXED 14 .08: 18 2 : 20 20 : 53248 N/A css-093 general ALLOCATED 19 .10: 20 0 : 20 42798 : 53248 N/A css-094 general MIXED 18 .16: 17 3 : 20 698 : 53248 N/A css-095 general MIXED 5 .02: 15 5 : 20 0 : 53248 N/A css-097 general ALLOCATED 20 .11: 20 0 : 20 24576 : 53248 N/A css-098 general MIXED 15 .08: 15 5 : 20 692 : 53248 N/A css-099 general ALLOCATED 17 .07: 20 0 : 20 9764 : 53248 N/A css-100 cukier MIXED 3 .00: 3 17 : 20 4096 : 53248 N/A css-101 cukier MIXED 3 .04: 3 17 : 20 4096 : 53248 N/A css-102 black MIXED 2 .82: 3 17 : 20 4096 : 53248 N/A css-103 black MIXED 6 .82: 7 13 : 20 0 : 53248 N/A css-106 yueqibuyin MIXED 2 .13: 2 18 : 20 20480 : 53248 N/A css-107 general ALLOCATED 14 .03: 20 0 : 20 8196 : 53248 N/A css-108 yueqibuyin MIXED 14 .01: 14 6 : 20 0 : 53248 N/A css-109 yueqibuyin MIXED 1 .01: 3 17 : 20 4288 : 53248 N/A css-111 yueqibuyin MIXED 14 .00: 14 6 : 20 0 : 53248 N/A css-112 yueqibuyin MIXED 1 .03: 1 19 : 20 12288 : 53248 N/A css-113 yueqibuyin MIXED 2 .93: 3 17 : 20 4096 : 53248 N/A css-114 yueqibuyin MIXED 14 .22: 14 6 : 20 0 : 53248 N/A css-115 yueqibuyin MIXED 14 .03: 14 6 : 20 0 : 53248 N/A css-116 yueqibuyin MIXED 1 .02: 1 19 : 20 12288 : 53248 N/A css-117 yueqibuyin MIXED 1 .08: 1 19 : 20 12288 : 53248 N/A css-118 general MIXED 17 .15: 19 1 : 20 92 : 53248 N/A css-119 yueqibuyin MIXED 14 .01: 14 6 : 20 0 : 53248 N/A css-120 yueqibuyin MIXED 5 .78: 8 12 : 20 20480 : 53248 N/A css-121 general MIXED 14 .01: 16 4 : 20 1442 : 53248 N/A css-122 baek MIXED 2 .98: 3 17 : 20 4096 : 53248 N/A css-123 baek MIXED 1 .10: 1 19 : 20 12288 : 53248 N/A css-124 general ALLOCATED 20 .26: 20 0 : 20 4312 : 53248 N/A css-125 aeimit MIXED 5 .06: 5 15 : 20 8192 : 53248 N/A css-126 general ALLOCATED 16 .13: 20 0 : 20 6418 : 53248 N/A css-127 toulson MIXED 1 .02: 1 19 : 20 12288 : 53248 N/A cs* = > 33 .3% ( buyin ) 91 .4% ( 162 ) 43 .6%: 59 .5% ( 3240 ) 69 .9% ( 17 .0Tb ) 97 % ( 78 ) Usage% ( Total ) lac-000 deyoungbuyin ALLOCATED 0 .06: 28 0 : 28 6012 : 118012 N/A lac-001 deyoungbuyin ALLOCATED 0 .07: 28 0 : 28 6012 : 118012 N/A lac-002 deyoungbuyin ALLOCATED 0 .11: 28 0 : 28 6012 : 118012 N/A lac-003 deyoungbuyin MIXED 25 .86: 26 2 : 28 5372 : 118012 N/A lac-004 deyoungbuyin ALLOCATED 0 .15: 28 0 : 28 6012 : 118012 N/A lac-005 deyoungbuyin ALLOCATED 0 .11: 28 0 : 28 6012 : 118012 N/A lac-006 deyoungbuyin DOWN+DRAIN 1 .00: 0 28 : 28 118012 : 118012 N/A NHC: check_hw_ib: No IB port is ACTIVE ( LinkUp 56 Gb/sec ) . [ root@2020-04-20T08:18:40 ] lac-007 deyoungbuyin ALLOCATED 0 .15: 28 0 : 28 6012 : 118012 N/A lac-008 deyoungbuyin ALLOCATED 0 .17: 28 0 : 28 6012 : 118012 N/A lac-009 deyoungbuyin ALLOCATED 0 .12: 28 0 : 28 6012 : 118012 N/A lac-010 deyoungbuyin ALLOCATED 0 .08: 28 0 : 28 6012 : 118012 N/A lac-011 deyoungbuyin ALLOCATED 0 .08: 28 0 : 28 6012 : 118012 N/A lac-012 deyoungbuyin ALLOCATED 0 .22: 28 0 : 28 6012 : 118012 N/A lac-013 deyoungbuyin ALLOCATED 0 .07: 28 0 : 28 6012 : 118012 N/A lac-014 deyoungbuyin ALLOCATED 0 .07: 28 0 : 28 6012 : 118012 N/A lac-015 deyoungbuyin ALLOCATED 0 .08: 28 0 : 28 6012 : 118012 N/A lac-016 deyoungbuyin ALLOCATED 0 .17: 28 0 : 28 6012 : 118012 N/A lac-017 deyoungbuyin ALLOCATED 0 .05: 28 0 : 28 6012 : 118012 N/A lac-018 deyoungbuyin ALLOCATED 0 .09: 28 0 : 28 6012 : 118012 N/A lac-019 deyoungbuyin ALLOCATED 0 .10: 28 0 : 28 6012 : 118012 N/A lac-020 deyoungbuyin ALLOCATED 0 .09: 28 0 : 28 6012 : 118012 N/A lac-021 deyoungbuyin ALLOCATED 0 .10: 28 0 : 28 6012 : 118012 N/A lac-022 deyoungbuyin ALLOCATED 0 .11: 28 0 : 28 6012 : 118012 N/A lac-023 deyoungbuyin ALLOCATED 0 .15: 28 0 : 28 6012 : 118012 N/A lac-024 deyoungbuyin MIXED 1 .39: 10 18 : 28 86640 : 246640 k80 ( 4 :8 ) lac-025 DicksonLab MIXED 6 .53: 16 12 : 28 6768 : 246640 k80 ( 1 :8 ) lac-026 DicksonLab MIXED 6 .98: 16 12 : 28 6768 : 246640 k80 ( 1 :8 ) lac-027 DicksonLab MIXED 6 .73: 16 12 : 28 6768 : 246640 k80 ( 1 :8 ) lac-028 DicksonLab MIXED 6 .79: 16 12 : 28 6768 : 246640 k80 ( 1 :8 ) lac-029 DicksonLab MIXED 3 .62: 13 15 : 28 55920 : 246640 k80 ( 1 :8 ) lac-030 feig-covid MIXED 1 .24: 14 14 : 28 66640 : 246640 k80 ( 3 :8 ) lac-031 merzjrke MIXED 4 .10: 10 18 : 28 197488 : 246640 k80 ( 0 :8 ) lac-032 allenmc MIXED 5 .15: 5 23 : 28 15612 : 118012 N/A lac-033 allenmc MIXED 5 .10: 5 23 : 28 15612 : 118012 N/A lac-034 allenmc MIXED 0 .29: 26 2 : 28 5372 : 118012 N/A lac-035 allenmc MIXED 7 .98: 26 2 : 28 5372 : 118012 N/A lac-036 allenmc ALLOCATED 28 .32: 28 0 : 28 60668 : 118012 N/A lac-037 allenmc ALLOCATED 28 .19: 28 0 : 28 60668 : 118012 N/A lac-038 general ALLOCATED 23 .36: 28 0 : 28 33028 : 118012 N/A lac-039 general ALLOCATED 24 .67: 28 0 : 28 260 : 118012 N/A lac-040 general MIXED 21 .14: 26 2 : 28 252 : 118012 N/A lac-041 general MIXED 9 .10: 9 19 : 28 252 : 118012 N/A lac-042 general MIXED 22 .86: 25 3 : 28 252 : 118012 N/A lac-043 general MIXED 20 .15: 21 7 : 28 252 : 118012 N/A lac-044 general MIXED 11 .51: 21 7 : 28 4540 : 118012 N/A lac-045 ptg MIXED 5 .22: 7 21 : 28 7612 : 118012 N/A lac-046 ptg MIXED 25 .51: 26 2 : 28 5372 : 118012 N/A lac-047 ptg MIXED 5 .28: 7 21 : 28 7612 : 118012 N/A lac-048 ptg MIXED 12 .23: 12 16 : 28 8444 : 118012 N/A lac-049 ptg MIXED 12 .37: 14 14 : 28 444 : 118012 N/A lac-050 ptg MIXED 11 .05: 13 15 : 28 1468 : 118012 N/A lac-051 ptg MIXED 5 .09: 5 23 : 28 15612 : 118012 N/A lac-052 ptg MIXED 5 .06: 7 21 : 28 7612 : 118012 N/A lac-053 ptg MIXED 5 .10: 5 23 : 28 15612 : 118012 N/A lac-054 ptg MIXED 12 .20: 12 16 : 28 8444 : 118012 N/A lac-055 ptg MIXED 12 .10: 14 14 : 28 444 : 118012 N/A lac-056 ptg MIXED 12 .41: 14 14 : 28 444 : 118012 N/A lac-057 ptg MIXED 12 .21: 14 14 : 28 444 : 118012 N/A lac-058 ptg MIXED 7 .08: 9 19 : 28 5564 : 118012 N/A lac-059 ptg ALLOCATED 23 .79: 28 0 : 28 15612 : 118012 N/A lac-060 ptg ALLOCATED 12 .93: 28 0 : 28 15612 : 118012 N/A lac-061 ptg ALLOCATED 13 .39: 28 0 : 28 15612 : 118012 N/A lac-062 ptg ALLOCATED 10 .32: 28 0 : 28 15612 : 118012 N/A lac-063 ptg ALLOCATED 12 .18: 28 0 : 28 15612 : 118012 N/A lac-064 ptg ALLOCATED 10 .46: 28 0 : 28 15612 : 118012 N/A lac-065 ptg ALLOCATED 16 .00: 28 0 : 28 15612 : 118012 N/A lac-066 ptg ALLOCATED 13 .01: 28 0 : 28 15612 : 118012 N/A lac-067 ptg MIXED 5 .36: 5 23 : 28 15612 : 118012 N/A lac-068 ptg ALLOCATED 21 .58: 28 0 : 28 15612 : 118012 N/A lac-069 ptg MIXED 12 .17: 14 14 : 28 444 : 118012 N/A lac-070 ptg MIXED 12 .19: 12 16 : 28 8444 : 118012 N/A lac-071 ptg MIXED 12 .08: 12 16 : 28 8444 : 118012 N/A lac-072 ptg MIXED 12 .17: 14 14 : 28 444 : 118012 N/A lac-073 ptg MIXED 7 .14: 7 21 : 28 13564 : 118012 N/A lac-074 ptg MIXED 5 .08: 8 20 : 28 2492 : 118012 N/A lac-075 ptg MIXED 5 .04: 7 21 : 28 7612 : 118012 N/A lac-076 ptg ALLOCATED 25 .55: 28 0 : 28 15612 : 118012 N/A lac-077 ptg MIXED 5 .18: 7 21 : 28 7612 : 118012 N/A lac-078 general MIXED 11 .38: 8 20 : 28 69884 : 118012 N/A lac-079 ptg ALLOCATED 22 .37: 28 0 : 28 15612 : 118012 N/A lac-080 merzjrke MIXED 2 .48: 16 12 : 28 50032 : 246640 k80 ( 0 :8 ) lac-081 merzjrke MIXED 13 .85: 23 5 : 28 9072 : 246640 k80 ( 0 :8 ) lac-082 merzjrke MIXED 5 .85: 18 10 : 28 131952 : 246640 k80 ( 0 :8 ) lac-083 merzjrke MIXED 4 .74: 19 9 : 28 880 : 246640 k80 ( 0 :8 ) lac-084 merzjrke MIXED 3 .43: 4 24 : 28 238448 : 246640 k80 ( 4 :8 ) lac-085 merzjrke MIXED 4 .68: 19 9 : 28 880 : 246640 k80 ( 0 :8 ) lac-086 merzjrke MIXED 1 .20: 16 12 : 28 50032 : 246640 k80 ( 0 :8 ) lac-087 general MIXED 18 .91: 19 9 : 28 9072 : 246640 k80 ( 0 :8 ) lac-088 ptg MIXED 4 .03: 6 22 : 28 7612 : 118012 N/A lac-089 ptg MIXED 4 .04: 6 22 : 28 7612 : 118012 N/A lac-090 ptg MIXED 4 .06: 6 22 : 28 7612 : 118012 N/A lac-091 ptg MIXED 4 .01: 6 22 : 28 7612 : 118012 N/A lac-092 ptg MIXED 4 .06: 6 22 : 28 7612 : 118012 N/A lac-093 ptg ALLOCATED 9 .42: 28 0 : 28 15612 : 118012 N/A lac-094 ptg ALLOCATED 14 .45: 28 0 : 28 15612 : 118012 N/A lac-095 ptg ALLOCATED 12 .50: 28 0 : 28 15612 : 118012 N/A lac-096 ptg ALLOCATED 6 .97: 28 0 : 28 15612 : 118012 N/A lac-097 ptg ALLOCATED 8 .46: 28 0 : 28 15612 : 118012 N/A lac-098 ptg ALLOCATED 6 .59: 28 0 : 28 15612 : 118012 N/A lac-099 ptg ALLOCATED 7 .12: 28 0 : 28 15612 : 118012 N/A lac-100 ptg ALLOCATED 19 .14: 28 0 : 28 15612 : 118012 N/A lac-101 ptg ALLOCATED 17 .22: 28 0 : 28 15612 : 118012 N/A lac-102 ptg MIXED 11 .19: 13 15 : 28 444 : 118012 N/A lac-103 ptg MIXED 7 .07: 9 19 : 28 4540 : 118012 N/A lac-104 ptg MIXED 5 .12: 7 21 : 28 444 : 118012 N/A lac-105 ptg MIXED 8 .21: 8 20 : 28 5372 : 118012 N/A lac-106 ptg ALLOCATED 20 .78: 28 0 : 28 15612 : 118012 N/A lac-107 ptg MIXED 11 .02: 13 15 : 28 444 : 118012 N/A lac-108 ptg MIXED 11 .22: 13 15 : 28 444 : 118012 N/A lac-109 ptg MIXED 11 .15: 11 17 : 28 8444 : 118012 N/A lac-110 ptg MIXED 11 .18: 13 15 : 28 444 : 118012 N/A lac-111 ptg MIXED 11 .17: 11 17 : 28 8444 : 118012 N/A lac-112 ptg MIXED 11 .09: 13 15 : 28 444 : 118012 N/A lac-113 ptg MIXED 11 .23: 13 15 : 28 444 : 118012 N/A lac-114 ptg MIXED 11 .01: 11 17 : 28 8444 : 118012 N/A lac-115 ptg MIXED 8 .07: 10 18 : 28 3516 : 118012 N/A lac-116 ptg MIXED 4 .89: 7 21 : 28 444 : 118012 N/A lac-117 ptg MIXED 7 .20: 7 21 : 28 37116 : 118012 N/A lac-118 ptg MIXED 11 .05: 13 15 : 28 444 : 118012 N/A lac-119 ptg MIXED 11 .16: 13 15 : 28 444 : 118012 N/A lac-120 ptg MIXED 4 .19: 4 24 : 28 40188 : 118012 N/A lac-121 ptg MIXED 5 .08: 7 21 : 28 6588 : 118012 N/A lac-122 ptg MIXED 4 .99: 5 23 : 28 8444 : 118012 N/A lac-123 general ALLOCATED 29 .19: 28 0 : 28 38904 : 118012 N/A lac-124 ptg MIXED 5 .18: 5 23 : 28 15612 : 118012 N/A lac-125 ptg MIXED 5 .07: 7 21 : 28 7612 : 118012 N/A lac-126 ptg MIXED 5 .29: 7 21 : 28 7612 : 118012 N/A lac-127 ptg ALLOCATED 22 .09: 28 0 : 28 15612 : 118012 N/A lac-128 ptg ALLOCATED 24 .32: 28 0 : 28 15612 : 118012 N/A lac-129 ptg MIXED 5 .15: 7 21 : 28 7612 : 118012 N/A lac-130 ptg ALLOCATED 24 .86: 28 0 : 28 15612 : 118012 N/A lac-131 ptg ALLOCATED 10 .80: 28 0 : 28 15612 : 118012 N/A lac-132 ptg MIXED 12 .01: 14 14 : 28 444 : 118012 N/A lac-133 ptg MIXED 12 .22: 14 14 : 28 444 : 118012 N/A lac-134 ptg MIXED 11 .25: 13 15 : 28 1468 : 118012 N/A lac-135 ptg MIXED 5 .20: 7 21 : 28 7612 : 118012 N/A lac-136 merzjrke MIXED+DRAIN 56 .36: 4 24 : 28 238448 : 246640 k80 ( 4 :8 ) NHC: Script timed out while executing \"icer_check_gpu_count 8 syslog die\" . [ root@2020-04-22T04:03:33 ] lac-137 feig-covid MIXED 3 .69: 13 15 : 28 74480 : 246640 k80 ( 3 :8 ) lac-138 merzjrke MIXED 9 .92: 1 27 : 28 181104 : 246640 k80 ( 4 :8 ) lac-139 merzjrke MIXED 17 .77: 11 17 : 28 66416 : 246640 k80 ( 0 :8 ) lac-140 merzjrke MIXED 15 .97: 24 4 : 28 206256 : 246640 k80 ( 0 :8 ) lac-141 merzjrke MIXED 5 .56: 8 20 : 28 181104 : 246640 k80 ( 0 :8 ) lac-142 merzjrke MIXED 17 .78: 8 20 : 28 230256 : 246640 k80 ( 0 :8 ) lac-143 general MIXED 11 .22: 11 17 : 28 7024 : 246640 k80 ( 7 :8 ) lac-144 ptg MIXED 4 .84: 7 21 : 28 444 : 118012 N/A lac-145 ptg MIXED 5 .07: 7 21 : 28 444 : 118012 N/A lac-146 ptg MIXED 5 .11: 5 23 : 28 8444 : 118012 N/A lac-147 ptg MIXED 4 .62: 7 21 : 28 444 : 118012 N/A lac-148 ptg MIXED 5 .05: 7 21 : 28 444 : 118012 N/A lac-149 ptg MIXED 5 .10: 7 21 : 28 444 : 118012 N/A lac-150 ptg MIXED 5 .03: 7 21 : 28 444 : 118012 N/A lac-151 ptg MIXED 5 .15: 5 23 : 28 8444 : 118012 N/A lac-152 ptg MIXED 5 .09: 7 21 : 28 444 : 118012 N/A lac-153 ptg MIXED 5 .02: 7 21 : 28 444 : 118012 N/A lac-154 ptg MIXED 5 .04: 7 21 : 28 444 : 118012 N/A lac-155 ptg MIXED 10 .08: 13 15 : 28 444 : 118012 N/A lac-156 ptg MIXED 5 .51: 7 21 : 28 444 : 118012 N/A lac-157 ptg MIXED 5 .06: 7 21 : 28 444 : 118012 N/A lac-158 ptg MIXED 5 .27: 5 23 : 28 8444 : 118012 N/A lac-159 ptg MIXED 5 .01: 7 21 : 28 444 : 118012 N/A lac-160 ptg MIXED 4 .26: 6 22 : 28 7612 : 118012 N/A lac-161 ptg MIXED 6 .34: 6 22 : 28 7420 : 118012 N/A lac-162 ptg MIXED 11 .08: 13 15 : 28 444 : 118012 N/A lac-163 ptg MIXED 11 .07: 13 15 : 28 444 : 118012 N/A lac-164 ptg MIXED 11 .26: 11 17 : 28 8444 : 118012 N/A lac-165 ptg MIXED 11 .11: 13 15 : 28 444 : 118012 N/A lac-166 ptg MIXED 11 .10: 13 15 : 28 444 : 118012 N/A lac-167 ptg MIXED 11 .07: 11 17 : 28 8444 : 118012 N/A lac-168 ptg MIXED 11 .37: 13 15 : 28 444 : 118012 N/A lac-169 ptg MIXED 11 .21: 13 15 : 28 444 : 118012 N/A lac-170 ptg MIXED 11 .15: 13 15 : 28 444 : 118012 N/A lac-171 ptg MIXED 11 .28: 13 15 : 28 444 : 118012 N/A lac-172 ptg MIXED 11 .06: 13 15 : 28 444 : 118012 N/A lac-173 ptg MIXED 11 .28: 13 15 : 28 444 : 118012 N/A lac-174 ptg MIXED 11 .21: 13 15 : 28 444 : 118012 N/A lac-175 ptg MIXED 11 .04: 13 15 : 28 444 : 118012 N/A lac-176 ptg MIXED 11 .07: 13 15 : 28 444 : 118012 N/A lac-177 ptg MIXED 12 .03: 14 14 : 28 444 : 118012 N/A lac-178 ptg MIXED 11 .10: 11 17 : 28 8444 : 118012 N/A lac-179 ptg MIXED 11 .04: 13 15 : 28 444 : 118012 N/A lac-180 ptg MIXED 11 .07: 13 15 : 28 444 : 118012 N/A lac-181 ptg MIXED 10 .06: 10 18 : 28 9468 : 118012 N/A lac-182 ptg MIXED 4 .24: 6 22 : 28 7612 : 118012 N/A lac-183 ptg MIXED 4 .01: 6 22 : 28 7612 : 118012 N/A lac-184 ptg MIXED 4 .05: 6 22 : 28 7612 : 118012 N/A lac-185 ptg MIXED 4 .05: 6 22 : 28 7612 : 118012 N/A lac-186 ptg MIXED 4 .04: 6 22 : 28 7612 : 118012 N/A lac-187 ptg MIXED 4 .23: 6 22 : 28 7612 : 118012 N/A lac-188 ptg MIXED 4 .24: 6 22 : 28 7612 : 118012 N/A lac-189 ptg ALLOCATED 8 .34: 28 0 : 28 15612 : 118012 N/A lac-190 ptg ALLOCATED 7 .25: 28 0 : 28 15612 : 118012 N/A lac-191 ptg ALLOCATED 8 .49: 28 0 : 28 15612 : 118012 N/A lac-192 general MIXED 5 .92: 15 13 : 28 23280 : 246640 k80 ( 2 :8 ) lac-193 guowei-search MIXED 2 .09: 22 6 : 28 33872 : 246640 k80 ( 3 :8 ) lac-194 guowei-search MIXED 6 .74: 20 8 : 28 2320 : 246640 k80 ( 5 :8 ) lac-195 guowei-search MIXED 7 .76: 17 11 : 28 1840 : 246640 k80 ( 6 :8 ) lac-196 guowei-search MIXED 3 .95: 17 11 : 28 13392 : 246640 k80 ( 3 :8 ) lac-197 guowei-search MIXED 3 .65: 17 11 : 28 13392 : 246640 k80 ( 3 :8 ) lac-198 general MIXED 0 .78: 10 18 : 28 86640 : 246640 k80 ( 4 :8 ) lac-199 general MIXED 1 .98: 11 17 : 28 66160 : 246640 k80 ( 3 :8 ) lac-200 ptg MIXED 11 .07: 13 15 : 28 444 : 118012 N/A lac-201 ptg MIXED 11 .21: 13 15 : 28 444 : 118012 N/A lac-202 ptg MIXED 11 .10: 13 15 : 28 444 : 118012 N/A lac-203 ptg MIXED 11 .08: 13 15 : 28 444 : 118012 N/A lac-204 ptg MIXED 11 .05: 13 15 : 28 444 : 118012 N/A lac-205 ptg MIXED 11 .21: 13 15 : 28 444 : 118012 N/A lac-206 ptg MIXED 11 .01: 13 15 : 28 444 : 118012 N/A lac-207 ptg MIXED 11 .01: 13 15 : 28 444 : 118012 N/A lac-208 ptg MIXED 8 .06: 10 18 : 28 3516 : 118012 N/A lac-209 general MIXED 3 .43: 10 18 : 28 27900 : 118012 N/A lac-210 ptg ALLOCATED 8 .52: 28 0 : 28 15612 : 118012 N/A lac-211 ptg ALLOCATED 8 .60: 28 0 : 28 15612 : 118012 N/A lac-212 ptg ALLOCATED 7 .35: 28 0 : 28 15612 : 118012 N/A lac-213 ptg ALLOCATED 7 .67: 28 0 : 28 15612 : 118012 N/A lac-214 ptg MIXED 26 .31: 26 2 : 28 5372 : 118012 N/A lac-215 tonggao MIXED 22 .49: 26 2 : 28 7612 : 118012 N/A lac-216 weilai ALLOCATED 24 .65: 28 0 : 28 5372 : 118012 N/A lac-217 general ALLOCATED 23 .03: 28 0 : 28 4420 : 118012 N/A lac-218 weilai ALLOCATED 25 .05: 28 0 : 28 5372 : 118012 N/A lac-219 weilai ALLOCATED 25 .12: 28 0 : 28 5372 : 118012 N/A lac-220 weilai MIXED 25 .44: 25 3 : 28 5372 : 118012 N/A lac-221 weilai ALLOCATED 25 .26: 28 0 : 28 5372 : 118012 N/A lac-222 weilai MIXED 16 .52: 26 2 : 28 5372 : 118012 N/A lac-223 weilai MIXED 8 .72: 9 19 : 28 1276 : 118012 N/A lac-224 weilai ALLOCATED 25 .00: 28 0 : 28 134000 : 246640 N/A lac-225 general MIXED 12 .28: 23 5 : 28 2928 : 246640 N/A lac-228 general MIXED 16 .26: 16 12 : 28 62320 : 246640 N/A lac-229 bazil MIXED 23 .93: 24 4 : 28 880 : 246640 N/A lac-230 general MIXED 41 .60: 18 10 : 28 131952 : 246640 N/A lac-231 general ALLOCATED 14 .49: 28 0 : 28 13168 : 246640 N/A lac-232 general MIXED 23 .93: 24 4 : 28 884 : 246640 N/A lac-233 general MIXED 26 .96: 24 4 : 28 8052 : 246640 N/A lac-234 general MIXED 2 .12: 2 26 : 28 164720 : 246640 N/A lac-235 general MIXED 5 .08: 5 23 : 28 74608 : 246640 N/A lac-236 cmich ALLOCATED 0 .53: 28 0 : 28 134640 : 246640 N/A lac-237 cmich ALLOCATED 4 .72: 28 0 : 28 34032 : 246640 N/A lac-238 cmich ALLOCATED 14 .00: 28 0 : 28 34800 : 246640 N/A lac-239 cmich MIXED 14 .06: 18 10 : 28 1264 : 246640 N/A lac-240 cmich ALLOCATED 8 .02: 28 0 : 28 35568 : 246640 N/A lac-241 cmich MIXED 15 .06: 15 13 : 28 880 : 246640 N/A lac-242 cmich MIXED 15 .28: 15 13 : 28 880 : 246640 N/A lac-243 cmich MIXED 15 .10: 15 13 : 28 880 : 246640 N/A lac-244 cmich MIXED 6 .06: 6 22 : 28 148336 : 246640 N/A lac-245 cmich MIXED 8 .08: 8 20 : 28 115568 : 246640 N/A lac-246 general ALLOCATED 16 .25: 28 0 : 28 17264 : 246640 N/A lac-247 general MIXED 6 .10: 6 22 : 28 79728 : 246640 N/A lac-248 iceradmin DOWN+DRAIN 0 .06: 0 28 : 28 246640 : 246640 N/A NHC: check_fs_mount: /mnt/gs18 not mounted [ root@2020-04-14T11:10:52 ] lac-250 lirac ALLOCATED 22 .19: 28 0 : 28 37152 : 503520 N/A lac-251 wenhuang ALLOCATED 26 .65: 28 0 : 28 382880 : 503520 N/A lac-252 general MIXED 0 .20: 18 10 : 28 431520 : 503520 N/A lac-253 general MIXED 14 .81: 15 13 : 28 257760 : 503520 N/A lac-254 beacon MIXED 6 .78: 7 21 : 28 3324 : 118012 N/A lac-255 chomiuk MIXED 10 .33: 13 15 : 28 3324 : 118012 N/A lac-256 quantgen ALLOCATED 4 .10: 28 0 : 28 290912 : 503520 N/A lac-257 hirn ALLOCATED 17 .81: 28 0 : 28 279200 : 503520 N/A lac-258 hirn ALLOCATED 28 .15: 28 0 : 28 216800 : 503520 N/A lac-259 quantgen ALLOCATED 28 .28: 28 0 : 28 216800 : 503520 N/A lac-260 quantgen ALLOCATED 5 .17: 28 0 : 28 391520 : 503520 N/A lac-261 ccg MIXED 25 .99: 25 3 : 28 260000 : 503520 N/A lac-276 general MIXED 15 .60: 16 12 : 28 598 : 118012 N/A lac-277 general ALLOCATED 20 .14: 28 0 : 28 56 : 118012 N/A lac-278 general MIXED 6 .11: 6 22 : 28 880 : 246640 N/A lac-279 general MIXED 15 .65: 25 3 : 28 2928 : 246640 N/A lac-280 general MIXED 18 .05: 15 13 : 28 880 : 246640 N/A lac-281 general MIXED 26 .14: 27 1 : 28 48 : 246640 N/A lac-282 general MIXED 6 .05: 6 22 : 28 880 : 246640 N/A lac-283 general MIXED 5 .02: 5 23 : 28 74608 : 246640 N/A lac-284 general ALLOCATED 25 .07: 28 0 : 28 4980 : 246640 N/A lac-285 qian MIXED 6 .10: 27 1 : 28 880 : 246640 N/A lac-286 scbbuyin ALLOCATED 7 .98: 28 0 : 28 17264 : 246640 k80 ( 8 :8 ) lac-287 general MIXED 1 .97: 11 17 : 28 70256 : 246640 k80 ( 3 :8 ) lac-288 general ALLOCATED 5 .97: 28 0 : 28 52080 : 246640 k80 ( 6 :8 ) lac-289 general MIXED 1 .02: 10 18 : 28 86640 : 246640 k80 ( 4 :8 ) lac-290 general MIXED 1 .10: 10 18 : 28 86640 : 246640 k80 ( 4 :8 ) lac-291 merzjrke MIXED 5 .79: 8 20 : 28 181104 : 246640 k80 ( 0 :8 ) lac-292 general MIXED 0 .86: 10 18 : 28 86640 : 246640 k80 ( 4 :8 ) lac-293 general MIXED 0 .65: 10 18 : 28 86640 : 246640 k80 ( 4 :8 ) lac-294 wang-krishnan MIXED 16 .83: 27 1 : 28 4976 : 246640 N/A lac-295 wang-krishnan MIXED 15 .76: 26 2 : 28 21360 : 246640 N/A lac-296 wang-krishnan MIXED 16 .59: 26 2 : 28 21360 : 246640 N/A lac-297 wang-krishnan MIXED 17 .39: 27 1 : 28 4976 : 246640 N/A lac-298 wang-krishnan MIXED 17 .44: 26 2 : 28 21360 : 246640 N/A lac-299 wang-krishnan MIXED 15 .17: 26 2 : 28 21360 : 246640 N/A lac-300 general MIXED 3 .06: 3 25 : 28 123760 : 246640 N/A lac-301 general MIXED 16 .10: 16 12 : 28 181104 : 246640 N/A lac-302 ccg MIXED 24 .21: 24 4 : 28 12000 : 503520 N/A lac-303 ccg MIXED 24 .06: 24 4 : 28 12000 : 503520 N/A lac-304 ccg MIXED 24 .13: 24 4 : 28 12000 : 503520 N/A lac-305 ccg MIXED 24 .05: 24 4 : 28 12000 : 503520 N/A lac-306 general ALLOCATED 28 .24: 28 0 : 28 216800 : 503520 N/A lac-307 ccg MIXED 24 .09: 26 2 : 28 4000 : 503520 N/A lac-308 ccg MIXED 24 .08: 26 2 : 28 4000 : 503520 N/A lac-309 ccg MIXED 24 .13: 26 2 : 28 4000 : 503520 N/A lac-310 ccg ALLOCATED 7 .82: 28 0 : 28 224992 : 503520 N/A lac-311 oakland-universi MIXED 24 .11: 24 4 : 28 12000 : 503520 N/A lac-312 oakland-universi MIXED 24 .10: 24 4 : 28 12000 : 503520 N/A lac-313 oakland-universi MIXED 24 .29: 24 4 : 28 12000 : 503520 N/A lac-314 oakland-universi ALLOCATED 2 .17: 28 0 : 28 349920 : 503520 N/A lac-315 ccg MIXED 24 .10: 24 4 : 28 12000 : 503520 N/A lac-316 cmich ALLOCATED 7 .50: 28 0 : 28 358112 : 503520 N/A lac-317 cmich ALLOCATED 0 .55: 28 0 : 28 391520 : 503520 N/A lac-318 cmich MIXED 21 .51: 26 2 : 28 5372 : 118012 N/A lac-319 cmich MIXED 15 .65: 16 12 : 28 252 : 118012 N/A lac-320 cmich MIXED 10 .17: 10 18 : 28 252 : 118012 N/A lac-321 cmich MIXED 10 .09: 10 18 : 28 252 : 118012 N/A lac-322 cmich MIXED 8 .04: 8 20 : 28 2300 : 118012 N/A lac-323 cmich MIXED 4 .15: 16 12 : 28 4476 : 118012 N/A lac-324 cmich MIXED 12 .69: 26 2 : 28 5372 : 118012 N/A lac-325 cmich ALLOCATED 0 .41: 28 0 : 28 6012 : 118012 N/A lac-326 cmich MIXED 7 .04: 7 21 : 28 3324 : 118012 N/A lac-327 cmich MIXED 0 .25: 26 2 : 28 5372 : 118012 N/A lac-328 cmich MIXED 12 .52: 26 2 : 28 5372 : 118012 N/A lac-329 cmich MIXED 9 .17: 13 15 : 28 636 : 118012 N/A lac-330 beacon MIXED 12 .16: 14 14 : 28 444 : 118012 N/A lac-331 beacon MIXED 12 .24: 14 14 : 28 444 : 118012 N/A lac-332 beacon MIXED 12 .12: 14 14 : 28 444 : 118012 N/A lac-333 beacon MIXED 11 .12: 13 15 : 28 1468 : 118012 N/A lac-334 ptg ALLOCATED 1 .82: 28 0 : 28 15612 : 118012 N/A lac-335 ptg ALLOCATED 1 .92: 28 0 : 28 15612 : 118012 N/A lac-336 general MIXED 20 .78: 21 7 : 28 3324 : 118012 N/A lac-337 general MIXED 12 .59: 26 2 : 28 252 : 118012 N/A lac-338 general MIXED 15 .39: 27 1 : 28 3328 : 118012 N/A lac-339 general ALLOCATED 13 .06: 28 0 : 28 29948 : 118012 N/A lac-340 ptg ALLOCATED 1 .87: 28 0 : 28 15612 : 118012 N/A lac-341 ptg ALLOCATED 1 .64: 28 0 : 28 15612 : 118012 N/A lac-342 general MIXED 1 .49: 14 14 : 28 66640 : 246640 k80 ( 3 :8 ) lac-343 merzjrke MIXED 33 .27: 25 3 : 28 124336 : 246640 k80 ( 0 :8 ) lac-344 merzjrke MIXED 12 .17: 22 6 : 28 25456 : 246640 k80 ( 0 :8 ) lac-345 merzjrke MIXED+COMPL 16 .13: 22 6 : 28 173488 : 246640 k80 ( 2 :8 ) lac-346 merzjrke MIXED 14 .60: 8 20 : 28 181104 : 246640 k80 ( 0 :8 ) lac-347 merzjrke DOWN* N/A: 0 28 : 28 246640 : 246640 k80 ( 8 :8 ) Climer - Need power reset 4 -21-20 [ climer@2020-04-21T07:58:48 ] lac-348 feig-covid MIXED 1 .12: 14 14 : 28 66640 : 246640 k80 ( 3 :8 ) lac-349 merzjrke MIXED+DRAIN 59 .62: 4 24 : 28 238448 : 246640 k80 ( 4 :8 ) NHC: Script timed out while executing \"icer_check_gpu_count 8 syslog die\" . [ root@2020-04-22T03:38:40 ] lac-350 phani MIXED 16 .54: 26 2 : 28 5372 : 118012 N/A lac-351 phani MIXED 15 .30: 26 2 : 28 5372 : 118012 N/A lac-352 phani MIXED 0 .36: 26 2 : 28 5372 : 118012 N/A lac-353 general ALLOCATED 27 .20: 28 0 : 28 1564 : 118012 N/A lac-354 general MIXED 12 .16: 15 13 : 28 7616 : 118012 N/A lac-355 general MIXED 8 .23: 23 5 : 28 252 : 118012 N/A lac-356 general MIXED 20 .96: 20 8 : 28 8444 : 118012 N/A lac-357 general MIXED 23 .99: 23 5 : 28 3328 : 118012 N/A lac-358 general MIXED 21 .31: 21 7 : 28 7420 : 118012 N/A lac-359 general MIXED 20 .98: 21 7 : 28 252 : 118012 N/A lac-360 general MIXED 12 .09: 17 11 : 28 2300 : 118012 N/A lac-361 junlin MIXED 10 .38: 13 15 : 28 3324 : 118012 N/A lac-362 junlin MIXED 6 .62: 7 21 : 28 3324 : 118012 N/A lac-363 general ALLOCATED 18 .79: 28 0 : 28 16640 : 118012 N/A lac-364 general MIXED 13 .36: 26 2 : 28 11790 : 118012 N/A lac-365 SPG ALLOCATED 28 .32: 28 0 : 28 15612 : 118012 N/A lac-366 SPG ALLOCATED 28 .36: 28 0 : 28 15612 : 118012 N/A lac-367 SPG ALLOCATED 28 .12: 28 0 : 28 15612 : 118012 N/A lac-368 hirn MIXED 4 .85: 26 2 : 28 5372 : 118012 N/A lac-369 hirn MIXED 19 .90: 26 2 : 28 5372 : 118012 N/A lac-372 general MIXED 22 .19: 26 2 : 28 1352 : 118012 N/A lac-374 general MIXED 20 .98: 22 6 : 28 256 : 118012 N/A lac-375 general MIXED 24 .19: 27 1 : 28 328 : 118012 N/A lac-376 general MIXED 23 .12: 23 5 : 28 5372 : 118012 N/A lac-377 general MIXED 26 .12: 26 2 : 28 4766 : 118012 N/A lac-378 general MIXED 11 .36: 13 15 : 28 1280 : 118012 N/A lac-379 general MIXED 13 .28: 14 14 : 28 51452 : 118012 N/A lac-380 general MIXED 18 .07: 26 2 : 28 328 : 118012 N/A lac-381 general MIXED 22 .88: 20 8 : 28 9208 : 118012 N/A lac-382 general ALLOCATED 15 .15: 28 0 : 28 1276 : 118012 N/A lac-383 general MIXED 7 .11: 16 12 : 28 85244 : 118012 N/A lac-384 general MIXED 2 .30: 2 26 : 28 75004 : 118012 N/A lac-385 general ALLOCATED 13 .14: 28 0 : 28 3612 : 118012 N/A lac-386 general ALLOCATED 24 .06: 28 0 : 28 11054 : 118012 N/A lac-387 general MIXED 16 .07: 27 1 : 28 13564 : 118012 N/A lac-388 general ALLOCATED 24 .46: 28 0 : 28 5396 : 118012 N/A lac-389 general MIXED 25 .71: 24 4 : 28 44284 : 118012 N/A lac-390 general MIXED 21 .11: 25 3 : 28 252 : 118012 N/A lac-391 general MIXED 26 .68: 27 1 : 28 6420 : 118012 N/A lac-392 general MIXED 16 .20: 16 12 : 28 256 : 118012 N/A lac-393 general MIXED 14 .26: 20 8 : 28 12540 : 118012 N/A lac-394 general MIXED 5 .98: 24 4 : 28 252 : 118012 N/A lac-395 general ALLOCATED 15 .00: 28 0 : 28 3324 : 118012 N/A lac-396 general MIXED 20 .06: 25 3 : 28 1280 : 118012 N/A lac-397 general MIXED 6 .43: 25 3 : 28 252 : 118012 N/A lac-398 general MIXED 17 .20: 17 11 : 28 11516 : 118012 N/A lac-399 general MIXED 20 .98: 25 3 : 28 6396 : 118012 N/A lac-400 tsang DOWN+DRAIN 0 .02: 0 28 : 28 118012 : 118012 N/A Testing climer-2-12-2- [ climer@2020-02-12T07:43:21 ] lac-401 general MIXED 29 .11: 24 4 : 28 4290 : 118012 N/A lac-402 general ALLOCATED 26 .10: 28 0 : 28 1300 : 118012 N/A lac-403 general MIXED 22 .28: 25 3 : 28 8444 : 118012 N/A lac-404 general ALLOCATED 25 .60: 28 0 : 28 1276 : 118012 N/A lac-405 general MIXED 9 .17: 15 13 : 28 3324 : 118012 N/A lac-406 general MIXED 17 .90: 25 3 : 28 3324 : 118012 N/A lac-407 general MIXED 21 .03: 25 3 : 28 252 : 118012 N/A lac-408 general MIXED 17 .83: 18 10 : 28 252 : 118012 N/A lac-409 general MIXED 26 .63: 24 4 : 28 3324 : 118012 N/A lac-410 general MIXED 15 .10: 18 10 : 28 252 : 118012 N/A lac-411 general MIXED 17 .07: 22 6 : 28 256 : 118012 N/A lac-412 general MIXED 17 .11: 27 1 : 28 252 : 118012 N/A lac-413 general MIXED 16 .18: 16 12 : 28 256 : 118012 N/A lac-414 general MIXED 17 .79: 18 10 : 28 252 : 118012 N/A lac-415 general MIXED 16 .32: 18 10 : 28 252 : 118012 N/A lac-416 general MIXED 9 .03: 9 19 : 28 252 : 118012 N/A lac-417 general MIXED 10 .28: 18 10 : 28 1276 : 118012 N/A lac-418 general MIXED 6 .25: 6 22 : 28 252 : 118012 N/A lac-419 general ALLOCATED 19 .17: 28 0 : 28 2300 : 118012 N/A lac-420 general ALLOCATED 14 .05: 28 0 : 28 8444 : 118012 N/A lac-421 classres MIXED 9 .21: 17 11 : 28 3324 : 118012 N/A lac-422 general MIXED 18 .12: 26 2 : 28 1016 : 118012 N/A lac-423 general MIXED 20 .79: 24 4 : 28 4348 : 118012 N/A lac-424 general MIXED 18 .08: 27 1 : 28 4348 : 118012 N/A lac-425 general MIXED 12 .11: 25 3 : 28 252 : 118012 N/A lac-426 general MIXED 12 .41: 19 9 : 28 1276 : 118012 N/A lac-427 general MIXED 9 .20: 9 19 : 28 252 : 118012 N/A lac-428 general DOWN* N/A: 0 28 : 28 118012 : 118012 N/A Climer - waiting repair 4 -8-20 [ climer@2020-04-08T12:49:48 ] lac-429 general MIXED 15 .89: 27 1 : 28 1276 : 118012 N/A lac-430 general MIXED 13 .12: 18 10 : 28 256 : 118012 N/A lac-431 general MIXED 24 .13: 26 2 : 28 1276 : 118012 N/A lac-432 general MIXED 5 .13: 11 17 : 28 252 : 118012 N/A lac-433 general MIXED 25 .08: 26 2 : 28 742 : 118012 N/A lac-434 general MIXED 22 .59: 26 2 : 28 2300 : 118012 N/A lac-435 general MIXED 17 .03: 24 4 : 28 252 : 118012 N/A lac-436 general MIXED 20 .95: 21 7 : 28 3328 : 118012 N/A lac-437 general MIXED 20 .11: 21 7 : 28 1280 : 118012 N/A lac-438 general MIXED 13 .43: 20 8 : 28 37116 : 118012 N/A lac-439 general MIXED 27 .93: 25 3 : 28 3324 : 118012 N/A lac-440 general ALLOCATED 24 .46: 28 0 : 28 2300 : 118012 N/A lac-441 general MIXED 13 .90: 14 14 : 28 252 : 118012 N/A lac-442 general ALLOCATED 28 .16: 28 0 : 28 3266 : 118012 N/A lac-443 general MIXED 9 .47: 17 11 : 28 252 : 118012 N/A lac-444 general MIXED 8 .46: 26 2 : 28 252 : 118012 N/A lac-445 general ALLOCATED 28 .01: 28 0 : 28 7444 : 118012 N/A lac = > 69 .0% ( buyin ) 98 .8% ( 426 ) 43 .5%: 65 .7% ( 11928 ) 81 .6% ( 68 .2Tb ) 70 % ( 384 ) Usage% ( Total ) nvl-000 piermaro MIXED 11 .84: 18 22 : 40 183394 : 376162 v100 ( 3 :8 ) nvl-001 cmse MIXED 39 .45: 36 4 : 40 228706 : 376162 v100 ( 8 :8 ) nvl-002 DicksonLab ALLOCATED 36 .61: 40 0 : 40 208706 : 376162 v100 ( 7 :8 ) nvl-003 DicksonLab ALLOCATED 36 .77: 40 0 : 40 208706 : 376162 v100 ( 7 :8 ) nvl-004 DicksonLab MIXED 10 .23: 18 22 : 40 183394 : 376162 v100 ( 3 :8 ) nvl-005 alexrd-covid MIXED 7 .60: 8 32 : 40 1378 : 376162 v100 ( 1 :8 ) nvl-006 alexrd-covid MIXED 10 .77: 24 16 : 40 239090 : 376162 v100 ( 0 :8 ) nvl-007 general MIXED 8 .04: 8 32 : 40 370162 : 376162 v100 ( 0 :8 ) nvl = > 87 .5% ( buyin ) 100 .0% ( 8 ) 50 .4%: 60 .0% ( 320 ) 46 .0% ( 2 .87Tb ) 55 % ( 64 ) Usage% ( Total ) qml-000 general ALLOCATED 40 .04: 48 0 : 48 446368 :3067808 N/A qml-001 gmiaslab ALLOCATED 48 .19: 48 0 : 48 1037264 :1528784 N/A qml-002 ged ALLOCATED 48 .05: 48 0 : 48 1037264 :1528784 N/A qml-003 horticulture ALLOCATED 43 .91: 48 0 : 48 909300 :1015776 N/A qml-004 mitchmcg ALLOCATED 48 .27: 48 0 : 48 1037264 :1528784 N/A qml-005 general MIXED 69 .44: 74 22 : 96 3198784 :6145856 N/A qml = > 66 .7% ( buyin ) 100 .0% ( 6 ) 88 .7%: 93 .5% ( 336 ) 48 .3% ( 14 .1Tb ) N/A ( 0 ) Usage% ( Total ) skl-000 devolab MIXED 17 .59: 5 35 : 40 35682 : 85858 N/A skl-001 devolab MIXED 11 .33: 7 33 : 40 33634 : 85858 N/A skl-002 devolab MIXED 5 .62: 4 36 : 40 36706 : 85858 N/A skl-003 devolab MIXED 12 .21: 12 28 : 40 3938 : 85858 N/A skl-004 tsangm ALLOCATED 3 .16: 40 0 : 40 3938 : 85858 N/A skl-005 plzbuyin MIXED 5 .22: 12 28 : 40 3938 : 85858 N/A skl-006 allenmc MIXED 9 .52: 6 34 : 40 61282 : 85858 N/A skl-007 allenmc MIXED 8 .55: 6 34 : 40 61282 : 85858 N/A skl-008 allenmc MIXED 9 .07: 9 31 : 40 12130 : 85858 N/A skl-009 allenmc MIXED 6 .03: 6 34 : 40 61282 : 85858 N/A skl-010 allenmc MIXED 12 .01: 12 28 : 40 3938 : 85858 N/A skl-011 allenmc MIXED 12 .03: 12 28 : 40 3938 : 85858 N/A skl-012 seiswei MIXED 12 .12: 12 28 : 40 3938 : 85858 N/A skl-013 junlin MIXED 24 .62: 6 34 : 40 61282 : 85858 N/A skl-014 junlin MIXED 6 .03: 6 34 : 40 61282 : 85858 N/A skl-015 junlin MIXED 34 .88: 34 6 : 40 3938 : 85858 N/A skl-016 junlin MIXED 5 .66: 12 28 : 40 3938 : 85858 N/A skl-017 klausner ALLOCATED 40 .05: 40 0 : 40 3938 : 85858 N/A skl-018 klausner MIXED 12 .16: 12 28 : 40 3938 : 85858 N/A skl-019 klausner MIXED 12 .34: 12 28 : 40 3938 : 85858 N/A skl-020 klausner MIXED 12 .18: 12 28 : 40 3938 : 85858 N/A skl-021 cbcclab ALLOCATED 32 .85: 40 0 : 40 21858 : 85858 N/A skl-022 cbcclab MIXED 12 .28: 12 28 : 40 3938 : 85858 N/A skl-023 guowei-search MIXED 5 .18: 12 28 : 40 3938 : 85858 N/A skl-024 edgerpat MIXED 7 .65: 38 2 : 40 3938 : 85858 N/A skl-025 edgerpat MIXED 12 .18: 12 28 : 40 3938 : 85858 N/A skl-026 guowei-search MIXED 5 .48: 12 28 : 40 3938 : 85858 N/A skl-027 guowei-search ALLOCATED 38 .56: 40 0 : 40 3938 : 85858 N/A skl-028 guowei-search MIXED 5 .07: 12 28 : 40 3938 : 85858 N/A skl-029 guowei-search MIXED 5 .14: 12 28 : 40 3938 : 85858 N/A skl-030 guowei-search MIXED 5 .11: 12 28 : 40 3938 : 85858 N/A skl-031 guowei-search MIXED 5 .07: 12 28 : 40 3938 : 85858 N/A skl-032 guowei-search MIXED 5 .12: 12 28 : 40 3938 : 85858 N/A skl-033 guowei-search MIXED 5 .06: 12 28 : 40 3938 : 85858 N/A skl-034 guowei-search MIXED 5 .23: 12 28 : 40 3938 : 85858 N/A skl-035 guowei-search MIXED 5 .17: 12 28 : 40 3938 : 85858 N/A skl-036 guowei-search MIXED 5 .12: 12 28 : 40 3938 : 85858 N/A skl-037 guowei-search MIXED 28 .03: 28 12 : 40 28514 : 85858 N/A skl-038 guowei-search MIXED 5 .27: 12 28 : 40 3938 : 85858 N/A skl-039 guowei-search MIXED 5 .23: 12 28 : 40 3938 : 85858 N/A skl-040 guowei-search MIXED 5 .13: 12 28 : 40 3938 : 85858 N/A skl-041 guowei-search MIXED 5 .07: 12 28 : 40 3938 : 85858 N/A skl-042 guowei-search MIXED 5 .08: 12 28 : 40 3938 : 85858 N/A skl-043 guowei-search MIXED 5 .07: 12 28 : 40 3938 : 85858 N/A skl-044 guowei-search MIXED 5 .10: 12 28 : 40 3938 : 85858 N/A skl-045 guowei-search MIXED 5 .10: 12 28 : 40 3938 : 85858 N/A skl-046 guowei-search MIXED 5 .14: 13 27 : 40 1890 : 85858 N/A skl-047 guowei-search MIXED 5 .06: 13 27 : 40 1890 : 85858 N/A skl-048 guowei-search MIXED 5 .16: 13 27 : 40 1890 : 85858 N/A skl-049 guowei-search MIXED 5 .07: 13 27 : 40 1890 : 85858 N/A skl-050 guowei-search MIXED 28 .02: 28 12 : 40 28514 : 85858 N/A skl-051 guowei-search MIXED 28 .83: 29 11 : 40 8034 : 85858 N/A skl-052 guowei-search MIXED 5 .21: 25 15 : 40 1890 : 85858 N/A skl-053 guowei-search MIXED 4 .79: 25 15 : 40 1890 : 85858 N/A skl-054 guowei-search MIXED 3 .27: 19 21 : 40 14178 : 85858 N/A skl-055 guowei-search MIXED 2 .68: 16 24 : 40 20322 : 85858 N/A skl-056 guowei-search MIXED 2 .66: 16 24 : 40 20322 : 85858 N/A skl-057 general ALLOCATED 77 .50: 40 0 : 40 2466 : 85858 N/A skl-058 general MIXED 21 .20: 27 13 : 40 874 : 85858 N/A skl-059 general MIXED 20 .06: 31 9 : 40 870 : 85858 N/A skl-060 general MIXED 8 .15: 7 33 : 40 866 : 85858 N/A skl-061 general MIXED 19 .24: 18 22 : 40 12130 : 85858 N/A skl-062 general MIXED 36 .90: 38 2 : 40 1914 : 85858 N/A skl-063 general MIXED 33 .71: 36 4 : 40 890 : 85858 N/A skl-064 general MIXED 25 .42: 26 14 : 40 890 : 85858 N/A skl-065 general MIXED 38 .71: 39 1 : 40 890 : 85858 N/A skl-066 general ALLOCATED 40 .02: 40 0 : 40 3938 : 85858 N/A skl-067 general ALLOCATED 40 .09: 40 0 : 40 3938 : 85858 N/A skl-068 general MIXED 15 .94: 25 15 : 40 1890 : 85858 N/A skl-069 general MIXED 21 .71: 23 17 : 40 3942 : 85858 N/A skl-070 general ALLOCATED 22 .31: 40 0 : 40 1894 : 85858 N/A skl-071 general MIXED 16 .65: 30 10 : 40 1890 : 85858 N/A skl-072 general MIXED 22 .15: 22 18 : 40 866 : 85858 N/A skl-073 general ALLOCATED 40 .09: 40 0 : 40 3938 : 85858 N/A skl-074 general MIXED 19 .91: 22 18 : 40 870 : 85858 N/A skl-075 general MIXED 37 .57: 36 4 : 40 4514 : 85858 N/A skl-076 general ALLOCATED 38 .08: 40 0 : 40 1442 : 85858 N/A skl-077 general MIXED 23 .94: 34 6 : 40 866 : 85858 N/A skl-078 general MIXED 25 .81: 28 12 : 40 870 : 85858 N/A skl-079 general MIXED 22 .93: 36 4 : 40 866 : 85858 N/A skl-080 general MIXED 16 .86: 20 20 : 40 866 : 85858 N/A skl-081 general MIXED 35 .24: 35 5 : 40 418 : 85858 N/A skl-082 general MIXED 24 .07: 30 10 : 40 866 : 85858 N/A skl-083 general MIXED 2 .61: 6 34 : 40 866 : 85858 N/A skl-084 general MIXED 21 .03: 23 17 : 40 866 : 85858 N/A skl-085 general MIXED 26 .66: 25 15 : 40 866 : 85858 N/A skl-086 general MIXED 39 .28: 39 1 : 40 1442 : 85858 N/A skl-087 general MIXED 17 .81: 22 18 : 40 1890 : 85858 N/A skl-088 general MIXED 5 .93: 2 38 : 40 3938 : 85858 N/A skl-089 general MIXED 16 .11: 15 25 : 40 866 : 85858 N/A skl-090 general MIXED 5 .74: 6 34 : 40 866 : 85858 N/A skl-091 general MIXED 8 .68: 14 26 : 40 866 : 85858 N/A skl-092 general MIXED 16 .46: 19 21 : 40 1890 : 85858 N/A skl-093 general MIXED 22 .51: 22 18 : 40 870 : 85858 N/A skl-094 general MIXED 31 .44: 39 1 : 40 1906 : 85858 N/A skl-095 general MIXED 27 .04: 29 11 : 40 870 : 85858 N/A skl-096 general MIXED 9 .76: 10 30 : 40 866 : 85858 N/A skl-097 general MIXED 9 .76: 10 30 : 40 866 : 85858 N/A skl-098 general MIXED 21 .58: 22 18 : 40 866 : 85858 N/A skl-099 general MIXED 24 .98: 36 4 : 40 870 : 85858 N/A skl-100 general MIXED 20 .48: 21 19 : 40 1890 : 85858 N/A skl-101 general MIXED 18 .06: 21 19 : 40 866 : 85858 N/A skl-102 general MIXED 7 .36: 15 25 : 40 866 : 85858 N/A skl-103 general MIXED 39 .21: 39 1 : 40 1442 : 85858 N/A skl-104 general MIXED 18 .07: 27 13 : 40 1890 : 85858 N/A skl-105 general ALLOCATED 9 .77: 40 0 : 40 3938 : 85858 N/A skl-106 general MIXED 10 .78: 6 34 : 40 1890 : 85858 N/A skl-107 general MIXED 16 .02: 18 22 : 40 870 : 85858 N/A skl-108 general MIXED 22 .08: 33 7 : 40 866 : 85858 N/A skl-109 general MIXED 6 .94: 11 29 : 40 866 : 85858 N/A skl-110 general MIXED 9 .05: 6 34 : 40 3938 : 85858 N/A skl-111 general MIXED 37 .53: 37 3 : 40 418 : 85858 N/A skl-112 general MIXED 5 .12: 4 36 : 40 866 : 85858 N/A skl-113 zayernouri_fmath MIXED 9 .68: 16 24 : 40 2402 : 182626 N/A skl-114 zayernouri_fmath MIXED 9 .21: 16 24 : 40 2402 : 182626 N/A skl-115 zayernouri_fmath MIXED 9 .39: 16 24 : 40 2402 : 182626 N/A skl-116 niederhu MIXED 9 .19: 16 24 : 40 2402 : 182626 N/A skl-117 daylab MIXED 12 .15: 12 28 : 40 2402 : 182626 N/A skl-118 junlin MIXED 12 .01: 12 28 : 40 2402 : 182626 N/A skl-119 pollyhsu MIXED 24 .21: 24 16 : 40 2402 : 182626 N/A skl-120 yueqibuyin ALLOCATED 40 .23: 40 0 : 40 18786 : 182626 N/A skl-121 yueqibuyin ALLOCATED 40 .05: 40 0 : 40 18786 : 182626 N/A skl-122 yueqibuyin ALLOCATED 40 .09: 40 0 : 40 18786 : 182626 N/A skl-123 yueqibuyin ALLOCATED 40 .09: 40 0 : 40 18786 : 182626 N/A skl-124 plzbuyin MIXED 12 .01: 12 28 : 40 2402 : 182626 N/A skl-125 hcy MIXED 12 .06: 12 28 : 40 2402 : 182626 N/A skl-126 hcy MIXED 12 .02: 12 28 : 40 2402 : 182626 N/A skl-127 hcy MIXED 10 .16: 10 30 : 40 10594 : 182626 N/A skl-128 junlin MIXED 37 .04: 37 3 : 40 14690 : 182626 N/A skl-129 zayernouri_fmath MIXED 11 .95: 30 10 : 40 20834 : 182626 N/A skl-130 zayernouri_fmath MIXED 4 .10: 4 36 : 40 133474 : 182626 N/A skl-131 zayernouri_fmath MIXED 6 .21: 12 28 : 40 67938 : 182626 N/A skl-132 qian MIXED 13 .45: 34 6 : 40 7522 : 376162 N/A skl-133 vmante IDLE+DRAIN 0 .01: 0 40 : 40 376162 : 376162 N/A Low RealMemory [ slurm@2020-04-22T09:05:12 ] skl-134 liulab MIXED 9 .15: 9 31 : 40 7522 : 376162 N/A skl-135 chenlab MIXED 11 .39: 24 16 : 40 81250 : 376162 N/A skl-136 junlin MIXED 18 .01: 18 22 : 40 7522 : 376162 N/A skl-137 mitchmcg MIXED 13 .91: 20 20 : 40 97634 : 376162 N/A skl-138 eisenlohr MIXED 8 .38: 8 32 : 40 245090 : 376162 N/A skl-139 davidroy MIXED 17 .32: 24 16 : 40 15714 : 376162 N/A skl-140 shadeash-colej MIXED 35 .14: 35 5 : 40 79202 : 763234 N/A skl-141 shadeash-colej ALLOCATED 40 .17: 40 0 : 40 566626 : 763234 N/A skl-142 cbcclab ALLOCATED 34 .51: 40 0 : 40 699234 : 763234 N/A skl-143 general MIXED 32 .10: 32 8 : 40 370018 : 763234 N/A skl-144 guowei-search MIXED 7 .21: 35 5 : 40 152930 : 763234 N/A skl-145 general ALLOCATED 32 .83: 40 0 : 40 304482 : 763234 N/A skl-146 guowei-search MIXED 36 .05: 36 4 : 40 615778 : 763234 N/A skl-147 general MIXED 3 .03: 13 27 : 40 42338 : 763234 N/A skl-148 davidroy MIXED 19 .03: 19 21 : 40 19810 : 376162 N/A skl-149 davidroy MIXED 19 .02: 19 21 : 40 19810 : 376162 N/A skl-150 davidroy MIXED 35 .41: 35 5 : 40 3426 : 376162 N/A skl-151 davidroy MIXED 18 .07: 37 3 : 40 42338 : 376162 N/A skl-152 davidroy MIXED 19 .02: 19 21 : 40 19810 : 376162 N/A skl-153 davidroy ALLOCATED 40 .38: 40 0 : 40 179554 : 376162 N/A skl-154 davidroy MIXED 35 .09: 35 5 : 40 3426 : 376162 N/A skl-155 davidroy MIXED 35 .24: 35 5 : 40 3426 : 376162 N/A skl-156 davidroy MIXED 19 .02: 19 21 : 40 19810 : 376162 N/A skl-157 davidroy MIXED 23 .04: 23 17 : 40 3426 : 376162 N/A skl-158 davidroy MIXED 23 .06: 23 17 : 40 3426 : 376162 N/A skl-159 davidroy ALLOCATED 40 .11: 40 0 : 40 179554 : 376162 N/A skl-160 davidroy ALLOCATED 40 .29: 40 0 : 40 146786 : 376162 N/A skl-161 zayernouri_fmath MIXED 22 .03: 22 18 : 40 3426 : 376162 N/A skl-162 general MIXED 37 .09: 37 3 : 40 3426 : 376162 N/A skl-163 general ALLOCATED 40 .48: 40 0 : 40 107874 : 376162 N/A skl-164 general ALLOCATED 40 .10: 40 0 : 40 83298 : 376162 N/A skl-165 general ALLOCATED 2 .42: 40 0 : 40 179554 : 376162 N/A skl-166 guowei-search MIXED 11 .29: 31 9 : 40 3426 : 376162 N/A skl-167 guowei-search MIXED 11 .12: 30 10 : 40 23906 : 376162 N/A skl = > 62 .5% ( buyin ) 99 .4% ( 168 ) 45 .6%: 55 .6% ( 6720 ) 80 .3% ( 28 .4Tb ) N/A ( 0 ) Usage% ( Total ) vim-000 hsu MIXED 1474 .82: 23 41 : 64 53152 :3067808 N/A vim-001 general MIXED 10 .42: 31 33 : 64 970656 :3067808 N/A vim-002 ccg MIXED 66 .14: 63 81 :144 5427008 :6145856 N/A intel14 = > 34 .5% ( buyin ) 91 .7% ( 168 ) 47 .8%: 62 .7% ( 3576 ) 60 .1% ( 31 .1Tb ) 97 % ( 78 ) Usage% ( Total ) intel16 = > 69 .0% ( buyin ) 98 .8% ( 429 ) 55 .2%: 65 .1% ( 12200 ) 76 .6% ( 79 .9Tb ) 70 % ( 384 ) Usage% ( Total ) intel18 = > 63 .6% ( buyin ) 99 .4% ( 176 ) 45 .8%: 55 .8% ( 7040 ) 77 .1% ( 31 .3Tb ) 55 % ( 64 ) Usage% ( Total ) Summary = > 60 .3% ( buyin ) 97 .4% ( 773 ) 51 .2%: 61 .9% ( 22816 ) 73 .1% ( 142Tb ) 72 % ( 526 ) Usage% ( Total )","title":"node_status"},{"location":"obtain_an_hpcc_account/","text":"Obtain an HPCC account Every user needs to have an account to use the HPCC. To obtain an HPCC account, follow the directions below. Current MSU-affiliated student or personnel HPCC accounts are free for all MSU researchers. To obtain (or re-activate) an account, a tenure-track faculty MSU Principal Investigator (PI) must complete a New Account Request for themselves and their collaborators (students, staff, post-docs, and collaborators with NetIDs.) Information required to complete the form includes: a list of the names and MSU NetIDs of collaborators (students, post-docs, visiting scholars and/or off-campus colleagues) requiring accounts. For collaborators without a MSU NetID, the PI can sponsor MSU Department Sponsored NetIDs by paying MSU ID Office with a department account number. See below section for more details on departmental NetIDs. Note All MSU Net IDs used to access the HPCC must be associated with a valid email address. a statement on whether or not export controlled software or data will be used a project abstract (description about the user's research.) By applying for a HPCC account, the Principal Investigator is agreeing that all group members will abide by MSU's Acceptable Use Policy . ICER workshops If you are going to attend an ICER workshop, you may get a temporary HPCC account. Please contact ICER External collaborators To request an HPCC account for an external collaborator, please obtain a login-only, non-email FPID NetID for the collaborator first. Please contact the ID Office at idoffice@msu.edu or 517-355-4500 for more information, or visit their website . Once the NetID is acquired, a MSU PI can follow the above instructions to apply for their HPCC account. Since the purchased NetID is not associated with a MSU e-mail account, the PI must provide a current contact e-mail address of the external collaborator when filling out the New Account Request form. Previous users affiliated with MSU If you were previously affiliated with MSU and had a HPCC account, then you may find that your HPCC account was disabled after a time. If you are still collaborating with a regular MSU faculty or staff member, you will need a Principal Investigator (PI) with a tenure-track appointment at MSU to sponsor the renewal of your account on an annual basis. The form for sponsoring the renewal of an existing HPCC account is at Sponsored Renewal . Only PIs may fill out this form. Other universities in Michigan If you are affiliated with the following universities, please contact the respective personnel to apply for a HPCC account: University Contact More information Oakland University ( oakland.edu ) Mario Nowak: nowak@oakland.edu Thomas Hajek: hajek@oakland.edu Oakland University info page Kettering University ( kettering.edu ) (kettering buy-in) Salomon Turgman Cohen: sturgman@gmail.com Kettering info (ryankettering buy-in) Gillian Ryan: gryan@kettering.edu Western Michigan University ( wmich.edu ) Joel Fletcher: joel.fletcher@wmich.edu Western Michigan info Central Michigan University ( cmich.edu ) Mel Taylor: taylo1ml@msu.edu or mel.taylor@cmich.edu Central Michigan info","title":"Obtain an HPCC account"},{"location":"obtain_an_hpcc_account/#obtain-an-hpcc-account","text":"Every user needs to have an account to use the HPCC. To obtain an HPCC account, follow the directions below.","title":"Obtain an HPCC account"},{"location":"obtain_an_hpcc_account/#current-msu-affiliated-student-or-personnel","text":"HPCC accounts are free for all MSU researchers. To obtain (or re-activate) an account, a tenure-track faculty MSU Principal Investigator (PI) must complete a New Account Request for themselves and their collaborators (students, staff, post-docs, and collaborators with NetIDs.) Information required to complete the form includes: a list of the names and MSU NetIDs of collaborators (students, post-docs, visiting scholars and/or off-campus colleagues) requiring accounts. For collaborators without a MSU NetID, the PI can sponsor MSU Department Sponsored NetIDs by paying MSU ID Office with a department account number. See below section for more details on departmental NetIDs. Note All MSU Net IDs used to access the HPCC must be associated with a valid email address. a statement on whether or not export controlled software or data will be used a project abstract (description about the user's research.) By applying for a HPCC account, the Principal Investigator is agreeing that all group members will abide by MSU's Acceptable Use Policy .","title":"Current MSU-affiliated student or personnel"},{"location":"obtain_an_hpcc_account/#icer-workshops","text":"If you are going to attend an ICER workshop, you may get a temporary HPCC account. Please contact ICER","title":"ICER workshops"},{"location":"obtain_an_hpcc_account/#external-collaborators","text":"To request an HPCC account for an external collaborator, please obtain a login-only, non-email FPID NetID for the collaborator first. Please contact the ID Office at idoffice@msu.edu or 517-355-4500 for more information, or visit their website . Once the NetID is acquired, a MSU PI can follow the above instructions to apply for their HPCC account. Since the purchased NetID is not associated with a MSU e-mail account, the PI must provide a current contact e-mail address of the external collaborator when filling out the New Account Request form.","title":"External collaborators"},{"location":"obtain_an_hpcc_account/#previous-users-affiliated-with-msu","text":"If you were previously affiliated with MSU and had a HPCC account, then you may find that your HPCC account was disabled after a time. If you are still collaborating with a regular MSU faculty or staff member, you will need a Principal Investigator (PI) with a tenure-track appointment at MSU to sponsor the renewal of your account on an annual basis. The form for sponsoring the renewal of an existing HPCC account is at Sponsored Renewal . Only PIs may fill out this form.","title":"Previous users affiliated with MSU"},{"location":"obtain_an_hpcc_account/#other-universities-in-michigan","text":"If you are affiliated with the following universities, please contact the respective personnel to apply for a HPCC account: University Contact More information Oakland University ( oakland.edu ) Mario Nowak: nowak@oakland.edu Thomas Hajek: hajek@oakland.edu Oakland University info page Kettering University ( kettering.edu ) (kettering buy-in) Salomon Turgman Cohen: sturgman@gmail.com Kettering info (ryankettering buy-in) Gillian Ryan: gryan@kettering.edu Western Michigan University ( wmich.edu ) Joel Fletcher: joel.fletcher@wmich.edu Western Michigan info Central Michigan University ( cmich.edu ) Mel Taylor: taylo1ml@msu.edu or mel.taylor@cmich.edu Central Michigan info","title":"Other universities in Michigan"},{"location":"orthomcl-pipeline%202/","text":"orthomcl-pipeline OrthoMCL Pipeline ( https://github.com/apetkau/orthomcl-pipeline ) is a wrapper that automates running of OrthoMCL. If you prefer to run OrthoMCL from scratch, please skip this tutorial. Installation guide You could install orthomcl pipeline to your home directory (or research space), following the instruction of installing OrthoMCL pipeline . All the Perl dependencies have been installed by iCER staff, and you only need to run a couple of commands to complete the installation. Importantly, it's assumed that you have already prepared your MySQL configuration file (see https://wiki.hpcc.msu.edu/x/aYe1 ). Sample installation I am going to install the pipeline in a subdirectory under my home ~/Software/ . Installing OrthoMCL Pipeline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 ssh dev-intel18 # Load necessary modules module purge module load icc/2016.3.210-GCC-5.4.0-2.26 impi/5.1.3.181 module load OrthoMCL/2.0.9-custom-Perl-5.24.0 module load BLAST/2.2.26-Linux_x86_64 module load GCCcore/5.4.0 libxml2/2.9.4 # Download source and configure cd Software git clone https://github.com/apetkau/orthomcl-pipeline.git cd orthomcl-pipeline perl scripts/orthomcl-pipeline-setup.pl # set paths to dependencies cat etc/orthomcl-pipeline.conf # parameters in this file can be adjusted; consult the instruction linked above # --- # blast: # F: 'm S' # b: '100000' # e: '1e-5' # v: '100000' # filter: # max_percent_stop: '20' # min_length: '10' # mcl: # inflation: '1.5' # path: # blastall: /opt/software/BLAST/2.2.26-Linux_x86_64/bin/blastall # formatdb: /opt/software/BLAST/2.2.26-Linux_x86_64/bin/formatdb # mcl: /opt/software/MCL/14.137-intel-2016b/bin/mcl # orthomcl: /opt/software/OrthoMCL/orthomclsoftware-custom/bin # scheduler: fork # split: '4' # Testing export PATH = ~/Software/orthomcl-pipeline/bin:~/Software/orthomcl-pipeline/scripts: $PATH perl t/test_pipeline.pl -m ~/Practice/general_test/orthomcl/my_orthomcl_dir/orthomcl.config -s fork -t ~/tmp # replace the path to orthomcl.config with your own Example: ortholog identification The tutorial is adapted from a tutorial hosted at https://github.com/apetkau/microbial-informatics-2014/tree/master/labs/orthomcl . We strongly recommend that you read it fully before starting the hands-on practice below, which is a much simplified version of the original one and serves as a demo only. The datasets containing a set of V. Cholerae genomes are located in mnt/research/common-data/Bio/orthomcl-data/ . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ssh dev-intel18 # Then go to your orthomcl working directory # Load necessary modules module purge module load icc/2016.3.210-GCC-5.4.0-2.26 impi/5.1.3.181 module load OrthoMCL/2.0.9-custom-Perl-5.24.0 module load BLAST/2.2.26-Linux_x86_64 module load GCCcore/5.4.0 libxml2/2.9.4 export PATH = ~/Software/orthomcl-pipeline/bin:~/Software/orthomcl-pipeline/scripts: $PATH # Run orthomcl pipeline (replace the path to orthomcl.config with your own) orthomcl-pipeline -i /mnt/research/common-data/Bio/orthomcl-data -o orthomcl_out_tmp -m ~/Practice/general_test/orthomcl/my_orthomcl_dir/orthomcl.config --nocompliant # Visualize the results by drawing a Venn Diagram using a pipeline utility script nml_parse_orthomcl.pl -i orthomcl_out_tmp/groups/groups.txt -g /mnt/research/common-data/Bio/orthomcl-data/genome-groups.txt -s --draw -o orthomcl-stats.txt --genes # View the Venn Diagram plot (just for demo; you should transfer the svg to your local computer for better display effect) java -jar /opt/software/batik/batik-1.9/batik-squiggle-1.9.jar genome-groups.txt.svg Side note: as mentioned in the full tutorial, you need to answer \"yes\" to the database removal question in the course of the run.","title":"orthomcl-pipeline"},{"location":"orthomcl-pipeline%202/#orthomcl-pipeline","text":"OrthoMCL Pipeline ( https://github.com/apetkau/orthomcl-pipeline ) is a wrapper that automates running of OrthoMCL. If you prefer to run OrthoMCL from scratch, please skip this tutorial.","title":"orthomcl-pipeline"},{"location":"orthomcl-pipeline%202/#installation-guide","text":"You could install orthomcl pipeline to your home directory (or research space), following the instruction of installing OrthoMCL pipeline . All the Perl dependencies have been installed by iCER staff, and you only need to run a couple of commands to complete the installation. Importantly, it's assumed that you have already prepared your MySQL configuration file (see https://wiki.hpcc.msu.edu/x/aYe1 ).","title":"Installation guide"},{"location":"orthomcl-pipeline%202/#sample-installation","text":"I am going to install the pipeline in a subdirectory under my home ~/Software/ . Installing OrthoMCL Pipeline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 ssh dev-intel18 # Load necessary modules module purge module load icc/2016.3.210-GCC-5.4.0-2.26 impi/5.1.3.181 module load OrthoMCL/2.0.9-custom-Perl-5.24.0 module load BLAST/2.2.26-Linux_x86_64 module load GCCcore/5.4.0 libxml2/2.9.4 # Download source and configure cd Software git clone https://github.com/apetkau/orthomcl-pipeline.git cd orthomcl-pipeline perl scripts/orthomcl-pipeline-setup.pl # set paths to dependencies cat etc/orthomcl-pipeline.conf # parameters in this file can be adjusted; consult the instruction linked above # --- # blast: # F: 'm S' # b: '100000' # e: '1e-5' # v: '100000' # filter: # max_percent_stop: '20' # min_length: '10' # mcl: # inflation: '1.5' # path: # blastall: /opt/software/BLAST/2.2.26-Linux_x86_64/bin/blastall # formatdb: /opt/software/BLAST/2.2.26-Linux_x86_64/bin/formatdb # mcl: /opt/software/MCL/14.137-intel-2016b/bin/mcl # orthomcl: /opt/software/OrthoMCL/orthomclsoftware-custom/bin # scheduler: fork # split: '4' # Testing export PATH = ~/Software/orthomcl-pipeline/bin:~/Software/orthomcl-pipeline/scripts: $PATH perl t/test_pipeline.pl -m ~/Practice/general_test/orthomcl/my_orthomcl_dir/orthomcl.config -s fork -t ~/tmp # replace the path to orthomcl.config with your own","title":"Sample installation"},{"location":"orthomcl-pipeline%202/#example-ortholog-identification","text":"The tutorial is adapted from a tutorial hosted at https://github.com/apetkau/microbial-informatics-2014/tree/master/labs/orthomcl . We strongly recommend that you read it fully before starting the hands-on practice below, which is a much simplified version of the original one and serves as a demo only. The datasets containing a set of V. Cholerae genomes are located in mnt/research/common-data/Bio/orthomcl-data/ . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ssh dev-intel18 # Then go to your orthomcl working directory # Load necessary modules module purge module load icc/2016.3.210-GCC-5.4.0-2.26 impi/5.1.3.181 module load OrthoMCL/2.0.9-custom-Perl-5.24.0 module load BLAST/2.2.26-Linux_x86_64 module load GCCcore/5.4.0 libxml2/2.9.4 export PATH = ~/Software/orthomcl-pipeline/bin:~/Software/orthomcl-pipeline/scripts: $PATH # Run orthomcl pipeline (replace the path to orthomcl.config with your own) orthomcl-pipeline -i /mnt/research/common-data/Bio/orthomcl-data -o orthomcl_out_tmp -m ~/Practice/general_test/orthomcl/my_orthomcl_dir/orthomcl.config --nocompliant # Visualize the results by drawing a Venn Diagram using a pipeline utility script nml_parse_orthomcl.pl -i orthomcl_out_tmp/groups/groups.txt -g /mnt/research/common-data/Bio/orthomcl-data/genome-groups.txt -s --draw -o orthomcl-stats.txt --genes # View the Venn Diagram plot (just for demo; you should transfer the svg to your local computer for better display effect) java -jar /opt/software/batik/batik-1.9/batik-squiggle-1.9.jar genome-groups.txt.svg Side note: as mentioned in the full tutorial, you need to answer \"yes\" to the database removal question in the course of the run.","title":"Example: ortholog identification"},{"location":"orthomcl-pipeline/","text":"orthomcl-pipeline OrthoMCL Pipeline ( https://github.com/apetkau/orthomcl-pipeline ) is a wrapper that automates running of OrthoMCL. If you prefer to run OrthoMCL from scratch, please skip this tutorial. Installation guide You could install orthomcl pipeline to your home directory (or research space), following the instruction of installing OrthoMCL pipeline . All the Perl dependencies have been installed by iCER staff, and you only need to run a couple of commands to complete the installation. Importantly, it's assumed that you have already prepared your MySQL configuration file (see https://wiki.hpcc.msu.edu/x/aYe1 ). Sample installation I am going to install the pipeline in a subdirectory under my home ~/Software/ . Installing OrthoMCL Pipeline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 ssh dev-intel18 # Load necessary modules module purge module load icc/2016.3.210-GCC-5.4.0-2.26 impi/5.1.3.181 module load OrthoMCL/2.0.9-custom-Perl-5.24.0 module load BLAST/2.2.26-Linux_x86_64 module load GCCcore/5.4.0 libxml2/2.9.4 # Download source and configure cd Software git clone https://github.com/apetkau/orthomcl-pipeline.git cd orthomcl-pipeline perl scripts/orthomcl-pipeline-setup.pl # set paths to dependencies cat etc/orthomcl-pipeline.conf # parameters in this file can be adjusted; consult the instruction linked above # --- # blast: # F: 'm S' # b: '100000' # e: '1e-5' # v: '100000' # filter: # max_percent_stop: '20' # min_length: '10' # mcl: # inflation: '1.5' # path: # blastall: /opt/software/BLAST/2.2.26-Linux_x86_64/bin/blastall # formatdb: /opt/software/BLAST/2.2.26-Linux_x86_64/bin/formatdb # mcl: /opt/software/MCL/14.137-intel-2016b/bin/mcl # orthomcl: /opt/software/OrthoMCL/orthomclsoftware-custom/bin # scheduler: fork # split: '4' # Testing export PATH = ~/Software/orthomcl-pipeline/bin:~/Software/orthomcl-pipeline/scripts: $PATH perl t/test_pipeline.pl -m ~/Practice/general_test/orthomcl/my_orthomcl_dir/orthomcl.config -s fork -t ~/tmp # replace the path to orthomcl.config with your own Example: ortholog identification The tutorial is adapted from a tutorial hosted at https://github.com/apetkau/microbial-informatics-2014/tree/master/labs/orthomcl . We strongly recommend that you read it fully before starting the hands-on practice below, which is a much simplified version of the original one and serves as a demo only. The datasets containing a set of V. Cholerae genomes are located in mnt/research/common-data/Bio/orthomcl-data/ . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ssh dev-intel18 # Then go to your orthomcl working directory # Load necessary modules module purge module load icc/2016.3.210-GCC-5.4.0-2.26 impi/5.1.3.181 module load OrthoMCL/2.0.9-custom-Perl-5.24.0 module load BLAST/2.2.26-Linux_x86_64 module load GCCcore/5.4.0 libxml2/2.9.4 export PATH = ~/Software/orthomcl-pipeline/bin:~/Software/orthomcl-pipeline/scripts: $PATH # Run orthomcl pipeline (replace the path to orthomcl.config with your own) orthomcl-pipeline -i /mnt/research/common-data/Bio/orthomcl-data -o orthomcl_out_tmp -m ~/Practice/general_test/orthomcl/my_orthomcl_dir/orthomcl.config --nocompliant # Visualize the results by drawing a Venn Diagram using a pipeline utility script nml_parse_orthomcl.pl -i orthomcl_out_tmp/groups/groups.txt -g /mnt/research/common-data/Bio/orthomcl-data/genome-groups.txt -s --draw -o orthomcl-stats.txt --genes # View the Venn Diagram plot (just for demo; you should transfer the svg to your local computer for better display effect) java -jar /opt/software/batik/batik-1.9/batik-squiggle-1.9.jar genome-groups.txt.svg Note As mentioned in the full tutorial, you need to answer \"yes\" to the database removal question in the course of the run.","title":"OrthoMCL pipeline"},{"location":"orthomcl-pipeline/#orthomcl-pipeline","text":"OrthoMCL Pipeline ( https://github.com/apetkau/orthomcl-pipeline ) is a wrapper that automates running of OrthoMCL. If you prefer to run OrthoMCL from scratch, please skip this tutorial.","title":"orthomcl-pipeline"},{"location":"orthomcl-pipeline/#installation-guide","text":"You could install orthomcl pipeline to your home directory (or research space), following the instruction of installing OrthoMCL pipeline . All the Perl dependencies have been installed by iCER staff, and you only need to run a couple of commands to complete the installation. Importantly, it's assumed that you have already prepared your MySQL configuration file (see https://wiki.hpcc.msu.edu/x/aYe1 ).","title":"Installation guide"},{"location":"orthomcl-pipeline/#sample-installation","text":"I am going to install the pipeline in a subdirectory under my home ~/Software/ . Installing OrthoMCL Pipeline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 ssh dev-intel18 # Load necessary modules module purge module load icc/2016.3.210-GCC-5.4.0-2.26 impi/5.1.3.181 module load OrthoMCL/2.0.9-custom-Perl-5.24.0 module load BLAST/2.2.26-Linux_x86_64 module load GCCcore/5.4.0 libxml2/2.9.4 # Download source and configure cd Software git clone https://github.com/apetkau/orthomcl-pipeline.git cd orthomcl-pipeline perl scripts/orthomcl-pipeline-setup.pl # set paths to dependencies cat etc/orthomcl-pipeline.conf # parameters in this file can be adjusted; consult the instruction linked above # --- # blast: # F: 'm S' # b: '100000' # e: '1e-5' # v: '100000' # filter: # max_percent_stop: '20' # min_length: '10' # mcl: # inflation: '1.5' # path: # blastall: /opt/software/BLAST/2.2.26-Linux_x86_64/bin/blastall # formatdb: /opt/software/BLAST/2.2.26-Linux_x86_64/bin/formatdb # mcl: /opt/software/MCL/14.137-intel-2016b/bin/mcl # orthomcl: /opt/software/OrthoMCL/orthomclsoftware-custom/bin # scheduler: fork # split: '4' # Testing export PATH = ~/Software/orthomcl-pipeline/bin:~/Software/orthomcl-pipeline/scripts: $PATH perl t/test_pipeline.pl -m ~/Practice/general_test/orthomcl/my_orthomcl_dir/orthomcl.config -s fork -t ~/tmp # replace the path to orthomcl.config with your own","title":"Sample installation"},{"location":"orthomcl-pipeline/#example-ortholog-identification","text":"The tutorial is adapted from a tutorial hosted at https://github.com/apetkau/microbial-informatics-2014/tree/master/labs/orthomcl . We strongly recommend that you read it fully before starting the hands-on practice below, which is a much simplified version of the original one and serves as a demo only. The datasets containing a set of V. Cholerae genomes are located in mnt/research/common-data/Bio/orthomcl-data/ . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ssh dev-intel18 # Then go to your orthomcl working directory # Load necessary modules module purge module load icc/2016.3.210-GCC-5.4.0-2.26 impi/5.1.3.181 module load OrthoMCL/2.0.9-custom-Perl-5.24.0 module load BLAST/2.2.26-Linux_x86_64 module load GCCcore/5.4.0 libxml2/2.9.4 export PATH = ~/Software/orthomcl-pipeline/bin:~/Software/orthomcl-pipeline/scripts: $PATH # Run orthomcl pipeline (replace the path to orthomcl.config with your own) orthomcl-pipeline -i /mnt/research/common-data/Bio/orthomcl-data -o orthomcl_out_tmp -m ~/Practice/general_test/orthomcl/my_orthomcl_dir/orthomcl.config --nocompliant # Visualize the results by drawing a Venn Diagram using a pipeline utility script nml_parse_orthomcl.pl -i orthomcl_out_tmp/groups/groups.txt -g /mnt/research/common-data/Bio/orthomcl-data/genome-groups.txt -s --draw -o orthomcl-stats.txt --genes # View the Venn Diagram plot (just for demo; you should transfer the svg to your local computer for better display effect) java -jar /opt/software/batik/batik-1.9/batik-squiggle-1.9.jar genome-groups.txt.svg Note As mentioned in the full tutorial, you need to answer \"yes\" to the database removal question in the course of the run.","title":"Example: ortholog identification"},{"location":"parallel_computing/","text":"Parallel Computing Here we introduce three basic parallel models: Shared Memory, Distributed Memory and Hybrid Model . Shared Memory with Threads A main program loads and acquires all of the necessary resources to run the \"heavy weight\" process. It performs some serial work, and then creates a number of threads (\"light weight\") running by CPU cores concurrently. Each thread can have local data, but also, shares the entire resources, including RAM memory of the main program. Threads communicate with each other through global memory (RAM). This requires synchronization operations to ensure that no than one thread is updating the same RAM address at any time. Threads can come and go, but the main program remains present to provide the necessary shared resources until the application has completed. Examples: POSIX Threads, OpenMP, CUDA threads for GPUs Distributed Memory with Tasks A main program creates a set of tasks that use their own local memory during computation. Multiple tasks can reside on the same physical machine and/or across an arbitrary number of machines. Tasks exchange data through communications by sending and receiving messages through fast network (e.g. infinite band). Data transfer usually requires cooperative operations to be performed by each process. For example, a send operation must have a matching receive operation. Synchronization operations are also required to prevent race condition. Example: Message Passing Interface (MPI) Hybrid Parallel A hybrid model combines more than one of the previously described programming models. A simple example is the combination of the message passing model (MPI) with the threads model (OpenMP). Threads perform computationally intensive kernels using local, on-node data Communications between processes on different nodes occurs over the network using MPI Works well to the most popular hardware environment of clustered multi/many-core machines. Other example: MPI with CPU-GPU (Graphics Processing Unit) Hybrid OpenMP-MPI Parallel Model: Hybrid CUDA-MPI Parallel Model: (Click Source )","title":"Parallel Computing"},{"location":"parallel_computing/#parallel-computing","text":"Here we introduce three basic parallel models: Shared Memory, Distributed Memory and Hybrid Model .","title":"Parallel Computing"},{"location":"parallel_computing/#shared-memory-with-threads","text":"A main program loads and acquires all of the necessary resources to run the \"heavy weight\" process. It performs some serial work, and then creates a number of threads (\"light weight\") running by CPU cores concurrently. Each thread can have local data, but also, shares the entire resources, including RAM memory of the main program. Threads communicate with each other through global memory (RAM). This requires synchronization operations to ensure that no than one thread is updating the same RAM address at any time. Threads can come and go, but the main program remains present to provide the necessary shared resources until the application has completed. Examples: POSIX Threads, OpenMP, CUDA threads for GPUs","title":"Shared Memory with Threads"},{"location":"parallel_computing/#distributed-memory-with-tasks","text":"A main program creates a set of tasks that use their own local memory during computation. Multiple tasks can reside on the same physical machine and/or across an arbitrary number of machines. Tasks exchange data through communications by sending and receiving messages through fast network (e.g. infinite band). Data transfer usually requires cooperative operations to be performed by each process. For example, a send operation must have a matching receive operation. Synchronization operations are also required to prevent race condition. Example: Message Passing Interface (MPI)","title":"Distributed Memory with Tasks"},{"location":"parallel_computing/#hybrid-parallel","text":"A hybrid model combines more than one of the previously described programming models. A simple example is the combination of the message passing model (MPI) with the threads model (OpenMP). Threads perform computationally intensive kernels using local, on-node data Communications between processes on different nodes occurs over the network using MPI Works well to the most popular hardware environment of clustered multi/many-core machines. Other example: MPI with CPU-GPU (Graphics Processing Unit) Hybrid OpenMP-MPI Parallel Model: Hybrid CUDA-MPI Parallel Model: (Click Source )","title":"Hybrid Parallel"},{"location":"qs/","text":"qs Display job list. 1 2 3 4 5 6 7 8 9 $ qs -h Usage: -a --> all jobs -F --> all fields -j --> specific job -u --> specific user -r --> all job array elements Default: -u $USER","title":"SLURM - display job list"},{"location":"qs/#qs","text":"Display job list. 1 2 3 4 5 6 7 8 9 $ qs -h Usage: -a --> all jobs -F --> all fields -j --> specific job -u --> specific user -r --> all job array elements Default: -u $USER","title":"qs"},{"location":"rjags/","text":"rjags To use {rjags}, first load R/3.5.1 and JAGS from a dev-node (dev-intel16 or dev-intel18) as follows: **Loading R/3.5.1 and JAGS** 1 2 3 module purge module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 module load JAGS/4.3.0 Next, we will run a short example of data analysis using rjags. This example comes from this tutorial which presents many Bayesian models using this package. To invoke R from the command line: 1 R --vanilla Then, in the R console, you can run the following codes (for detailed explanation refer to the tutorial mentioned above): **Sample R code using {rjags} commands** 1 2 3 4 5 6 7 8 9 10 11 library ( rjags ) data <- read . csv ( \"data1.csv\" ) N <- length ( data $y ) dat <- list ( \"N\" = N , \"y\" = data $y , \"V\" = data $V ) inits <- list ( d = 0 . 0 ) jags . m <- jags . model ( file = \"aspirinFE.txt\" , data = dat , inits = inits , n . chains = 1 , n . adapt = 500 ) params <- c ( \"d\" , \"OR\" ) samps <- coda . samples ( jags . m , params , n . iter = 10000 ) summary ( window ( samps , start = 5001 )) plot ( samps ) where the two input files, data1.csv and aspirinFE.txt, need to be located in the working directory. The content of the two files is below. **data1.csv** 1 2 3 4 5 6 7 8 N,y,V 1,0.3289011,0.0388957 2,0.3845458,0.0411673 3,0.2195622,0.0204915 4,0.2222206,0.0647646 5,0.2254672,0.0351996 6,-0.1246363,0.0096167 7,0.1109658,0.0015062 **aspirinFE.txt** 1 2 3 4 5 6 7 8 9 10 11 12 13 14 model { for ( i in 1:N ) { P[i] <- 1/V[i] y[i] ~ dnorm(d, P[i]) } ### Define the priors d ~ dnorm(0, 0.00001) ### Transform the ln(OR) to OR OR <- exp(d) } A screen shot of the entire run including the output figures is attached here. ## Attachments: [rjags.png](attachments/22709657/22709656.png) (image/png)","title":"rjags"},{"location":"rjags/#rjags","text":"To use {rjags}, first load R/3.5.1 and JAGS from a dev-node (dev-intel16 or dev-intel18) as follows: **Loading R/3.5.1 and JAGS** 1 2 3 module purge module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 module load JAGS/4.3.0 Next, we will run a short example of data analysis using rjags. This example comes from this tutorial which presents many Bayesian models using this package. To invoke R from the command line: 1 R --vanilla Then, in the R console, you can run the following codes (for detailed explanation refer to the tutorial mentioned above): **Sample R code using {rjags} commands** 1 2 3 4 5 6 7 8 9 10 11 library ( rjags ) data <- read . csv ( \"data1.csv\" ) N <- length ( data $y ) dat <- list ( \"N\" = N , \"y\" = data $y , \"V\" = data $V ) inits <- list ( d = 0 . 0 ) jags . m <- jags . model ( file = \"aspirinFE.txt\" , data = dat , inits = inits , n . chains = 1 , n . adapt = 500 ) params <- c ( \"d\" , \"OR\" ) samps <- coda . samples ( jags . m , params , n . iter = 10000 ) summary ( window ( samps , start = 5001 )) plot ( samps ) where the two input files, data1.csv and aspirinFE.txt, need to be located in the working directory. The content of the two files is below. **data1.csv** 1 2 3 4 5 6 7 8 N,y,V 1,0.3289011,0.0388957 2,0.3845458,0.0411673 3,0.2195622,0.0204915 4,0.2222206,0.0647646 5,0.2254672,0.0351996 6,-0.1246363,0.0096167 7,0.1109658,0.0015062 **aspirinFE.txt** 1 2 3 4 5 6 7 8 9 10 11 12 13 14 model { for ( i in 1:N ) { P[i] <- 1/V[i] y[i] ~ dnorm(d, P[i]) } ### Define the priors d ~ dnorm(0, 0.00001) ### Transform the ln(OR) to OR OR <- exp(d) } A screen shot of the entire run including the output figures is attached here. ## Attachments: [rjags.png](attachments/22709657/22709656.png) (image/png)","title":"rjags"},{"location":"rstan/","text":"rstan The example here follows that in RStan Getting Started . To test rstan on the HPCC, first load R 3.6.2: module purge module load GCC/8.3.0 OpenMPI/3.1.4 R/3.6.2-X11-20180604 As of February 2020, the rstan version is 2.19.2. The stan model file \"8schools.stan\" contains: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 data { int< lower = 0 > J ; // number of schools real y [ J ] ; // estimated treatment effects real< lower = 0 > sigma [ J ] ; // standard error of effect estimates } parameters { real mu ; // population treatment effect real< lower = 0 > tau ; // standard deviation in treatment effects vector [ J ] eta ; // unscaled deviation from mu by school } transformed parameters { vector [ J ] theta = mu + tau * eta ; // school treatment effects } model { target += normal_lpdf ( eta | 0 , 1 ) ; // prior log-density target += normal_lpdf ( y | theta, sigma ) ; // log-likelihood } The R code (\"run.R\") to run stan model contains: 1 2 3 4 5 6 7 8 9 10 11 12 library ( \"rstan\" ) options ( mc . cores = parallel :: detectCores ()) rstan_options ( auto_write = TRUE ) schools_dat <- list ( J = 8 , y = c ( 28 , 8 , - 3 , 7 , - 1 , 1 , 18 , 12 ), sigma = c ( 15 , 10 , 16 , 11 , 9 , 11 , 10 , 18 )) fit <- stan ( file = '8schools.stan' , data = schools_dat ) print ( fit ) pairs ( fit , pars = c ( \"mu\" , \"tau\" , \"lp__\" )) la <- extract ( fit , permuted = TRUE ) # return a list of arrays mu <- la $mu To run the model from the command line: Rscript run.R In addition to the results printed to the stdout, there is an R object file named 8schools.rds generated. This is due to that we've set auto_write to TRUE in run.R. More about the auto_write option: Logical, defaulting to the value of rstan_options(\"auto_write\"), indicating whether to write the object to the hard disk using saveRDS. Although this argument is FALSE by default, we recommend calling rstan_options(\"auto_write\" = TRUE) in order to avoid unnecessary recompilations. If file is supplied and its dirname is writable, then the object will be written to that same directory, substituting a .rds extension for the .stan extension. Otherwise, the object will be written to the tempdir.","title":"rstan"},{"location":"rstan/#rstan","text":"The example here follows that in RStan Getting Started . To test rstan on the HPCC, first load R 3.6.2: module purge module load GCC/8.3.0 OpenMPI/3.1.4 R/3.6.2-X11-20180604 As of February 2020, the rstan version is 2.19.2. The stan model file \"8schools.stan\" contains: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 data { int< lower = 0 > J ; // number of schools real y [ J ] ; // estimated treatment effects real< lower = 0 > sigma [ J ] ; // standard error of effect estimates } parameters { real mu ; // population treatment effect real< lower = 0 > tau ; // standard deviation in treatment effects vector [ J ] eta ; // unscaled deviation from mu by school } transformed parameters { vector [ J ] theta = mu + tau * eta ; // school treatment effects } model { target += normal_lpdf ( eta | 0 , 1 ) ; // prior log-density target += normal_lpdf ( y | theta, sigma ) ; // log-likelihood } The R code (\"run.R\") to run stan model contains: 1 2 3 4 5 6 7 8 9 10 11 12 library ( \"rstan\" ) options ( mc . cores = parallel :: detectCores ()) rstan_options ( auto_write = TRUE ) schools_dat <- list ( J = 8 , y = c ( 28 , 8 , - 3 , 7 , - 1 , 1 , 18 , 12 ), sigma = c ( 15 , 10 , 16 , 11 , 9 , 11 , 10 , 18 )) fit <- stan ( file = '8schools.stan' , data = schools_dat ) print ( fit ) pairs ( fit , pars = c ( \"mu\" , \"tau\" , \"lp__\" )) la <- extract ( fit , permuted = TRUE ) # return a list of arrays mu <- la $mu To run the model from the command line: Rscript run.R In addition to the results printed to the stdout, there is an R object file named 8schools.rds generated. This is due to that we've set auto_write to TRUE in run.R. More about the auto_write option: Logical, defaulting to the value of rstan_options(\"auto_write\"), indicating whether to write the object to the hard disk using saveRDS. Although this argument is FALSE by default, we recommend calling rstan_options(\"auto_write\" = TRUE) in order to avoid unnecessary recompilations. If file is supplied and its dirname is writable, then the object will be written to that same directory, substituting a .rds extension for the .stan extension. Otherwise, the object will be written to the tempdir.","title":"rstan"},{"location":"singularity_01/","text":"Singularity: I. Introduction Singularity is installed on our HPCC. However, you may want to develop your own containers first on a local machine. Many HPC centers including MSU HPCC do not allow Docker containers through Docker. However, Singularity is compatible with Docker, and you can use Docker containers through Singularity. There are a few distinct difference between Docker and Singularity. Docker: Inside a Docker image, the user's privilege is escalated to root on the host system. This privilege is not supported by most HPCC including MSU HPCC. It means that Docker will not be installed on our system. Singularity: User has root privileges if elevated with \"sudo\" when a container runs. Can run and modify Docker images and containers These key difference make Singularity be installed on most HPCC. In addition, virtually all Docker containers can be run through Singularity, users can effectively run Docker on MSU HPCC. ### Installation Singularity exits as two major version, 2 and 3. Current version on MSU HPCC is 3.5.3. Therefore, in this tutorial, I will use version 3. To Install Singularity on your local machine, click here: https://www.sylabs.io/guides/3.0/user-guideinstallation.html#installation The version of singularity on MSU HPCC is currently 3.4.1. The official documentation for this version is at https://www.sylabs.io/guides/3.4/user-guide/index.html. All singularity commands are built into the system such as 'singularity shell' and 'singularity exec', which means you can invoke these commands directly from the command line. Check installation When you install Singularity on your local machine, then you can check the installation with 1 2 3 $ singularity pull shub://vsoch/hello-world INFO: Downloading shub image 59.75 MiB / 59.75 MiB [========================================================================================] 100.00% 10.46 MiB/s 5s In the above example, I used the Singularity Hub \u201cunique resource identifier,\u201d or uri, \"shub://\" which tells the software to run an image from Singularity Hub. To get help, you can use the help command which gives a general overview of Singularity options and subcommands as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 $ singularity --help Linux container platform optimized for High Performance Computing (HPC) and Enterprise Performance Computing (EPC) Usage: singularity [global options...] Description: Singularity containers provide an application virtualization layer enabling mobility of compute via both application and environment portability. With Singularity one is capable of building a root file system that runs on any other Linux system where Singularity is installed. Options: -d, --debug print debugging information (highest verbosity) -h, --help help for singularity --nocolor print without color output (default False) -q, --quiet suppress normal output -s, --silent only print errors -v, --verbose print additional information --version version for singularity ... You can use the help command if you want to see the information about subcommands. For example, to see the pull command help, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ singularity help pullPull an image from a URI Usage: singularity pull [pull options...] [output file] <URI> Description: The 'pull' command allows you to download or build a container from a given URI. Supported URIs include: library: Pull an image from the currently configured library library://user/collection/container[:tag] docker: Pull an image from Docker Hub docker://user/image:tag shub: Pull an image from Singularity Hub shub://user/image:tag oras: Pull a SIF image from a supporting OCI registry oras://registry/namespace/image:tag http, https: Pull an image using the http(s?) protocol https://library.sylabs.io/v1/imagefile/library/default/alpine:latest ... Downloading pre-built images I already downloaded a pre-built image \"hello-world\" from shub, one of the registries, using pull command. This is the easiest way to use Singularity. You can use the pull command to download pre-built images from a number of Container Registries, here we\u2019ll be focusing on the Singularity-Hub or DockerHub. The following are some of container registries. library - images hosted on Sylabs Cloud shub - images hosted on Singularity Hub docker - images hosted on Docker Hub localimage - images saved on your machine yum - yum based systems such as CentOS and Scientific Linux debootstrap - apt based systems such as Debian and Ubuntu arch - Arch Linux busybox - BusyBox zypper - zypper based systems such as Suse and OpenSuse Pulling an images from Sylabs cloud library In this example, I will pull a base Alpine container from Sylabs cloud: 1 2 3 $ singularity pull library://sylabsed/linux/alpine INFO: Downloading library image 2.08 MiB / 2.08 MiB [===========================================================================================] 100.00% 4.74 MiB/s 0s You can rename the container using the \u2013name flag: 1 2 3 $ singularity pull --name my_alpine.sif library://sylabsed/linux/alpine INFO: Downloading library image 2.08 MiB / 2.08 MiB [===========================================================================================] 100.00% 9.65 MiB/s 0s The above example will save the image as \"my_alpine.sif\" Pulling an images from Docker hub This example pulls an Alpine image from Docker hub 1 2 3 4 5 6 7 8 9 10 11 $ singularity pull docker://alpine INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob df20fa9351a1 done Copying config 0f5f445df8 done Writing manifest to image destination Storing signatures 2020/08/20 15:53:52 info unpack layer: sha256:df20fa9351a15782c64e6dddb2d4a6f50bf6d3688060a34c4014b0d9a752eb4c INFO: Creating SIF file... INFO: Build complete: alpine_latest.sif interact with images You can interact with images with shell , exec , and run commands. To learn how to interact with images, let's first pull an image \"lolcow_latest.sif\" from the libray. 1 $ singularity pull library://sylabsed/examples/lolcow shell The shell command allows you to spawn a new shell within your container and interact with it as if it is a virtual machine. 1 2 $singularity shell lolcow_latest.sif Singularity> The change in prompt indicates that you have entered the container. Once inside of a container, you are the same user as you are on the host system. 1 2 Singularity>whoami choiyj To exit from a container, type exit . 1 2 Singularity>exit $ exec The exec command allows you to execute a custom command within a container by specifying the image file. For instance, to execute the cowsay program within the lolcow_latest.sif container: 1 2 3 4 5 6 7 8 9 $ singularity exec lolcow_latest.sif cowsay container camp rocks ______________________ < container camp rocks > ---------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || You can also use shell command to run the program in the container. 1 2 3 4 5 6 7 8 9 Singularity> cowsay container camp rocks ______________________ < container camp rocks > ---------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || run Singularity containers contain runscripts. These are user defined scripts which define the actions of a container when user runs it. The runscript can be performed with the run command, or simply by calling the container as though it were an executable. 1 2 3 4 5 6 7 8 9 10 $ singularity run lolcow_latest.sif _________________________________________ / You're ugly and your mother dresses you \\ \\ funny. / ----------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || Cache setting By default, Singularity uses a temporary directory to save Docker files as tarballs: 1 2 3 4 5 6 7 8 $ ls ~/.singularity cache/ docker/ metadata/ $ ls .singularity/docker/ sha256:0efe07335a049e6afcd757db2d17ba37a12b717eb807acb03ddf3cd756b9fc2a.tar.gz sha256:43a98c1873995475a895f3d79f405232ef5230076b3f610c949c2e8341743af7.tar.gz sha256:6b1bb01b3a3b72463ae8ac5666d57b28f1a21d5256271910ac8df841aa04ecd1.tar.gz sha256:c2ca09a1934b951505ecc4d6b2e4ab7f9bf27bcdfb8999d0181deca74daf7683.tar.gz sha256:d6c3619d2153ffdefa4a9c19f15c5d566ce271b397a84537baa9ee45b24178f2.tar.gz You can change these by theses cache directories by specifying the location on your localhost as following: 1 2 3 $ mkdir -p $SCRATCH/singularity_tmp $ mkdir -p $SCRATCH/singularity_scratch $ SINGULARITY_TMPDIR=$SCRATCH/singularity_scratch SINGULARITY_CACHEDIR=$SCRATCH/singularity_tmp singularity --debug pull --name ubuntu-tmpdir.sif docker://ubuntu Creating writable containers with --sandbox options If you want to build a container within a writable directory (called a sandbox), you can do that with --sandbox option. We\u2019ve already covered how you can pull an existing container from Docker Hub, but we can also build a Singularity container from docker using the build command: 1 $ singularity build --sandbox ubuntu-latest/ docker://ubuntu With --sandbox option, you can changes and write files within the directory, but those changes will not remain when you finished using the container. To make those changes persistent, use --writable option when you start a container. 1 $ singularity shell --writable ubuntu-latest/","title":"Singularity I: Introduction"},{"location":"singularity_01/#singularity-i-introduction","text":"Singularity is installed on our HPCC. However, you may want to develop your own containers first on a local machine. Many HPC centers including MSU HPCC do not allow Docker containers through Docker. However, Singularity is compatible with Docker, and you can use Docker containers through Singularity. There are a few distinct difference between Docker and Singularity. Docker: Inside a Docker image, the user's privilege is escalated to root on the host system. This privilege is not supported by most HPCC including MSU HPCC. It means that Docker will not be installed on our system. Singularity: User has root privileges if elevated with \"sudo\" when a container runs. Can run and modify Docker images and containers These key difference make Singularity be installed on most HPCC. In addition, virtually all Docker containers can be run through Singularity, users can effectively run Docker on MSU HPCC. ### Installation Singularity exits as two major version, 2 and 3. Current version on MSU HPCC is 3.5.3. Therefore, in this tutorial, I will use version 3. To Install Singularity on your local machine, click here: https://www.sylabs.io/guides/3.0/user-guideinstallation.html#installation The version of singularity on MSU HPCC is currently 3.4.1. The official documentation for this version is at https://www.sylabs.io/guides/3.4/user-guide/index.html. All singularity commands are built into the system such as 'singularity shell' and 'singularity exec', which means you can invoke these commands directly from the command line.","title":"Singularity: I. Introduction"},{"location":"singularity_01/#check-installation","text":"When you install Singularity on your local machine, then you can check the installation with 1 2 3 $ singularity pull shub://vsoch/hello-world INFO: Downloading shub image 59.75 MiB / 59.75 MiB [========================================================================================] 100.00% 10.46 MiB/s 5s In the above example, I used the Singularity Hub \u201cunique resource identifier,\u201d or uri, \"shub://\" which tells the software to run an image from Singularity Hub. To get help, you can use the help command which gives a general overview of Singularity options and subcommands as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 $ singularity --help Linux container platform optimized for High Performance Computing (HPC) and Enterprise Performance Computing (EPC) Usage: singularity [global options...] Description: Singularity containers provide an application virtualization layer enabling mobility of compute via both application and environment portability. With Singularity one is capable of building a root file system that runs on any other Linux system where Singularity is installed. Options: -d, --debug print debugging information (highest verbosity) -h, --help help for singularity --nocolor print without color output (default False) -q, --quiet suppress normal output -s, --silent only print errors -v, --verbose print additional information --version version for singularity ... You can use the help command if you want to see the information about subcommands. For example, to see the pull command help, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ singularity help pullPull an image from a URI Usage: singularity pull [pull options...] [output file] <URI> Description: The 'pull' command allows you to download or build a container from a given URI. Supported URIs include: library: Pull an image from the currently configured library library://user/collection/container[:tag] docker: Pull an image from Docker Hub docker://user/image:tag shub: Pull an image from Singularity Hub shub://user/image:tag oras: Pull a SIF image from a supporting OCI registry oras://registry/namespace/image:tag http, https: Pull an image using the http(s?) protocol https://library.sylabs.io/v1/imagefile/library/default/alpine:latest ...","title":"Check installation"},{"location":"singularity_01/#downloading-pre-built-images","text":"I already downloaded a pre-built image \"hello-world\" from shub, one of the registries, using pull command. This is the easiest way to use Singularity. You can use the pull command to download pre-built images from a number of Container Registries, here we\u2019ll be focusing on the Singularity-Hub or DockerHub. The following are some of container registries. library - images hosted on Sylabs Cloud shub - images hosted on Singularity Hub docker - images hosted on Docker Hub localimage - images saved on your machine yum - yum based systems such as CentOS and Scientific Linux debootstrap - apt based systems such as Debian and Ubuntu arch - Arch Linux busybox - BusyBox zypper - zypper based systems such as Suse and OpenSuse","title":"Downloading pre-built images"},{"location":"singularity_01/#pulling-an-images-from-sylabs-cloud-library","text":"In this example, I will pull a base Alpine container from Sylabs cloud: 1 2 3 $ singularity pull library://sylabsed/linux/alpine INFO: Downloading library image 2.08 MiB / 2.08 MiB [===========================================================================================] 100.00% 4.74 MiB/s 0s You can rename the container using the \u2013name flag: 1 2 3 $ singularity pull --name my_alpine.sif library://sylabsed/linux/alpine INFO: Downloading library image 2.08 MiB / 2.08 MiB [===========================================================================================] 100.00% 9.65 MiB/s 0s The above example will save the image as \"my_alpine.sif\"","title":"Pulling an images from Sylabs cloud library"},{"location":"singularity_01/#pulling-an-images-from-docker-hub","text":"This example pulls an Alpine image from Docker hub 1 2 3 4 5 6 7 8 9 10 11 $ singularity pull docker://alpine INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob df20fa9351a1 done Copying config 0f5f445df8 done Writing manifest to image destination Storing signatures 2020/08/20 15:53:52 info unpack layer: sha256:df20fa9351a15782c64e6dddb2d4a6f50bf6d3688060a34c4014b0d9a752eb4c INFO: Creating SIF file... INFO: Build complete: alpine_latest.sif","title":"Pulling an images from Docker hub"},{"location":"singularity_01/#interact-with-images","text":"You can interact with images with shell , exec , and run commands. To learn how to interact with images, let's first pull an image \"lolcow_latest.sif\" from the libray. 1 $ singularity pull library://sylabsed/examples/lolcow shell The shell command allows you to spawn a new shell within your container and interact with it as if it is a virtual machine. 1 2 $singularity shell lolcow_latest.sif Singularity> The change in prompt indicates that you have entered the container. Once inside of a container, you are the same user as you are on the host system. 1 2 Singularity>whoami choiyj To exit from a container, type exit . 1 2 Singularity>exit $ exec The exec command allows you to execute a custom command within a container by specifying the image file. For instance, to execute the cowsay program within the lolcow_latest.sif container: 1 2 3 4 5 6 7 8 9 $ singularity exec lolcow_latest.sif cowsay container camp rocks ______________________ < container camp rocks > ---------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || You can also use shell command to run the program in the container. 1 2 3 4 5 6 7 8 9 Singularity> cowsay container camp rocks ______________________ < container camp rocks > ---------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || run Singularity containers contain runscripts. These are user defined scripts which define the actions of a container when user runs it. The runscript can be performed with the run command, or simply by calling the container as though it were an executable. 1 2 3 4 5 6 7 8 9 10 $ singularity run lolcow_latest.sif _________________________________________ / You're ugly and your mother dresses you \\ \\ funny. / ----------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || ||","title":"interact with images"},{"location":"singularity_01/#cache-setting","text":"By default, Singularity uses a temporary directory to save Docker files as tarballs: 1 2 3 4 5 6 7 8 $ ls ~/.singularity cache/ docker/ metadata/ $ ls .singularity/docker/ sha256:0efe07335a049e6afcd757db2d17ba37a12b717eb807acb03ddf3cd756b9fc2a.tar.gz sha256:43a98c1873995475a895f3d79f405232ef5230076b3f610c949c2e8341743af7.tar.gz sha256:6b1bb01b3a3b72463ae8ac5666d57b28f1a21d5256271910ac8df841aa04ecd1.tar.gz sha256:c2ca09a1934b951505ecc4d6b2e4ab7f9bf27bcdfb8999d0181deca74daf7683.tar.gz sha256:d6c3619d2153ffdefa4a9c19f15c5d566ce271b397a84537baa9ee45b24178f2.tar.gz You can change these by theses cache directories by specifying the location on your localhost as following: 1 2 3 $ mkdir -p $SCRATCH/singularity_tmp $ mkdir -p $SCRATCH/singularity_scratch $ SINGULARITY_TMPDIR=$SCRATCH/singularity_scratch SINGULARITY_CACHEDIR=$SCRATCH/singularity_tmp singularity --debug pull --name ubuntu-tmpdir.sif docker://ubuntu","title":"Cache setting"},{"location":"singularity_01/#creating-writable-containers-with-sandbox-options","text":"If you want to build a container within a writable directory (called a sandbox), you can do that with --sandbox option. We\u2019ve already covered how you can pull an existing container from Docker Hub, but we can also build a Singularity container from docker using the build command: 1 $ singularity build --sandbox ubuntu-latest/ docker://ubuntu With --sandbox option, you can changes and write files within the directory, but those changes will not remain when you finished using the container. To make those changes persistent, use --writable option when you start a container. 1 $ singularity shell --writable ubuntu-latest/","title":"Creating writable containers with --sandbox options"},{"location":"tags/","text":"Tags Docs pages organized by tag. antismash Lab Notebook: AntiSMASH explanation File Systems how-to guide CUDA Example lab notebook Lab Notebook: AntiSMASH reference Web access SLURM - buyin information tutorial Connect to the HPCC","title":"Tags"},{"location":"tags/#tags","text":"Docs pages organized by tag.","title":"Tags"},{"location":"tags/#antismash","text":"Lab Notebook: AntiSMASH","title":"antismash"},{"location":"tags/#explanation","text":"File Systems","title":"explanation"},{"location":"tags/#how-to-guide","text":"CUDA Example","title":"how-to guide"},{"location":"tags/#lab-notebook","text":"Lab Notebook: AntiSMASH","title":"lab notebook"},{"location":"tags/#reference","text":"Web access SLURM - buyin information","title":"reference"},{"location":"tags/#tutorial","text":"Connect to the HPCC","title":"tutorial"},{"location":"ufs18_File_Systems/","text":"ufs18 File Systems The ufs18 file system is a 2018 IBM General Parallel File System (GPFS) installed in HPCC for storing our home or research spaces. While it is faster and more stable, users need to learn its differences from other file systems and understand how to use it. Space quota The only way to get quota information of ufs18 file space is to run the command quota : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ quota Home Directory: Space Space Space Space Files Files Files Files Quota Used Available % Used Quota Used Available % Used ----------------------------------------------------------------------------------------------------------------------------------- /mnt/home/UserName 1024G 1213G -198G 118 % 1048576 540389 508187 52 % Research Groups: Space Space Space Space Files Files Files Files Quota Used Available % Used Quota Used Available % Used ----------------------------------------------------------------------------------------------------------------------------------- ResearchSpace1 2048G 1778G 270G 87 % 2097152 432525 1664627 21 % ResearchSpace2 1024G 126G 898G 12 % 1048576 11755 1036821 1 % Temporary Filesystems: ----------------------------------------------------------------------------------------------------------------------------------- /mnt/scratch ( /mnt/gs18 ) Space Quota Space Used Space Free Space % Used Filess Quota Files Used Files Free Files % Used 51200G 64G 51136G 0 % 1048576 5724 1042852 1 % /mnt/ffs17 Used Quota 35 .66GiB 100 .00GiB /mnt/ls15 ( legacy scratch ) Inodes Used Quota Free 125721 1000000 874279 where all file spaces accessible to the user are listed, including home, research, scratch and ffs17. In each space, the information of quota, usage and availablility on space size and number of files can be found. If \"Free\" or \"Available\" is a negative value (such as \"Space Available\" in home directory of the above example), the usage is over the quota. Please remove, transfer or compress some files so the used space or the file count can be lower than the \"Quota\" value. Since GPFS uses a different compression algorithm, you may notice higher space size after files are copied to the ufs18 file system. Actual disk usage different from quota results The new file system has a smallest file block size of 64k. This means that files between 2K and 64K will occupy 64K of space. This causes space usage inflated greatly for users with large amounts of small files. One suggested solution would be to compress many small files into one large file (at least larger than 64K). If you still have any difficulty, a temporarily larger quota can be requested by a user if their quota is at 1T with many small files. Limit on number of files Besides the quota on the size of space, users are also limited to 1 million files in their home or research directory. We need to set this limit because with a great number of files the file system will spend too much time on back-up to be able to function normally. If possible, users can compress many files into one to reduce the file number. If users do not wish to have the limit, they can request an extra space under /mnt/ufs18/nodr/ or /mnt/ufs18/nodr/research/ where there is no limit on the file count yet no back-up on the files either. Users will be responsible for their own back-up on the nodr space. By default, one half of the allowed quota on space size will be assigned to the requested nodr space. The quota of the original space under /mnt/home/ or /mnt/research/ is then downsized to its half so the total disk space quota remains the same. A different proportion on their sizes can also be requested as user's desire. Once this nodr space is created, the path and the quota information can be found from the results of the quota command mentioned above. Quota setting on research space The quota setting on research space is based on the group ownership of the files. Any files with the group ownership the same as the research space are followed by the quota command. However, any files (larger than 8 MB) with a group ownership different from the research space are not allowed to exist. Due to this reason, even though there is no over-quota issue from the results of quota command, users might still get an error message such as \u201c failed to ... ... Disk quota exceeded \u201d while create, copy or write a file to his research space. To resolve this \" Disk quota exceeded \" problem, users may do the following: Make sure the directory to which files are copied has the same group ownership as the research space and has the set-group-ID bit. For example, you get the error message when trying to transfer files from your local computer to a directory Drctry in your research space /mnt/research/Group . Use ls -ld command to check the group ownership and the access permission of the directory Drctry and the research space Group : 1 2 3 $ ls -ld /mnt/research/Group/Drctry /mnt/research/Group drwxrwx--- 2 UserName Prmry 5464 Feb 27 11 :34 /mnt/research/Group/Drctry drwxrws--- 9 UserName Group 8192 Jul 10 15 :34 /mnt/research/Group where you can see their differences. For the group ownership, Prmry of the directory Drctry is different from Group of the research space /mnt/research/Group . For the permission, rwxrwx--- of the directory does not have the set-group-ID bit rwxrws--- as the research space. The owner UserName of the directory can run the commands: 1 2 $ chgrp -R Group /mnt/research/Group/Drctry # Change the group ownership to Group $ chmod g+s /mnt/research/Group/Drctry # Set up set-group-ID bit to change the two attributes of the directory. (A further instruction about file permission can be reviewed from the wiki page File Permissions on HPCC .) Once the settings are corrected: 1 2 $ ls -ld /mnt/research/Group/Drctry drwxrws--- 2 UserName Group 5464 Feb 27 11 :34 /mnt/research/Group/Drctry the file transfer can proceed to the directory. If the file has already existed, its group ownership needs to be changed to the group of the research space. For example, you try running a command to copy, transfer or write a file foo to a directory Drctry of your research space. However, a file with the same file name foo has already existed in the directory Drctry . In order to make the command work, foo in the directory Drctry must have the same group ownership as the research space. Otherwise, the owner of the file can use chgrp command mentioned above to correct the group ownership or you have to rename or remove foo in the directory Drctry . If the file is going to be created, user's primary group may need to be set to the group of the research space. Users can use newgrp command to reset his primary group temporarily. For more information, please refer to Change Primary Group page. Samba mapping path for local computer Users mounting their home directories or research spaces will need to update their SMB/Samba/Windows File Sharing paths in their clients. To determine the mount path, log into HPCC, ssh to a development node and run: 1 show-samba-home To determine the mount point for your research space, use the following command: 1 show-samba-research researchspacename You may also use the powertools command to see all paths of your home and research spaces: 1 2 ml powertools # if powertools is not loaded show-samba-paths To map your home or research directory, please refer to the Mapping HPC Drives with Samba page. ACL for GPFS If you are using access control list (ACL), you will need to update them to NFSv4 ACLs. You will need to use the mmgetacl , mmputacl , and mmeditacl commands. Please refer to the GPFS Commands page for more details.","title":"UFS18 file system"},{"location":"ufs18_File_Systems/#ufs18-file-systems","text":"The ufs18 file system is a 2018 IBM General Parallel File System (GPFS) installed in HPCC for storing our home or research spaces. While it is faster and more stable, users need to learn its differences from other file systems and understand how to use it.","title":"ufs18 File Systems"},{"location":"ufs18_File_Systems/#space-quota","text":"The only way to get quota information of ufs18 file space is to run the command quota : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ quota Home Directory: Space Space Space Space Files Files Files Files Quota Used Available % Used Quota Used Available % Used ----------------------------------------------------------------------------------------------------------------------------------- /mnt/home/UserName 1024G 1213G -198G 118 % 1048576 540389 508187 52 % Research Groups: Space Space Space Space Files Files Files Files Quota Used Available % Used Quota Used Available % Used ----------------------------------------------------------------------------------------------------------------------------------- ResearchSpace1 2048G 1778G 270G 87 % 2097152 432525 1664627 21 % ResearchSpace2 1024G 126G 898G 12 % 1048576 11755 1036821 1 % Temporary Filesystems: ----------------------------------------------------------------------------------------------------------------------------------- /mnt/scratch ( /mnt/gs18 ) Space Quota Space Used Space Free Space % Used Filess Quota Files Used Files Free Files % Used 51200G 64G 51136G 0 % 1048576 5724 1042852 1 % /mnt/ffs17 Used Quota 35 .66GiB 100 .00GiB /mnt/ls15 ( legacy scratch ) Inodes Used Quota Free 125721 1000000 874279 where all file spaces accessible to the user are listed, including home, research, scratch and ffs17. In each space, the information of quota, usage and availablility on space size and number of files can be found. If \"Free\" or \"Available\" is a negative value (such as \"Space Available\" in home directory of the above example), the usage is over the quota. Please remove, transfer or compress some files so the used space or the file count can be lower than the \"Quota\" value. Since GPFS uses a different compression algorithm, you may notice higher space size after files are copied to the ufs18 file system.","title":"Space quota"},{"location":"ufs18_File_Systems/#actual-disk-usage-different-from-quota-results","text":"The new file system has a smallest file block size of 64k. This means that files between 2K and 64K will occupy 64K of space. This causes space usage inflated greatly for users with large amounts of small files. One suggested solution would be to compress many small files into one large file (at least larger than 64K). If you still have any difficulty, a temporarily larger quota can be requested by a user if their quota is at 1T with many small files.","title":"Actual disk usage different from quota results"},{"location":"ufs18_File_Systems/#limit-on-number-of-files","text":"Besides the quota on the size of space, users are also limited to 1 million files in their home or research directory. We need to set this limit because with a great number of files the file system will spend too much time on back-up to be able to function normally. If possible, users can compress many files into one to reduce the file number. If users do not wish to have the limit, they can request an extra space under /mnt/ufs18/nodr/ or /mnt/ufs18/nodr/research/ where there is no limit on the file count yet no back-up on the files either. Users will be responsible for their own back-up on the nodr space. By default, one half of the allowed quota on space size will be assigned to the requested nodr space. The quota of the original space under /mnt/home/ or /mnt/research/ is then downsized to its half so the total disk space quota remains the same. A different proportion on their sizes can also be requested as user's desire. Once this nodr space is created, the path and the quota information can be found from the results of the quota command mentioned above.","title":"Limit on number of files"},{"location":"ufs18_File_Systems/#quota-setting-on-research-space","text":"The quota setting on research space is based on the group ownership of the files. Any files with the group ownership the same as the research space are followed by the quota command. However, any files (larger than 8 MB) with a group ownership different from the research space are not allowed to exist. Due to this reason, even though there is no over-quota issue from the results of quota command, users might still get an error message such as \u201c failed to ... ... Disk quota exceeded \u201d while create, copy or write a file to his research space. To resolve this \" Disk quota exceeded \" problem, users may do the following:","title":"Quota setting on research space"},{"location":"ufs18_File_Systems/#make-sure-the-directory-to-which-files-are-copied-has-the-same-group-ownership-as-the-research-space-and-has-the-set-group-id-bit","text":"For example, you get the error message when trying to transfer files from your local computer to a directory Drctry in your research space /mnt/research/Group . Use ls -ld command to check the group ownership and the access permission of the directory Drctry and the research space Group : 1 2 3 $ ls -ld /mnt/research/Group/Drctry /mnt/research/Group drwxrwx--- 2 UserName Prmry 5464 Feb 27 11 :34 /mnt/research/Group/Drctry drwxrws--- 9 UserName Group 8192 Jul 10 15 :34 /mnt/research/Group where you can see their differences. For the group ownership, Prmry of the directory Drctry is different from Group of the research space /mnt/research/Group . For the permission, rwxrwx--- of the directory does not have the set-group-ID bit rwxrws--- as the research space. The owner UserName of the directory can run the commands: 1 2 $ chgrp -R Group /mnt/research/Group/Drctry # Change the group ownership to Group $ chmod g+s /mnt/research/Group/Drctry # Set up set-group-ID bit to change the two attributes of the directory. (A further instruction about file permission can be reviewed from the wiki page File Permissions on HPCC .) Once the settings are corrected: 1 2 $ ls -ld /mnt/research/Group/Drctry drwxrws--- 2 UserName Group 5464 Feb 27 11 :34 /mnt/research/Group/Drctry the file transfer can proceed to the directory.","title":"Make sure the directory to which files are copied has the same group ownership as the research space and has the set-group-ID bit."},{"location":"ufs18_File_Systems/#if-the-file-has-already-existed-its-group-ownership-needs-to-be-changed-to-the-group-of-the-research-space","text":"For example, you try running a command to copy, transfer or write a file foo to a directory Drctry of your research space. However, a file with the same file name foo has already existed in the directory Drctry . In order to make the command work, foo in the directory Drctry must have the same group ownership as the research space. Otherwise, the owner of the file can use chgrp command mentioned above to correct the group ownership or you have to rename or remove foo in the directory Drctry .","title":"If the file has already existed, its group ownership needs to be changed to the group of the research space."},{"location":"ufs18_File_Systems/#if-the-file-is-going-to-be-created-users-primary-group-may-need-to-be-set-to-the-group-of-the-research-space","text":"Users can use newgrp command to reset his primary group temporarily. For more information, please refer to Change Primary Group page.","title":"If the file is going to be created, user's primary group may need to be set to the group of the research space."},{"location":"ufs18_File_Systems/#samba-mapping-path-for-local-computer","text":"Users mounting their home directories or research spaces will need to update their SMB/Samba/Windows File Sharing paths in their clients. To determine the mount path, log into HPCC, ssh to a development node and run: 1 show-samba-home To determine the mount point for your research space, use the following command: 1 show-samba-research researchspacename You may also use the powertools command to see all paths of your home and research spaces: 1 2 ml powertools # if powertools is not loaded show-samba-paths To map your home or research directory, please refer to the Mapping HPC Drives with Samba page.","title":"Samba mapping path for local computer"},{"location":"ufs18_File_Systems/#acl-for-gpfs","text":"If you are using access control list (ACL), you will need to update them to NFSv4 ACLs. You will need to use the mmgetacl , mmputacl , and mmeditacl commands. Please refer to the GPFS Commands page for more details.","title":"ACL for GPFS"},{"location":"using_DDD/","text":"Using DDD DDD stands for 'Data Display Debbuger'. It is a GUI front end of GDB, the GNU debugger. The main advantage of DDD over GDB is that DDD offers GUI. In this tutorial, we will learn about setting and removing breakpoints tracing through programs examining data at various points in execution. The DDD interface When you start DDD, you would see a DDD window like this: The DDD window consists of 4 sections: data window source window machine code window GDB console You can show/hide each of them in View menu. You can customize the DDD environment in Edit \u2192 Preferences menu. For example, to display line numbers in source window, Edit \u2192 Preferences \u2192 Source: check ' Display Source Line Numbers' Getting started To use DDD, we need a program to debug. Let's use the following code. debug_ex.c 1 2 3 4 5 6 7 8 9 #include <stdio.h> int main(int argc, char** argv){ for(int i = 0; i < 10; i++){ int j = i*i; printf(\"%d \", j); } printf(\"\\n\"); } Basic steps First, you need to compile this code with -g option to include the debug symbols such as 1 gcc -g debug_ex.c -o debug_ex Now, run the DDD with an executable such as 1 ddd debug_ex Even though you open an executable such as debug_ex , but the DDD will show the source file name such as debug_ex.c . Breakpoints stop your program in the middle of running to examine the current state of variables and data structures. You can continue from where you set the breakpoint to finish program execution. To set a breakpoint, double click to the left of the source line in the source window. A STOP icon will appear next to it. Click Run to start execution. Now, click 'Run' button or type 'run' on GDB console. The green arrow will appear as soon as you hit the breakpoint. The breakpoints you set can be deleted or disabled by right-clicking on the line just as before. Except this time, you'll choose either the \"disable breakpoint\" or \"delete breakpoint\" options. In order to set breakpoints in other files (ie, not in the main() function), choose the \"Open Source\" option from the File menu of DDD. The file dialog should appear. The figure shows that the program ran to the line number 5, and waits your input on line number 6. You can run the code a line by line with 'next' command (you can click the button, or just type on GDB console). To see the variable value, type 'print variable_name' oin GDB console. For example, 'print i' will show 'I' variable's value. To go to the next break point, click 'cont' button or type 'cont' on GDB console. When you find bugs, edit your source code in your editor of choice and recompile the code. Reload the new source code into DDD using the Source menu: Source \u2192 Reload Source. Common commands DDD offers command buttons, but you can also type commands directly on GDB console. command Description help help documentation for topics and commands help breakpoint Lists help information about breakpoints break sets breakpoint break line number Sets breakpoint at a line number break function name Sets breakpoint at the begining of function name enable, disable, delete/clear Enable, disable, or delete one or more breakpoints. disable 3 Disables breakpoint number 3 clear line_number Clears breakpoint at line_number delete 3 Deletes breakpoint number 3 delete Deletes all beakpoints run Starts program running from the begining. continue (or cont) Continues execution from the current line to the next breakpoint step (or s) Execute next line(s) of program step Executes one line of a program step number Executes next number of lines of program next (or n) Like step, but treats a function as a single line. next Execute the next line next number Executes next number of lines of program until line_number Executes program until line number quit quit DDD list Lists program source code condition Conditional breakpoints print Display program values, results of expressions whatis List type of an expression whatis j Shows data type of expression 'j' info Get information info locals Shows local variables in current stack frame info args Shows the argument variable of current stack frame info break Show breakpoints set Change values of variables, memory, registers set x = 123*y Set variable x's value to 123*y Examining data While the program is running, you may want to examine the contents of variables. You can do this by right-clicking on a variable name in the DDD window. Upon right-clicking, select \"Display\". If you want to display the value of a pointer. In this case, use the \"Display*\" menu item. Right-clicking on a variable name offers other capabilities such as print, lookup, what is (showing the data type), break, and clear. Instead of right-clicking, you can peek at memory contents also. To do that, click Data \u2192 Memory. The following window will pop up. Some useful resources The official DDD Manual http://www.gnu.org/manual/ddd/html_mono/ddd.html A good debugging tutorial using DDD http://heather.cs.ucdavis.edu/~matloff/debug.html","title":"Using DDD"},{"location":"using_DDD/#using-ddd","text":"DDD stands for 'Data Display Debbuger'. It is a GUI front end of GDB, the GNU debugger. The main advantage of DDD over GDB is that DDD offers GUI. In this tutorial, we will learn about setting and removing breakpoints tracing through programs examining data at various points in execution.","title":"Using DDD"},{"location":"using_DDD/#the-ddd-interface","text":"When you start DDD, you would see a DDD window like this: The DDD window consists of 4 sections: data window source window machine code window GDB console You can show/hide each of them in View menu. You can customize the DDD environment in Edit \u2192 Preferences menu. For example, to display line numbers in source window, Edit \u2192 Preferences \u2192 Source: check ' Display Source Line Numbers'","title":"The DDD interface"},{"location":"using_DDD/#getting-started","text":"To use DDD, we need a program to debug. Let's use the following code. debug_ex.c 1 2 3 4 5 6 7 8 9 #include <stdio.h> int main(int argc, char** argv){ for(int i = 0; i < 10; i++){ int j = i*i; printf(\"%d \", j); } printf(\"\\n\"); }","title":"Getting started"},{"location":"using_DDD/#basic-steps","text":"First, you need to compile this code with -g option to include the debug symbols such as 1 gcc -g debug_ex.c -o debug_ex Now, run the DDD with an executable such as 1 ddd debug_ex Even though you open an executable such as debug_ex , but the DDD will show the source file name such as debug_ex.c . Breakpoints stop your program in the middle of running to examine the current state of variables and data structures. You can continue from where you set the breakpoint to finish program execution. To set a breakpoint, double click to the left of the source line in the source window. A STOP icon will appear next to it. Click Run to start execution. Now, click 'Run' button or type 'run' on GDB console. The green arrow will appear as soon as you hit the breakpoint. The breakpoints you set can be deleted or disabled by right-clicking on the line just as before. Except this time, you'll choose either the \"disable breakpoint\" or \"delete breakpoint\" options. In order to set breakpoints in other files (ie, not in the main() function), choose the \"Open Source\" option from the File menu of DDD. The file dialog should appear. The figure shows that the program ran to the line number 5, and waits your input on line number 6. You can run the code a line by line with 'next' command (you can click the button, or just type on GDB console). To see the variable value, type 'print variable_name' oin GDB console. For example, 'print i' will show 'I' variable's value. To go to the next break point, click 'cont' button or type 'cont' on GDB console. When you find bugs, edit your source code in your editor of choice and recompile the code. Reload the new source code into DDD using the Source menu: Source \u2192 Reload Source.","title":"Basic steps"},{"location":"using_DDD/#common-commands","text":"DDD offers command buttons, but you can also type commands directly on GDB console. command Description help help documentation for topics and commands help breakpoint Lists help information about breakpoints break sets breakpoint break line number Sets breakpoint at a line number break function name Sets breakpoint at the begining of function name enable, disable, delete/clear Enable, disable, or delete one or more breakpoints. disable 3 Disables breakpoint number 3 clear line_number Clears breakpoint at line_number delete 3 Deletes breakpoint number 3 delete Deletes all beakpoints run Starts program running from the begining. continue (or cont) Continues execution from the current line to the next breakpoint step (or s) Execute next line(s) of program step Executes one line of a program step number Executes next number of lines of program next (or n) Like step, but treats a function as a single line. next Execute the next line next number Executes next number of lines of program until line_number Executes program until line number quit quit DDD list Lists program source code condition Conditional breakpoints print Display program values, results of expressions whatis List type of an expression whatis j Shows data type of expression 'j' info Get information info locals Shows local variables in current stack frame info args Shows the argument variable of current stack frame info break Show breakpoints set Change values of variables, memory, registers set x = 123*y Set variable x's value to 123*y","title":"Common commands"},{"location":"using_DDD/#examining-data","text":"While the program is running, you may want to examine the contents of variables. You can do this by right-clicking on a variable name in the DDD window. Upon right-clicking, select \"Display\". If you want to display the value of a pointer. In this case, use the \"Display*\" menu item. Right-clicking on a variable name offers other capabilities such as print, lookup, what is (showing the data type), break, and clear. Instead of right-clicking, you can peek at memory contents also. To do that, click Data \u2192 Memory. The following window will pop up.","title":"Examining data"},{"location":"using_DDD/#some-useful-resources","text":"The official DDD Manual http://www.gnu.org/manual/ddd/html_mono/ddd.html A good debugging tutorial using DDD http://heather.cs.ucdavis.edu/~matloff/debug.html","title":"Some useful resources"},{"location":"virtual_help_desk/","text":"Virtual Help Desk by Microsoft Teams and Zoom ICER offers virtual helpdesk office hours (every Monday and Thursday 1:00-2:00pm) online without walk-in. User can reach us either through Microsoft Teams App or just a web browser. Please click on the ICER Help Desk link . It will take you to the launcher web site of Microsoft Teams: You can now choose to use a web browser or Microsoft Teams to access our Help Desk channel. If you do not want to install and use Microsoft Teams, you can click on Use the web app instead to enter ICER Help Desk channel: If you would like to use Microsoft Teams but have not installed one in your computer yet, please click on Get the Teams app . If Microsoft Teams is installed already, you can click on Launch it now . If a \"Launch Appliction\" window pop out: you can also choose Microsoft Teams to open our Help Desk channel link. Once you are in the channel, please ask your questions in the text bar located at the bottom of the window: Click on the button so we are able to see the message and help you. We can start a conversation and arrange a Zoom Zoom's Microsoft Teams integration has been set up to start or join an instant meeting right from our conversation. You may enter \"@zoom help\" or type \"@zoom\" in the text bar and click on Zoom to see a list of commands. To find out how to download or use Zoom, please visit the MSU Zoom page .","title":"Virtual Help Desk by Microsoft Teams and Zoom"},{"location":"virtual_help_desk/#virtual-help-desk-by-microsoft-teams-and-zoom","text":"ICER offers virtual helpdesk office hours (every Monday and Thursday 1:00-2:00pm) online without walk-in. User can reach us either through Microsoft Teams App or just a web browser. Please click on the ICER Help Desk link . It will take you to the launcher web site of Microsoft Teams: You can now choose to use a web browser or Microsoft Teams to access our Help Desk channel. If you do not want to install and use Microsoft Teams, you can click on Use the web app instead to enter ICER Help Desk channel: If you would like to use Microsoft Teams but have not installed one in your computer yet, please click on Get the Teams app . If Microsoft Teams is installed already, you can click on Launch it now . If a \"Launch Appliction\" window pop out: you can also choose Microsoft Teams to open our Help Desk channel link. Once you are in the channel, please ask your questions in the text bar located at the bottom of the window: Click on the button so we are able to see the message and help you. We can start a conversation and arrange a Zoom Zoom's Microsoft Teams integration has been set up to start or join an instant meeting right from our conversation. You may enter \"@zoom help\" or type \"@zoom\" in the text bar and click on Zoom to see a list of commands. To find out how to download or use Zoom, please visit the MSU Zoom page .","title":"Virtual Help Desk by Microsoft Teams and Zoom"},{"location":"workshop_slides/","text":"Workshop slides Introduction to HPCC (last update 7/2022, by Nanye Long): pdf Writing SLURM job scripts (last update 7/2022, by Nanye Long): pdf From PC to HPC (by Xiaoge Wang): pdf","title":"Workshop slides"},{"location":"workshop_slides/#workshop-slides","text":"Introduction to HPCC (last update 7/2022, by Nanye Long): pdf Writing SLURM job scripts (last update 7/2022, by Nanye Long): pdf From PC to HPC (by Xiaoge Wang): pdf","title":"Workshop slides"},{"location":"tags/","text":"Tags Docs pages organized by tag. antismash Lab Notebook: AntiSMASH explanation File Systems how-to guide CUDA Example lab notebook Lab Notebook: AntiSMASH reference Web access SLURM - buyin information tutorial Connect to the HPCC","title":"Tags"},{"location":"tags/#tags","text":"Docs pages organized by tag.","title":"Tags"},{"location":"tags/#antismash","text":"Lab Notebook: AntiSMASH","title":"antismash"},{"location":"tags/#explanation","text":"File Systems","title":"explanation"},{"location":"tags/#how-to-guide","text":"CUDA Example","title":"how-to guide"},{"location":"tags/#lab-notebook","text":"Lab Notebook: AntiSMASH","title":"lab notebook"},{"location":"tags/#reference","text":"Web access SLURM - buyin information","title":"reference"},{"location":"tags/#tutorial","text":"Connect to the HPCC","title":"tutorial"}]}